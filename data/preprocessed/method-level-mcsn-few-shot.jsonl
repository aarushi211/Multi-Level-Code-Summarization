{"repo_name":"apache\/airflow","method_name":"BaseExecutor.has_task","method_code":"def has_task(self, task_instance):\n        \"\"\"\"\"\"\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True","method_summary":"Checks if a task is either queued or running in this executor","original_method_code":"def has_task(self, task_instance):\n        \"\"\"\n        Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor\n        \"\"\"\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True","method_path":"https:\/\/github.com\/apache\/airflow\/blob\/b69c686ad8a0c89b9136bb4b31767257eb7b2597\/airflow\/executors\/base_executor.py#L97-L105"}
{"repo_name":"apache\/airflow","method_name":"SnowflakeHook._get_conn_params","method_code":"def _get_conn_params(self):\n        \"\"\"\"\"\"\n        conn = self.get_connection(self.snowflake_conn_id)\n        account = conn.extra_dejson.get('account', None)\n        warehouse = conn.extra_dejson.get('warehouse', None)\n        database = conn.extra_dejson.get('database', None)\n        region = conn.extra_dejson.get(\"region\", None)\n        role = conn.extra_dejson.get('role', None)\n\n        conn_config = {\n            \"user\": conn.login,\n            \"password\": conn.password or '',\n            \"schema\": conn.schema or '',\n            \"database\": self.database or database or '',\n            \"account\": self.account or account or '',\n            \"warehouse\": self.warehouse or warehouse or '',\n            \"region\": self.region or region or '',\n            \"role\": self.role or role or '',\n        }\n\n        \"\"\"\"\"\"\n        private_key_file = conn.extra_dejson.get('private_key_file', None)\n        if private_key_file:\n            with open(private_key_file, \"rb\") as key:\n                passphrase = None\n                if conn.password:\n                    passphrase = conn.password.strip().encode()\n\n                p_key = serialization.load_pem_private_key(\n                    key.read(),\n                    password=passphrase,\n                    backend=default_backend()\n                )\n\n            pkb = p_key.private_bytes(encoding=serialization.Encoding.DER,\n                                      format=serialization.PrivateFormat.PKCS8,\n                                      encryption_algorithm=serialization.NoEncryption())\n\n            conn_config['private_key'] = pkb\n            conn_config.pop('password', None)\n\n        return conn_config","method_summary":"one method to fetch connection params as a dict used in get_uri() and get_connection()","original_method_code":"def _get_conn_params(self):\n        \"\"\"\n        one method to fetch connection params as a dict\n        used in get_uri() and get_connection()\n        \"\"\"\n        conn = self.get_connection(self.snowflake_conn_id)\n        account = conn.extra_dejson.get('account', None)\n        warehouse = conn.extra_dejson.get('warehouse', None)\n        database = conn.extra_dejson.get('database', None)\n        region = conn.extra_dejson.get(\"region\", None)\n        role = conn.extra_dejson.get('role', None)\n\n        conn_config = {\n            \"user\": conn.login,\n            \"password\": conn.password or '',\n            \"schema\": conn.schema or '',\n            \"database\": self.database or database or '',\n            \"account\": self.account or account or '',\n            \"warehouse\": self.warehouse or warehouse or '',\n            \"region\": self.region or region or '',\n            \"role\": self.role or role or '',\n        }\n\n        \"\"\"\n        If private_key_file is specified in the extra json, load the contents of the file as a private\n        key and specify that in the connection configuration. The connection password then becomes the\n        passphrase for the private key. If your private key file is not encrypted (not recommended), then\n        leave the password empty.\n        \"\"\"\n        private_key_file = conn.extra_dejson.get('private_key_file', None)\n        if private_key_file:\n            with open(private_key_file, \"rb\") as key:\n                passphrase = None\n                if conn.password:\n                    passphrase = conn.password.strip().encode()\n\n                p_key = serialization.load_pem_private_key(\n                    key.read(),\n                    password=passphrase,\n                    backend=default_backend()\n                )\n\n            pkb = p_key.private_bytes(encoding=serialization.Encoding.DER,\n                                      format=serialization.PrivateFormat.PKCS8,\n                                      encryption_algorithm=serialization.NoEncryption())\n\n            conn_config['private_key'] = pkb\n            conn_config.pop('password', None)\n\n        return conn_config","method_path":"https:\/\/github.com\/apache\/airflow\/blob\/b69c686ad8a0c89b9136bb4b31767257eb7b2597\/airflow\/contrib\/hooks\/snowflake_hook.py#L47-L96"}
{"repo_name":"apache\/airflow","method_name":"SnowflakeHook.get_uri","method_code":"def get_uri(self):\n        \"\"\"\"\"\"\n        conn_config = self._get_conn_params()\n        uri = 'snowflake:\/\/{user}:{password}@{account}\/{database}\/'\n        uri += '{schema}?warehouse={warehouse}&role={role}'\n        return uri.format(**conn_config)","method_summary":"override DbApiHook get_uri method for get_sqlalchemy_engine()","original_method_code":"def get_uri(self):\n        \"\"\"\n        override DbApiHook get_uri method for get_sqlalchemy_engine()\n        \"\"\"\n        conn_config = self._get_conn_params()\n        uri = 'snowflake:\/\/{user}:{password}@{account}\/{database}\/'\n        uri += '{schema}?warehouse={warehouse}&role={role}'\n        return uri.format(**conn_config)","method_path":"https:\/\/github.com\/apache\/airflow\/blob\/b69c686ad8a0c89b9136bb4b31767257eb7b2597\/airflow\/contrib\/hooks\/snowflake_hook.py#L98-L105"}
{"repo_name":"apache\/airflow","method_name":"SnowflakeHook._get_aws_credentials","method_code":"def _get_aws_credentials(self):\n        \"\"\"\"\"\"\n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if 'aws_secret_access_key' in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    'aws_access_key_id')\n                aws_secret_access_key = connection_object.extra_dejson.get(\n                    'aws_secret_access_key')\n        return aws_access_key_id, aws_secret_access_key","method_summary":"returns aws_access_key_id, aws_secret_access_key from extra intended to be used by external import and export statements","original_method_code":"def _get_aws_credentials(self):\n        \"\"\"\n        returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements\n        \"\"\"\n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if 'aws_secret_access_key' in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    'aws_access_key_id')\n                aws_secret_access_key = connection_object.extra_dejson.get(\n                    'aws_secret_access_key')\n        return aws_access_key_id, aws_secret_access_key","method_path":"https:\/\/github.com\/apache\/airflow\/blob\/b69c686ad8a0c89b9136bb4b31767257eb7b2597\/airflow\/contrib\/hooks\/snowflake_hook.py#L115-L129"}
{"repo_name":"apache\/airflow","method_name":"PostgresHook.copy_expert","method_code":"def copy_expert(self, sql, filename, open=open):\n        \"\"\"\"\"\"\n        if not os.path.isfile(filename):\n            with open(filename, 'w'):\n                pass\n\n        with open(filename, 'r+') as f:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()","method_summary":"Executes SQL using psycopg2 copy_expert method. Necessary to execute COPY command without access to a superuser.","original_method_code":"def copy_expert(self, sql, filename, open=open):\n        \"\"\"\n        Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.\n        \"\"\"\n        if not os.path.isfile(filename):\n            with open(filename, 'w'):\n                pass\n\n        with open(filename, 'r+') as f:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()","method_path":"https:\/\/github.com\/apache\/airflow\/blob\/b69c686ad8a0c89b9136bb4b31767257eb7b2597\/airflow\/hooks\/postgres_hook.py#L63-L83"}
{"repo_name":"apache\/airflow","method_name":"PostgresHook.bulk_load","method_code":"def bulk_load(self, table, tmp_file):\n        \"\"\"\"\"\"\n        self.copy_expert(\"COPY {table} FROM STDIN\".format(table=table), tmp_file)","method_summary":"Loads a tab-delimited file into a database table","original_method_code":"def bulk_load(self, table, tmp_file):\n        \"\"\"\n        Loads a tab-delimited file into a database table\n        \"\"\"\n        self.copy_expert(\"COPY {table} FROM STDIN\".format(table=table), tmp_file)","method_path":"https:\/\/github.com\/apache\/airflow\/blob\/b69c686ad8a0c89b9136bb4b31767257eb7b2597\/airflow\/hooks\/postgres_hook.py#L85-L89"}
{"repo_name":"apache\/airflow","method_name":"PostgresHook.bulk_dump","method_code":"def bulk_dump(self, table, tmp_file):\n        \"\"\"\"\"\"\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)","method_summary":"Dumps a database table into a tab-delimited file","original_method_code":"def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)","method_path":"https:\/\/github.com\/apache\/airflow\/blob\/b69c686ad8a0c89b9136bb4b31767257eb7b2597\/airflow\/hooks\/postgres_hook.py#L91-L95"}
{"repo_name":"apache\/airflow","method_name":"FileToGoogleCloudStorageOperator.execute","method_code":"def execute(self, context):\n        \"\"\"\"\"\"\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,\n            filename=self.src,\n            gzip=self.gzip,\n        )","method_summary":"Uploads the file to Google cloud storage","original_method_code":"def execute(self, context):\n        \"\"\"\n        Uploads the file to Google cloud storage\n        \"\"\"\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,\n            filename=self.src,\n            gzip=self.gzip,\n        )","method_path":"https:\/\/github.com\/apache\/airflow\/blob\/b69c686ad8a0c89b9136bb4b31767257eb7b2597\/airflow\/contrib\/operators\/file_to_gcs.py#L68-L82"}
{"repo_name":"apache\/airflow","method_name":"max_partition","method_code":"def max_partition(\n        table, schema=\"default\", field=None, filter_map=None,\n        metastore_conn_id='metastore_default'):\n    \"\"\"\"\"\"\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    return hh.max_partition(\n        schema=schema, table_name=table, field=field, filter_map=filter_map)","method_summary":"Gets the max partition for a table.","original_method_code":"def max_partition(\n        table, schema=\"default\", field=None, filter_map=None,\n        metastore_conn_id='metastore_default'):\n    \"\"\"\n    Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'\n    \"\"\"\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    return hh.max_partition(\n        schema=schema, table_name=table, field=field, filter_map=filter_map)","method_path":"https:\/\/github.com\/apache\/airflow\/blob\/b69c686ad8a0c89b9136bb4b31767257eb7b2597\/airflow\/macros\/hive.py#L23-L55"}
{"repo_name":"apache\/airflow","method_name":"_closest_date","method_code":"def _closest_date(target_dt, date_list, before_target=None):\n    \"\"\"\"\"\"\n    fb = lambda d: target_dt - d if d <= target_dt else datetime.timedelta.max\n    fa = lambda d: d - target_dt if d >= target_dt else datetime.timedelta.max\n    fnone = lambda d: target_dt - d if d < target_dt else d - target_dt\n    if before_target is None:\n        return min(date_list, key=fnone).date()\n    if before_target:\n        return min(date_list, key=fb).date()\n    else:\n        return min(date_list, key=fa).date()","method_summary":"This function finds the date in a list closest to the target date. An optional parameter can be given to get the closest before or after.","original_method_code":"def _closest_date(target_dt, date_list, before_target=None):\n    \"\"\"\n    This function finds the date in a list closest to the target date.\n    An optional parameter can be given to get the closest before or after.\n\n    :param target_dt: The target date\n    :type target_dt: datetime.date\n    :param date_list: The list of dates to search\n    :type date_list: list[datetime.date]\n    :param before_target: closest before or after the target\n    :type before_target: bool or None\n    :returns: The closest date\n    :rtype: datetime.date or None\n    \"\"\"\n    fb = lambda d: target_dt - d if d <= target_dt else datetime.timedelta.max\n    fa = lambda d: d - target_dt if d >= target_dt else datetime.timedelta.max\n    fnone = lambda d: target_dt - d if d < target_dt else d - target_dt\n    if before_target is None:\n        return min(date_list, key=fnone).date()\n    if before_target:\n        return min(date_list, key=fb).date()\n    else:\n        return min(date_list, key=fa).date()","method_path":"https:\/\/github.com\/apache\/airflow\/blob\/b69c686ad8a0c89b9136bb4b31767257eb7b2597\/airflow\/macros\/hive.py#L58-L80"}
{"repo_name":"tensorflow\/probability","method_name":"custom_gradient","method_code":"def custom_gradient(fx, gx, x, fx_gx_manually_stopped=False, name=None):\n  \"\"\"\"\"\"\n  def maybe_stop(x):\n    if fx_gx_manually_stopped:\n      return x\n    return tf.stop_gradient(x)\n\n  with tf.compat.v1.name_scope(name, 'custom_gradient', [fx, gx, x]):\n    fx = tf.convert_to_tensor(value=fx, name='fx')\n    \n    \n    with tf.control_dependencies([fx]):\n      if is_list_like(x):\n        x = [identity(x_, name='x') for x_ in x]\n      else:\n        x = [identity(x, name='x')]\n\n      if is_list_like(gx):\n        gx = [identity(gx_, dtype=fx.dtype, name='gx')\n              for gx_ in gx]\n      else:\n        gx = [identity(gx, dtype=fx.dtype, name='gx')]\n\n      override_grad = []\n      for x_, gx_ in zip(x, gx):\n        \n        \n        equal_shape = tf.compat.v1.assert_equal(\n            tf.shape(input=x_),\n            tf.shape(input=gx_),\n            message='Each `x` must have the same shape as each `gx`.')\n        with tf.control_dependencies([equal_shape]):\n          \n          \n          \n          \n          \n          \n          \n          zeros_like_x_ = x_ - tf.stop_gradient(x_)\n          override_grad.append(\n              tf.reduce_sum(input_tensor=maybe_stop(gx_) * zeros_like_x_))\n      override_grad = sum(override_grad)\n      override_grad \/= tf.cast(tf.size(input=fx), dtype=fx.dtype.base_dtype)\n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      return maybe_stop(fx) + override_grad","method_summary":"Embeds a custom gradient into a `Tensor`. This function works by clever application of `stop_gradient`. I.e., observe","original_method_code":"def custom_gradient(fx, gx, x, fx_gx_manually_stopped=False, name=None):\n  \"\"\"Embeds a custom gradient into a `Tensor`.\n\n  This function works by clever application of `stop_gradient`. I.e., observe\n  that:\n\n  ```none\n  h(x) = stop_gradient(f(x)) + stop_gradient(g(x)) * (x - stop_gradient(x))\n  ```\n\n  is such that `h(x) == stop_gradient(f(x))` and\n  `grad[h(x), x] == stop_gradient(g(x)).`\n\n  In addition to scalar-domain\/scalar-range functions, this function also\n  supports tensor-domain\/scalar-range functions.\n\n  Partial Custom Gradient:\n\n  Suppose `h(x) = htilde(x, y)`. Note that `dh\/dx = stop(g(x))` but `dh\/dy =\n  None`. This is because a `Tensor` cannot have only a portion of its gradient\n  stopped. To circumvent this issue, one must manually `stop_gradient` the\n  relevant portions of `f`, `g`. For example see the unit-test,\n  `test_works_correctly_fx_gx_manually_stopped`.\n\n  Args:\n    fx: `Tensor`. Output of function evaluated at `x`.\n    gx: `Tensor` or list of `Tensor`s. Gradient of function at (each) `x`.\n    x: `Tensor` or list of `Tensor`s. Args of evaluation for `f`.\n    fx_gx_manually_stopped: Python `bool` indicating that `fx`, `gx` manually\n      have `stop_gradient` applied.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    fx: Floating-type `Tensor` equal to `f(x)` but which has gradient\n      `stop_gradient(g(x))`.\n  \"\"\"\n  def maybe_stop(x):\n    if fx_gx_manually_stopped:\n      return x\n    return tf.stop_gradient(x)\n\n  with tf.compat.v1.name_scope(name, 'custom_gradient', [fx, gx, x]):\n    fx = tf.convert_to_tensor(value=fx, name='fx')\n    # We don't want to bother eagerly computing `gx` since we may not even need\n    # it.\n    with tf.control_dependencies([fx]):\n      if is_list_like(x):\n        x = [identity(x_, name='x') for x_ in x]\n      else:\n        x = [identity(x, name='x')]\n\n      if is_list_like(gx):\n        gx = [identity(gx_, dtype=fx.dtype, name='gx')\n              for gx_ in gx]\n      else:\n        gx = [identity(gx, dtype=fx.dtype, name='gx')]\n\n      override_grad = []\n      for x_, gx_ in zip(x, gx):\n        # Observe: tf.gradients(f(x), x)[i].shape == x[i].shape\n        # thus we check that the user is supplying correct shapes.\n        equal_shape = tf.compat.v1.assert_equal(\n            tf.shape(input=x_),\n            tf.shape(input=gx_),\n            message='Each `x` must have the same shape as each `gx`.')\n        with tf.control_dependencies([equal_shape]):\n          # IEEE754 ensures `(x-x)==0.` and that `0.*x==0.` so we make sure to\n          # write the code this way, rather than, e.g.,\n          # `sum_x * stop(gx) + stop(fx - sum_x * gx)`.\n          # For more discussion regarding the relevant portions of the IEEE754\n          # standard, see the StackOverflow question,\n          # \"Is there a floating point value of x, for which x-x == 0 is false?\"\n          # http:\/\/stackoverflow.com\/q\/2686644\n          zeros_like_x_ = x_ - tf.stop_gradient(x_)\n          override_grad.append(\n              tf.reduce_sum(input_tensor=maybe_stop(gx_) * zeros_like_x_))\n      override_grad = sum(override_grad)\n      override_grad \/= tf.cast(tf.size(input=fx), dtype=fx.dtype.base_dtype)\n\n      # Proof of correctness:\n      #\n      #  f(x) = x * stop[gx] + stop[fx - x * gx]\n      #       = stop[fx]\n      #\n      #  g(x) = grad[fx]\n      #       = stop[gx] + grad[stop[fx - x * gx]]\n      #       = stop[gx] + 0\n      #\n      # Notice that when x is zero it still works:\n      # grad[x * stop(gx) + stop(fx - x * gx)] = 1 * stop[gx] + 0 = stop[gx]\n      #\n      # The proof is similar for the tensor-domain case, except that we\n      # `reduce_sum` the `stop[gx] * (x - stop[x])` then rescale by\n      # `tf.size(fx)` since this reduced version is broadcast to `fx`.\n      return maybe_stop(fx) + override_grad","method_path":"https:\/\/github.com\/tensorflow\/probability\/blob\/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5\/tensorflow_probability\/python\/math\/custom_gradient.py#L39-L133"}
{"repo_name":"tensorflow\/probability","method_name":"value_and_gradient","method_code":"def value_and_gradient(f, xs, use_gradient_tape=False, name=None):\n  \"\"\"\"\"\"\n  with tf.compat.v1.name_scope(name, 'value_and_gradient', [xs]):\n    is_xs_list_like = isinstance(xs, (tuple, list))\n    if not is_xs_list_like:\n      xs = [xs]\n    xs = [tf.convert_to_tensor(value=x, name='x{}'.format(i))\n          for i, x in enumerate(xs)]\n    if tf.executing_eagerly() or use_gradient_tape:\n      with tf.GradientTape(watch_accessed_variables=False) as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      dydx = tape.gradient(y, xs)\n    else:\n      y = f(*xs)\n      dydx = tf.gradients(ys=y, xs=xs)\n    if not is_xs_list_like:\n      dydx = dydx[0]\n    return y, dydx","method_summary":"Computes `f(*xs)` and its gradients wrt to `*xs`.","original_method_code":"def value_and_gradient(f, xs, use_gradient_tape=False, name=None):\n  \"\"\"Computes `f(*xs)` and its gradients wrt to `*xs`.\n\n  Args:\n    f: Python `callable` to be differentiated. If `f` returns a scalar, this\n      scalar will be differentiated. If `f` returns a tensor or list of tensors,\n      by default a scalar will be computed by adding all their values to produce\n      a single scalar. If desired, the tensors can be elementwise multiplied by\n      the tensors passed as the `dy` keyword argument to the returned gradient\n      function.\n    xs: Python list of parameters of f for which to differentiate. (Can also\n      be single `Tensor`.)\n    use_gradient_tape: Python `bool` indicating that `tf.GradientTape`\n      should be used regardless of `tf.executing_eagerly()` status.\n      Default value: `False`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., `'value_and_gradient'`).\n\n  Returns:\n    y: `y = f(*xs)`.\n    dydx: Gradient of `y` wrt each of `xs`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'value_and_gradient', [xs]):\n    is_xs_list_like = isinstance(xs, (tuple, list))\n    if not is_xs_list_like:\n      xs = [xs]\n    xs = [tf.convert_to_tensor(value=x, name='x{}'.format(i))\n          for i, x in enumerate(xs)]\n    if tf.executing_eagerly() or use_gradient_tape:\n      with tf.GradientTape(watch_accessed_variables=False) as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      dydx = tape.gradient(y, xs)\n    else:\n      y = f(*xs)\n      dydx = tf.gradients(ys=y, xs=xs)\n    if not is_xs_list_like:\n      dydx = dydx[0]\n    return y, dydx","method_path":"https:\/\/github.com\/tensorflow\/probability\/blob\/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5\/tensorflow_probability\/python\/math\/gradient.py#L30-L69"}
{"repo_name":"tensorflow\/probability","method_name":"mvn","method_code":"def mvn(*args, **kwargs):\n  \"\"\"\"\"\"\n  \n  return tfd.Independent(tfd.Normal(*args, **kwargs),\n                         reinterpreted_batch_ndims=1)","method_summary":"Convenience function to efficiently construct a MultivariateNormalDiag.","original_method_code":"def mvn(*args, **kwargs):\n  \"\"\"Convenience function to efficiently construct a MultivariateNormalDiag.\"\"\"\n  # Faster than using `tfd.MultivariateNormalDiag`.\n  return tfd.Independent(tfd.Normal(*args, **kwargs),\n                         reinterpreted_batch_ndims=1)","method_path":"https:\/\/github.com\/tensorflow\/probability\/blob\/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5\/tensorflow_probability\/python\/mcmc\/eight_schools_hmc.py#L37-L41"}
{"repo_name":"tensorflow\/probability","method_name":"eight_schools_joint_log_prob","method_code":"def eight_schools_joint_log_prob(\n    treatment_effects, treatment_stddevs,\n    avg_effect, avg_stddev, school_effects_standard):\n  \"\"\"\"\"\"\n  rv_avg_effect = tfd.Normal(loc=0., scale=10.)\n  rv_avg_stddev = tfd.Normal(loc=5., scale=1.)\n  rv_school_effects_standard = mvn(\n      loc=tf.zeros_like(school_effects_standard),\n      scale=tf.ones_like(school_effects_standard))\n  rv_treatment_effects = mvn(\n      loc=(avg_effect + tf.exp(avg_stddev) * school_effects_standard),\n      scale=treatment_stddevs)\n  return (\n      rv_avg_effect.log_prob(avg_effect) +\n      rv_avg_stddev.log_prob(avg_stddev) +\n      rv_school_effects_standard.log_prob(school_effects_standard) +\n      rv_treatment_effects.log_prob(treatment_effects))","method_summary":"Eight-schools joint log-prob.","original_method_code":"def eight_schools_joint_log_prob(\n    treatment_effects, treatment_stddevs,\n    avg_effect, avg_stddev, school_effects_standard):\n  \"\"\"Eight-schools joint log-prob.\"\"\"\n  rv_avg_effect = tfd.Normal(loc=0., scale=10.)\n  rv_avg_stddev = tfd.Normal(loc=5., scale=1.)\n  rv_school_effects_standard = mvn(\n      loc=tf.zeros_like(school_effects_standard),\n      scale=tf.ones_like(school_effects_standard))\n  rv_treatment_effects = mvn(\n      loc=(avg_effect + tf.exp(avg_stddev) * school_effects_standard),\n      scale=treatment_stddevs)\n  return (\n      rv_avg_effect.log_prob(avg_effect) +\n      rv_avg_stddev.log_prob(avg_stddev) +\n      rv_school_effects_standard.log_prob(school_effects_standard) +\n      rv_treatment_effects.log_prob(treatment_effects))","method_path":"https:\/\/github.com\/tensorflow\/probability\/blob\/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5\/tensorflow_probability\/python\/mcmc\/eight_schools_hmc.py#L44-L60"}
{"repo_name":"tensorflow\/probability","method_name":"benchmark_eight_schools_hmc","method_code":"def benchmark_eight_schools_hmc(\n    num_results=int(5e3),\n    num_burnin_steps=int(3e3),\n    num_leapfrog_steps=3,\n    step_size=0.4):\n  \"\"\"\"\"\"\n\n  num_schools = 8\n  treatment_effects = tf.constant(\n      [28, 8, -3, 7, -1, 1, 18, 12],\n      dtype=np.float32,\n      name='treatment_effects')\n  treatment_stddevs = tf.constant(\n      [15, 10, 16, 11, 9, 11, 10, 18],\n      dtype=np.float32,\n      name='treatment_stddevs')\n\n  def unnormalized_posterior_log_prob(\n      avg_effect, avg_stddev, school_effects_standard):\n    \"\"\"\"\"\"\n    return eight_schools_joint_log_prob(\n        treatment_effects, treatment_stddevs,\n        avg_effect, avg_stddev, school_effects_standard)\n\n  if tf.executing_eagerly():\n    sample_chain = tf.function(tfp.mcmc.sample_chain)\n  else:\n    sample_chain = tfp.mcmc.sample_chain\n\n  def computation():\n    \"\"\"\"\"\"\n    _, kernel_results = sample_chain(\n        num_results=num_results,\n        num_burnin_steps=num_burnin_steps,\n        current_state=(\n            tf.zeros([], name='init_avg_effect'),\n            tf.zeros([], name='init_avg_stddev'),\n            tf.ones([num_schools], name='init_school_effects_standard'),\n        ),\n        kernel=tfp.mcmc.HamiltonianMonteCarlo(\n            target_log_prob_fn=unnormalized_posterior_log_prob,\n            step_size=step_size,\n            num_leapfrog_steps=num_leapfrog_steps))\n\n    return kernel_results.is_accepted\n\n  \n  \n  is_accepted_tensor = computation()\n  if not tf.executing_eagerly():\n    session = tf.compat.v1.Session()\n    session.run(is_accepted_tensor)\n\n  start_time = time.time()\n  if tf.executing_eagerly():\n    is_accepted = computation()\n  else:\n    is_accepted = session.run(is_accepted_tensor)\n  wall_time = time.time() - start_time\n\n  num_accepted = np.sum(is_accepted)\n  acceptance_rate = np.float32(num_accepted) \/ np.float32(num_results)\n\n  return dict(\n      iters=(num_results + num_burnin_steps) * num_leapfrog_steps,\n      extras={'acceptance_rate': acceptance_rate},\n      wall_time=wall_time)","method_summary":"Runs HMC on the eight-schools unnormalized posterior.","original_method_code":"def benchmark_eight_schools_hmc(\n    num_results=int(5e3),\n    num_burnin_steps=int(3e3),\n    num_leapfrog_steps=3,\n    step_size=0.4):\n  \"\"\"Runs HMC on the eight-schools unnormalized posterior.\"\"\"\n\n  num_schools = 8\n  treatment_effects = tf.constant(\n      [28, 8, -3, 7, -1, 1, 18, 12],\n      dtype=np.float32,\n      name='treatment_effects')\n  treatment_stddevs = tf.constant(\n      [15, 10, 16, 11, 9, 11, 10, 18],\n      dtype=np.float32,\n      name='treatment_stddevs')\n\n  def unnormalized_posterior_log_prob(\n      avg_effect, avg_stddev, school_effects_standard):\n    \"\"\"Eight-schools unnormalized log posterior.\"\"\"\n    return eight_schools_joint_log_prob(\n        treatment_effects, treatment_stddevs,\n        avg_effect, avg_stddev, school_effects_standard)\n\n  if tf.executing_eagerly():\n    sample_chain = tf.function(tfp.mcmc.sample_chain)\n  else:\n    sample_chain = tfp.mcmc.sample_chain\n\n  def computation():\n    \"\"\"The benchmark computation.\"\"\"\n    _, kernel_results = sample_chain(\n        num_results=num_results,\n        num_burnin_steps=num_burnin_steps,\n        current_state=(\n            tf.zeros([], name='init_avg_effect'),\n            tf.zeros([], name='init_avg_stddev'),\n            tf.ones([num_schools], name='init_school_effects_standard'),\n        ),\n        kernel=tfp.mcmc.HamiltonianMonteCarlo(\n            target_log_prob_fn=unnormalized_posterior_log_prob,\n            step_size=step_size,\n            num_leapfrog_steps=num_leapfrog_steps))\n\n    return kernel_results.is_accepted\n\n  # Let's force evaluation of graph to ensure build time is not part of our time\n  # trial.\n  is_accepted_tensor = computation()\n  if not tf.executing_eagerly():\n    session = tf.compat.v1.Session()\n    session.run(is_accepted_tensor)\n\n  start_time = time.time()\n  if tf.executing_eagerly():\n    is_accepted = computation()\n  else:\n    is_accepted = session.run(is_accepted_tensor)\n  wall_time = time.time() - start_time\n\n  num_accepted = np.sum(is_accepted)\n  acceptance_rate = np.float32(num_accepted) \/ np.float32(num_results)\n\n  return dict(\n      iters=(num_results + num_burnin_steps) * num_leapfrog_steps,\n      extras={'acceptance_rate': acceptance_rate},\n      wall_time=wall_time)","method_path":"https:\/\/github.com\/tensorflow\/probability\/blob\/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5\/tensorflow_probability\/python\/mcmc\/eight_schools_hmc.py#L63-L129"}
{"repo_name":"tensorflow\/probability","method_name":"expand_docstring","method_code":"def expand_docstring(**kwargs):\n  \"\"\"\"\"\"\n  def _fn_wrapped(fn):\n    \"\"\"\"\"\"\n    doc = inspect.cleandoc(fn.__doc__)\n    for k, v in six.iteritems(kwargs):\n      \n      \n      \n      pattern = r'\\$\\{' + str(k) + r'\\}'\n      doc = re.sub(pattern, lambda match: v, doc)  \n    fn.__doc__ = doc\n    return fn\n  return _fn_wrapped","method_summary":"Decorator to programmatically expand the docstring.","original_method_code":"def expand_docstring(**kwargs):\n  \"\"\"Decorator to programmatically expand the docstring.\n\n  Args:\n    **kwargs: Keyword arguments to set. For each key-value pair `k` and `v`,\n      the key is found as `${k}` in the docstring and replaced with `v`.\n\n  Returns:\n    Decorated function.\n  \"\"\"\n  def _fn_wrapped(fn):\n    \"\"\"Original function with modified `__doc__` attribute.\"\"\"\n    doc = inspect.cleandoc(fn.__doc__)\n    for k, v in six.iteritems(kwargs):\n      # Capture each ${k} reference to replace with v.\n      # We wrap the replacement in a function so no backslash escapes\n      # are processed.\n      pattern = r'\\$\\{' + str(k) + r'\\}'\n      doc = re.sub(pattern, lambda match: v, doc)  # pylint: disable=cell-var-from-loop\n    fn.__doc__ = doc\n    return fn\n  return _fn_wrapped","method_path":"https:\/\/github.com\/tensorflow\/probability\/blob\/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5\/tensorflow_probability\/python\/util\/docstring.py#L30-L51"}
{"repo_name":"tensorflow\/probability","method_name":"_simple_name","method_code":"def _simple_name(distribution):\n  \"\"\"\"\"\"\n  simple_name = distribution.name\n\n  \n  if simple_name.endswith('\/'):\n    simple_name = simple_name.split('\/')[-2]\n\n  \n  parts = simple_name.split('_')\n  if parts[-1].isdigit():\n    simple_name = '_'.join(parts[:-1])\n\n  return simple_name","method_summary":"Infer the original name passed into a distribution constructor. Distributions typically follow the pattern of with.name_scope(name) as","original_method_code":"def _simple_name(distribution):\n  \"\"\"Infer the original name passed into a distribution constructor.\n\n  Distributions typically follow the pattern of\n  with.name_scope(name) as name:\n    super(name=name)\n  so we attempt to reverse the name-scope transformation to allow\n  addressing of RVs by the distribution's original, user-visible\n  name kwarg.\n\n  Args:\n    distribution: a tfd.Distribution instance.\n  Returns:\n    simple_name: the original name passed into the Distribution.\n\n  #### Example\n\n  ```\n  d1 = tfd.Normal(0., 1., name='x') # d1.name = 'x\/'\n  d2 = tfd.Normal(0., 1., name='x') # d2.name = 'x_2\/'\n  _simple_name(d2) # returns 'x'\n\n  ```\n\n  \"\"\"\n  simple_name = distribution.name\n\n  # turn 'scope\/x\/' into 'x'\n  if simple_name.endswith('\/'):\n    simple_name = simple_name.split('\/')[-2]\n\n  # turn 'x_3' into 'x'\n  parts = simple_name.split('_')\n  if parts[-1].isdigit():\n    simple_name = '_'.join(parts[:-1])\n\n  return simple_name","method_path":"https:\/\/github.com\/tensorflow\/probability\/blob\/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5\/tensorflow_probability\/python\/edward2\/generated_random_variables.py#L43-L79"}
{"repo_name":"tensorflow\/probability","method_name":"_build_custom_rv","method_code":"def _build_custom_rv(distribution, sample_shape, value, name):\n  \"\"\"\"\"\"\n  \n  \n  \n  \n  \n  \n  del name  \n  return RandomVariable(distribution=distribution,\n                        sample_shape=sample_shape,\n                        value=value)","method_summary":"RandomVariable constructor with a dummy name argument.","original_method_code":"def _build_custom_rv(distribution, sample_shape, value, name):\n  \"\"\"RandomVariable constructor with a dummy name argument.\"\"\"\n  # Program transformations (e.g., `make_log_joint_fn`) assume that\n  # the traced constructor has `name` and `value` kwargs, enabling\n  # them to override the value of an RV according to its name.\n  # User-defined RVs inherit their name from the provided\n  # distribution; this helper method exposes the name as a dummy kwarg\n  # so that it's visible to program transformations.\n  del name  # unused\n  return RandomVariable(distribution=distribution,\n                        sample_shape=sample_shape,\n                        value=value)","method_path":"https:\/\/github.com\/tensorflow\/probability\/blob\/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5\/tensorflow_probability\/python\/edward2\/generated_random_variables.py#L83-L94"}
{"repo_name":"tensorflow\/probability","method_name":"_make_random_variable","method_code":"def _make_random_variable(distribution_cls):\n  \"\"\"\"\"\"\n\n  @interceptable\n  @functools.wraps(distribution_cls, assigned=('__module__', '__name__'))\n  @docstring_util.expand_docstring(\n      cls=distribution_cls.__name__,\n      doc=inspect.cleandoc(distribution_cls.__init__.__doc__ or ''))\n  def func(*args, **kwargs):\n    \n    \"\"\"\"\"\"\n    \n    sample_shape = kwargs.pop('sample_shape', ())\n    value = kwargs.pop('value', None)\n    return RandomVariable(distribution=distribution_cls(*args, **kwargs),\n                          sample_shape=sample_shape,\n                          value=value)\n  return func","method_summary":"Factory function to make random variable given distribution class.","original_method_code":"def _make_random_variable(distribution_cls):\n  \"\"\"Factory function to make random variable given distribution class.\"\"\"\n\n  @interceptable\n  @functools.wraps(distribution_cls, assigned=('__module__', '__name__'))\n  @docstring_util.expand_docstring(\n      cls=distribution_cls.__name__,\n      doc=inspect.cleandoc(distribution_cls.__init__.__doc__ or ''))\n  def func(*args, **kwargs):\n    # pylint: disable=g-doc-args\n    \"\"\"Create a random variable for ${cls}.\n\n    See ${cls} for more details.\n\n    Returns:\n      RandomVariable.\n\n    #### Original Docstring for Distribution\n\n    ${doc}\n    \"\"\"\n    # pylint: enable=g-doc-args\n    sample_shape = kwargs.pop('sample_shape', ())\n    value = kwargs.pop('value', None)\n    return RandomVariable(distribution=distribution_cls(*args, **kwargs),\n                          sample_shape=sample_shape,\n                          value=value)\n  return func","method_path":"https:\/\/github.com\/tensorflow\/probability\/blob\/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5\/tensorflow_probability\/python\/edward2\/generated_random_variables.py#L148-L175"}
{"repo_name":"tensorflow\/probability","method_name":"VectorExponentialLinearOperator._mode_mean_shape","method_code":"def _mode_mean_shape(self):\n    \"\"\"\"\"\"\n    shape = tensorshape_util.concatenate(self.batch_shape, self.event_shape)\n    has_static_shape = tensorshape_util.is_fully_defined(shape)\n    if not has_static_shape:\n      shape = tf.concat([\n          self.batch_shape_tensor(),\n          self.event_shape_tensor(),\n      ], 0)\n    return shape","method_summary":"Shape for the mode\/mean Tensors.","original_method_code":"def _mode_mean_shape(self):\n    \"\"\"Shape for the mode\/mean Tensors.\"\"\"\n    shape = tensorshape_util.concatenate(self.batch_shape, self.event_shape)\n    has_static_shape = tensorshape_util.is_fully_defined(shape)\n    if not has_static_shape:\n      shape = tf.concat([\n          self.batch_shape_tensor(),\n          self.event_shape_tensor(),\n      ], 0)\n    return shape","method_path":"https:\/\/github.com\/tensorflow\/probability\/blob\/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5\/tensorflow_probability\/python\/distributions\/vector_exponential_linear_operator.py#L278-L287"}
{"repo_name":"intel-analytics\/BigDL","method_name":"SequentialSchedule.add","method_code":"def add(self, scheduler, max_iteration, bigdl_type=\"float\"):\n        \"\"\"\"\"\"\n        return callBigDlFunc(bigdl_type, \"addScheduler\", self.value, scheduler, max_iteration)","method_summary":"Add a learning rate scheduler to the contained `schedules`","original_method_code":"def add(self, scheduler, max_iteration, bigdl_type=\"float\"):\n        \"\"\"\n        Add a learning rate scheduler to the contained `schedules`\n\n        :param scheduler: learning rate scheduler to be add\n        :param max_iteration: iteration numbers this scheduler will run\n        \"\"\"\n        return callBigDlFunc(bigdl_type, \"addScheduler\", self.value, scheduler, max_iteration)","method_path":"https:\/\/github.com\/intel-analytics\/BigDL\/blob\/e9c19788285986ab789a2e2998f9a85d7524779f\/pyspark\/bigdl\/optim\/optimizer.py#L425-L432"}
{"repo_name":"intel-analytics\/BigDL","method_name":"OptimMethod.save","method_code":"def save(self, path, overWrite):\n        \"\"\"\"\"\"\n        method=self.value\n        return callBigDlFunc(self.bigdl_type, \"saveOptimMethod\", method, path, overWrite)","method_summary":"save OptimMethod","original_method_code":"def save(self, path, overWrite):\n        \"\"\"\n        save OptimMethod\n        :param path      path\n        :param overWrite whether to overwrite\n        \"\"\"\n        method=self.value\n        return callBigDlFunc(self.bigdl_type, \"saveOptimMethod\", method, path, overWrite)","method_path":"https:\/\/github.com\/intel-analytics\/BigDL\/blob\/e9c19788285986ab789a2e2998f9a85d7524779f\/pyspark\/bigdl\/optim\/optimizer.py#L453-L460"}
{"repo_name":"intel-analytics\/BigDL","method_name":"BaseOptimizer.set_checkpoint","method_code":"def set_checkpoint(self, checkpoint_trigger,\n                       checkpoint_path, isOverWrite=True):\n        \"\"\"\"\"\"\n        if not os.path.exists(checkpoint_path):\n            mkpath(checkpoint_path)\n        callBigDlFunc(self.bigdl_type, \"setCheckPoint\", self.value,\n                      checkpoint_trigger, checkpoint_path, isOverWrite)","method_summary":"Configure checkpoint settings.","original_method_code":"def set_checkpoint(self, checkpoint_trigger,\n                       checkpoint_path, isOverWrite=True):\n        \"\"\"\n        Configure checkpoint settings.\n\n\n        :param checkpoint_trigger: the interval to write snapshots\n        :param checkpoint_path: the path to write snapshots into\n        :param isOverWrite: whether to overwrite existing snapshots in path.default is True\n        \"\"\"\n        if not os.path.exists(checkpoint_path):\n            mkpath(checkpoint_path)\n        callBigDlFunc(self.bigdl_type, \"setCheckPoint\", self.value,\n                      checkpoint_trigger, checkpoint_path, isOverWrite)","method_path":"https:\/\/github.com\/intel-analytics\/BigDL\/blob\/e9c19788285986ab789a2e2998f9a85d7524779f\/pyspark\/bigdl\/optim\/optimizer.py#L719-L732"}
{"repo_name":"intel-analytics\/BigDL","method_name":"BaseOptimizer.set_gradclip_const","method_code":"def set_gradclip_const(self, min_value, max_value):\n        \"\"\"\"\"\"\n        callBigDlFunc(self.bigdl_type, \"setConstantClip\", self.value, min_value, max_value)","method_summary":"Configure constant clipping settings.","original_method_code":"def set_gradclip_const(self, min_value, max_value):\n        \"\"\"\n        Configure constant clipping settings.\n\n\n        :param min_value: the minimum value to clip by\n        :param max_value: the maxmimum value to clip by\n        \"\"\"\n        callBigDlFunc(self.bigdl_type, \"setConstantClip\", self.value, min_value, max_value)","method_path":"https:\/\/github.com\/intel-analytics\/BigDL\/blob\/e9c19788285986ab789a2e2998f9a85d7524779f\/pyspark\/bigdl\/optim\/optimizer.py#L734-L742"}
{"repo_name":"intel-analytics\/BigDL","method_name":"BaseOptimizer.optimize","method_code":"def optimize(self):\n        \"\"\"\"\"\"\n        jmodel = callJavaFunc(self.value.optimize)\n        from bigdl.nn.layer import Layer\n        return Layer.of(jmodel)","method_summary":"Do an optimization.","original_method_code":"def optimize(self):\n        \"\"\"\n        Do an optimization.\n        \"\"\"\n        jmodel = callJavaFunc(self.value.optimize)\n        from bigdl.nn.layer import Layer\n        return Layer.of(jmodel)","method_path":"https:\/\/github.com\/intel-analytics\/BigDL\/blob\/e9c19788285986ab789a2e2998f9a85d7524779f\/pyspark\/bigdl\/optim\/optimizer.py#L760-L766"}
{"repo_name":"intel-analytics\/BigDL","method_name":"Optimizer.create","method_code":"def create(model,\n               training_set,\n               criterion,\n               end_trigger=None,\n               batch_size=32,\n               optim_method=None,\n               cores=None,\n               bigdl_type=\"float\"):\n        \"\"\"\"\"\"\n        if not end_trigger:\n            end_trigger = MaxEpoch(1)\n        if not optim_method:\n            optim_method = SGD()\n        if isinstance(training_set, RDD) or isinstance(training_set, DataSet):\n            return DistriOptimizer(model=model,\n                                   training_rdd=training_set,\n                                   criterion=criterion,\n                                   end_trigger=end_trigger,\n                                   batch_size=batch_size,\n                                   optim_method=optim_method,\n                                   bigdl_type=bigdl_type)\n        elif isinstance(training_set, tuple) and len(training_set) == 2:\n            x, y = training_set\n            return LocalOptimizer(X=x,\n                                  Y=y,\n                                  model=model,\n                                  criterion=criterion,\n                                  end_trigger=end_trigger,\n                                  batch_size=batch_size,\n                                  optim_method=optim_method,\n                                  cores=cores,\n                                  bigdl_type=\"float\")\n        else:\n            raise Exception(\"Not supported training set: %s\" % type(training_set))","method_summary":"Create an optimizer. Depend on the input type, the returning optimizer can be a local optimizer \\ or a distributed optimizer.","original_method_code":"def create(model,\n               training_set,\n               criterion,\n               end_trigger=None,\n               batch_size=32,\n               optim_method=None,\n               cores=None,\n               bigdl_type=\"float\"):\n        \"\"\"\n        Create an optimizer.\n        Depend on the input type, the returning optimizer can be a local optimizer \\\n        or a distributed optimizer.\n\n        :param model: the neural net model\n        :param training_set: (features, label) for local mode. RDD[Sample] for distributed mode.\n        :param criterion: the loss function\n        :param optim_method: the algorithm to use for optimization,\n           e.g. SGD, Adagrad, etc. If optim_method is None, the default algorithm is SGD.\n        :param end_trigger: when to end the optimization. default value is MapEpoch(1)\n        :param batch_size: training batch size\n        :param cores: This is for local optimizer only and use total physical cores as the default value\n        \"\"\"\n        if not end_trigger:\n            end_trigger = MaxEpoch(1)\n        if not optim_method:\n            optim_method = SGD()\n        if isinstance(training_set, RDD) or isinstance(training_set, DataSet):\n            return DistriOptimizer(model=model,\n                                   training_rdd=training_set,\n                                   criterion=criterion,\n                                   end_trigger=end_trigger,\n                                   batch_size=batch_size,\n                                   optim_method=optim_method,\n                                   bigdl_type=bigdl_type)\n        elif isinstance(training_set, tuple) and len(training_set) == 2:\n            x, y = training_set\n            return LocalOptimizer(X=x,\n                                  Y=y,\n                                  model=model,\n                                  criterion=criterion,\n                                  end_trigger=end_trigger,\n                                  batch_size=batch_size,\n                                  optim_method=optim_method,\n                                  cores=cores,\n                                  bigdl_type=\"float\")\n        else:\n            raise Exception(\"Not supported training set: %s\" % type(training_set))","method_path":"https:\/\/github.com\/intel-analytics\/BigDL\/blob\/e9c19788285986ab789a2e2998f9a85d7524779f\/pyspark\/bigdl\/optim\/optimizer.py#L848-L894"}
{"repo_name":"intel-analytics\/BigDL","method_name":"Optimizer.set_validation","method_code":"def set_validation(self, batch_size, val_rdd, trigger, val_method=None):\n        \"\"\"\"\"\"\n        if val_method is None:\n            val_method = [Top1Accuracy()]\n        func_name = \"setValidation\"\n        if isinstance(val_rdd, DataSet):\n            func_name = \"setValidationFromDataSet\"\n        callBigDlFunc(self.bigdl_type, func_name, self.value, batch_size,\n                      trigger, val_rdd, to_list(val_method))","method_summary":"Configure validation settings.","original_method_code":"def set_validation(self, batch_size, val_rdd, trigger, val_method=None):\n        \"\"\"\n        Configure validation settings.\n\n\n        :param batch_size: validation batch size\n        :param val_rdd: validation dataset\n        :param trigger: validation interval\n        :param val_method: the ValidationMethod to use,e.g. \"Top1Accuracy\", \"Top5Accuracy\", \"Loss\"\n        \"\"\"\n        if val_method is None:\n            val_method = [Top1Accuracy()]\n        func_name = \"setValidation\"\n        if isinstance(val_rdd, DataSet):\n            func_name = \"setValidationFromDataSet\"\n        callBigDlFunc(self.bigdl_type, func_name, self.value, batch_size,\n                      trigger, val_rdd, to_list(val_method))","method_path":"https:\/\/github.com\/intel-analytics\/BigDL\/blob\/e9c19788285986ab789a2e2998f9a85d7524779f\/pyspark\/bigdl\/optim\/optimizer.py#L896-L912"}
{"repo_name":"intel-analytics\/BigDL","method_name":"Optimizer.set_traindata","method_code":"def set_traindata(self, training_rdd, batch_size):\n        \"\"\"\"\"\"\n        callBigDlFunc(self.bigdl_type, \"setTrainData\", self.value,\n                     training_rdd, batch_size)","method_summary":"Set new training dataset, for optimizer reuse","original_method_code":"def set_traindata(self, training_rdd, batch_size):\n        \"\"\"\n        Set new training dataset, for optimizer reuse\n\n        :param training_rdd: the training dataset\n        :param batch_size: training batch size\n        :return:\n        \"\"\"\n        callBigDlFunc(self.bigdl_type, \"setTrainData\", self.value,\n                     training_rdd, batch_size)","method_path":"https:\/\/github.com\/intel-analytics\/BigDL\/blob\/e9c19788285986ab789a2e2998f9a85d7524779f\/pyspark\/bigdl\/optim\/optimizer.py#L914-L923"}
{"repo_name":"intel-analytics\/BigDL","method_name":"LocalOptimizer.set_validation","method_code":"def set_validation(self, batch_size, X_val, Y_val, trigger, val_method=None):\n        \"\"\"\"\"\"\n        if val_method is None:\n            val_method = [Top1Accuracy()]\n        callBigDlFunc(self.bigdl_type, \"setValidation\", self.value, batch_size,\n                      trigger, [JTensor.from_ndarray(X) for X in to_list(X_val)],\n                      JTensor.from_ndarray(Y_val), to_list(val_method))","method_summary":"Configure validation settings.","original_method_code":"def set_validation(self, batch_size, X_val, Y_val, trigger, val_method=None):\n        \"\"\"\n        Configure validation settings.\n\n        :param batch_size: validation batch size\n        :param X_val: features of validation dataset\n        :param Y_val: label of validation dataset\n        :param trigger: validation interval\n        :param val_method: the ValidationMethod to use,e.g. \"Top1Accuracy\", \"Top5Accuracy\", \"Loss\"\n        \"\"\"\n        if val_method is None:\n            val_method = [Top1Accuracy()]\n        callBigDlFunc(self.bigdl_type, \"setValidation\", self.value, batch_size,\n                      trigger, [JTensor.from_ndarray(X) for X in to_list(X_val)],\n                      JTensor.from_ndarray(Y_val), to_list(val_method))","method_path":"https:\/\/github.com\/intel-analytics\/BigDL\/blob\/e9c19788285986ab789a2e2998f9a85d7524779f\/pyspark\/bigdl\/optim\/optimizer.py#L1009-L1023"}
{"repo_name":"intel-analytics\/BigDL","method_name":"TrainSummary.set_summary_trigger","method_code":"def set_summary_trigger(self, name, trigger):\n        \"\"\"\"\"\"\n        return callBigDlFunc(self.bigdl_type, \"summarySetTrigger\", self.value,\n                             name, trigger)","method_summary":"Set the interval of recording for each indicator.","original_method_code":"def set_summary_trigger(self, name, trigger):\n        \"\"\"\n        Set the interval of recording for each indicator.\n\n\n        :param tag: tag name. Supported tag names are \"LearningRate\", \"Loss\",\"Throughput\", \"Parameters\". \"Parameters\" is an umbrella tag thatincludes weight, bias, gradWeight, gradBias, and some running status(eg. runningMean and runningVar in BatchNormalization). If youdidn't set any triggers, we will by default record Loss and Throughputin each iteration, while *NOT* recording LearningRate and Parameters,as recording parameters may introduce substantial overhead when themodel is very big, LearningRate is not a public attribute for allOptimMethod.\n        :param trigger: trigger\n        \"\"\"\n        return callBigDlFunc(self.bigdl_type, \"summarySetTrigger\", self.value,\n                             name, trigger)","method_path":"https:\/\/github.com\/intel-analytics\/BigDL\/blob\/e9c19788285986ab789a2e2998f9a85d7524779f\/pyspark\/bigdl\/optim\/optimizer.py#L1062-L1071"}
{"repo_name":"deepmipt\/DeepPavlov","method_name":"expand_tile","method_code":"def expand_tile(units, axis):\n    \"\"\"\"\"\"\n    assert axis in (1, 2)\n    n_time_steps = K.int_shape(units)[1]\n    repetitions = [1, 1, 1, 1]\n    repetitions[axis] = n_time_steps\n    if axis == 1:\n        expanded = Reshape(target_shape=( (1,) + K.int_shape(units)[1:] ))(units)\n    else:\n        expanded = Reshape(target_shape=(K.int_shape(units)[1:2] + (1,) + K.int_shape(units)[2:]))(units)\n    return K.tile(expanded, repetitions)","method_summary":"Expand and tile tensor along given axis","original_method_code":"def expand_tile(units, axis):\n    \"\"\"\n    Expand and tile tensor along given axis\n\n    Args:\n        units: tf tensor with dimensions [batch_size, time_steps, n_input_features]\n        axis: axis along which expand and tile. Must be 1 or 2\n\n    \"\"\"\n    assert axis in (1, 2)\n    n_time_steps = K.int_shape(units)[1]\n    repetitions = [1, 1, 1, 1]\n    repetitions[axis] = n_time_steps\n    if axis == 1:\n        expanded = Reshape(target_shape=( (1,) + K.int_shape(units)[1:] ))(units)\n    else:\n        expanded = Reshape(target_shape=(K.int_shape(units)[1:2] + (1,) + K.int_shape(units)[2:]))(units)\n    return K.tile(expanded, repetitions)","method_path":"https:\/\/github.com\/deepmipt\/DeepPavlov\/blob\/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c\/deeppavlov\/core\/layers\/keras_layers.py#L22-L39"}
{"repo_name":"deepmipt\/DeepPavlov","method_name":"additive_self_attention","method_code":"def additive_self_attention(units, n_hidden=None, n_output_features=None, activation=None):\n    \"\"\"\"\"\"\n    n_input_features = K.int_shape(units)[2]\n    if n_hidden is None:\n        n_hidden = n_input_features\n    if n_output_features is None:\n        n_output_features = n_input_features\n    exp1 = Lambda(lambda x: expand_tile(x, axis=1))(units)\n    exp2 = Lambda(lambda x: expand_tile(x, axis=2))(units)\n    units_pairs = Concatenate(axis=3)([exp1, exp2])\n    query = Dense(n_hidden, activation=\"tanh\")(units_pairs)\n    attention = Dense(1, activation=lambda x: softmax(x, axis=2))(query)\n    attended_units = Lambda(lambda x: K.sum(attention * x, axis=2))(exp1)\n    output = Dense(n_output_features, activation=activation)(attended_units)\n    return output","method_summary":"Compute additive self attention for time series of vectors (with batch dimension) the","original_method_code":"def additive_self_attention(units, n_hidden=None, n_output_features=None, activation=None):\n    \"\"\"\n    Compute additive self attention for time series of vectors (with batch dimension)\n            the formula: score(h_i, h_j) = <v, tanh(W_1 h_i + W_2 h_j)>\n            v is a learnable vector of n_hidden dimensionality,\n            W_1 and W_2 are learnable [n_hidden, n_input_features] matrices\n\n    Args:\n        units: tf tensor with dimensionality [batch_size, time_steps, n_input_features]\n        n_hidden: number of2784131 units in hidden representation of similarity measure\n        n_output_features: number of features in output dense layer\n        activation: activation at the output\n\n    Returns:\n        output: self attended tensor with dimensionality [batch_size, time_steps, n_output_features]\n        \"\"\"\n    n_input_features = K.int_shape(units)[2]\n    if n_hidden is None:\n        n_hidden = n_input_features\n    if n_output_features is None:\n        n_output_features = n_input_features\n    exp1 = Lambda(lambda x: expand_tile(x, axis=1))(units)\n    exp2 = Lambda(lambda x: expand_tile(x, axis=2))(units)\n    units_pairs = Concatenate(axis=3)([exp1, exp2])\n    query = Dense(n_hidden, activation=\"tanh\")(units_pairs)\n    attention = Dense(1, activation=lambda x: softmax(x, axis=2))(query)\n    attended_units = Lambda(lambda x: K.sum(attention * x, axis=2))(exp1)\n    output = Dense(n_output_features, activation=activation)(attended_units)\n    return output","method_path":"https:\/\/github.com\/deepmipt\/DeepPavlov\/blob\/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c\/deeppavlov\/core\/layers\/keras_layers.py#L42-L70"}
{"repo_name":"deepmipt\/DeepPavlov","method_name":"multiplicative_self_attention","method_code":"def multiplicative_self_attention(units, n_hidden=None, n_output_features=None, activation=None):\n    \"\"\"\"\"\"\n    n_input_features = K.int_shape(units)[2]\n    if n_hidden is None:\n        n_hidden = n_input_features\n    if n_output_features is None:\n        n_output_features = n_input_features\n    exp1 = Lambda(lambda x: expand_tile(x, axis=1))(units)\n    exp2 = Lambda(lambda x: expand_tile(x, axis=2))(units)\n    queries = Dense(n_hidden)(exp1)\n    keys = Dense(n_hidden)(exp2)\n    scores = Lambda(lambda x: K.sum(queries * x, axis=3, keepdims=True))(keys)\n    attention = Lambda(lambda x: softmax(x, axis=2))(scores)\n    mult = Multiply()([attention, exp1])\n    attended_units = Lambda(lambda x: K.sum(x, axis=2))(mult)\n    output = Dense(n_output_features, activation=activation)(attended_units)\n    return output","method_summary":"Compute multiplicative self attention for time series of vectors (with batch dimension) the","original_method_code":"def multiplicative_self_attention(units, n_hidden=None, n_output_features=None, activation=None):\n    \"\"\"\n    Compute multiplicative self attention for time series of vectors (with batch dimension)\n    the formula: score(h_i, h_j) = <W_1 h_i,  W_2 h_j>,  W_1 and W_2 are learnable matrices\n    with dimensionality [n_hidden, n_input_features]\n\n    Args:\n        units: tf tensor with dimensionality [batch_size, time_steps, n_input_features]\n        n_hidden: number of units in hidden representation of similarity measure\n        n_output_features: number of features in output dense layer\n        activation: activation at the output\n\n    Returns:\n        output: self attended tensor with dimensionality [batch_size, time_steps, n_output_features]\n    \"\"\"\n    n_input_features = K.int_shape(units)[2]\n    if n_hidden is None:\n        n_hidden = n_input_features\n    if n_output_features is None:\n        n_output_features = n_input_features\n    exp1 = Lambda(lambda x: expand_tile(x, axis=1))(units)\n    exp2 = Lambda(lambda x: expand_tile(x, axis=2))(units)\n    queries = Dense(n_hidden)(exp1)\n    keys = Dense(n_hidden)(exp2)\n    scores = Lambda(lambda x: K.sum(queries * x, axis=3, keepdims=True))(keys)\n    attention = Lambda(lambda x: softmax(x, axis=2))(scores)\n    mult = Multiply()([attention, exp1])\n    attended_units = Lambda(lambda x: K.sum(x, axis=2))(mult)\n    output = Dense(n_output_features, activation=activation)(attended_units)\n    return output","method_path":"https:\/\/github.com\/deepmipt\/DeepPavlov\/blob\/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c\/deeppavlov\/core\/layers\/keras_layers.py#L73-L102"}
{"repo_name":"deepmipt\/DeepPavlov","method_name":"precompute_future_symbols","method_code":"def precompute_future_symbols(trie, n, allow_spaces=False):\n    \"\"\"\"\"\"\n    if n == 0:\n        return\n    if trie.is_terminated and trie.precompute_symbols:\n        \n        return\n    for index, final in enumerate(trie.final):\n        trie.data[index] = [set() for i in range(n)]\n    for index, (node_data, final) in enumerate(zip(trie.data, trie.final)):\n        node_data[0] = set(trie._get_letters(index))\n        if allow_spaces and final:\n            node_data[0].add(\" \")\n    for d in range(1, n):\n        for index, (node_data, final) in enumerate(zip(trie.data, trie.final)):\n            children = set(trie._get_children(index))\n            for child in children:\n                node_data[d] |= trie.data[child][d - 1]\n            \n            if allow_spaces and final:\n                node_data[d] |= trie.data[trie.root][d - 1]\n    trie.terminated = True","method_summary":"Collecting possible continuations of length <= n for every node","original_method_code":"def precompute_future_symbols(trie, n, allow_spaces=False):\n    \"\"\"\n    Collecting possible continuations of length <= n for every node\n    \"\"\"\n    if n == 0:\n        return\n    if trie.is_terminated and trie.precompute_symbols:\n        # \u0441\u0438\u043c\u0432\u043e\u043b\u044b \u0443\u0436\u0435 \u043f\u0440\u0435\u0434\u043f\u043e\u0441\u0447\u0438\u0442\u0430\u043d\u044b\n        return\n    for index, final in enumerate(trie.final):\n        trie.data[index] = [set() for i in range(n)]\n    for index, (node_data, final) in enumerate(zip(trie.data, trie.final)):\n        node_data[0] = set(trie._get_letters(index))\n        if allow_spaces and final:\n            node_data[0].add(\" \")\n    for d in range(1, n):\n        for index, (node_data, final) in enumerate(zip(trie.data, trie.final)):\n            children = set(trie._get_children(index))\n            for child in children:\n                node_data[d] |= trie.data[child][d - 1]\n            # \u0432 \u0441\u043b\u0443\u0447\u0430\u0435, \u0435\u0441\u043b\u0438 \u0440\u0430\u0437\u0440\u0435\u0448\u0451\u043d \u0432\u043e\u0437\u0432\u0440\u0430\u0442 \u043f\u043e \u043f\u0440\u043e\u0431\u0435\u043b\u0443 \u0432 \u0441\u0442\u0430\u0440\u0442\u043e\u0432\u043e\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\n            if allow_spaces and final:\n                node_data[d] |= trie.data[trie.root][d - 1]\n    trie.terminated = True","method_path":"https:\/\/github.com\/deepmipt\/DeepPavlov\/blob\/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c\/deeppavlov\/models\/spelling_correction\/levenshtein\/tabled_trie.py#L465-L488"}
{"repo_name":"deepmipt\/DeepPavlov","method_name":"Trie.save","method_code":"def save(self, outfile):\n        \"\"\"\"\"\"\n        with open(outfile, \"w\", encoding=\"utf8\") as fout:\n            attr_values = [getattr(self, attr) for attr in Trie.ATTRS]\n            attr_values.append(any(x is not None for x in self.data))\n            fout.write(\"{}\\n{}\\t{}\\n\".format(\n                \" \".join(\"T\" if x else \"F\" for x in attr_values),\n                self.nodes_number, self.root))\n            fout.write(\" \".join(str(a) for a in self.alphabet) + \"\\n\")\n            for index, label in enumerate(self.final):\n                letters = self._get_letters(index, return_indexes=True)\n                children = self._get_children(index)\n                fout.write(\"{}\\t{}\\n\".format(\n                    \"T\" if label else \"F\", \" \".join(\"{}:{}\".format(*elem)\n                                                    for elem in zip(letters, children))))\n            if self.precompute_symbols is not None:\n                for elem in self.data:\n                    fout.write(\":\".join(\",\".join(\n                        map(str, symbols)) for symbols in elem) + \"\\n\")\n        return","method_summary":"\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u0434\u0435\u0440\u0435\u0432\u043e \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0433\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f","original_method_code":"def save(self, outfile):\n        \"\"\"\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u0434\u0435\u0440\u0435\u0432\u043e \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0433\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f\n        \"\"\"\n        with open(outfile, \"w\", encoding=\"utf8\") as fout:\n            attr_values = [getattr(self, attr) for attr in Trie.ATTRS]\n            attr_values.append(any(x is not None for x in self.data))\n            fout.write(\"{}\\n{}\\t{}\\n\".format(\n                \" \".join(\"T\" if x else \"F\" for x in attr_values),\n                self.nodes_number, self.root))\n            fout.write(\" \".join(str(a) for a in self.alphabet) + \"\\n\")\n            for index, label in enumerate(self.final):\n                letters = self._get_letters(index, return_indexes=True)\n                children = self._get_children(index)\n                fout.write(\"{}\\t{}\\n\".format(\n                    \"T\" if label else \"F\", \" \".join(\"{}:{}\".format(*elem)\n                                                    for elem in zip(letters, children))))\n            if self.precompute_symbols is not None:\n                for elem in self.data:\n                    fout.write(\":\".join(\",\".join(\n                        map(str, symbols)) for symbols in elem) + \"\\n\")\n        return","method_path":"https:\/\/github.com\/deepmipt\/DeepPavlov\/blob\/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c\/deeppavlov\/models\/spelling_correction\/levenshtein\/tabled_trie.py#L61-L82"}
{"repo_name":"deepmipt\/DeepPavlov","method_name":"Trie.make_cashed","method_code":"def make_cashed(self):\n        \"\"\"\"\"\"\n        self._descendance_cash = [dict() for _ in self.graph]\n        self.descend = self._descend_cashed","method_summary":"\u0412\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u043a\u044d\u0448\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a descend","original_method_code":"def make_cashed(self):\n        \"\"\"\n        \u0412\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u043a\u044d\u0448\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a descend\n        \"\"\"\n        self._descendance_cash = [dict() for _ in self.graph]\n        self.descend = self._descend_cashed","method_path":"https:\/\/github.com\/deepmipt\/DeepPavlov\/blob\/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c\/deeppavlov\/models\/spelling_correction\/levenshtein\/tabled_trie.py#L84-L89"}
{"repo_name":"deepmipt\/DeepPavlov","method_name":"Trie.add","method_code":"def add(self, s):\n        \"\"\"\"\"\"\n        if self.is_terminated:\n            raise TypeError(\"Impossible to add string to fitted trie\")\n        if s == \"\":\n            self._set_final(self.root)\n            return\n        curr = self.root\n        for i, a in enumerate(s):\n            code = self.alphabet_codes[a]\n            next = self.graph[curr][code]\n            if next == Trie.NO_NODE:\n                curr = self._add_descendant(curr, s[i:])\n                break\n            else:\n                curr = next\n        self._set_final(curr)\n        return self","method_summary":"\u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u043e\u043a\u0438 s \u0432 \u043f\u0440\u0435\u0444\u0438\u043a\u0441\u043d\u044b\u0439 \u0431\u043e\u0440","original_method_code":"def add(self, s):\n        \"\"\"\n        \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u043e\u043a\u0438 s \u0432 \u043f\u0440\u0435\u0444\u0438\u043a\u0441\u043d\u044b\u0439 \u0431\u043e\u0440\n        \"\"\"\n        if self.is_terminated:\n            raise TypeError(\"Impossible to add string to fitted trie\")\n        if s == \"\":\n            self._set_final(self.root)\n            return\n        curr = self.root\n        for i, a in enumerate(s):\n            code = self.alphabet_codes[a]\n            next = self.graph[curr][code]\n            if next == Trie.NO_NODE:\n                curr = self._add_descendant(curr, s[i:])\n                break\n            else:\n                curr = next\n        self._set_final(curr)\n        return self","method_path":"https:\/\/github.com\/deepmipt\/DeepPavlov\/blob\/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c\/deeppavlov\/models\/spelling_correction\/levenshtein\/tabled_trie.py#L96-L115"}
{"repo_name":"deepmipt\/DeepPavlov","method_name":"Trie.words","method_code":"def words(self):\n        \"\"\"\"\"\"\n        branch, word, indexes = [self.root], [], [0]\n        letters_with_children = [self._get_children_and_letters(self.root)]\n        while len(branch) > 0:\n            if self.is_final(branch[-1]):\n                yield \"\".join(word)\n            while indexes[-1] == len(letters_with_children[-1]):\n                indexes.pop()\n                letters_with_children.pop()\n                branch.pop()\n                if len(indexes) == 0:\n                    raise StopIteration()\n                word.pop()\n            next_letter, next_child = letters_with_children[-1][indexes[-1]]\n            indexes[-1] += 1\n            indexes.append(0)\n            word.append(next_letter)\n            branch.append(next_child)\n            letters_with_children.append(self._get_children_and_letters(branch[-1]))","method_summary":"\u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0438\u0442\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u043e \u0441\u043b\u043e\u0432\u0430\u043c, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u043c\u0441\u044f \u0432 \u0431\u043e\u0440\u0435","original_method_code":"def words(self):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0438\u0442\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u043e \u0441\u043b\u043e\u0432\u0430\u043c, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u043c\u0441\u044f \u0432 \u0431\u043e\u0440\u0435\n        \"\"\"\n        branch, word, indexes = [self.root], [], [0]\n        letters_with_children = [self._get_children_and_letters(self.root)]\n        while len(branch) > 0:\n            if self.is_final(branch[-1]):\n                yield \"\".join(word)\n            while indexes[-1] == len(letters_with_children[-1]):\n                indexes.pop()\n                letters_with_children.pop()\n                branch.pop()\n                if len(indexes) == 0:\n                    raise StopIteration()\n                word.pop()\n            next_letter, next_child = letters_with_children[-1][indexes[-1]]\n            indexes[-1] += 1\n            indexes.append(0)\n            word.append(next_letter)\n            branch.append(next_child)\n            letters_with_children.append(self._get_children_and_letters(branch[-1]))","method_path":"https:\/\/github.com\/deepmipt\/DeepPavlov\/blob\/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c\/deeppavlov\/models\/spelling_correction\/levenshtein\/tabled_trie.py#L139-L160"}
{"repo_name":"deepmipt\/DeepPavlov","method_name":"Trie.find_partitions","method_code":"def find_partitions(self, s, max_count=1):\n        \"\"\"\"\"\"\n        curr_agenda = [(self.root, [], 0)]\n        for i, a in enumerate(s):\n            next_agenda = []\n            for curr, borders, cost in curr_agenda:\n                if cost >= max_count:\n                    continue\n                child = self.graph[curr][self.alphabet_codes[a]]\n                \n                if child == Trie.NO_NODE:\n                    continue\n                next_agenda.append((child, borders, cost))\n                if self.is_final(child):\n                    next_agenda.append((self.root, borders + [i+1], cost+1))\n            curr_agenda = next_agenda\n        answer = []\n        for curr, borders, cost in curr_agenda:\n            if curr == self.root:\n                borders = [0] + borders\n                answer.append([s[left:borders[i+1]] for i, left in enumerate(borders[:-1])])\n        return answer","method_summary":"\u041d\u0430\u0445\u043e\u0434\u0438\u0442 \u0432\u0441\u0435 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f s = s_1","original_method_code":"def find_partitions(self, s, max_count=1):\n        \"\"\"\n        \u041d\u0430\u0445\u043e\u0434\u0438\u0442 \u0432\u0441\u0435 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f s = s_1 ... s_m \u043d\u0430 \u0441\u043b\u043e\u0432\u0430\u0440\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 s_1, ..., s_m\n        \u0434\u043b\u044f m <= max_count\n        \"\"\"\n        curr_agenda = [(self.root, [], 0)]\n        for i, a in enumerate(s):\n            next_agenda = []\n            for curr, borders, cost in curr_agenda:\n                if cost >= max_count:\n                    continue\n                child = self.graph[curr][self.alphabet_codes[a]]\n                # child = self.graph[curr][a]\n                if child == Trie.NO_NODE:\n                    continue\n                next_agenda.append((child, borders, cost))\n                if self.is_final(child):\n                    next_agenda.append((self.root, borders + [i+1], cost+1))\n            curr_agenda = next_agenda\n        answer = []\n        for curr, borders, cost in curr_agenda:\n            if curr == self.root:\n                borders = [0] + borders\n                answer.append([s[left:borders[i+1]] for i, left in enumerate(borders[:-1])])\n        return answer","method_path":"https:\/\/github.com\/deepmipt\/DeepPavlov\/blob\/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c\/deeppavlov\/models\/spelling_correction\/levenshtein\/tabled_trie.py#L175-L199"}
{"repo_name":"deepmipt\/DeepPavlov","method_name":"Trie._add_empty_child","method_code":"def _add_empty_child(self, parent, code, final=False):\n        \"\"\"\"\"\"\n        self.graph[parent][code] = self.nodes_number\n        self.graph.append(self._make_default_node())\n        self.data.append(None)\n        self.final.append(final)\n        self.nodes_number += 1\n        return (self.nodes_number - 1)","method_summary":"\u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0431\u0451\u043d\u043a\u0430 \u043a \u0432\u0435\u0440\u0448\u0438\u043d\u0435 parent \u043f\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u0443 \u0441 \u043a\u043e\u0434\u043e\u043c code","original_method_code":"def _add_empty_child(self, parent, code, final=False):\n        \"\"\"\n        \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0431\u0451\u043d\u043a\u0430 \u043a \u0432\u0435\u0440\u0448\u0438\u043d\u0435 parent \u043f\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u0443 \u0441 \u043a\u043e\u0434\u043e\u043c code\n        \"\"\"\n        self.graph[parent][code] = self.nodes_number\n        self.graph.append(self._make_default_node())\n        self.data.append(None)\n        self.final.append(final)\n        self.nodes_number += 1\n        return (self.nodes_number - 1)","method_path":"https:\/\/github.com\/deepmipt\/DeepPavlov\/blob\/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c\/deeppavlov\/models\/spelling_correction\/levenshtein\/tabled_trie.py#L224-L233"}
{"repo_name":"h2oai\/h2o-3","method_name":"H2OGenericEstimator.from_file","method_code":"def from_file(file=str):\n        \"\"\"\"\"\"\n        from h2o import lazy_import, get_frame\n        model_key = lazy_import(file)\n        model_bytes_frame = get_frame(model_key[0])\n        model = H2OGenericEstimator(model_key = model_bytes_frame)\n        model.train()\n\n        return model","method_summary":"Creates new Generic model by loading existing embedded model into library, e.g. from H2O MOJO. The imported model must be supported by H2O.","original_method_code":"def from_file(file=str):\n        \"\"\"\n        Creates new Generic model by loading existing embedded model into library, e.g. from H2O MOJO.\n        The imported model must be supported by H2O.\n        :param file: A string containing path to the file to create the model from\n        :return: H2OGenericEstimator instance representing the generic model\n        \"\"\"\n        from h2o import lazy_import, get_frame\n        model_key = lazy_import(file)\n        model_bytes_frame = get_frame(model_key[0])\n        model = H2OGenericEstimator(model_key = model_bytes_frame)\n        model.train()\n\n        return model","method_path":"https:\/\/github.com\/h2oai\/h2o-3\/blob\/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8\/h2o-py\/h2o\/estimators\/generic.py#L62-L75"}
{"repo_name":"h2oai\/h2o-3","method_name":"H2OGeneralizedLinearEstimator.getGLMRegularizationPath","method_code":"def getGLMRegularizationPath(model):\n        \"\"\"\"\"\"\n        x = h2o.api(\"GET \/3\/GetGLMRegPath\", data={\"model\": model._model_json[\"model_id\"][\"name\"]})\n        ns = x.pop(\"coefficient_names\")\n        res = {\n            \"lambdas\": x[\"lambdas\"],\n            \"explained_deviance_train\": x[\"explained_deviance_train\"],\n            \"explained_deviance_valid\": x[\"explained_deviance_valid\"],\n            \"coefficients\": [dict(zip(ns, y)) for y in x[\"coefficients\"]],\n        }\n        if \"coefficients_std\" in x:\n            res[\"coefficients_std\"] = [dict(zip(ns, y)) for y in x[\"coefficients_std\"]]\n        return res","method_summary":"Extract full regularization path explored during lambda search from glm model.","original_method_code":"def getGLMRegularizationPath(model):\n        \"\"\"\n        Extract full regularization path explored during lambda search from glm model.\n\n        :param model: source lambda search model\n        \"\"\"\n        x = h2o.api(\"GET \/3\/GetGLMRegPath\", data={\"model\": model._model_json[\"model_id\"][\"name\"]})\n        ns = x.pop(\"coefficient_names\")\n        res = {\n            \"lambdas\": x[\"lambdas\"],\n            \"explained_deviance_train\": x[\"explained_deviance_train\"],\n            \"explained_deviance_valid\": x[\"explained_deviance_valid\"],\n            \"coefficients\": [dict(zip(ns, y)) for y in x[\"coefficients\"]],\n        }\n        if \"coefficients_std\" in x:\n            res[\"coefficients_std\"] = [dict(zip(ns, y)) for y in x[\"coefficients_std\"]]\n        return res","method_path":"https:\/\/github.com\/h2oai\/h2o-3\/blob\/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8\/h2o-py\/h2o\/estimators\/glm.py#L860-L876"}
{"repo_name":"h2oai\/h2o-3","method_name":"H2OGeneralizedLinearEstimator.makeGLMModel","method_code":"def makeGLMModel(model, coefs, threshold=.5):\n        \"\"\"\"\"\"\n        model_json = h2o.api(\n            \"POST \/3\/MakeGLMModel\",\n            data={\"model\": model._model_json[\"model_id\"][\"name\"],\n                  \"names\": list(coefs.keys()),\n                  \"beta\": list(coefs.values()),\n                  \"threshold\": threshold}\n        )\n        m = H2OGeneralizedLinearEstimator()\n        m._resolve_model(model_json[\"model_id\"][\"name\"], model_json)\n        return m","method_summary":"Create a custom GLM model using the given coefficients. Needs to be passed source model trained on the dataset to extract the dataset information from.","original_method_code":"def makeGLMModel(model, coefs, threshold=.5):\n        \"\"\"\n        Create a custom GLM model using the given coefficients.\n\n        Needs to be passed source model trained on the dataset to extract the dataset information from.\n\n        :param model: source model, used for extracting dataset information\n        :param coefs: dictionary containing model coefficients\n        :param threshold: (optional, only for binomial) decision threshold used for classification\n        \"\"\"\n        model_json = h2o.api(\n            \"POST \/3\/MakeGLMModel\",\n            data={\"model\": model._model_json[\"model_id\"][\"name\"],\n                  \"names\": list(coefs.keys()),\n                  \"beta\": list(coefs.values()),\n                  \"threshold\": threshold}\n        )\n        m = H2OGeneralizedLinearEstimator()\n        m._resolve_model(model_json[\"model_id\"][\"name\"], model_json)\n        return m","method_path":"https:\/\/github.com\/h2oai\/h2o-3\/blob\/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8\/h2o-py\/h2o\/estimators\/glm.py#L879-L898"}
{"repo_name":"h2oai\/h2o-3","method_name":"H2OCluster.from_kvs","method_code":"def from_kvs(keyvals):\n        \"\"\"\"\"\"\n        obj = H2OCluster()\n        obj._retrieved_at = time.time()\n        for k, v in keyvals:\n            if k in {\"__meta\", \"_exclude_fields\", \"__schema\"}: continue\n            if k in _cloud_v3_valid_keys:\n                obj._props[k] = v\n            else:\n                raise AttributeError(\"Attribute %s cannot be set on H2OCluster (= %r)\" % (k, v))\n        return obj","method_summary":"Create H2OCluster object from a list of key-value pairs.","original_method_code":"def from_kvs(keyvals):\n        \"\"\"\n        Create H2OCluster object from a list of key-value pairs.\n\n        TODO: This method should be moved into the base H2OResponse class.\n        \"\"\"\n        obj = H2OCluster()\n        obj._retrieved_at = time.time()\n        for k, v in keyvals:\n            if k in {\"__meta\", \"_exclude_fields\", \"__schema\"}: continue\n            if k in _cloud_v3_valid_keys:\n                obj._props[k] = v\n            else:\n                raise AttributeError(\"Attribute %s cannot be set on H2OCluster (= %r)\" % (k, v))\n        return obj","method_path":"https:\/\/github.com\/h2oai\/h2o-3\/blob\/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8\/h2o-py\/h2o\/backend\/cluster.py#L34-L48"}
{"repo_name":"h2oai\/h2o-3","method_name":"H2OCluster.shutdown","method_code":"def shutdown(self, prompt=False):\n        \"\"\"\"\"\"\n        if not self.is_running(): return\n        assert_is_type(prompt, bool)\n        if prompt:\n            question = \"Are you sure you want to shutdown the H2O instance running at %s (Y\/N)? \" \\\n                       % h2o.connection().base_url\n            response = input(question)  \n        else:\n            response = \"Y\"\n        if response.lower() in {\"y\", \"yes\"}:\n            h2o.api(\"POST \/3\/Shutdown\")\n            h2o.connection().close()","method_summary":"Shut down the server. This method checks if the H2O cluster is still running, and if it does shuts it down (via a REST API call).","original_method_code":"def shutdown(self, prompt=False):\n        \"\"\"\n        Shut down the server.\n\n        This method checks if the H2O cluster is still running, and if it does shuts it down (via a REST API call).\n\n        :param prompt: A logical value indicating whether to prompt the user before shutting down the H2O server.\n        \"\"\"\n        if not self.is_running(): return\n        assert_is_type(prompt, bool)\n        if prompt:\n            question = \"Are you sure you want to shutdown the H2O instance running at %s (Y\/N)? \" \\\n                       % h2o.connection().base_url\n            response = input(question)  # works in Py2 & Py3 because redefined in h2o.utils.compatibility module\n        else:\n            response = \"Y\"\n        if response.lower() in {\"y\", \"yes\"}:\n            h2o.api(\"POST \/3\/Shutdown\")\n            h2o.connection().close()","method_path":"https:\/\/github.com\/h2oai\/h2o-3\/blob\/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8\/h2o-py\/h2o\/backend\/cluster.py#L176-L194"}
{"repo_name":"h2oai\/h2o-3","method_name":"H2OCluster.is_running","method_code":"def is_running(self):\n        \"\"\"\"\"\"\n        try:\n            if h2o.connection().local_server and not h2o.connection().local_server.is_running(): return False\n            h2o.api(\"GET \/\")\n            return True\n        except (H2OConnectionError, H2OServerError):\n            return False","method_summary":"Determine if the H2O cluster is running or not.","original_method_code":"def is_running(self):\n        \"\"\"\n        Determine if the H2O cluster is running or not.\n\n        :returns: True if the cluster is up; False otherwise\n        \"\"\"\n        try:\n            if h2o.connection().local_server and not h2o.connection().local_server.is_running(): return False\n            h2o.api(\"GET \/\")\n            return True\n        except (H2OConnectionError, H2OServerError):\n            return False","method_path":"https:\/\/github.com\/h2oai\/h2o-3\/blob\/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8\/h2o-py\/h2o\/backend\/cluster.py#L197-L208"}
{"repo_name":"h2oai\/h2o-3","method_name":"H2OCluster.show_status","method_code":"def show_status(self, detailed=False):\n        \"\"\"\"\"\"\n        if self._retrieved_at + self.REFRESH_INTERVAL < time.time():\n            \n            new_info = h2o.api(\"GET \/3\/Cloud\")\n            self._fill_from_h2ocluster(new_info)\n        ncpus = sum(node[\"num_cpus\"] for node in self.nodes)\n        allowed_cpus = sum(node[\"cpus_allowed\"] for node in self.nodes)\n        free_mem = sum(node[\"free_mem\"] for node in self.nodes)\n        unhealthy_nodes = sum(not node[\"healthy\"] for node in self.nodes)\n        status = \"locked\" if self.locked else \"accepting new members\"\n        if unhealthy_nodes == 0:\n            status += \", healthy\"\n        else:\n            status += \", %d nodes are not healthy\" % unhealthy_nodes\n        api_extensions = self.list_api_extensions()\n        H2ODisplay([\n            [\"H2O cluster uptime:\",        get_human_readable_time(self.cloud_uptime_millis)],\n            [\"H2O cluster timezone:\",      self.cloud_internal_timezone],\n            [\"H2O data parsing timezone:\", self.datafile_parser_timezone],\n            [\"H2O cluster version:\",       self.version],\n            [\"H2O cluster version age:\",   \"{} {}\".format(self.build_age, (\"!!!\" if self.build_too_old else \"\"))],\n            [\"H2O cluster name:\",          self.cloud_name],\n            [\"H2O cluster total nodes:\",   self.cloud_size],\n            [\"H2O cluster free memory:\",   get_human_readable_bytes(free_mem)],\n            [\"H2O cluster total cores:\",   str(ncpus)],\n            [\"H2O cluster allowed cores:\", str(allowed_cpus)],\n            [\"H2O cluster status:\",        status],\n            [\"H2O connection url:\",        h2o.connection().base_url],\n            [\"H2O connection proxy:\",      h2o.connection().proxy],\n            [\"H2O internal security:\",     self.internal_security_enabled],\n            [\"H2O API Extensions:\",        ', '.join(api_extensions)],\n            [\"Python version:\",            \"%d.%d.%d %s\" % tuple(sys.version_info[:4])],\n        ])\n\n        if detailed:\n            keys = [\"h2o\", \"healthy\", \"last_ping\", \"num_cpus\", \"sys_load\", \"mem_value_size\", \"free_mem\", \"pojo_mem\",\n                    \"swap_mem\", \"free_disk\", \"max_disk\", \"pid\", \"num_keys\", \"tcps_active\", \"open_fds\", \"rpcs_active\"]\n            header = [\"Nodes info:\"] + [\"Node %d\" % (i + 1) for i in range(len(self.nodes))]\n            table = [[k] for k in keys]\n            for node in self.nodes:\n                for i, k in enumerate(keys):\n                    table[i].append(node[k])\n            H2ODisplay(table=table, header=header)","method_summary":"Print current cluster status information.","original_method_code":"def show_status(self, detailed=False):\n        \"\"\"\n        Print current cluster status information.\n\n        :param detailed: if True, then also print detailed information about each node.\n        \"\"\"\n        if self._retrieved_at + self.REFRESH_INTERVAL < time.time():\n            # Info is stale, need to refresh\n            new_info = h2o.api(\"GET \/3\/Cloud\")\n            self._fill_from_h2ocluster(new_info)\n        ncpus = sum(node[\"num_cpus\"] for node in self.nodes)\n        allowed_cpus = sum(node[\"cpus_allowed\"] for node in self.nodes)\n        free_mem = sum(node[\"free_mem\"] for node in self.nodes)\n        unhealthy_nodes = sum(not node[\"healthy\"] for node in self.nodes)\n        status = \"locked\" if self.locked else \"accepting new members\"\n        if unhealthy_nodes == 0:\n            status += \", healthy\"\n        else:\n            status += \", %d nodes are not healthy\" % unhealthy_nodes\n        api_extensions = self.list_api_extensions()\n        H2ODisplay([\n            [\"H2O cluster uptime:\",        get_human_readable_time(self.cloud_uptime_millis)],\n            [\"H2O cluster timezone:\",      self.cloud_internal_timezone],\n            [\"H2O data parsing timezone:\", self.datafile_parser_timezone],\n            [\"H2O cluster version:\",       self.version],\n            [\"H2O cluster version age:\",   \"{} {}\".format(self.build_age, (\"!!!\" if self.build_too_old else \"\"))],\n            [\"H2O cluster name:\",          self.cloud_name],\n            [\"H2O cluster total nodes:\",   self.cloud_size],\n            [\"H2O cluster free memory:\",   get_human_readable_bytes(free_mem)],\n            [\"H2O cluster total cores:\",   str(ncpus)],\n            [\"H2O cluster allowed cores:\", str(allowed_cpus)],\n            [\"H2O cluster status:\",        status],\n            [\"H2O connection url:\",        h2o.connection().base_url],\n            [\"H2O connection proxy:\",      h2o.connection().proxy],\n            [\"H2O internal security:\",     self.internal_security_enabled],\n            [\"H2O API Extensions:\",        ', '.join(api_extensions)],\n            [\"Python version:\",            \"%d.%d.%d %s\" % tuple(sys.version_info[:4])],\n        ])\n\n        if detailed:\n            keys = [\"h2o\", \"healthy\", \"last_ping\", \"num_cpus\", \"sys_load\", \"mem_value_size\", \"free_mem\", \"pojo_mem\",\n                    \"swap_mem\", \"free_disk\", \"max_disk\", \"pid\", \"num_keys\", \"tcps_active\", \"open_fds\", \"rpcs_active\"]\n            header = [\"Nodes info:\"] + [\"Node %d\" % (i + 1) for i in range(len(self.nodes))]\n            table = [[k] for k in keys]\n            for node in self.nodes:\n                for i, k in enumerate(keys):\n                    table[i].append(node[k])\n            H2ODisplay(table=table, header=header)","method_path":"https:\/\/github.com\/h2oai\/h2o-3\/blob\/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8\/h2o-py\/h2o\/backend\/cluster.py#L211-L258"}
{"repo_name":"h2oai\/h2o-3","method_name":"H2OCluster.list_jobs","method_code":"def list_jobs(self):\n        \"\"\"\"\"\"\n        res = h2o.api(\"GET \/3\/Jobs\")\n        table = [[\"type\"], [\"dest\"], [\"description\"], [\"status\"]]\n        for job in res[\"jobs\"]:\n            job_dest = job[\"dest\"]\n            table[0].append(self._translate_job_type(job_dest[\"type\"]))\n            table[1].append(job_dest[\"name\"])\n            table[2].append(job[\"description\"])\n            table[3].append(job[\"status\"])\n        return table","method_summary":"List all jobs performed by the cluster.","original_method_code":"def list_jobs(self):\n        \"\"\"List all jobs performed by the cluster.\"\"\"\n        res = h2o.api(\"GET \/3\/Jobs\")\n        table = [[\"type\"], [\"dest\"], [\"description\"], [\"status\"]]\n        for job in res[\"jobs\"]:\n            job_dest = job[\"dest\"]\n            table[0].append(self._translate_job_type(job_dest[\"type\"]))\n            table[1].append(job_dest[\"name\"])\n            table[2].append(job[\"description\"])\n            table[3].append(job[\"status\"])\n        return table","method_path":"https:\/\/github.com\/h2oai\/h2o-3\/blob\/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8\/h2o-py\/h2o\/backend\/cluster.py#L266-L276"}
{"repo_name":"h2oai\/h2o-3","method_name":"H2OCluster.list_timezones","method_code":"def list_timezones(self):\n        \"\"\"\"\"\"\n        from h2o.expr import ExprNode\n        return h2o.H2OFrame._expr(expr=ExprNode(\"listTimeZones\"))._frame()","method_summary":"Return the list of all known timezones.","original_method_code":"def list_timezones(self):\n        \"\"\"Return the list of all known timezones.\"\"\"\n        from h2o.expr import ExprNode\n        return h2o.H2OFrame._expr(expr=ExprNode(\"listTimeZones\"))._frame()","method_path":"https:\/\/github.com\/h2oai\/h2o-3\/blob\/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8\/h2o-py\/h2o\/backend\/cluster.py#L303-L306"}
{"repo_name":"h2oai\/h2o-3","method_name":"H2OCluster._fill_from_h2ocluster","method_code":"def _fill_from_h2ocluster(self, other):\n        \"\"\"\"\"\"\n        self._props = other._props\n        self._retrieved_at = other._retrieved_at\n        other._props = {}\n        other._retrieved_at = None","method_summary":"Update information in this object from another H2OCluster instance.","original_method_code":"def _fill_from_h2ocluster(self, other):\n        \"\"\"\n        Update information in this object from another H2OCluster instance.\n\n        :param H2OCluster other: source of the new information for this object.\n        \"\"\"\n        self._props = other._props\n        self._retrieved_at = other._retrieved_at\n        other._props = {}\n        other._retrieved_at = None","method_path":"https:\/\/github.com\/h2oai\/h2o-3\/blob\/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8\/h2o-py\/h2o\/backend\/cluster.py#L313-L322"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"Message.schedule","method_code":"def schedule(self, schedule_time):\n        \"\"\"\"\"\"\n        if not self.properties.message_id:\n            self.properties.message_id = str(uuid.uuid4())\n        if not self.message.annotations:\n            self.message.annotations = {}\n        self.message.annotations[types.AMQPSymbol(self._x_OPT_SCHEDULED_ENQUEUE_TIME)] = schedule_time","method_summary":"Add a specific enqueue time to the message.","original_method_code":"def schedule(self, schedule_time):\n        \"\"\"Add a specific enqueue time to the message.\n\n        :param schedule_time: The scheduled time to enqueue the message.\n        :type schedule_time: ~datetime.datetime\n        \"\"\"\n        if not self.properties.message_id:\n            self.properties.message_id = str(uuid.uuid4())\n        if not self.message.annotations:\n            self.message.annotations = {}\n        self.message.annotations[types.AMQPSymbol(self._x_OPT_SCHEDULED_ENQUEUE_TIME)] = schedule_time","method_path":"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/d7306fde32f60a293a7567678692bdad31e4b667\/azure-servicebus\/azure\/servicebus\/common\/message.py#L274-L284"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"VpnSitesConfigurationOperations.download","method_code":"def download(\n            self, resource_group_name, virtual_wan_name, vpn_sites=None, output_blob_sas_url=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"\"\"\"\n        raw_result = self._download_initial(\n            resource_group_name=resource_group_name,\n            virtual_wan_name=virtual_wan_name,\n            vpn_sites=vpn_sites,\n            output_blob_sas_url=output_blob_sas_url,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Gives the sas-url to download the configurations for vpn-sites in a resource group.","original_method_code":"def download(\n            self, resource_group_name, virtual_wan_name, vpn_sites=None, output_blob_sas_url=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Gives the sas-url to download the configurations for vpn-sites in a\n        resource group.\n\n        :param resource_group_name: The resource group name.\n        :type resource_group_name: str\n        :param virtual_wan_name: The name of the VirtualWAN for which\n         configuration of all vpn-sites is needed.\n        :type virtual_wan_name: str\n        :param vpn_sites: List of resource-ids of the vpn-sites for which\n         config is to be downloaded.\n        :type vpn_sites:\n         list[~azure.mgmt.network.v2018_04_01.models.SubResource]\n        :param output_blob_sas_url: The sas-url to download the configurations\n         for vpn-sites\n        :type output_blob_sas_url: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorException<azure.mgmt.network.v2018_04_01.models.ErrorException>`\n        \"\"\"\n        raw_result = self._download_initial(\n            resource_group_name=resource_group_name,\n            virtual_wan_name=virtual_wan_name,\n            vpn_sites=vpn_sites,\n            output_blob_sas_url=output_blob_sas_url,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/d7306fde32f60a293a7567678692bdad31e4b667\/azure-mgmt-network\/azure\/mgmt\/network\/v2018_04_01\/operations\/vpn_sites_configuration_operations.py#L83-L133"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ComputeManagementClient.models","method_code":"def models(cls, api_version=DEFAULT_API_VERSION):\n        \"\"\"\"\"\"\n        if api_version == '2015-06-15':\n            from .v2015_06_15 import models\n            return models\n        elif api_version == '2016-03-30':\n            from .v2016_03_30 import models\n            return models\n        elif api_version == '2016-04-30-preview':\n            from .v2016_04_30_preview import models\n            return models\n        elif api_version == '2017-03-30':\n            from .v2017_03_30 import models\n            return models\n        elif api_version == '2017-09-01':\n            from .v2017_09_01 import models\n            return models\n        elif api_version == '2017-12-01':\n            from .v2017_12_01 import models\n            return models\n        elif api_version == '2018-04-01':\n            from .v2018_04_01 import models\n            return models\n        elif api_version == '2018-06-01':\n            from .v2018_06_01 import models\n            return models\n        elif api_version == '2018-09-30':\n            from .v2018_09_30 import models\n            return models\n        elif api_version == '2018-10-01':\n            from .v2018_10_01 import models\n            return models\n        elif api_version == '2019-03-01':\n            from .v2019_03_01 import models\n            return models\n        elif api_version == '2019-04-01':\n            from .v2019_04_01 import models\n            return models\n        raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))","method_summary":"Module depends on the API","original_method_code":"def models(cls, api_version=DEFAULT_API_VERSION):\n        \"\"\"Module depends on the API version:\n\n           * 2015-06-15: :mod:`v2015_06_15.models<azure.mgmt.compute.v2015_06_15.models>`\n           * 2016-03-30: :mod:`v2016_03_30.models<azure.mgmt.compute.v2016_03_30.models>`\n           * 2016-04-30-preview: :mod:`v2016_04_30_preview.models<azure.mgmt.compute.v2016_04_30_preview.models>`\n           * 2017-03-30: :mod:`v2017_03_30.models<azure.mgmt.compute.v2017_03_30.models>`\n           * 2017-09-01: :mod:`v2017_09_01.models<azure.mgmt.compute.v2017_09_01.models>`\n           * 2017-12-01: :mod:`v2017_12_01.models<azure.mgmt.compute.v2017_12_01.models>`\n           * 2018-04-01: :mod:`v2018_04_01.models<azure.mgmt.compute.v2018_04_01.models>`\n           * 2018-06-01: :mod:`v2018_06_01.models<azure.mgmt.compute.v2018_06_01.models>`\n           * 2018-09-30: :mod:`v2018_09_30.models<azure.mgmt.compute.v2018_09_30.models>`\n           * 2018-10-01: :mod:`v2018_10_01.models<azure.mgmt.compute.v2018_10_01.models>`\n           * 2019-03-01: :mod:`v2019_03_01.models<azure.mgmt.compute.v2019_03_01.models>`\n           * 2019-04-01: :mod:`v2019_04_01.models<azure.mgmt.compute.v2019_04_01.models>`\n        \"\"\"\n        if api_version == '2015-06-15':\n            from .v2015_06_15 import models\n            return models\n        elif api_version == '2016-03-30':\n            from .v2016_03_30 import models\n            return models\n        elif api_version == '2016-04-30-preview':\n            from .v2016_04_30_preview import models\n            return models\n        elif api_version == '2017-03-30':\n            from .v2017_03_30 import models\n            return models\n        elif api_version == '2017-09-01':\n            from .v2017_09_01 import models\n            return models\n        elif api_version == '2017-12-01':\n            from .v2017_12_01 import models\n            return models\n        elif api_version == '2018-04-01':\n            from .v2018_04_01 import models\n            return models\n        elif api_version == '2018-06-01':\n            from .v2018_06_01 import models\n            return models\n        elif api_version == '2018-09-30':\n            from .v2018_09_30 import models\n            return models\n        elif api_version == '2018-10-01':\n            from .v2018_10_01 import models\n            return models\n        elif api_version == '2019-03-01':\n            from .v2019_03_01 import models\n            return models\n        elif api_version == '2019-04-01':\n            from .v2019_04_01 import models\n            return models\n        raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))","method_path":"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/d7306fde32f60a293a7567678692bdad31e4b667\/azure-mgmt-compute\/azure\/mgmt\/compute\/compute_management_client.py#L111-L163"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ComputeManagementClient.disks","method_code":"def disks(self):\n        \"\"\"\"\"\"\n        api_version = self._get_api_version('disks')\n        if api_version == '2016-04-30-preview':\n            from .v2016_04_30_preview.operations import DisksOperations as OperationClass\n        elif api_version == '2017-03-30':\n            from .v2017_03_30.operations import DisksOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import DisksOperations as OperationClass\n        elif api_version == '2018-06-01':\n            from .v2018_06_01.operations import DisksOperations as OperationClass\n        elif api_version == '2018-09-30':\n            from .v2018_09_30.operations import DisksOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_summary":"Instance depends on the API","original_method_code":"def disks(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2016-04-30-preview: :class:`DisksOperations<azure.mgmt.compute.v2016_04_30_preview.operations.DisksOperations>`\n           * 2017-03-30: :class:`DisksOperations<azure.mgmt.compute.v2017_03_30.operations.DisksOperations>`\n           * 2018-04-01: :class:`DisksOperations<azure.mgmt.compute.v2018_04_01.operations.DisksOperations>`\n           * 2018-06-01: :class:`DisksOperations<azure.mgmt.compute.v2018_06_01.operations.DisksOperations>`\n           * 2018-09-30: :class:`DisksOperations<azure.mgmt.compute.v2018_09_30.operations.DisksOperations>`\n        \"\"\"\n        api_version = self._get_api_version('disks')\n        if api_version == '2016-04-30-preview':\n            from .v2016_04_30_preview.operations import DisksOperations as OperationClass\n        elif api_version == '2017-03-30':\n            from .v2017_03_30.operations import DisksOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import DisksOperations as OperationClass\n        elif api_version == '2018-06-01':\n            from .v2018_06_01.operations import DisksOperations as OperationClass\n        elif api_version == '2018-09-30':\n            from .v2018_09_30.operations import DisksOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_path":"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/d7306fde32f60a293a7567678692bdad31e4b667\/azure-mgmt-compute\/azure\/mgmt\/compute\/compute_management_client.py#L203-L225"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ComputeManagementClient.galleries","method_code":"def galleries(self):\n        \"\"\"\"\"\"\n        api_version = self._get_api_version('galleries')\n        if api_version == '2018-06-01':\n            from .v2018_06_01.operations import GalleriesOperations as OperationClass\n        elif api_version == '2019-03-01':\n            from .v2019_03_01.operations import GalleriesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_summary":"Instance depends on the API","original_method_code":"def galleries(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-06-01: :class:`GalleriesOperations<azure.mgmt.compute.v2018_06_01.operations.GalleriesOperations>`\n           * 2019-03-01: :class:`GalleriesOperations<azure.mgmt.compute.v2019_03_01.operations.GalleriesOperations>`\n        \"\"\"\n        api_version = self._get_api_version('galleries')\n        if api_version == '2018-06-01':\n            from .v2018_06_01.operations import GalleriesOperations as OperationClass\n        elif api_version == '2019-03-01':\n            from .v2019_03_01.operations import GalleriesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_path":"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/d7306fde32f60a293a7567678692bdad31e4b667\/azure-mgmt-compute\/azure\/mgmt\/compute\/compute_management_client.py#L228-L241"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ComputeManagementClient.gallery_image_versions","method_code":"def gallery_image_versions(self):\n        \"\"\"\"\"\"\n        api_version = self._get_api_version('gallery_image_versions')\n        if api_version == '2018-06-01':\n            from .v2018_06_01.operations import GalleryImageVersionsOperations as OperationClass\n        elif api_version == '2019-03-01':\n            from .v2019_03_01.operations import GalleryImageVersionsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_summary":"Instance depends on the API","original_method_code":"def gallery_image_versions(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-06-01: :class:`GalleryImageVersionsOperations<azure.mgmt.compute.v2018_06_01.operations.GalleryImageVersionsOperations>`\n           * 2019-03-01: :class:`GalleryImageVersionsOperations<azure.mgmt.compute.v2019_03_01.operations.GalleryImageVersionsOperations>`\n        \"\"\"\n        api_version = self._get_api_version('gallery_image_versions')\n        if api_version == '2018-06-01':\n            from .v2018_06_01.operations import GalleryImageVersionsOperations as OperationClass\n        elif api_version == '2019-03-01':\n            from .v2019_03_01.operations import GalleryImageVersionsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_path":"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/d7306fde32f60a293a7567678692bdad31e4b667\/azure-mgmt-compute\/azure\/mgmt\/compute\/compute_management_client.py#L244-L257"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ComputeManagementClient.gallery_images","method_code":"def gallery_images(self):\n        \"\"\"\"\"\"\n        api_version = self._get_api_version('gallery_images')\n        if api_version == '2018-06-01':\n            from .v2018_06_01.operations import GalleryImagesOperations as OperationClass\n        elif api_version == '2019-03-01':\n            from .v2019_03_01.operations import GalleryImagesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_summary":"Instance depends on the API","original_method_code":"def gallery_images(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-06-01: :class:`GalleryImagesOperations<azure.mgmt.compute.v2018_06_01.operations.GalleryImagesOperations>`\n           * 2019-03-01: :class:`GalleryImagesOperations<azure.mgmt.compute.v2019_03_01.operations.GalleryImagesOperations>`\n        \"\"\"\n        api_version = self._get_api_version('gallery_images')\n        if api_version == '2018-06-01':\n            from .v2018_06_01.operations import GalleryImagesOperations as OperationClass\n        elif api_version == '2019-03-01':\n            from .v2019_03_01.operations import GalleryImagesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_path":"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/d7306fde32f60a293a7567678692bdad31e4b667\/azure-mgmt-compute\/azure\/mgmt\/compute\/compute_management_client.py#L260-L273"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ComputeManagementClient.images","method_code":"def images(self):\n        \"\"\"\"\"\"\n        api_version = self._get_api_version('images')\n        if api_version == '2016-04-30-preview':\n            from .v2016_04_30_preview.operations import ImagesOperations as OperationClass\n        elif api_version == '2017-03-30':\n            from .v2017_03_30.operations import ImagesOperations as OperationClass\n        elif api_version == '2017-12-01':\n            from .v2017_12_01.operations import ImagesOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import ImagesOperations as OperationClass\n        elif api_version == '2018-06-01':\n            from .v2018_06_01.operations import ImagesOperations as OperationClass\n        elif api_version == '2018-10-01':\n            from .v2018_10_01.operations import ImagesOperations as OperationClass\n        elif api_version == '2019-03-01':\n            from .v2019_03_01.operations import ImagesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_summary":"Instance depends on the API","original_method_code":"def images(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2016-04-30-preview: :class:`ImagesOperations<azure.mgmt.compute.v2016_04_30_preview.operations.ImagesOperations>`\n           * 2017-03-30: :class:`ImagesOperations<azure.mgmt.compute.v2017_03_30.operations.ImagesOperations>`\n           * 2017-12-01: :class:`ImagesOperations<azure.mgmt.compute.v2017_12_01.operations.ImagesOperations>`\n           * 2018-04-01: :class:`ImagesOperations<azure.mgmt.compute.v2018_04_01.operations.ImagesOperations>`\n           * 2018-06-01: :class:`ImagesOperations<azure.mgmt.compute.v2018_06_01.operations.ImagesOperations>`\n           * 2018-10-01: :class:`ImagesOperations<azure.mgmt.compute.v2018_10_01.operations.ImagesOperations>`\n           * 2019-03-01: :class:`ImagesOperations<azure.mgmt.compute.v2019_03_01.operations.ImagesOperations>`\n        \"\"\"\n        api_version = self._get_api_version('images')\n        if api_version == '2016-04-30-preview':\n            from .v2016_04_30_preview.operations import ImagesOperations as OperationClass\n        elif api_version == '2017-03-30':\n            from .v2017_03_30.operations import ImagesOperations as OperationClass\n        elif api_version == '2017-12-01':\n            from .v2017_12_01.operations import ImagesOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import ImagesOperations as OperationClass\n        elif api_version == '2018-06-01':\n            from .v2018_06_01.operations import ImagesOperations as OperationClass\n        elif api_version == '2018-10-01':\n            from .v2018_10_01.operations import ImagesOperations as OperationClass\n        elif api_version == '2019-03-01':\n            from .v2019_03_01.operations import ImagesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_path":"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/d7306fde32f60a293a7567678692bdad31e4b667\/azure-mgmt-compute\/azure\/mgmt\/compute\/compute_management_client.py#L276-L304"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ComputeManagementClient.log_analytics","method_code":"def log_analytics(self):\n        \"\"\"\"\"\"\n        api_version = self._get_api_version('log_analytics')\n        if api_version == '2017-12-01':\n            from .v2017_12_01.operations import LogAnalyticsOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import LogAnalyticsOperations as OperationClass\n        elif api_version == '2018-06-01':\n            from .v2018_06_01.operations import LogAnalyticsOperations as OperationClass\n        elif api_version == '2018-10-01':\n            from .v2018_10_01.operations import LogAnalyticsOperations as OperationClass\n        elif api_version == '2019-03-01':\n            from .v2019_03_01.operations import LogAnalyticsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_summary":"Instance depends on the API","original_method_code":"def log_analytics(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-12-01: :class:`LogAnalyticsOperations<azure.mgmt.compute.v2017_12_01.operations.LogAnalyticsOperations>`\n           * 2018-04-01: :class:`LogAnalyticsOperations<azure.mgmt.compute.v2018_04_01.operations.LogAnalyticsOperations>`\n           * 2018-06-01: :class:`LogAnalyticsOperations<azure.mgmt.compute.v2018_06_01.operations.LogAnalyticsOperations>`\n           * 2018-10-01: :class:`LogAnalyticsOperations<azure.mgmt.compute.v2018_10_01.operations.LogAnalyticsOperations>`\n           * 2019-03-01: :class:`LogAnalyticsOperations<azure.mgmt.compute.v2019_03_01.operations.LogAnalyticsOperations>`\n        \"\"\"\n        api_version = self._get_api_version('log_analytics')\n        if api_version == '2017-12-01':\n            from .v2017_12_01.operations import LogAnalyticsOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import LogAnalyticsOperations as OperationClass\n        elif api_version == '2018-06-01':\n            from .v2018_06_01.operations import LogAnalyticsOperations as OperationClass\n        elif api_version == '2018-10-01':\n            from .v2018_10_01.operations import LogAnalyticsOperations as OperationClass\n        elif api_version == '2019-03-01':\n            from .v2019_03_01.operations import LogAnalyticsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_path":"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/d7306fde32f60a293a7567678692bdad31e4b667\/azure-mgmt-compute\/azure\/mgmt\/compute\/compute_management_client.py#L307-L329"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ComputeManagementClient.resource_skus","method_code":"def resource_skus(self):\n        \"\"\"\"\"\"\n        api_version = self._get_api_version('resource_skus')\n        if api_version == '2017-03-30':\n            from .v2017_03_30.operations import ResourceSkusOperations as OperationClass\n        elif api_version == '2017-09-01':\n            from .v2017_09_01.operations import ResourceSkusOperations as OperationClass\n        elif api_version == '2019-04-01':\n            from .v2019_04_01.operations import ResourceSkusOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_summary":"Instance depends on the API","original_method_code":"def resource_skus(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-03-30: :class:`ResourceSkusOperations<azure.mgmt.compute.v2017_03_30.operations.ResourceSkusOperations>`\n           * 2017-09-01: :class:`ResourceSkusOperations<azure.mgmt.compute.v2017_09_01.operations.ResourceSkusOperations>`\n           * 2019-04-01: :class:`ResourceSkusOperations<azure.mgmt.compute.v2019_04_01.operations.ResourceSkusOperations>`\n        \"\"\"\n        api_version = self._get_api_version('resource_skus')\n        if api_version == '2017-03-30':\n            from .v2017_03_30.operations import ResourceSkusOperations as OperationClass\n        elif api_version == '2017-09-01':\n            from .v2017_09_01.operations import ResourceSkusOperations as OperationClass\n        elif api_version == '2019-04-01':\n            from .v2019_04_01.operations import ResourceSkusOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","method_path":"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/d7306fde32f60a293a7567678692bdad31e4b667\/azure-mgmt-compute\/azure\/mgmt\/compute\/compute_management_client.py#L379-L395"}
{"repo_name":"streamlink\/streamlink","method_name":"Plugin.streams","method_code":"def streams(self, stream_types=None, sorting_excludes=None):\n        \"\"\"\"\"\"\n\n        try:\n            ostreams = self._get_streams()\n            if isinstance(ostreams, dict):\n                ostreams = ostreams.items()\n\n            \n            if ostreams:\n                ostreams = list(ostreams)\n        except NoStreamsError:\n            return {}\n        except (IOError, OSError, ValueError) as err:\n            raise PluginError(err)\n\n        if not ostreams:\n            return {}\n\n        if stream_types is None:\n            stream_types = self.default_stream_types(ostreams)\n\n        \n        sorted_streams = sorted(iterate_streams(ostreams),\n                                key=partial(stream_type_priority,\n                                            stream_types))\n\n        streams = {}\n        for name, stream in sorted_streams:\n            stream_type = type(stream).shortname()\n\n            \n            if \"*\" not in stream_types and stream_type not in stream_types:\n                continue\n\n            \n            if name.endswith(\"_alt\"):\n                name = name[:-len(\"_alt\")]\n\n            existing = streams.get(name)\n            if existing:\n                existing_stream_type = type(existing).shortname()\n                if existing_stream_type != stream_type:\n                    name = \"{0}_{1}\".format(name, stream_type)\n\n                if name in streams:\n                    name = \"{0}_alt\".format(name)\n                    num_alts = len(list(filter(lambda n: n.startswith(name), streams.keys())))\n\n                    \n                    if num_alts >= 2:\n                        continue\n                    elif num_alts > 0:\n                        name = \"{0}{1}\".format(name, num_alts + 1)\n\n            \n            match = re.match(\"([A-z0-9_+]+)\", name)\n            if match:\n                name = match.group(1)\n            else:\n                self.logger.debug(\"The stream '{0}' has been ignored \"\n                                  \"since it is badly named.\", name)\n                continue\n\n            \n            streams[name.lower()] = stream\n\n        \n        def stream_weight_only(s):\n            return (self.stream_weight(s)[0] or\n                    (len(streams) == 1 and 1))\n\n        stream_names = filter(stream_weight_only, streams.keys())\n        sorted_streams = sorted(stream_names, key=stream_weight_only)\n        unfiltered_sorted_streams = sorted_streams\n\n        if isinstance(sorting_excludes, list):\n            for expr in sorting_excludes:\n                filter_func = stream_sorting_filter(expr, self.stream_weight)\n                sorted_streams = list(filter(filter_func, sorted_streams))\n        elif callable(sorting_excludes):\n            sorted_streams = list(filter(sorting_excludes, sorted_streams))\n\n        final_sorted_streams = OrderedDict()\n\n        for stream_name in sorted(streams, key=stream_weight_only):\n            final_sorted_streams[stream_name] = streams[stream_name]\n\n        if len(sorted_streams) > 0:\n            best = sorted_streams[-1]\n            worst = sorted_streams[0]\n            final_sorted_streams[\"worst\"] = streams[worst]\n            final_sorted_streams[\"best\"] = streams[best]\n        elif len(unfiltered_sorted_streams) > 0:\n            best = unfiltered_sorted_streams[-1]\n            worst = unfiltered_sorted_streams[0]\n            final_sorted_streams[\"worst-unfiltered\"] = streams[worst]\n            final_sorted_streams[\"best-unfiltered\"] = streams[best]\n\n        return final_sorted_streams","method_summary":"Attempts to extract available streams.","original_method_code":"def streams(self, stream_types=None, sorting_excludes=None):\n        \"\"\"Attempts to extract available streams.\n\n        Returns a :class:`dict` containing the streams, where the key is\n        the name of the stream, most commonly the quality and the value\n        is a :class:`Stream` object.\n\n        The result can contain the synonyms **best** and **worst** which\n        points to the streams which are likely to be of highest and\n        lowest quality respectively.\n\n        If multiple streams with the same name are found, the order of\n        streams specified in *stream_types* will determine which stream\n        gets to keep the name while the rest will be renamed to\n        \"<name>_<stream type>\".\n\n        The synonyms can be fine tuned with the *sorting_excludes*\n        parameter. This can be either of these types:\n\n            - A list of filter expressions in the format\n              *[operator]<value>*. For example the filter \">480p\" will\n              exclude streams ranked higher than \"480p\" from the list\n              used in the synonyms ranking. Valid operators are >, >=, <\n              and <=. If no operator is specified then equality will be\n              tested.\n\n            - A function that is passed to filter() with a list of\n              stream names as input.\n\n\n        :param stream_types: A list of stream types to return.\n        :param sorting_excludes: Specify which streams to exclude from\n                                 the best\/worst synonyms.\n\n        \"\"\"\n\n        try:\n            ostreams = self._get_streams()\n            if isinstance(ostreams, dict):\n                ostreams = ostreams.items()\n\n            # Flatten the iterator to a list so we can reuse it.\n            if ostreams:\n                ostreams = list(ostreams)\n        except NoStreamsError:\n            return {}\n        except (IOError, OSError, ValueError) as err:\n            raise PluginError(err)\n\n        if not ostreams:\n            return {}\n\n        if stream_types is None:\n            stream_types = self.default_stream_types(ostreams)\n\n        # Add streams depending on stream type and priorities\n        sorted_streams = sorted(iterate_streams(ostreams),\n                                key=partial(stream_type_priority,\n                                            stream_types))\n\n        streams = {}\n        for name, stream in sorted_streams:\n            stream_type = type(stream).shortname()\n\n            # Use * as wildcard to match other stream types\n            if \"*\" not in stream_types and stream_type not in stream_types:\n                continue\n\n            # drop _alt from any stream names\n            if name.endswith(\"_alt\"):\n                name = name[:-len(\"_alt\")]\n\n            existing = streams.get(name)\n            if existing:\n                existing_stream_type = type(existing).shortname()\n                if existing_stream_type != stream_type:\n                    name = \"{0}_{1}\".format(name, stream_type)\n\n                if name in streams:\n                    name = \"{0}_alt\".format(name)\n                    num_alts = len(list(filter(lambda n: n.startswith(name), streams.keys())))\n\n                    # We shouldn't need more than 2 alt streams\n                    if num_alts >= 2:\n                        continue\n                    elif num_alts > 0:\n                        name = \"{0}{1}\".format(name, num_alts + 1)\n\n            # Validate stream name and discard the stream if it's bad.\n            match = re.match(\"([A-z0-9_+]+)\", name)\n            if match:\n                name = match.group(1)\n            else:\n                self.logger.debug(\"The stream '{0}' has been ignored \"\n                                  \"since it is badly named.\", name)\n                continue\n\n            # Force lowercase name and replace space with underscore.\n            streams[name.lower()] = stream\n\n        # Create the best\/worst synonmys\n        def stream_weight_only(s):\n            return (self.stream_weight(s)[0] or\n                    (len(streams) == 1 and 1))\n\n        stream_names = filter(stream_weight_only, streams.keys())\n        sorted_streams = sorted(stream_names, key=stream_weight_only)\n        unfiltered_sorted_streams = sorted_streams\n\n        if isinstance(sorting_excludes, list):\n            for expr in sorting_excludes:\n                filter_func = stream_sorting_filter(expr, self.stream_weight)\n                sorted_streams = list(filter(filter_func, sorted_streams))\n        elif callable(sorting_excludes):\n            sorted_streams = list(filter(sorting_excludes, sorted_streams))\n\n        final_sorted_streams = OrderedDict()\n\n        for stream_name in sorted(streams, key=stream_weight_only):\n            final_sorted_streams[stream_name] = streams[stream_name]\n\n        if len(sorted_streams) > 0:\n            best = sorted_streams[-1]\n            worst = sorted_streams[0]\n            final_sorted_streams[\"worst\"] = streams[worst]\n            final_sorted_streams[\"best\"] = streams[best]\n        elif len(unfiltered_sorted_streams) > 0:\n            best = unfiltered_sorted_streams[-1]\n            worst = unfiltered_sorted_streams[0]\n            final_sorted_streams[\"worst-unfiltered\"] = streams[worst]\n            final_sorted_streams[\"best-unfiltered\"] = streams[best]\n\n        return final_sorted_streams","method_path":"https:\/\/github.com\/streamlink\/streamlink\/blob\/c8ed1daff14ac03195870238b9b900c1109dd5c1\/src\/streamlink\/plugin\/plugin.py#L280-L412"}
{"repo_name":"streamlink\/streamlink","method_name":"Plugin.save_cookies","method_code":"def save_cookies(self, cookie_filter=None, default_expires=60 * 60 * 24 * 7):\n        \"\"\"\"\"\"\n        if not self.session or not self.cache:\n            raise RuntimeError(\"Cannot cache cookies in unbound plugin\")\n\n        cookie_filter = cookie_filter or (lambda c: True)\n        saved = []\n\n        for cookie in filter(cookie_filter, self.session.http.cookies):\n            cookie_dict = {}\n            for attr in (\"version\", \"name\", \"value\", \"port\", \"domain\", \"path\", \"secure\", \"expires\", \"discard\",\n                         \"comment\", \"comment_url\", \"rfc2109\"):\n                cookie_dict[attr] = getattr(cookie, attr, None)\n            cookie_dict[\"rest\"] = getattr(cookie, \"rest\", getattr(cookie, \"_rest\", None))\n\n            expires = default_expires\n            if cookie_dict['expires']:\n                expires = int(cookie_dict['expires'] - time.time())\n            key = \"__cookie:{0}:{1}:{2}:{3}\".format(cookie.name,\n                                                    cookie.domain,\n                                                    cookie.port_specified and cookie.port or \"80\",\n                                                    cookie.path_specified and cookie.path or \"*\")\n            self.cache.set(key, cookie_dict, expires)\n            saved.append(cookie.name)\n\n        if saved:\n            self.logger.debug(\"Saved cookies: {0}\".format(\", \".join(saved)))\n        return saved","method_summary":"Store the cookies from ``http`` in the plugin cache until they expire. The cookies can be filtered by supplying a filter method. eg. ``lambda","original_method_code":"def save_cookies(self, cookie_filter=None, default_expires=60 * 60 * 24 * 7):\n        \"\"\"\n        Store the cookies from ``http`` in the plugin cache until they expire. The cookies can be filtered\n        by supplying a filter method. eg. ``lambda c: \"auth\" in c.name``. If no expiry date is given in the\n        cookie then the ``default_expires`` value will be used.\n\n        :param cookie_filter: a function to filter the cookies\n        :type cookie_filter: function\n        :param default_expires: time (in seconds) until cookies with no expiry will expire\n        :type default_expires: int\n        :return: list of the saved cookie names\n        \"\"\"\n        if not self.session or not self.cache:\n            raise RuntimeError(\"Cannot cache cookies in unbound plugin\")\n\n        cookie_filter = cookie_filter or (lambda c: True)\n        saved = []\n\n        for cookie in filter(cookie_filter, self.session.http.cookies):\n            cookie_dict = {}\n            for attr in (\"version\", \"name\", \"value\", \"port\", \"domain\", \"path\", \"secure\", \"expires\", \"discard\",\n                         \"comment\", \"comment_url\", \"rfc2109\"):\n                cookie_dict[attr] = getattr(cookie, attr, None)\n            cookie_dict[\"rest\"] = getattr(cookie, \"rest\", getattr(cookie, \"_rest\", None))\n\n            expires = default_expires\n            if cookie_dict['expires']:\n                expires = int(cookie_dict['expires'] - time.time())\n            key = \"__cookie:{0}:{1}:{2}:{3}\".format(cookie.name,\n                                                    cookie.domain,\n                                                    cookie.port_specified and cookie.port or \"80\",\n                                                    cookie.path_specified and cookie.path or \"*\")\n            self.cache.set(key, cookie_dict, expires)\n            saved.append(cookie.name)\n\n        if saved:\n            self.logger.debug(\"Saved cookies: {0}\".format(\", \".join(saved)))\n        return saved","method_path":"https:\/\/github.com\/streamlink\/streamlink\/blob\/c8ed1daff14ac03195870238b9b900c1109dd5c1\/src\/streamlink\/plugin\/plugin.py#L426-L463"}
{"repo_name":"streamlink\/streamlink","method_name":"Plugin.load_cookies","method_code":"def load_cookies(self):\n        \"\"\"\"\"\"\n        if not self.session or not self.cache:\n            raise RuntimeError(\"Cannot loaded cached cookies in unbound plugin\")\n\n        restored = []\n\n        for key, value in self.cache.get_all().items():\n            if key.startswith(\"__cookie\"):\n                cookie = requests.cookies.create_cookie(**value)\n                self.session.http.cookies.set_cookie(cookie)\n                restored.append(cookie.name)\n\n        if restored:\n            self.logger.debug(\"Restored cookies: {0}\".format(\", \".join(restored)))\n        return restored","method_summary":"Load any stored cookies for the plugin that have not expired.","original_method_code":"def load_cookies(self):\n        \"\"\"\n        Load any stored cookies for the plugin that have not expired.\n\n        :return: list of the restored cookie names\n        \"\"\"\n        if not self.session or not self.cache:\n            raise RuntimeError(\"Cannot loaded cached cookies in unbound plugin\")\n\n        restored = []\n\n        for key, value in self.cache.get_all().items():\n            if key.startswith(\"__cookie\"):\n                cookie = requests.cookies.create_cookie(**value)\n                self.session.http.cookies.set_cookie(cookie)\n                restored.append(cookie.name)\n\n        if restored:\n            self.logger.debug(\"Restored cookies: {0}\".format(\", \".join(restored)))\n        return restored","method_path":"https:\/\/github.com\/streamlink\/streamlink\/blob\/c8ed1daff14ac03195870238b9b900c1109dd5c1\/src\/streamlink\/plugin\/plugin.py#L465-L484"}
{"repo_name":"streamlink\/streamlink","method_name":"Plugin.clear_cookies","method_code":"def clear_cookies(self, cookie_filter=None):\n        \"\"\"\"\"\"\n        if not self.session or not self.cache:\n            raise RuntimeError(\"Cannot loaded cached cookies in unbound plugin\")\n\n        cookie_filter = cookie_filter or (lambda c: True)\n        removed = []\n\n        for key, value in sorted(self.cache.get_all().items(), key=operator.itemgetter(0), reverse=True):\n            if key.startswith(\"__cookie\"):\n                cookie = requests.cookies.create_cookie(**value)\n                if cookie_filter(cookie):\n                    del self.session.http.cookies[cookie.name]\n                    self.cache.set(key, None, 0)\n                    removed.append(key)\n\n        return removed","method_summary":"Removes all of the saved cookies for this Plugin. To filter the cookies that are deleted specify the ``cookie_filter`` argument (see :func:`save_cookies`).","original_method_code":"def clear_cookies(self, cookie_filter=None):\n        \"\"\"\n        Removes all of the saved cookies for this Plugin. To filter the cookies that are deleted\n        specify the ``cookie_filter`` argument (see :func:`save_cookies`).\n\n        :param cookie_filter: a function to filter the cookies\n        :type cookie_filter: function\n        :return: list of the removed cookie names\n        \"\"\"\n        if not self.session or not self.cache:\n            raise RuntimeError(\"Cannot loaded cached cookies in unbound plugin\")\n\n        cookie_filter = cookie_filter or (lambda c: True)\n        removed = []\n\n        for key, value in sorted(self.cache.get_all().items(), key=operator.itemgetter(0), reverse=True):\n            if key.startswith(\"__cookie\"):\n                cookie = requests.cookies.create_cookie(**value)\n                if cookie_filter(cookie):\n                    del self.session.http.cookies[cookie.name]\n                    self.cache.set(key, None, 0)\n                    removed.append(key)\n\n        return removed","method_path":"https:\/\/github.com\/streamlink\/streamlink\/blob\/c8ed1daff14ac03195870238b9b900c1109dd5c1\/src\/streamlink\/plugin\/plugin.py#L486-L509"}
{"repo_name":"streamlink\/streamlink","method_name":"shlex_quote","method_code":"def shlex_quote(s):\n    \"\"\"\"\"\"\n\n    if is_py3:  \n        return quote(s)\n\n    if not s:\n        return \"''\"\n    if _find_unsafe(s) is None:\n        return s\n\n    \n    \n    return \"'\" + s.replace(\"'\", \"'\\\"'\\\"'\") + \"'\"","method_summary":"Return a shell-escaped version of the string *s*. Backported from Python 3.3 standard library module shlex.","original_method_code":"def shlex_quote(s):\n    \"\"\"Return a shell-escaped version of the string *s*.\n\n    Backported from Python 3.3 standard library module shlex.\n    \"\"\"\n\n    if is_py3:  # use the latest version instead of backporting if it's available\n        return quote(s)\n\n    if not s:\n        return \"''\"\n    if _find_unsafe(s) is None:\n        return s\n\n    # use single quotes, and put single quotes into double quotes\n    # the string $'b is then quoted as '$'\"'\"'b'\n    return \"'\" + s.replace(\"'\", \"'\\\"'\\\"'\") + \"'\"","method_path":"https:\/\/github.com\/streamlink\/streamlink\/blob\/c8ed1daff14ac03195870238b9b900c1109dd5c1\/src\/streamlink_cli\/compat.py#L27-L43"}
{"repo_name":"streamlink\/streamlink","method_name":"get_cut_prefix","method_code":"def get_cut_prefix(value, max_len):\n    \"\"\"\"\"\"\n    should_convert = isinstance(value, bytes)\n    if should_convert:\n        value = value.decode(\"utf8\", \"ignore\")\n    for i in range(len(value)):\n        if terminal_width(value[i:]) <= max_len:\n            break\n    return value[i:].encode(\"utf8\", \"ignore\") if should_convert else value[i:]","method_summary":"Drops Characters by unicode not by bytes.","original_method_code":"def get_cut_prefix(value, max_len):\n    \"\"\"Drops Characters by unicode not by bytes.\"\"\"\n    should_convert = isinstance(value, bytes)\n    if should_convert:\n        value = value.decode(\"utf8\", \"ignore\")\n    for i in range(len(value)):\n        if terminal_width(value[i:]) <= max_len:\n            break\n    return value[i:].encode(\"utf8\", \"ignore\") if should_convert else value[i:]","method_path":"https:\/\/github.com\/streamlink\/streamlink\/blob\/c8ed1daff14ac03195870238b9b900c1109dd5c1\/src\/streamlink_cli\/utils\/progress.py#L46-L54"}
{"repo_name":"streamlink\/streamlink","method_name":"print_inplace","method_code":"def print_inplace(msg):\n    \"\"\"\"\"\"\n    term_width = get_terminal_size().columns\n    spacing = term_width - terminal_width(msg)\n\n    \n    if is_win32:\n        spacing -= 1\n\n    sys.stderr.write(\"\\r{0}\".format(msg))\n    sys.stderr.write(\" \" * max(0, spacing))\n    sys.stderr.flush()","method_summary":"Clears out the previous line and prints a new one.","original_method_code":"def print_inplace(msg):\n    \"\"\"Clears out the previous line and prints a new one.\"\"\"\n    term_width = get_terminal_size().columns\n    spacing = term_width - terminal_width(msg)\n\n    # On windows we need one less space or we overflow the line for some reason.\n    if is_win32:\n        spacing -= 1\n\n    sys.stderr.write(\"\\r{0}\".format(msg))\n    sys.stderr.write(\" \" * max(0, spacing))\n    sys.stderr.flush()","method_path":"https:\/\/github.com\/streamlink\/streamlink\/blob\/c8ed1daff14ac03195870238b9b900c1109dd5c1\/src\/streamlink_cli\/utils\/progress.py#L57-L68"}
{"repo_name":"streamlink\/streamlink","method_name":"format_filesize","method_code":"def format_filesize(size):\n    \"\"\"\"\"\"\n    for suffix in (\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"):\n        if size < 1024.0:\n            if suffix in (\"GB\", \"TB\"):\n                return \"{0:3.2f} {1}\".format(size, suffix)\n            else:\n                return \"{0:3.1f} {1}\".format(size, suffix)\n\n        size \/= 1024.0","method_summary":"Formats the file size into a human readable format.","original_method_code":"def format_filesize(size):\n    \"\"\"Formats the file size into a human readable format.\"\"\"\n    for suffix in (\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"):\n        if size < 1024.0:\n            if suffix in (\"GB\", \"TB\"):\n                return \"{0:3.2f} {1}\".format(size, suffix)\n            else:\n                return \"{0:3.1f} {1}\".format(size, suffix)\n\n        size \/= 1024.0","method_path":"https:\/\/github.com\/streamlink\/streamlink\/blob\/c8ed1daff14ac03195870238b9b900c1109dd5c1\/src\/streamlink_cli\/utils\/progress.py#L71-L80"}
{"repo_name":"streamlink\/streamlink","method_name":"format_time","method_code":"def format_time(elapsed):\n    \"\"\"\"\"\"\n    hours = int(elapsed \/ (60 * 60))\n    minutes = int((elapsed % (60 * 60)) \/ 60)\n    seconds = int(elapsed % 60)\n\n    rval = \"\"\n    if hours:\n        rval += \"{0}h\".format(hours)\n\n    if elapsed > 60:\n        rval += \"{0}m\".format(minutes)\n\n    rval += \"{0}s\".format(seconds)\n    return rval","method_summary":"Formats elapsed seconds into a human readable format.","original_method_code":"def format_time(elapsed):\n    \"\"\"Formats elapsed seconds into a human readable format.\"\"\"\n    hours = int(elapsed \/ (60 * 60))\n    minutes = int((elapsed % (60 * 60)) \/ 60)\n    seconds = int(elapsed % 60)\n\n    rval = \"\"\n    if hours:\n        rval += \"{0}h\".format(hours)\n\n    if elapsed > 60:\n        rval += \"{0}m\".format(minutes)\n\n    rval += \"{0}s\".format(seconds)\n    return rval","method_path":"https:\/\/github.com\/streamlink\/streamlink\/blob\/c8ed1daff14ac03195870238b9b900c1109dd5c1\/src\/streamlink_cli\/utils\/progress.py#L83-L97"}
{"repo_name":"streamlink\/streamlink","method_name":"create_status_line","method_code":"def create_status_line(**params):\n    \"\"\"\"\"\"\n    max_size = get_terminal_size().columns - 1\n\n    for fmt in PROGRESS_FORMATS:\n        status = fmt.format(**params)\n\n        if len(status) <= max_size:\n            break\n\n    return status","method_summary":"Creates a status line with appropriate size.","original_method_code":"def create_status_line(**params):\n    \"\"\"Creates a status line with appropriate size.\"\"\"\n    max_size = get_terminal_size().columns - 1\n\n    for fmt in PROGRESS_FORMATS:\n        status = fmt.format(**params)\n\n        if len(status) <= max_size:\n            break\n\n    return status","method_path":"https:\/\/github.com\/streamlink\/streamlink\/blob\/c8ed1daff14ac03195870238b9b900c1109dd5c1\/src\/streamlink_cli\/utils\/progress.py#L100-L110"}
{"repo_name":"vaexio\/vaex","method_name":"polychrome","method_code":"def polychrome(I, colors, vmin=None, vmax=None, axis=-1):\n    \"\"\"\"\"\"\n    axes_length = len(I.shape)\n    allaxes = list(range(axes_length))\n    otheraxes = list(allaxes)\n    otheraxes.remove((axis + axes_length) % axes_length)\n    otheraxes = tuple(otheraxes)\n\n    if vmin is None:\n        vmin = np.nanmin(I, axis=otheraxes)\n    if vmax is None:\n        vmax = np.nanmax(I, axis=otheraxes)\n    normalized = (I - vmin) \/ (vmax - vmin)\n    return np.clip(normalized, 0, 1).dot(colors)","method_summary":"Similar to monochrome, but now do it for multiple colors","original_method_code":"def polychrome(I, colors, vmin=None, vmax=None, axis=-1):\n    \"\"\"Similar to monochrome, but now do it for multiple colors\n\n    Example\n    >>> I = np.arange(32.).reshape(4,4,2)\n    >>> colors = [(0, 0, 1), (0, 1, 0)] # red and green\n    >>> rgb = vx.image.polychrome(I, colors) # shape is (4,4,3)\n\n    :param I: ndarray of any shape (3d will result in a 2d image)\n    :param colors: sequence of [(r,g,b), ...] values\n    :param vmin: normalization minimum for I, or np.nanmin(I) when None\n    :param vmax: normalization maximum for I, or np.nanmax(I) when None\n    :param axis: axis which to sum over, by default the last\n    :return:\n    \"\"\"\n    axes_length = len(I.shape)\n    allaxes = list(range(axes_length))\n    otheraxes = list(allaxes)\n    otheraxes.remove((axis + axes_length) % axes_length)\n    otheraxes = tuple(otheraxes)\n\n    if vmin is None:\n        vmin = np.nanmin(I, axis=otheraxes)\n    if vmax is None:\n        vmax = np.nanmax(I, axis=otheraxes)\n    normalized = (I - vmin) \/ (vmax - vmin)\n    return np.clip(normalized, 0, 1).dot(colors)","method_path":"https:\/\/github.com\/vaexio\/vaex\/blob\/a45b672f8287afca2ada8e36b74b604b9b28dd85\/packages\/vaex-core\/vaex\/image.py#L145-L171"}
{"repo_name":"vaexio\/vaex","method_name":"parallelize","method_code":"def parallelize(cores=None, fork=True, flatten=False, info=False, infoclass=InfoThreadProgressBar, init=None, *args, **kwargs):\n\t\"\"\"\"\"\"\n\tif cores == None:\n\t\tcores = multiprocessing.cpu_count()\n\tdef wrapper(f):\n\t\tdef execute(*multiargs):\n\t\t\tresults = []\n\t\t\tlen(list(zip(*multiargs)))\n\t\t\tN = len(multiargs[0])\n\t\t\tif info:\n\t\t\t\tprint(\"running %i jobs on %i cores\" % (N, cores))\n\t\t\ttaskQueue = queue.Queue(len(multiargs[0]))\n\t\t\t\n\t\t\t\n\t\t\tfor tasknr, _args in enumerate(zip(*multiargs)):\n\t\t\t\ttaskQueue.put((tasknr, list(_args)))\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\texecutions = [Execution(taskQueue, fork, f, init, corenr, args, kwargs) for corenr in range(cores)]\n\t\t\tif info:\n\t\t\t\tinfoobj = infoclass(len(multiargs[0]), executions)\n\t\t\t\tinfoobj.start()\n\t\t\tfor i, execution in enumerate(executions):\n\t\t\t\texecution.setName(\"T-%d\" % i)\n\t\t\t\texecution.start()\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\terror = False\n\t\t\tfor execution in executions:\n\t\t\t\tlog(\"joining:\",execution.getName())\n\t\t\t\ttry:\n\t\t\t\t\texecution.join()\n\t\t\t\texcept BaseException:\n\t\t\t\t\terror = True\n\t\t\t\tresults.extend(execution.results)\n\t\t\t\tif execution.error:\n\t\t\t\t\terror = True \n\t\t\tif info:\n\t\t\t\tinfoobj.join()\n\t\t\tif error:\n\t\t\t\tprint(\"error\", file=sys.stderr)\n\t\t\t\tresults = None\n\t\t\t\traise Exception(\"error in one or more of the executors\")\n\t\t\telse:\n\t\t\t\tresults.sort(cmp=lambda a, b: cmp(a[0], b[0]))\n\t\t\t\tresults = [k[1] for k in results]\n\t\t\t\t\n\t\t\t\tif flatten:\n\t\t\t\t\tflatresults = []\n\t\t\t\t\tfor result in results:\n\t\t\t\t\t\tflatresults.extend(result)\n\t\t\t\t\tresults = flatresults\n\t\t\treturn results\n\t\treturn execute\n\treturn wrapper","method_summary":"Function decorater that executes the function in parallel","original_method_code":"def parallelize(cores=None, fork=True, flatten=False, info=False, infoclass=InfoThreadProgressBar, init=None, *args, **kwargs):\n\t\"\"\"Function decorater that executes the function in parallel\n\t\n\tUsage::\n\n\t\t@parallelize(cores=10, info=True)\n\t\tdef f(x):\n\t\t\treturn x**2\n\t\t\n\t\tx = numpy.arange(0, 100, 0.1)\n\t\ty = f(x) # this gets executed parallel\n\n\t:param cores: number of cpus\/cores to use (if None, it counts the cores using \/proc\/cpuinfo)\n\t:param fork: fork a process (should always be true since of the GIT, but can be used with c modules that release the GIT)\n\t:param flatten: if False and each return value is a list, final result will be a list of lists, if True, all lists are combined to one big list\n\t:param info: show progress bar (see infoclass)\n\t:param infoclass: class to instantiate that shows the progress (default shows progressbar)\n\t:param init: function to be called in each forked process before executing, can be used to set the seed, takes a integer as parameter (number that identifies the process)\n\t:param args: extra arguments passed to function\n\t:param kwargs: extra keyword arguments passed to function\n\t\t\t\n\tExample::\n\t\t\t\n\t\t@parallelize(cores=10, info=True, n=2)\n\t\tdef f(x, n):\n\t\t\treturn x**n\n\t\t\n\t\tx = numpy.arange(0, 100, 0.1)\n\t\ty = f(x) # this gets executed parallel\n\t\t\t\n\t\n\t\n\t\"\"\"\n\tif cores == None:\n\t\tcores = multiprocessing.cpu_count()\n\tdef wrapper(f):\n\t\tdef execute(*multiargs):\n\t\t\tresults = []\n\t\t\tlen(list(zip(*multiargs)))\n\t\t\tN = len(multiargs[0])\n\t\t\tif info:\n\t\t\t\tprint(\"running %i jobs on %i cores\" % (N, cores))\n\t\t\ttaskQueue = queue.Queue(len(multiargs[0]))\n\t\t\t#for timenr in range(times):\n\t\t\t#\ttaskQueue.put(timenr)\n\t\t\tfor tasknr, _args in enumerate(zip(*multiargs)):\n\t\t\t\ttaskQueue.put((tasknr, list(_args)))\n\t\t\t#for timenr in range(times):\n\t\t\t#\tresult = f(*args, **kwargs)\n\t\t\t#\tresults.append(result)\n\t\t\texecutions = [Execution(taskQueue, fork, f, init, corenr, args, kwargs) for corenr in range(cores)]\n\t\t\tif info:\n\t\t\t\tinfoobj = infoclass(len(multiargs[0]), executions)\n\t\t\t\tinfoobj.start()\n\t\t\tfor i, execution in enumerate(executions):\n\t\t\t\texecution.setName(\"T-%d\" % i)\n\t\t\t\texecution.start()\n\t\t\t#if 1:\n\t\t\t#\twatchdog = Watchdog(executions)\n\t\t\t#\twatchdog.start()\n\t\t\terror = False\n\t\t\tfor execution in executions:\n\t\t\t\tlog(\"joining:\",execution.getName())\n\t\t\t\ttry:\n\t\t\t\t\texecution.join()\n\t\t\t\texcept BaseException:\n\t\t\t\t\terror = True\n\t\t\t\tresults.extend(execution.results)\n\t\t\t\tif execution.error:\n\t\t\t\t\terror = True \n\t\t\tif info:\n\t\t\t\tinfoobj.join()\n\t\t\tif error:\n\t\t\t\tprint(\"error\", file=sys.stderr)\n\t\t\t\tresults = None\n\t\t\t\traise Exception(\"error in one or more of the executors\")\n\t\t\telse:\n\t\t\t\tresults.sort(cmp=lambda a, b: cmp(a[0], b[0]))\n\t\t\t\tresults = [k[1] for k in results]\n\t\t\t\t#print \"bla\", results\n\t\t\t\tif flatten:\n\t\t\t\t\tflatresults = []\n\t\t\t\t\tfor result in results:\n\t\t\t\t\t\tflatresults.extend(result)\n\t\t\t\t\tresults = flatresults\n\t\t\treturn results\n\t\treturn execute\n\treturn wrapper","method_path":"https:\/\/github.com\/vaexio\/vaex\/blob\/a45b672f8287afca2ada8e36b74b604b9b28dd85\/packages\/vaex-core\/vaex\/misc\/parallelize.py#L335-L422"}
{"repo_name":"vaexio\/vaex","method_name":"arrow_table_from_vaex_df","method_code":"def arrow_table_from_vaex_df(ds, column_names=None, selection=None, strings=True, virtual=False):\n    \"\"\"\"\"\"\n    names = []\n    arrays = []\n    for name, array in ds.to_items(column_names=column_names, selection=selection, strings=strings, virtual=virtual):\n        names.append(name)\n        arrays.append(arrow_array_from_numpy_array(array))\n    return pyarrow.Table.from_arrays(arrays, names)","method_summary":"Implementation of Dataset.to_arrow_table","original_method_code":"def arrow_table_from_vaex_df(ds, column_names=None, selection=None, strings=True, virtual=False):\n    \"\"\"Implementation of Dataset.to_arrow_table\"\"\"\n    names = []\n    arrays = []\n    for name, array in ds.to_items(column_names=column_names, selection=selection, strings=strings, virtual=virtual):\n        names.append(name)\n        arrays.append(arrow_array_from_numpy_array(array))\n    return pyarrow.Table.from_arrays(arrays, names)","method_path":"https:\/\/github.com\/vaexio\/vaex\/blob\/a45b672f8287afca2ada8e36b74b604b9b28dd85\/packages\/vaex-arrow\/vaex_arrow\/convert.py#L81-L88"}
{"repo_name":"vaexio\/vaex","method_name":"patch","method_code":"def patch(f):\n    ''''''\n    name = f.__name__\n    Dataset.__hidden__[name] = f\n    return f","method_summary":"Adds method f to the Dataset class","original_method_code":"def patch(f):\n    '''Adds method f to the Dataset class'''\n    name = f.__name__\n    Dataset.__hidden__[name] = f\n    return f","method_path":"https:\/\/github.com\/vaexio\/vaex\/blob\/a45b672f8287afca2ada8e36b74b604b9b28dd85\/packages\/vaex-astro\/vaex\/astro\/transformations.py#L5-L9"}
{"repo_name":"vaexio\/vaex","method_name":"add_virtual_columns_eq2ecl","method_code":"def add_virtual_columns_eq2ecl(self, long_in=\"ra\", lat_in=\"dec\", long_out=\"lambda_\", lat_out=\"beta\", name_prefix=\"__celestial_eq2ecl\", radians=False):\n    \"\"\"\"\"\"\n\n    self.add_virtual_columns_celestial(long_in, lat_in, long_out, lat_out, name_prefix=name_prefix, radians=radians, _matrix='eq2ecl')","method_summary":"Add ecliptic coordates (long_out, lat_out) from equatorial coordinates.","original_method_code":"def add_virtual_columns_eq2ecl(self, long_in=\"ra\", lat_in=\"dec\", long_out=\"lambda_\", lat_out=\"beta\", name_prefix=\"__celestial_eq2ecl\", radians=False):\n    \"\"\"Add ecliptic coordates (long_out, lat_out) from equatorial coordinates.\n\n    :param long_in: Name\/expression for right ascension\n    :param lat_in: Name\/expression for declination\n    :param long_out:  Output name for lambda coordinate\n    :param lat_out: Output name for beta coordinate\n    :param name_prefix:\n    :param radians: input and output in radians (True), or degrees (False)\n    :return:\n    \"\"\"\n\n    self.add_virtual_columns_celestial(long_in, lat_in, long_out, lat_out, name_prefix=name_prefix, radians=radians, _matrix='eq2ecl')","method_path":"https:\/\/github.com\/vaexio\/vaex\/blob\/a45b672f8287afca2ada8e36b74b604b9b28dd85\/packages\/vaex-astro\/vaex\/astro\/transformations.py#L23-L35"}
{"repo_name":"vaexio\/vaex","method_name":"add_virtual_columns_distance_from_parallax","method_code":"def add_virtual_columns_distance_from_parallax(self, parallax=\"parallax\", distance_name=\"distance\", parallax_uncertainty=None, uncertainty_postfix=\"_uncertainty\"):\n    \"\"\"\"\"\"\n    \"\"\"\"\"\"\n    import astropy.units\n    unit = self.unit(parallax)\n    \n    \n    \n    \n    distance_expression = \"1\/%s\" % (parallax)\n    self.ucds[distance_name] = \"pos.distance\"\n    self.descriptions[distance_name] = \"Derived from parallax (%s)\" % parallax\n    if unit:\n        if unit == astropy.units.milliarcsecond:\n            self.units[distance_name] = astropy.units.kpc\n        if unit == astropy.units.arcsecond:\n            self.units[distance_name] = astropy.units.parsec\n    self.add_virtual_column(distance_name, distance_expression)\n    if parallax_uncertainty:\n        \"\"\"\"\"\"\n        name = distance_name + uncertainty_postfix\n        distance_uncertainty_expression = \"{parallax_uncertainty}\/({parallax})**2\".format(**locals())\n        self.add_virtual_column(name, distance_uncertainty_expression)\n        self.descriptions[name] = \"Uncertainty on parallax (%s)\" % parallax\n        self.ucds[name] = \"stat.error;pos.distance\"","method_summary":"Convert parallax to distance (i.e. 1\/parallax)","original_method_code":"def add_virtual_columns_distance_from_parallax(self, parallax=\"parallax\", distance_name=\"distance\", parallax_uncertainty=None, uncertainty_postfix=\"_uncertainty\"):\n    \"\"\"Convert parallax to distance (i.e. 1\/parallax)\n\n    :param parallax: expression for the parallax, e.g. \"parallax\"\n    :param distance_name: name for the virtual column of the distance, e.g. \"distance\"\n    :param parallax_uncertainty: expression for the uncertainty on the parallax, e.g. \"parallax_error\"\n    :param uncertainty_postfix: distance_name + uncertainty_postfix is the name for the virtual column, e.g. \"distance_uncertainty\" by default\n    :return:\n    \"\"\"\n    \"\"\"\n\n\n    \"\"\"\n    import astropy.units\n    unit = self.unit(parallax)\n    # if unit:\n    #   convert = unit.to(astropy.units.mas)\n    #   distance_expression = \"%f\/(%s)\" % (convert, parallax)\n    # else:\n    distance_expression = \"1\/%s\" % (parallax)\n    self.ucds[distance_name] = \"pos.distance\"\n    self.descriptions[distance_name] = \"Derived from parallax (%s)\" % parallax\n    if unit:\n        if unit == astropy.units.milliarcsecond:\n            self.units[distance_name] = astropy.units.kpc\n        if unit == astropy.units.arcsecond:\n            self.units[distance_name] = astropy.units.parsec\n    self.add_virtual_column(distance_name, distance_expression)\n    if parallax_uncertainty:\n        \"\"\"\n        y = 1\/x\n        sigma_y**2 = (1\/x**2)**2 sigma_x**2\n        sigma_y = (1\/x**2) sigma_x\n        sigma_y = y**2 sigma_x\n        sigma_y\/y = (1\/x) sigma_x\n        \"\"\"\n        name = distance_name + uncertainty_postfix\n        distance_uncertainty_expression = \"{parallax_uncertainty}\/({parallax})**2\".format(**locals())\n        self.add_virtual_column(name, distance_uncertainty_expression)\n        self.descriptions[name] = \"Uncertainty on parallax (%s)\" % parallax\n        self.ucds[name] = \"stat.error;pos.distance\"","method_path":"https:\/\/github.com\/vaexio\/vaex\/blob\/a45b672f8287afca2ada8e36b74b604b9b28dd85\/packages\/vaex-astro\/vaex\/astro\/transformations.py#L70-L110"}
{"repo_name":"vaexio\/vaex","method_name":"add_virtual_columns_cartesian_velocities_to_pmvr","method_code":"def add_virtual_columns_cartesian_velocities_to_pmvr(self, x=\"x\", y=\"y\", z=\"z\", vx=\"vx\", vy=\"vy\", vz=\"vz\", vr=\"vr\", pm_long=\"pm_long\", pm_lat=\"pm_lat\", distance=None):\n    \"\"\"\"\"\"\n    if distance is None:\n        distance = \"sqrt({x}**2+{y}**2+{z}**2)\".format(**locals())\n    k = 4.74057\n    self.add_variable(\"k\", k, overwrite=False)\n    self.add_virtual_column(vr, \"({x}*{vx}+{y}*{vy}+{z}*{vz})\/{distance}\".format(**locals()))\n    self.add_virtual_column(pm_long, \"-({vx}*{y}-{x}*{vy})\/sqrt({x}**2+{y}**2)\/{distance}\/k\".format(**locals()))\n    self.add_virtual_column(pm_lat, \"-({z}*({x}*{vx}+{y}*{vy}) - ({x}**2+{y}**2)*{vz})\/( ({x}**2+{y}**2+{z}**2) * sqrt({x}**2+{y}**2) )\/k\".format(**locals()))","method_summary":"Concert velocities from a cartesian system to proper motions and radial velocities","original_method_code":"def add_virtual_columns_cartesian_velocities_to_pmvr(self, x=\"x\", y=\"y\", z=\"z\", vx=\"vx\", vy=\"vy\", vz=\"vz\", vr=\"vr\", pm_long=\"pm_long\", pm_lat=\"pm_lat\", distance=None):\n    \"\"\"Concert velocities from a cartesian system to proper motions and radial velocities\n\n    TODO: errors\n\n    :param x: name of x column (input)\n    :param y:         y\n    :param z:         z\n    :param vx:       vx\n    :param vy:       vy\n    :param vz:       vz\n    :param vr: name of the column for the radial velocity in the r direction (output)\n    :param pm_long: name of the column for the proper motion component in the longitude direction  (output)\n    :param pm_lat: name of the column for the proper motion component in the latitude direction, positive points to the north pole (output)\n    :param distance: Expression for distance, if not given defaults to sqrt(x**2+y**2+z**2), but if this column already exists, passing this expression may lead to a better performance\n    :return:\n    \"\"\"\n    if distance is None:\n        distance = \"sqrt({x}**2+{y}**2+{z}**2)\".format(**locals())\n    k = 4.74057\n    self.add_variable(\"k\", k, overwrite=False)\n    self.add_virtual_column(vr, \"({x}*{vx}+{y}*{vy}+{z}*{vz})\/{distance}\".format(**locals()))\n    self.add_virtual_column(pm_long, \"-({vx}*{y}-{x}*{vy})\/sqrt({x}**2+{y}**2)\/{distance}\/k\".format(**locals()))\n    self.add_virtual_column(pm_lat, \"-({z}*({x}*{vx}+{y}*{vy}) - ({x}**2+{y}**2)*{vz})\/( ({x}**2+{y}**2+{z}**2) * sqrt({x}**2+{y}**2) )\/k\".format(**locals()))","method_path":"https:\/\/github.com\/vaexio\/vaex\/blob\/a45b672f8287afca2ada8e36b74b604b9b28dd85\/packages\/vaex-astro\/vaex\/astro\/transformations.py#L114-L137"}
{"repo_name":"vaexio\/vaex","method_name":"add_virtual_columns_proper_motion_eq2gal","method_code":"def add_virtual_columns_proper_motion_eq2gal(self, long_in=\"ra\", lat_in=\"dec\", pm_long=\"pm_ra\", pm_lat=\"pm_dec\", pm_long_out=\"pm_l\", pm_lat_out=\"pm_b\",\n                                            name_prefix=\"__proper_motion_eq2gal\",\n                                            right_ascension_galactic_pole=192.85,\n                                            declination_galactic_pole=27.12,\n                                            propagate_uncertainties=False,\n                                            radians=False, inverse=False):\n    \"\"\"\"\"\"\n    \"\"\"\"\"\"\n    long_in_original = long_in = self._expr(long_in)\n    lat_in_original = lat_in = self._expr(lat_in)\n    pm_long = self._expr(pm_long)\n    pm_lat = self._expr(pm_lat)\n    if not radians:\n        long_in = long_in * np.pi\/180\n        lat_in = lat_in * np.pi\/180\n    c1_name = name_prefix + \"_C1\"\n    c2_name = name_prefix + \"_C2\"\n    right_ascension_galactic_pole = math.radians(right_ascension_galactic_pole)\n    declination_galactic_pole = math.radians(declination_galactic_pole)\n    self[c1_name] = c1 = np.sin(declination_galactic_pole) * np.cos(lat_in) - np.cos(declination_galactic_pole)*np.sin(lat_in)*np.cos(long_in-right_ascension_galactic_pole)\n    self[c2_name] = c2 = np.cos(declination_galactic_pole) * np.sin(long_in - right_ascension_galactic_pole)\n    c1 = self[c1_name]\n    c2 = self[c2_name]\n    if inverse:\n        self[pm_long_out] = ( c1 * pm_long + -c2 * pm_lat)\/np.sqrt(c1**2+c2**2)\n        self[pm_lat_out] =  ( c2 * pm_long +  c1 * pm_lat)\/np.sqrt(c1**2+c2**2)\n    else:\n        self[pm_long_out] = ( c1 * pm_long + c2 * pm_lat)\/np.sqrt(c1**2+c2**2)\n        self[pm_lat_out] =  (-c2 * pm_long + c1 * pm_lat)\/np.sqrt(c1**2+c2**2)\n    if propagate_uncertainties:\n        self.propagate_uncertainties([self[pm_long_out], self[pm_lat_out]])","method_summary":"Transform\/rotate proper motions from equatorial to galactic coordinates Taken from","original_method_code":"def add_virtual_columns_proper_motion_eq2gal(self, long_in=\"ra\", lat_in=\"dec\", pm_long=\"pm_ra\", pm_lat=\"pm_dec\", pm_long_out=\"pm_l\", pm_lat_out=\"pm_b\",\n                                            name_prefix=\"__proper_motion_eq2gal\",\n                                            right_ascension_galactic_pole=192.85,\n                                            declination_galactic_pole=27.12,\n                                            propagate_uncertainties=False,\n                                            radians=False, inverse=False):\n    \"\"\"Transform\/rotate proper motions from equatorial to galactic coordinates\n\n    Taken from http:\/\/arxiv.org\/abs\/1306.2945\n\n    :param long_in: Name\/expression for right ascension\n    :param lat_in: Name\/expression for declination\n    :param pm_long: Proper motion for ra\n    :param pm_lat: Proper motion for dec\n    :param pm_long_out:  Output name for output proper motion on l direction\n    :param pm_lat_out: Output name for output proper motion on b direction\n    :param name_prefix:\n    :param radians: input and output in radians (True), or degrees (False)\n    :parap inverse: (For internal use) convert from galactic to equatorial instead\n    :return:\n    \"\"\"\n    \"\"\"mu_gb =  mu_dec*(cdec*sdp-sdec*cdp*COS(ras))\/cgb $\n      - mu_ra*cdp*SIN(ras)\/cgb\"\"\"\n    long_in_original = long_in = self._expr(long_in)\n    lat_in_original = lat_in = self._expr(lat_in)\n    pm_long = self._expr(pm_long)\n    pm_lat = self._expr(pm_lat)\n    if not radians:\n        long_in = long_in * np.pi\/180\n        lat_in = lat_in * np.pi\/180\n    c1_name = name_prefix + \"_C1\"\n    c2_name = name_prefix + \"_C2\"\n    right_ascension_galactic_pole = math.radians(right_ascension_galactic_pole)\n    declination_galactic_pole = math.radians(declination_galactic_pole)\n    self[c1_name] = c1 = np.sin(declination_galactic_pole) * np.cos(lat_in) - np.cos(declination_galactic_pole)*np.sin(lat_in)*np.cos(long_in-right_ascension_galactic_pole)\n    self[c2_name] = c2 = np.cos(declination_galactic_pole) * np.sin(long_in - right_ascension_galactic_pole)\n    c1 = self[c1_name]\n    c2 = self[c2_name]\n    if inverse:\n        self[pm_long_out] = ( c1 * pm_long + -c2 * pm_lat)\/np.sqrt(c1**2+c2**2)\n        self[pm_lat_out] =  ( c2 * pm_long +  c1 * pm_lat)\/np.sqrt(c1**2+c2**2)\n    else:\n        self[pm_long_out] = ( c1 * pm_long + c2 * pm_lat)\/np.sqrt(c1**2+c2**2)\n        self[pm_lat_out] =  (-c2 * pm_long + c1 * pm_lat)\/np.sqrt(c1**2+c2**2)\n    if propagate_uncertainties:\n        self.propagate_uncertainties([self[pm_long_out], self[pm_lat_out]])","method_path":"https:\/\/github.com\/vaexio\/vaex\/blob\/a45b672f8287afca2ada8e36b74b604b9b28dd85\/packages\/vaex-astro\/vaex\/astro\/transformations.py#L141-L186"}
{"repo_name":"vaexio\/vaex","method_name":"add_virtual_columns_proper_motion_gal2eq","method_code":"def add_virtual_columns_proper_motion_gal2eq(self, long_in=\"ra\", lat_in=\"dec\", pm_long=\"pm_l\", pm_lat=\"pm_b\", pm_long_out=\"pm_ra\", pm_lat_out=\"pm_dec\",\n                                            name_prefix=\"__proper_motion_gal2eq\",\n                                            right_ascension_galactic_pole=192.85,\n                                            declination_galactic_pole=27.12,\n                                            propagate_uncertainties=False,\n                                            radians=False):\n    \"\"\"\"\"\"\n    kwargs = dict(**locals())\n    kwargs.pop('self')\n    kwargs['inverse'] = True\n    self.add_virtual_columns_proper_motion_eq2gal(**kwargs)","method_summary":"Transform\/rotate proper motions from galactic to equatorial coordinates. Inverse of :py:`add_virtual_columns_proper_motion_eq2gal`","original_method_code":"def add_virtual_columns_proper_motion_gal2eq(self, long_in=\"ra\", lat_in=\"dec\", pm_long=\"pm_l\", pm_lat=\"pm_b\", pm_long_out=\"pm_ra\", pm_lat_out=\"pm_dec\",\n                                            name_prefix=\"__proper_motion_gal2eq\",\n                                            right_ascension_galactic_pole=192.85,\n                                            declination_galactic_pole=27.12,\n                                            propagate_uncertainties=False,\n                                            radians=False):\n    \"\"\"Transform\/rotate proper motions from galactic to equatorial coordinates.\n\n    Inverse of :py:`add_virtual_columns_proper_motion_eq2gal`\n    \"\"\"\n    kwargs = dict(**locals())\n    kwargs.pop('self')\n    kwargs['inverse'] = True\n    self.add_virtual_columns_proper_motion_eq2gal(**kwargs)","method_path":"https:\/\/github.com\/vaexio\/vaex\/blob\/a45b672f8287afca2ada8e36b74b604b9b28dd85\/packages\/vaex-astro\/vaex\/astro\/transformations.py#L189-L202"}
{"repo_name":"vaexio\/vaex","method_name":"add_virtual_columns_lbrvr_proper_motion2vcartesian","method_code":"def add_virtual_columns_lbrvr_proper_motion2vcartesian(self, long_in=\"l\", lat_in=\"b\", distance=\"distance\", pm_long=\"pm_l\", pm_lat=\"pm_b\",\n                                                       vr=\"vr\", vx=\"vx\", vy=\"vy\", vz=\"vz\",\n                                                       center_v=(0, 0, 0),\n                                                       propagate_uncertainties=False, radians=False):\n    \"\"\"\"\"\"\n    k = 4.74057\n    a, d, distance = self._expr(long_in, lat_in, distance)\n    pm_long, pm_lat, vr = self._expr(pm_long, pm_lat, vr)\n    if not radians:\n        a = a * np.pi\/180\n        d = d * np.pi\/180\n    A = [[np.cos(a)*np.cos(d), -np.sin(a), -np.cos(a)*np.sin(d)],\n         [np.sin(a)*np.cos(d),  np.cos(a), -np.sin(a)*np.sin(d)],\n         [np.sin(d), d*0, np.cos(d)]]\n    self.add_virtual_columns_matrix3d(vr, k * pm_long * distance, k * pm_lat * distance, vx, vy, vz, A, translation=center_v)\n    if propagate_uncertainties:\n        self.propagate_uncertainties([self[vx], self[vy], self[vz]])","method_summary":"Convert radial velocity and galactic proper motions (and positions) to cartesian velocities wrt the center_v Based on","original_method_code":"def add_virtual_columns_lbrvr_proper_motion2vcartesian(self, long_in=\"l\", lat_in=\"b\", distance=\"distance\", pm_long=\"pm_l\", pm_lat=\"pm_b\",\n                                                       vr=\"vr\", vx=\"vx\", vy=\"vy\", vz=\"vz\",\n                                                       center_v=(0, 0, 0),\n                                                       propagate_uncertainties=False, radians=False):\n    \"\"\"Convert radial velocity and galactic proper motions (and positions) to cartesian velocities wrt the center_v\n\n    Based on http:\/\/adsabs.harvard.edu\/abs\/1987AJ.....93..864J\n\n\n    :param long_in: Name\/expression for galactic longitude\n    :param lat_in: Name\/expression for galactic latitude\n    :param distance: Name\/expression for heliocentric distance\n    :param pm_long: Name\/expression for the galactic proper motion in latitude direction (pm_l*, so cosine(b) term should be included)\n    :param pm_lat: Name\/expression for the galactic proper motion in longitude direction\n    :param vr: Name\/expression for the radial velocity\n    :param vx: Output name for the cartesian velocity x-component\n    :param vy: Output name for the cartesian velocity y-component\n    :param vz: Output name for the cartesian velocity z-component\n    :param center_v: Extra motion that should be added, for instance lsr + motion of the sun wrt the galactic restframe\n    :param radians: input and output in radians (True), or degrees (False)\n    :return:\n    \"\"\"\n    k = 4.74057\n    a, d, distance = self._expr(long_in, lat_in, distance)\n    pm_long, pm_lat, vr = self._expr(pm_long, pm_lat, vr)\n    if not radians:\n        a = a * np.pi\/180\n        d = d * np.pi\/180\n    A = [[np.cos(a)*np.cos(d), -np.sin(a), -np.cos(a)*np.sin(d)],\n         [np.sin(a)*np.cos(d),  np.cos(a), -np.sin(a)*np.sin(d)],\n         [np.sin(d), d*0, np.cos(d)]]\n    self.add_virtual_columns_matrix3d(vr, k * pm_long * distance, k * pm_lat * distance, vx, vy, vz, A, translation=center_v)\n    if propagate_uncertainties:\n        self.propagate_uncertainties([self[vx], self[vy], self[vz]])","method_path":"https:\/\/github.com\/vaexio\/vaex\/blob\/a45b672f8287afca2ada8e36b74b604b9b28dd85\/packages\/vaex-astro\/vaex\/astro\/transformations.py#L206-L239"}
{"repo_name":"librosa\/librosa","method_name":"show_versions","method_code":"def show_versions():\n    ''''''\n\n    core_deps = ['audioread',\n                 'numpy',\n                 'scipy',\n                 'sklearn',\n                 'joblib',\n                 'decorator',\n                 'six',\n                 'soundfile',\n                 'resampy',\n                 'numba']\n\n    extra_deps = ['numpydoc',\n                  'sphinx',\n                  'sphinx_rtd_theme',\n                  'sphinxcontrib.versioning',\n                  'sphinx-gallery',\n                  'pytest',\n                  'pytest-mpl',\n                  'pytest-cov',\n                  'matplotlib']\n\n    print('INSTALLED VERSIONS')\n    print('------------------')\n    print('python: {}\\n'.format(sys.version))\n    print('librosa: {}\\n'.format(version))\n    for dep in core_deps:\n        print('{}: {}'.format(dep, __get_mod_version(dep)))\n    print('')\n    for dep in extra_deps:\n        print('{}: {}'.format(dep, __get_mod_version(dep)))\n    pass","method_summary":"Return the version information for all librosa dependencies.","original_method_code":"def show_versions():\n    '''Return the version information for all librosa dependencies.'''\n\n    core_deps = ['audioread',\n                 'numpy',\n                 'scipy',\n                 'sklearn',\n                 'joblib',\n                 'decorator',\n                 'six',\n                 'soundfile',\n                 'resampy',\n                 'numba']\n\n    extra_deps = ['numpydoc',\n                  'sphinx',\n                  'sphinx_rtd_theme',\n                  'sphinxcontrib.versioning',\n                  'sphinx-gallery',\n                  'pytest',\n                  'pytest-mpl',\n                  'pytest-cov',\n                  'matplotlib']\n\n    print('INSTALLED VERSIONS')\n    print('------------------')\n    print('python: {}\\n'.format(sys.version))\n    print('librosa: {}\\n'.format(version))\n    for dep in core_deps:\n        print('{}: {}'.format(dep, __get_mod_version(dep)))\n    print('')\n    for dep in extra_deps:\n        print('{}: {}'.format(dep, __get_mod_version(dep)))\n    pass","method_path":"https:\/\/github.com\/librosa\/librosa\/blob\/180e8e6eb8f958fa6b20b8cba389f7945d508247\/librosa\/version.py#L28-L61"}
{"repo_name":"librosa\/librosa","method_name":"rename_kw","method_code":"def rename_kw(old_name, old_value, new_name, new_value,\n              version_deprecated, version_removed):\n    ''''''\n    if isinstance(old_value, Deprecated):\n        return new_value\n    else:\n        stack = inspect.stack()\n        dep_func = stack[1]\n        caller = stack[2]\n\n        warnings.warn_explicit(\"{:s}() keyword argument '{:s}' has been \"\n                               \"renamed to '{:s}' in version {:}.\"\n                               \"\\n\\tThis alias will be removed in version \"\n                               \"{:}.\".format(dep_func[3],\n                                             old_name, new_name,\n                                             version_deprecated,\n                                             version_removed),\n                               category=DeprecationWarning,\n                               filename=caller[1],\n                               lineno=caller[2])\n\n        return old_value","method_summary":"Handle renamed arguments.","original_method_code":"def rename_kw(old_name, old_value, new_name, new_value,\n              version_deprecated, version_removed):\n    '''Handle renamed arguments.\n\n    Parameters\n    ----------\n    old_name : str\n    old_value\n        The name and value of the old argument\n\n    new_name : str\n    new_value\n        The name and value of the new argument\n\n    version_deprecated : str\n        The version at which the old name became deprecated\n\n    version_removed : str\n        The version at which the old name will be removed\n\n    Returns\n    -------\n    value\n        - `new_value` if `old_value` of type `Deprecated`\n        - `old_value` otherwise\n\n    Warnings\n    --------\n    if `old_value` is not of type `Deprecated`\n\n    '''\n    if isinstance(old_value, Deprecated):\n        return new_value\n    else:\n        stack = inspect.stack()\n        dep_func = stack[1]\n        caller = stack[2]\n\n        warnings.warn_explicit(\"{:s}() keyword argument '{:s}' has been \"\n                               \"renamed to '{:s}' in version {:}.\"\n                               \"\\n\\tThis alias will be removed in version \"\n                               \"{:}.\".format(dep_func[3],\n                                             old_name, new_name,\n                                             version_deprecated,\n                                             version_removed),\n                               category=DeprecationWarning,\n                               filename=caller[1],\n                               lineno=caller[2])\n\n        return old_value","method_path":"https:\/\/github.com\/librosa\/librosa\/blob\/180e8e6eb8f958fa6b20b8cba389f7945d508247\/librosa\/util\/deprecation.py#L15-L64"}
{"repo_name":"librosa\/librosa","method_name":"set_fftlib","method_code":"def set_fftlib(lib=None):\n    ''''''\n\n    global __FFTLIB\n    if lib is None:\n        from numpy import fft\n        lib = fft\n\n    __FFTLIB = lib","method_summary":"Set the FFT library used by librosa.","original_method_code":"def set_fftlib(lib=None):\n    '''Set the FFT library used by librosa.\n\n    Parameters\n    ----------\n    lib : None or module\n        Must implement an interface compatible with `numpy.fft`.\n        If `None`, reverts to `numpy.fft`.\n\n    Examples\n    --------\n    Use `pyfftw`:\n\n    >>> import pyfftw\n    >>> librosa.set_fftlib(pyfftw.interfaces.numpy_fft)\n\n    Reset to default `numpy` implementation\n\n    >>> librosa.set_fftlib()\n\n    '''\n\n    global __FFTLIB\n    if lib is None:\n        from numpy import fft\n        lib = fft\n\n    __FFTLIB = lib","method_path":"https:\/\/github.com\/librosa\/librosa\/blob\/180e8e6eb8f958fa6b20b8cba389f7945d508247\/librosa\/core\/fft.py#L11-L38"}
{"repo_name":"librosa\/librosa","method_name":"beat_track","method_code":"def beat_track(input_file, output_csv):\n    ''''''\n\n    print('Loading ', input_file)\n    y, sr = librosa.load(input_file, sr=22050)\n\n    \n    hop_length = 512\n\n    \n    print('Tracking beats')\n    tempo, beats = librosa.beat.beat_track(y=y, sr=sr, hop_length=hop_length)\n\n    print('Estimated tempo: {:0.2f} beats per minute'.format(tempo))\n\n    \n    \n    beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=hop_length)\n\n    print('Saving output to ', output_csv)\n    librosa.output.times_csv(output_csv, beat_times)\n    print('done!')","method_summary":"Beat tracking function","original_method_code":"def beat_track(input_file, output_csv):\n    '''Beat tracking function\n\n    :parameters:\n      - input_file : str\n          Path to input audio file (wav, mp3, m4a, flac, etc.)\n\n      - output_file : str\n          Path to save beat event timestamps as a CSV file\n    '''\n\n    print('Loading ', input_file)\n    y, sr = librosa.load(input_file, sr=22050)\n\n    # Use a default hop size of 512 samples @ 22KHz ~= 23ms\n    hop_length = 512\n\n    # This is the window length used by default in stft\n    print('Tracking beats')\n    tempo, beats = librosa.beat.beat_track(y=y, sr=sr, hop_length=hop_length)\n\n    print('Estimated tempo: {:0.2f} beats per minute'.format(tempo))\n\n    # save output\n    # 'beats' will contain the frame numbers of beat events.\n    beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=hop_length)\n\n    print('Saving output to ', output_csv)\n    librosa.output.times_csv(output_csv, beat_times)\n    print('done!')","method_path":"https:\/\/github.com\/librosa\/librosa\/blob\/180e8e6eb8f958fa6b20b8cba389f7945d508247\/examples\/beat_tracker.py#L16-L45"}
{"repo_name":"librosa\/librosa","method_name":"adjust_tuning","method_code":"def adjust_tuning(input_file, output_file):\n    ''''''\n    print('Loading ', input_file)\n    y, sr = librosa.load(input_file)\n\n    print('Separating harmonic component ... ')\n    y_harm = librosa.effects.harmonic(y)\n\n    print('Estimating tuning ... ')\n    \n    tuning = librosa.estimate_tuning(y=y_harm, sr=sr)\n\n    print('{:+0.2f} cents'.format(100 * tuning))\n    print('Applying pitch-correction of {:+0.2f} cents'.format(-100 * tuning))\n    y_tuned = librosa.effects.pitch_shift(y, sr, -tuning)\n\n    print('Saving tuned audio to: ', output_file)\n    librosa.output.write_wav(output_file, y_tuned, sr)","method_summary":"Load audio, estimate tuning, apply pitch correction, and save.","original_method_code":"def adjust_tuning(input_file, output_file):\n    '''Load audio, estimate tuning, apply pitch correction, and save.'''\n    print('Loading ', input_file)\n    y, sr = librosa.load(input_file)\n\n    print('Separating harmonic component ... ')\n    y_harm = librosa.effects.harmonic(y)\n\n    print('Estimating tuning ... ')\n    # Just track the pitches associated with high magnitude\n    tuning = librosa.estimate_tuning(y=y_harm, sr=sr)\n\n    print('{:+0.2f} cents'.format(100 * tuning))\n    print('Applying pitch-correction of {:+0.2f} cents'.format(-100 * tuning))\n    y_tuned = librosa.effects.pitch_shift(y, sr, -tuning)\n\n    print('Saving tuned audio to: ', output_file)\n    librosa.output.write_wav(output_file, y_tuned, sr)","method_path":"https:\/\/github.com\/librosa\/librosa\/blob\/180e8e6eb8f958fa6b20b8cba389f7945d508247\/examples\/adjust_tuning.py#L15-L32"}
{"repo_name":"librosa\/librosa","method_name":"frames_to_samples","method_code":"def frames_to_samples(frames, hop_length=512, n_fft=None):\n    \"\"\"\"\"\"\n\n    offset = 0\n    if n_fft is not None:\n        offset = int(n_fft \/\/ 2)\n\n    return (np.asanyarray(frames) * hop_length + offset).astype(int)","method_summary":"Converts frame indices to audio sample indices.","original_method_code":"def frames_to_samples(frames, hop_length=512, n_fft=None):\n    \"\"\"Converts frame indices to audio sample indices.\n\n    Parameters\n    ----------\n    frames     : number or np.ndarray [shape=(n,)]\n        frame index or vector of frame indices\n\n    hop_length : int > 0 [scalar]\n        number of samples between successive frames\n\n    n_fft : None or int > 0 [scalar]\n        Optional: length of the FFT window.\n        If given, time conversion will include an offset of `n_fft \/ 2`\n        to counteract windowing effects when using a non-centered STFT.\n\n    Returns\n    -------\n    times : number or np.ndarray\n        time (in samples) of each given frame number:\n        `times[i] = frames[i] * hop_length`\n\n    See Also\n    --------\n    frames_to_time : convert frame indices to time values\n    samples_to_frames : convert sample indices to frame indices\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> tempo, beats = librosa.beat.beat_track(y, sr=sr)\n    >>> beat_samples = librosa.frames_to_samples(beats)\n    \"\"\"\n\n    offset = 0\n    if n_fft is not None:\n        offset = int(n_fft \/\/ 2)\n\n    return (np.asanyarray(frames) * hop_length + offset).astype(int)","method_path":"https:\/\/github.com\/librosa\/librosa\/blob\/180e8e6eb8f958fa6b20b8cba389f7945d508247\/librosa\/core\/time_frequency.py#L30-L68"}
{"repo_name":"librosa\/librosa","method_name":"samples_to_frames","method_code":"def samples_to_frames(samples, hop_length=512, n_fft=None):\n    \"\"\"\"\"\"\n\n    offset = 0\n    if n_fft is not None:\n        offset = int(n_fft \/\/ 2)\n\n    samples = np.asanyarray(samples)\n    return np.floor((samples - offset) \/\/ hop_length).astype(int)","method_summary":"Converts sample indices into STFT frames.","original_method_code":"def samples_to_frames(samples, hop_length=512, n_fft=None):\n    \"\"\"Converts sample indices into STFT frames.\n\n    Examples\n    --------\n    >>> # Get the frame numbers for every 256 samples\n    >>> librosa.samples_to_frames(np.arange(0, 22050, 256))\n    array([ 0,  0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,\n            7,  7,  8,  8,  9,  9, 10, 10, 11, 11, 12, 12, 13, 13,\n           14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20,\n           21, 21, 22, 22, 23, 23, 24, 24, 25, 25, 26, 26, 27, 27,\n           28, 28, 29, 29, 30, 30, 31, 31, 32, 32, 33, 33, 34, 34,\n           35, 35, 36, 36, 37, 37, 38, 38, 39, 39, 40, 40, 41, 41,\n           42, 42, 43])\n\n    Parameters\n    ----------\n    samples : int or np.ndarray [shape=(n,)]\n        sample index or vector of sample indices\n\n    hop_length : int > 0 [scalar]\n        number of samples between successive frames\n\n    n_fft : None or int > 0 [scalar]\n        Optional: length of the FFT window.\n        If given, time conversion will include an offset of `- n_fft \/ 2`\n        to counteract windowing effects in STFT.\n\n        .. note:: This may result in negative frame indices.\n\n    Returns\n    -------\n    frames : int or np.ndarray [shape=(n,), dtype=int]\n        Frame numbers corresponding to the given times:\n        `frames[i] = floor( samples[i] \/ hop_length )`\n\n    See Also\n    --------\n    samples_to_time : convert sample indices to time values\n    frames_to_samples : convert frame indices to sample indices\n    \"\"\"\n\n    offset = 0\n    if n_fft is not None:\n        offset = int(n_fft \/\/ 2)\n\n    samples = np.asanyarray(samples)\n    return np.floor((samples - offset) \/\/ hop_length).astype(int)","method_path":"https:\/\/github.com\/librosa\/librosa\/blob\/180e8e6eb8f958fa6b20b8cba389f7945d508247\/librosa\/core\/time_frequency.py#L71-L118"}
{"repo_name":"librosa\/librosa","method_name":"frames_to_time","method_code":"def frames_to_time(frames, sr=22050, hop_length=512, n_fft=None):\n    \"\"\"\"\"\"\n\n    samples = frames_to_samples(frames,\n                                hop_length=hop_length,\n                                n_fft=n_fft)\n\n    return samples_to_time(samples, sr=sr)","method_summary":"Converts frame counts to time (seconds).","original_method_code":"def frames_to_time(frames, sr=22050, hop_length=512, n_fft=None):\n    \"\"\"Converts frame counts to time (seconds).\n\n    Parameters\n    ----------\n    frames     : np.ndarray [shape=(n,)]\n        frame index or vector of frame indices\n\n    sr         : number > 0 [scalar]\n        audio sampling rate\n\n    hop_length : int > 0 [scalar]\n        number of samples between successive frames\n\n    n_fft : None or int > 0 [scalar]\n        Optional: length of the FFT window.\n        If given, time conversion will include an offset of `n_fft \/ 2`\n        to counteract windowing effects when using a non-centered STFT.\n\n    Returns\n    -------\n    times : np.ndarray [shape=(n,)]\n        time (in seconds) of each given frame number:\n        `times[i] = frames[i] * hop_length \/ sr`\n\n    See Also\n    --------\n    time_to_frames : convert time values to frame indices\n    frames_to_samples : convert frame indices to sample indices\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> tempo, beats = librosa.beat.beat_track(y, sr=sr)\n    >>> beat_times = librosa.frames_to_time(beats, sr=sr)\n    \"\"\"\n\n    samples = frames_to_samples(frames,\n                                hop_length=hop_length,\n                                n_fft=n_fft)\n\n    return samples_to_time(samples, sr=sr)","method_path":"https:\/\/github.com\/librosa\/librosa\/blob\/180e8e6eb8f958fa6b20b8cba389f7945d508247\/librosa\/core\/time_frequency.py#L121-L162"}
{"repo_name":"librosa\/librosa","method_name":"time_to_frames","method_code":"def time_to_frames(times, sr=22050, hop_length=512, n_fft=None):\n    \"\"\"\"\"\"\n\n    samples = time_to_samples(times, sr=sr)\n\n    return samples_to_frames(samples, hop_length=hop_length, n_fft=n_fft)","method_summary":"Converts time stamps into STFT frames.","original_method_code":"def time_to_frames(times, sr=22050, hop_length=512, n_fft=None):\n    \"\"\"Converts time stamps into STFT frames.\n\n    Parameters\n    ----------\n    times : np.ndarray [shape=(n,)]\n        time (in seconds) or vector of time values\n\n    sr : number > 0 [scalar]\n        audio sampling rate\n\n    hop_length : int > 0 [scalar]\n        number of samples between successive frames\n\n    n_fft : None or int > 0 [scalar]\n        Optional: length of the FFT window.\n        If given, time conversion will include an offset of `- n_fft \/ 2`\n        to counteract windowing effects in STFT.\n\n        .. note:: This may result in negative frame indices.\n\n    Returns\n    -------\n    frames : np.ndarray [shape=(n,), dtype=int]\n        Frame numbers corresponding to the given times:\n        `frames[i] = floor( times[i] * sr \/ hop_length )`\n\n    See Also\n    --------\n    frames_to_time : convert frame indices to time values\n    time_to_samples : convert time values to sample indices\n\n    Examples\n    --------\n    Get the frame numbers for every 100ms\n\n    >>> librosa.time_to_frames(np.arange(0, 1, 0.1),\n    ...                         sr=22050, hop_length=512)\n    array([ 0,  4,  8, 12, 17, 21, 25, 30, 34, 38])\n\n    \"\"\"\n\n    samples = time_to_samples(times, sr=sr)\n\n    return samples_to_frames(samples, hop_length=hop_length, n_fft=n_fft)","method_path":"https:\/\/github.com\/librosa\/librosa\/blob\/180e8e6eb8f958fa6b20b8cba389f7945d508247\/librosa\/core\/time_frequency.py#L165-L209"}
{"repo_name":"librosa\/librosa","method_name":"midi_to_note","method_code":"def midi_to_note(midi, octave=True, cents=False):\n    ''''''\n\n    if cents and not octave:\n        raise ParameterError('Cannot encode cents without octave information.')\n\n    if not np.isscalar(midi):\n        return [midi_to_note(x, octave=octave, cents=cents) for x in midi]\n\n    note_map = ['C', 'C#', 'D', 'D#',\n                'E', 'F', 'F#', 'G',\n                'G#', 'A', 'A#', 'B']\n\n    note_num = int(np.round(midi))\n    note_cents = int(100 * np.around(midi - note_num, 2))\n\n    note = note_map[note_num % 12]\n\n    if octave:\n        note = '{:s}{:0d}'.format(note, int(note_num \/ 12) - 1)\n    if cents:\n        note = '{:s}{:+02d}'.format(note, note_cents)\n\n    return note","method_summary":"Convert one or more MIDI numbers to note strings. MIDI numbers will be rounded to the nearest integer. Notes will be of the format 'C0', 'C#0', 'D0',","original_method_code":"def midi_to_note(midi, octave=True, cents=False):\n    '''Convert one or more MIDI numbers to note strings.\n\n    MIDI numbers will be rounded to the nearest integer.\n\n    Notes will be of the format 'C0', 'C#0', 'D0', ...\n\n    Examples\n    --------\n    >>> librosa.midi_to_note(0)\n    'C-1'\n    >>> librosa.midi_to_note(37)\n    'C#2'\n    >>> librosa.midi_to_note(-2)\n    'A#-2'\n    >>> librosa.midi_to_note(104.7)\n    'A7'\n    >>> librosa.midi_to_note(104.7, cents=True)\n    'A7-30'\n    >>> librosa.midi_to_note(list(range(12, 24)))\n    ['C0', 'C#0', 'D0', 'D#0', 'E0', 'F0', 'F#0', 'G0', 'G#0', 'A0', 'A#0', 'B0']\n\n    Parameters\n    ----------\n    midi : int or iterable of int\n        Midi numbers to convert.\n\n    octave: bool\n        If True, include the octave number\n\n    cents: bool\n        If true, cent markers will be appended for fractional notes.\n        Eg, `midi_to_note(69.3, cents=True)` == `A4+03`\n\n    Returns\n    -------\n    notes : str or iterable of str\n        Strings describing each midi note.\n\n    Raises\n    ------\n    ParameterError\n        if `cents` is True and `octave` is False\n\n    See Also\n    --------\n    midi_to_hz\n    note_to_midi\n    hz_to_note\n    '''\n\n    if cents and not octave:\n        raise ParameterError('Cannot encode cents without octave information.')\n\n    if not np.isscalar(midi):\n        return [midi_to_note(x, octave=octave, cents=cents) for x in midi]\n\n    note_map = ['C', 'C#', 'D', 'D#',\n                'E', 'F', 'F#', 'G',\n                'G#', 'A', 'A#', 'B']\n\n    note_num = int(np.round(midi))\n    note_cents = int(100 * np.around(midi - note_num, 2))\n\n    note = note_map[note_num % 12]\n\n    if octave:\n        note = '{:s}{:0d}'.format(note, int(note_num \/ 12) - 1)\n    if cents:\n        note = '{:s}{:+02d}'.format(note, note_cents)\n\n    return note","method_path":"https:\/\/github.com\/librosa\/librosa\/blob\/180e8e6eb8f958fa6b20b8cba389f7945d508247\/librosa\/core\/time_frequency.py#L407-L478"}
{"repo_name":"open-mmlab\/mmcv","method_name":"frames2video","method_code":"def frames2video(frame_dir,\n                 video_file,\n                 fps=30,\n                 fourcc='XVID',\n                 filename_tmpl='{:06d}.jpg',\n                 start=0,\n                 end=0,\n                 show_progress=True):\n    \"\"\"\"\"\"\n    if end == 0:\n        ext = filename_tmpl.split('.')[-1]\n        end = len([name for name in scandir(frame_dir, ext)])\n    first_file = osp.join(frame_dir, filename_tmpl.format(start))\n    check_file_exist(first_file, 'The start frame not found: ' + first_file)\n    img = cv2.imread(first_file)\n    height, width = img.shape[:2]\n    resolution = (width, height)\n    vwriter = cv2.VideoWriter(video_file, VideoWriter_fourcc(*fourcc), fps,\n                              resolution)\n\n    def write_frame(file_idx):\n        filename = osp.join(frame_dir, filename_tmpl.format(file_idx))\n        img = cv2.imread(filename)\n        vwriter.write(img)\n\n    if show_progress:\n        track_progress(write_frame, range(start, end))\n    else:\n        for i in range(start, end):\n            filename = osp.join(frame_dir, filename_tmpl.format(i))\n            img = cv2.imread(filename)\n            vwriter.write(img)\n    vwriter.release()","method_summary":"Read the frame images from a directory and join them as a video","original_method_code":"def frames2video(frame_dir,\n                 video_file,\n                 fps=30,\n                 fourcc='XVID',\n                 filename_tmpl='{:06d}.jpg',\n                 start=0,\n                 end=0,\n                 show_progress=True):\n    \"\"\"Read the frame images from a directory and join them as a video\n\n    Args:\n        frame_dir (str): The directory containing video frames.\n        video_file (str): Output filename.\n        fps (float): FPS of the output video.\n        fourcc (str): Fourcc of the output video, this should be compatible\n            with the output file type.\n        filename_tmpl (str): Filename template with the index as the variable.\n        start (int): Starting frame index.\n        end (int): Ending frame index.\n        show_progress (bool): Whether to show a progress bar.\n    \"\"\"\n    if end == 0:\n        ext = filename_tmpl.split('.')[-1]\n        end = len([name for name in scandir(frame_dir, ext)])\n    first_file = osp.join(frame_dir, filename_tmpl.format(start))\n    check_file_exist(first_file, 'The start frame not found: ' + first_file)\n    img = cv2.imread(first_file)\n    height, width = img.shape[:2]\n    resolution = (width, height)\n    vwriter = cv2.VideoWriter(video_file, VideoWriter_fourcc(*fourcc), fps,\n                              resolution)\n\n    def write_frame(file_idx):\n        filename = osp.join(frame_dir, filename_tmpl.format(file_idx))\n        img = cv2.imread(filename)\n        vwriter.write(img)\n\n    if show_progress:\n        track_progress(write_frame, range(start, end))\n    else:\n        for i in range(start, end):\n            filename = osp.join(frame_dir, filename_tmpl.format(i))\n            img = cv2.imread(filename)\n            vwriter.write(img)\n    vwriter.release()","method_path":"https:\/\/github.com\/open-mmlab\/mmcv\/blob\/0d77f61450aab4dde8b8585a577cc496acb95d7f\/mmcv\/video\/io.py#L288-L332"}
{"repo_name":"open-mmlab\/mmcv","method_name":"VideoReader.read","method_code":"def read(self):\n        \"\"\"\"\"\"\n        \n        if self._cache:\n            img = self._cache.get(self._position)\n            if img is not None:\n                ret = True\n            else:\n                if self._position != self._get_real_position():\n                    self._set_real_position(self._position)\n                ret, img = self._vcap.read()\n                if ret:\n                    self._cache.put(self._position, img)\n        else:\n            ret, img = self._vcap.read()\n        if ret:\n            self._position += 1\n        return img","method_summary":"Read the next frame. If the next frame have been decoded before and in the cache, then return it directly, otherwise decode, cache and return it.","original_method_code":"def read(self):\n        \"\"\"Read the next frame.\n\n        If the next frame have been decoded before and in the cache, then\n        return it directly, otherwise decode, cache and return it.\n\n        Returns:\n            ndarray or None: Return the frame if successful, otherwise None.\n        \"\"\"\n        # pos = self._position\n        if self._cache:\n            img = self._cache.get(self._position)\n            if img is not None:\n                ret = True\n            else:\n                if self._position != self._get_real_position():\n                    self._set_real_position(self._position)\n                ret, img = self._vcap.read()\n                if ret:\n                    self._cache.put(self._position, img)\n        else:\n            ret, img = self._vcap.read()\n        if ret:\n            self._position += 1\n        return img","method_path":"https:\/\/github.com\/open-mmlab\/mmcv\/blob\/0d77f61450aab4dde8b8585a577cc496acb95d7f\/mmcv\/video\/io.py#L142-L166"}
{"repo_name":"open-mmlab\/mmcv","method_name":"VideoReader.get_frame","method_code":"def get_frame(self, frame_id):\n        \"\"\"\"\"\"\n        if frame_id < 0 or frame_id >= self._frame_cnt:\n            raise IndexError(\n                '\"frame_id\" must be between 0 and {}'.format(self._frame_cnt -\n                                                             1))\n        if frame_id == self._position:\n            return self.read()\n        if self._cache:\n            img = self._cache.get(frame_id)\n            if img is not None:\n                self._position = frame_id + 1\n                return img\n        self._set_real_position(frame_id)\n        ret, img = self._vcap.read()\n        if ret:\n            if self._cache:\n                self._cache.put(self._position, img)\n            self._position += 1\n        return img","method_summary":"Get frame by index.","original_method_code":"def get_frame(self, frame_id):\n        \"\"\"Get frame by index.\n\n        Args:\n            frame_id (int): Index of the expected frame, 0-based.\n\n        Returns:\n            ndarray or None: Return the frame if successful, otherwise None.\n        \"\"\"\n        if frame_id < 0 or frame_id >= self._frame_cnt:\n            raise IndexError(\n                '\"frame_id\" must be between 0 and {}'.format(self._frame_cnt -\n                                                             1))\n        if frame_id == self._position:\n            return self.read()\n        if self._cache:\n            img = self._cache.get(frame_id)\n            if img is not None:\n                self._position = frame_id + 1\n                return img\n        self._set_real_position(frame_id)\n        ret, img = self._vcap.read()\n        if ret:\n            if self._cache:\n                self._cache.put(self._position, img)\n            self._position += 1\n        return img","method_path":"https:\/\/github.com\/open-mmlab\/mmcv\/blob\/0d77f61450aab4dde8b8585a577cc496acb95d7f\/mmcv\/video\/io.py#L168-L194"}
{"repo_name":"open-mmlab\/mmcv","method_name":"VideoReader.cvt2frames","method_code":"def cvt2frames(self,\n                   frame_dir,\n                   file_start=0,\n                   filename_tmpl='{:06d}.jpg',\n                   start=0,\n                   max_num=0,\n                   show_progress=True):\n        \"\"\"\"\"\"\n        mkdir_or_exist(frame_dir)\n        if max_num == 0:\n            task_num = self.frame_cnt - start\n        else:\n            task_num = min(self.frame_cnt - start, max_num)\n        if task_num <= 0:\n            raise ValueError('start must be less than total frame number')\n        if start > 0:\n            self._set_real_position(start)\n\n        def write_frame(file_idx):\n            img = self.read()\n            filename = osp.join(frame_dir, filename_tmpl.format(file_idx))\n            cv2.imwrite(filename, img)\n\n        if show_progress:\n            track_progress(write_frame, range(file_start,\n                                              file_start + task_num))\n        else:\n            for i in range(task_num):\n                img = self.read()\n                if img is None:\n                    break\n                filename = osp.join(frame_dir,\n                                    filename_tmpl.format(i + file_start))\n                cv2.imwrite(filename, img)","method_summary":"Convert a video to frame images","original_method_code":"def cvt2frames(self,\n                   frame_dir,\n                   file_start=0,\n                   filename_tmpl='{:06d}.jpg',\n                   start=0,\n                   max_num=0,\n                   show_progress=True):\n        \"\"\"Convert a video to frame images\n\n        Args:\n            frame_dir (str): Output directory to store all the frame images.\n            file_start (int): Filenames will start from the specified number.\n            filename_tmpl (str): Filename template with the index as the\n                placeholder.\n            start (int): The starting frame index.\n            max_num (int): Maximum number of frames to be written.\n            show_progress (bool): Whether to show a progress bar.\n        \"\"\"\n        mkdir_or_exist(frame_dir)\n        if max_num == 0:\n            task_num = self.frame_cnt - start\n        else:\n            task_num = min(self.frame_cnt - start, max_num)\n        if task_num <= 0:\n            raise ValueError('start must be less than total frame number')\n        if start > 0:\n            self._set_real_position(start)\n\n        def write_frame(file_idx):\n            img = self.read()\n            filename = osp.join(frame_dir, filename_tmpl.format(file_idx))\n            cv2.imwrite(filename, img)\n\n        if show_progress:\n            track_progress(write_frame, range(file_start,\n                                              file_start + task_num))\n        else:\n            for i in range(task_num):\n                img = self.read()\n                if img is None:\n                    break\n                filename = osp.join(frame_dir,\n                                    filename_tmpl.format(i + file_start))\n                cv2.imwrite(filename, img)","method_path":"https:\/\/github.com\/open-mmlab\/mmcv\/blob\/0d77f61450aab4dde8b8585a577cc496acb95d7f\/mmcv\/video\/io.py#L207-L250"}
{"repo_name":"open-mmlab\/mmcv","method_name":"track_progress","method_code":"def track_progress(func, tasks, bar_width=50, **kwargs):\n    \"\"\"\"\"\"\n    if isinstance(tasks, tuple):\n        assert len(tasks) == 2\n        assert isinstance(tasks[0], collections_abc.Iterable)\n        assert isinstance(tasks[1], int)\n        task_num = tasks[1]\n        tasks = tasks[0]\n    elif isinstance(tasks, collections_abc.Iterable):\n        task_num = len(tasks)\n    else:\n        raise TypeError(\n            '\"tasks\" must be an iterable object or a (iterator, int) tuple')\n    prog_bar = ProgressBar(task_num, bar_width)\n    results = []\n    for task in tasks:\n        results.append(func(task, **kwargs))\n        prog_bar.update()\n    sys.stdout.write('\\n')\n    return results","method_summary":"Track the progress of tasks execution with a progress bar. Tasks are done with a simple for-loop.","original_method_code":"def track_progress(func, tasks, bar_width=50, **kwargs):\n    \"\"\"Track the progress of tasks execution with a progress bar.\n\n    Tasks are done with a simple for-loop.\n\n    Args:\n        func (callable): The function to be applied to each task.\n        tasks (list or tuple[Iterable, int]): A list of tasks or\n            (tasks, total num).\n        bar_width (int): Width of progress bar.\n\n    Returns:\n        list: The task results.\n    \"\"\"\n    if isinstance(tasks, tuple):\n        assert len(tasks) == 2\n        assert isinstance(tasks[0], collections_abc.Iterable)\n        assert isinstance(tasks[1], int)\n        task_num = tasks[1]\n        tasks = tasks[0]\n    elif isinstance(tasks, collections_abc.Iterable):\n        task_num = len(tasks)\n    else:\n        raise TypeError(\n            '\"tasks\" must be an iterable object or a (iterator, int) tuple')\n    prog_bar = ProgressBar(task_num, bar_width)\n    results = []\n    for task in tasks:\n        results.append(func(task, **kwargs))\n        prog_bar.update()\n    sys.stdout.write('\\n')\n    return results","method_path":"https:\/\/github.com\/open-mmlab\/mmcv\/blob\/0d77f61450aab4dde8b8585a577cc496acb95d7f\/mmcv\/utils\/progressbar.py#L63-L94"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imflip","method_code":"def imflip(img, direction='horizontal'):\n    \"\"\"\"\"\"\n    assert direction in ['horizontal', 'vertical']\n    if direction == 'horizontal':\n        return np.flip(img, axis=1)\n    else:\n        return np.flip(img, axis=0)","method_summary":"Flip an image horizontally or vertically.","original_method_code":"def imflip(img, direction='horizontal'):\n    \"\"\"Flip an image horizontally or vertically.\n\n    Args:\n        img (ndarray): Image to be flipped.\n        direction (str): The flip direction, either \"horizontal\" or \"vertical\".\n\n    Returns:\n        ndarray: The flipped image.\n    \"\"\"\n    assert direction in ['horizontal', 'vertical']\n    if direction == 'horizontal':\n        return np.flip(img, axis=1)\n    else:\n        return np.flip(img, axis=0)","method_path":"https:\/\/github.com\/open-mmlab\/mmcv\/blob\/0d77f61450aab4dde8b8585a577cc496acb95d7f\/mmcv\/image\/transforms\/geometry.py#L7-L21"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imrotate","method_code":"def imrotate(img,\n             angle,\n             center=None,\n             scale=1.0,\n             border_value=0,\n             auto_bound=False):\n    \"\"\"\"\"\"\n    if center is not None and auto_bound:\n        raise ValueError('`auto_bound` conflicts with `center`')\n    h, w = img.shape[:2]\n    if center is None:\n        center = ((w - 1) * 0.5, (h - 1) * 0.5)\n    assert isinstance(center, tuple)\n\n    matrix = cv2.getRotationMatrix2D(center, -angle, scale)\n    if auto_bound:\n        cos = np.abs(matrix[0, 0])\n        sin = np.abs(matrix[0, 1])\n        new_w = h * sin + w * cos\n        new_h = h * cos + w * sin\n        matrix[0, 2] += (new_w - w) * 0.5\n        matrix[1, 2] += (new_h - h) * 0.5\n        w = int(np.round(new_w))\n        h = int(np.round(new_h))\n    rotated = cv2.warpAffine(img, matrix, (w, h), borderValue=border_value)\n    return rotated","method_summary":"Rotate an image.","original_method_code":"def imrotate(img,\n             angle,\n             center=None,\n             scale=1.0,\n             border_value=0,\n             auto_bound=False):\n    \"\"\"Rotate an image.\n\n    Args:\n        img (ndarray): Image to be rotated.\n        angle (float): Rotation angle in degrees, positive values mean\n            clockwise rotation.\n        center (tuple): Center of the rotation in the source image, by default\n            it is the center of the image.\n        scale (float): Isotropic scale factor.\n        border_value (int): Border value.\n        auto_bound (bool): Whether to adjust the image size to cover the whole\n            rotated image.\n\n    Returns:\n        ndarray: The rotated image.\n    \"\"\"\n    if center is not None and auto_bound:\n        raise ValueError('`auto_bound` conflicts with `center`')\n    h, w = img.shape[:2]\n    if center is None:\n        center = ((w - 1) * 0.5, (h - 1) * 0.5)\n    assert isinstance(center, tuple)\n\n    matrix = cv2.getRotationMatrix2D(center, -angle, scale)\n    if auto_bound:\n        cos = np.abs(matrix[0, 0])\n        sin = np.abs(matrix[0, 1])\n        new_w = h * sin + w * cos\n        new_h = h * cos + w * sin\n        matrix[0, 2] += (new_w - w) * 0.5\n        matrix[1, 2] += (new_h - h) * 0.5\n        w = int(np.round(new_w))\n        h = int(np.round(new_h))\n    rotated = cv2.warpAffine(img, matrix, (w, h), borderValue=border_value)\n    return rotated","method_path":"https:\/\/github.com\/open-mmlab\/mmcv\/blob\/0d77f61450aab4dde8b8585a577cc496acb95d7f\/mmcv\/image\/transforms\/geometry.py#L24-L64"}
{"repo_name":"open-mmlab\/mmcv","method_name":"bbox_clip","method_code":"def bbox_clip(bboxes, img_shape):\n    \"\"\"\"\"\"\n    assert bboxes.shape[-1] % 4 == 0\n    clipped_bboxes = np.empty_like(bboxes, dtype=bboxes.dtype)\n    clipped_bboxes[..., 0::2] = np.maximum(\n        np.minimum(bboxes[..., 0::2], img_shape[1] - 1), 0)\n    clipped_bboxes[..., 1::2] = np.maximum(\n        np.minimum(bboxes[..., 1::2], img_shape[0] - 1), 0)\n    return clipped_bboxes","method_summary":"Clip bboxes to fit the image shape.","original_method_code":"def bbox_clip(bboxes, img_shape):\n    \"\"\"Clip bboxes to fit the image shape.\n\n    Args:\n        bboxes (ndarray): Shape (..., 4*k)\n        img_shape (tuple): (height, width) of the image.\n\n    Returns:\n        ndarray: Clipped bboxes.\n    \"\"\"\n    assert bboxes.shape[-1] % 4 == 0\n    clipped_bboxes = np.empty_like(bboxes, dtype=bboxes.dtype)\n    clipped_bboxes[..., 0::2] = np.maximum(\n        np.minimum(bboxes[..., 0::2], img_shape[1] - 1), 0)\n    clipped_bboxes[..., 1::2] = np.maximum(\n        np.minimum(bboxes[..., 1::2], img_shape[0] - 1), 0)\n    return clipped_bboxes","method_path":"https:\/\/github.com\/open-mmlab\/mmcv\/blob\/0d77f61450aab4dde8b8585a577cc496acb95d7f\/mmcv\/image\/transforms\/geometry.py#L67-L83"}
{"repo_name":"open-mmlab\/mmcv","method_name":"bbox_scaling","method_code":"def bbox_scaling(bboxes, scale, clip_shape=None):\n    \"\"\"\"\"\"\n    if float(scale) == 1.0:\n        scaled_bboxes = bboxes.copy()\n    else:\n        w = bboxes[..., 2] - bboxes[..., 0] + 1\n        h = bboxes[..., 3] - bboxes[..., 1] + 1\n        dw = (w * (scale - 1)) * 0.5\n        dh = (h * (scale - 1)) * 0.5\n        scaled_bboxes = bboxes + np.stack((-dw, -dh, dw, dh), axis=-1)\n    if clip_shape is not None:\n        return bbox_clip(scaled_bboxes, clip_shape)\n    else:\n        return scaled_bboxes","method_summary":"Scaling bboxes w.r.t the box center.","original_method_code":"def bbox_scaling(bboxes, scale, clip_shape=None):\n    \"\"\"Scaling bboxes w.r.t the box center.\n\n    Args:\n        bboxes (ndarray): Shape(..., 4).\n        scale (float): Scaling factor.\n        clip_shape (tuple, optional): If specified, bboxes that exceed the\n            boundary will be clipped according to the given shape (h, w).\n\n    Returns:\n        ndarray: Scaled bboxes.\n    \"\"\"\n    if float(scale) == 1.0:\n        scaled_bboxes = bboxes.copy()\n    else:\n        w = bboxes[..., 2] - bboxes[..., 0] + 1\n        h = bboxes[..., 3] - bboxes[..., 1] + 1\n        dw = (w * (scale - 1)) * 0.5\n        dh = (h * (scale - 1)) * 0.5\n        scaled_bboxes = bboxes + np.stack((-dw, -dh, dw, dh), axis=-1)\n    if clip_shape is not None:\n        return bbox_clip(scaled_bboxes, clip_shape)\n    else:\n        return scaled_bboxes","method_path":"https:\/\/github.com\/open-mmlab\/mmcv\/blob\/0d77f61450aab4dde8b8585a577cc496acb95d7f\/mmcv\/image\/transforms\/geometry.py#L86-L109"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imcrop","method_code":"def imcrop(img, bboxes, scale=1.0, pad_fill=None):\n    \"\"\"\"\"\"\n    chn = 1 if img.ndim == 2 else img.shape[2]\n    if pad_fill is not None:\n        if isinstance(pad_fill, (int, float)):\n            pad_fill = [pad_fill for _ in range(chn)]\n        assert len(pad_fill) == chn\n\n    _bboxes = bboxes[None, ...] if bboxes.ndim == 1 else bboxes\n    scaled_bboxes = bbox_scaling(_bboxes, scale).astype(np.int32)\n    clipped_bbox = bbox_clip(scaled_bboxes, img.shape)\n\n    patches = []\n    for i in range(clipped_bbox.shape[0]):\n        x1, y1, x2, y2 = tuple(clipped_bbox[i, :])\n        if pad_fill is None:\n            patch = img[y1:y2 + 1, x1:x2 + 1, ...]\n        else:\n            _x1, _y1, _x2, _y2 = tuple(scaled_bboxes[i, :])\n            if chn == 2:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1)\n            else:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1, chn)\n            patch = np.array(\n                pad_fill, dtype=img.dtype) * np.ones(\n                    patch_shape, dtype=img.dtype)\n            x_start = 0 if _x1 >= 0 else -_x1\n            y_start = 0 if _y1 >= 0 else -_y1\n            w = x2 - x1 + 1\n            h = y2 - y1 + 1\n            patch[y_start:y_start + h, x_start:x_start +\n                  w, ...] = img[y1:y1 + h, x1:x1 + w, ...]\n        patches.append(patch)\n\n    if bboxes.ndim == 1:\n        return patches[0]\n    else:\n        return patches","method_summary":"Crop image patches. 3","original_method_code":"def imcrop(img, bboxes, scale=1.0, pad_fill=None):\n    \"\"\"Crop image patches.\n\n    3 steps: scale the bboxes -> clip bboxes -> crop and pad.\n\n    Args:\n        img (ndarray): Image to be cropped.\n        bboxes (ndarray): Shape (k, 4) or (4, ), location of cropped bboxes.\n        scale (float, optional): Scale ratio of bboxes, the default value\n            1.0 means no padding.\n        pad_fill (number or list): Value to be filled for padding, None for\n            no padding.\n\n    Returns:\n        list or ndarray: The cropped image patches.\n    \"\"\"\n    chn = 1 if img.ndim == 2 else img.shape[2]\n    if pad_fill is not None:\n        if isinstance(pad_fill, (int, float)):\n            pad_fill = [pad_fill for _ in range(chn)]\n        assert len(pad_fill) == chn\n\n    _bboxes = bboxes[None, ...] if bboxes.ndim == 1 else bboxes\n    scaled_bboxes = bbox_scaling(_bboxes, scale).astype(np.int32)\n    clipped_bbox = bbox_clip(scaled_bboxes, img.shape)\n\n    patches = []\n    for i in range(clipped_bbox.shape[0]):\n        x1, y1, x2, y2 = tuple(clipped_bbox[i, :])\n        if pad_fill is None:\n            patch = img[y1:y2 + 1, x1:x2 + 1, ...]\n        else:\n            _x1, _y1, _x2, _y2 = tuple(scaled_bboxes[i, :])\n            if chn == 2:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1)\n            else:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1, chn)\n            patch = np.array(\n                pad_fill, dtype=img.dtype) * np.ones(\n                    patch_shape, dtype=img.dtype)\n            x_start = 0 if _x1 >= 0 else -_x1\n            y_start = 0 if _y1 >= 0 else -_y1\n            w = x2 - x1 + 1\n            h = y2 - y1 + 1\n            patch[y_start:y_start + h, x_start:x_start +\n                  w, ...] = img[y1:y1 + h, x1:x1 + w, ...]\n        patches.append(patch)\n\n    if bboxes.ndim == 1:\n        return patches[0]\n    else:\n        return patches","method_path":"https:\/\/github.com\/open-mmlab\/mmcv\/blob\/0d77f61450aab4dde8b8585a577cc496acb95d7f\/mmcv\/image\/transforms\/geometry.py#L112-L163"}
{"repo_name":"Microsoft\/botbuilder-python","method_name":"MemoryStorage.__should_write_changes","method_code":"def __should_write_changes(self, old_value: StoreItem, new_value: StoreItem) -> bool:\n        \"\"\"\"\"\"\n\n        \n        if old_value is None or (hasattr(new_value, 'e_tag') and new_value.e_tag == '*'):\n            return True\n        \n        elif hasattr(new_value, 'e_tag') and hasattr(old_value, 'e_tag'):\n            if new_value.e_tag is not None and old_value.e_tag is None:\n                return True\n            \n            if old_value.e_tag == new_value.e_tag or int(old_value.e_tag) <= int(new_value.e_tag):\n                return True\n            else:\n                return False\n        else:\n            return False","method_summary":"Helper method that compares two StoreItems and their e_tags and returns True if the new_value should overwrite the old_value. Otherwise returns False.","original_method_code":"def __should_write_changes(self, old_value: StoreItem, new_value: StoreItem) -> bool:\n        \"\"\"\n        Helper method that compares two StoreItems and their e_tags and returns True if the new_value should overwrite\n        the old_value. Otherwise returns False.\n        :param old_value:\n        :param new_value:\n        :return:\n        \"\"\"\n\n        # If old_value is none or if the new_value's e_tag is '*', then we return True\n        if old_value is None or (hasattr(new_value, 'e_tag') and new_value.e_tag == '*'):\n            return True\n        # If none of the above cases, we verify that e_tags exist on both arguments\n        elif hasattr(new_value, 'e_tag') and hasattr(old_value, 'e_tag'):\n            if new_value.e_tag is not None and old_value.e_tag is None:\n                return True\n            # And then we do a comparing between the old and new e_tag values to decide if the new data will be written\n            if old_value.e_tag == new_value.e_tag or int(old_value.e_tag) <= int(new_value.e_tag):\n                return True\n            else:\n                return False\n        else:\n            return False","method_path":"https:\/\/github.com\/Microsoft\/botbuilder-python\/blob\/274663dd91c811bae6ac4488915ba5880771b0a7\/libraries\/botbuilder-core\/botbuilder\/core\/memory_storage.py#L68-L90"}
{"repo_name":"Microsoft\/botbuilder-python","method_name":"BotAdapter.run_middleware","method_code":"async def run_middleware(self, context: TurnContext, callback: Callable=None):\n        \"\"\"\"\"\"\n        return await self._middleware.receive_activity_with_status(context, callback)","method_summary":"Called by the parent class to run the adapters middleware set and calls the passed in `callback()` handler at the end of the chain.","original_method_code":"async def run_middleware(self, context: TurnContext, callback: Callable=None):\n        \"\"\"\n        Called by the parent class to run the adapters middleware set and calls the passed in `callback()` handler at\n        the end of the chain.\n        :param context:\n        :param callback:\n        :return:\n        \"\"\"\n        return await self._middleware.receive_activity_with_status(context, callback)","method_path":"https:\/\/github.com\/Microsoft\/botbuilder-python\/blob\/274663dd91c811bae6ac4488915ba5880771b0a7\/libraries\/botbuilder-core\/botbuilder\/core\/bot_adapter.py#L51-L59"}
{"repo_name":"Microsoft\/botbuilder-python","method_name":"MiddlewareSet.use","method_code":"def use(self, *middleware: Middleware):\n        \"\"\"\"\"\"\n        for (idx, m) in enumerate(middleware):\n            if hasattr(m, 'on_process_request') and callable(m.on_process_request):\n                self._middleware.append(m)\n                return self\n            else:\n                raise TypeError('MiddlewareSet.use(): invalid middleware at index \"%s\" being added.' % idx)","method_summary":"Registers middleware plugin(s) with the bot or set.","original_method_code":"def use(self, *middleware: Middleware):\n        \"\"\"\n        Registers middleware plugin(s) with the bot or set.\n        :param middleware :\n        :return:\n        \"\"\"\n        for (idx, m) in enumerate(middleware):\n            if hasattr(m, 'on_process_request') and callable(m.on_process_request):\n                self._middleware.append(m)\n                return self\n            else:\n                raise TypeError('MiddlewareSet.use(): invalid middleware at index \"%s\" being added.' % idx)","method_path":"https:\/\/github.com\/Microsoft\/botbuilder-python\/blob\/274663dd91c811bae6ac4488915ba5880771b0a7\/libraries\/botbuilder-core\/botbuilder\/core\/middleware_set.py#L35-L46"}
{"repo_name":"Microsoft\/botbuilder-python","method_name":"ApplicationInsightsTelemetryClient.track_pageview","method_code":"def track_pageview(self, name: str, url:str, duration: int = 0, properties : Dict[str, object]=None, \n                        measurements: Dict[str, object]=None) -> None:\n        \"\"\"\"\"\"\n        self._client.track_pageview(name, url, duration, properties, measurements)","method_summary":"Send information about the page viewed in the application (a web page for instance).","original_method_code":"def track_pageview(self, name: str, url:str, duration: int = 0, properties : Dict[str, object]=None, \n                        measurements: Dict[str, object]=None) -> None:\n        \"\"\"\n        Send information about the page viewed in the application (a web page for instance).\n        :param name: the name of the page that was viewed.\n        :param url: the URL of the page that was viewed.\n        :param duration: the duration of the page view in milliseconds. (defaults to: 0)\n        :param properties: the set of custom properties the client wants attached to this data item. (defaults to: None)\n        :param measurements: the set of custom measurements the client wants to attach to this data item. (defaults to: None)\n        \"\"\"\n        self._client.track_pageview(name, url, duration, properties, measurements)","method_path":"https:\/\/github.com\/Microsoft\/botbuilder-python\/blob\/274663dd91c811bae6ac4488915ba5880771b0a7\/libraries\/botbuilder-applicationinsights\/botbuilder\/applicationinsights\/application_insights_telemetry_client.py#L38-L48"}
{"repo_name":"Microsoft\/botbuilder-python","method_name":"ApplicationInsightsTelemetryClient.track_exception","method_code":"def track_exception(self, type_exception: type = None, value : Exception =None, tb : traceback =None, \n                        properties: Dict[str, object]=None, measurements: Dict[str, object]=None) -> None:\n        \"\"\"\"\"\"\n        self._client.track_exception(type_exception, value, tb, properties, measurements)","method_summary":"Send information about a single exception that occurred in the application.","original_method_code":"def track_exception(self, type_exception: type = None, value : Exception =None, tb : traceback =None, \n                        properties: Dict[str, object]=None, measurements: Dict[str, object]=None) -> None:\n        \"\"\" \n        Send information about a single exception that occurred in the application.\n        :param type_exception: the type of the exception that was thrown.\n        :param value: the exception that the client wants to send.\n        :param tb: the traceback information as returned by :func:`sys.exc_info`.\n        :param properties: the set of custom properties the client wants attached to this data item. (defaults to: None)\n        :param measurements: the set of custom measurements the client wants to attach to this data item. (defaults to: None)\n        \"\"\"\n        self._client.track_exception(type_exception, value, tb, properties, measurements)","method_path":"https:\/\/github.com\/Microsoft\/botbuilder-python\/blob\/274663dd91c811bae6ac4488915ba5880771b0a7\/libraries\/botbuilder-applicationinsights\/botbuilder\/applicationinsights\/application_insights_telemetry_client.py#L50-L60"}
{"repo_name":"Microsoft\/botbuilder-python","method_name":"ApplicationInsightsTelemetryClient.track_event","method_code":"def track_event(self, name: str, properties: Dict[str, object] = None, \n                    measurements: Dict[str, object] = None) -> None:\n        \"\"\"\"\"\"\n        self._client.track_event(name, properties, measurements)","method_summary":"Send information about a single event that has occurred in the context of the application.","original_method_code":"def track_event(self, name: str, properties: Dict[str, object] = None, \n                    measurements: Dict[str, object] = None) -> None:\n        \"\"\" \n        Send information about a single event that has occurred in the context of the application.\n        :param name: the data to associate to this event.\n        :param properties: the set of custom properties the client wants attached to this data item. (defaults to: None)\n        :param measurements: the set of custom measurements the client wants to attach to this data item. (defaults to: None)\n        \"\"\"\n        self._client.track_event(name, properties, measurements)","method_path":"https:\/\/github.com\/Microsoft\/botbuilder-python\/blob\/274663dd91c811bae6ac4488915ba5880771b0a7\/libraries\/botbuilder-applicationinsights\/botbuilder\/applicationinsights\/application_insights_telemetry_client.py#L62-L70"}
{"repo_name":"Microsoft\/botbuilder-python","method_name":"ApplicationInsightsTelemetryClient.track_metric","method_code":"def track_metric(self, name: str, value: float, type: TelemetryDataPointType =None, \n                    count: int =None, min: float=None, max: float=None, std_dev: float=None,\n                    properties: Dict[str, object]=None) -> NotImplemented:\n        \"\"\"\"\"\"\n        self._client.track_metric(name, value, type, count, min, max, std_dev, properties)","method_summary":"Send information about a single metric data point that was captured for the application.","original_method_code":"def track_metric(self, name: str, value: float, type: TelemetryDataPointType =None, \n                    count: int =None, min: float=None, max: float=None, std_dev: float=None,\n                    properties: Dict[str, object]=None) -> NotImplemented:\n        \"\"\"\n        Send information about a single metric data point that was captured for the application.\n        :param name: The name of the metric that was captured.\n        :param value: The value of the metric that was captured.\n        :param type: The type of the metric. (defaults to: TelemetryDataPointType.aggregation`)\n        :param count: the number of metrics that were aggregated into this data point. (defaults to: None)\n        :param min: the minimum of all metrics collected that were aggregated into this data point. (defaults to: None)\n        :param max: the maximum of all metrics collected that were aggregated into this data point. (defaults to: None)\n        :param std_dev: the standard deviation of all metrics collected that were aggregated into this data point. (defaults to: None)\n        :param properties: the set of custom properties the client wants attached to this data item. (defaults to: None)\n        \"\"\"\n        self._client.track_metric(name, value, type, count, min, max, std_dev, properties)","method_path":"https:\/\/github.com\/Microsoft\/botbuilder-python\/blob\/274663dd91c811bae6ac4488915ba5880771b0a7\/libraries\/botbuilder-applicationinsights\/botbuilder\/applicationinsights\/application_insights_telemetry_client.py#L72-L86"}
{"repo_name":"Microsoft\/botbuilder-python","method_name":"ApplicationInsightsTelemetryClient.track_trace","method_code":"def track_trace(self, name: str, properties: Dict[str, object]=None, severity=None):\n        \"\"\"\"\"\"\n        self._client.track_trace(name, properties, severity)","method_summary":"Sends a single trace statement.","original_method_code":"def track_trace(self, name: str, properties: Dict[str, object]=None, severity=None):\n        \"\"\"\n        Sends a single trace statement.\n        :param name: the trace statement.\\n\n        :param properties: the set of custom properties the client wants attached to this data item. (defaults to: None)\\n\n        :param severity: the severity level of this trace, one of DEBUG, INFO, WARNING, ERROR, CRITICAL\n        \"\"\"\n        self._client.track_trace(name, properties, severity)","method_path":"https:\/\/github.com\/Microsoft\/botbuilder-python\/blob\/274663dd91c811bae6ac4488915ba5880771b0a7\/libraries\/botbuilder-applicationinsights\/botbuilder\/applicationinsights\/application_insights_telemetry_client.py#L88-L95"}
{"repo_name":"Microsoft\/botbuilder-python","method_name":"ApplicationInsightsTelemetryClient.track_request","method_code":"def track_request(self, name: str, url: str, success: bool, start_time: str=None, \n                    duration: int=None, response_code: str =None, http_method: str=None, \n                    properties: Dict[str, object]=None, measurements: Dict[str, object]=None, \n                    request_id: str=None):\n        \"\"\"\"\"\"\n        self._client.track_request(name, url, success, start_time, duration, response_code, http_method, properties,\n                                    measurements, request_id)","method_summary":"Sends a single request that was captured for the application.","original_method_code":"def track_request(self, name: str, url: str, success: bool, start_time: str=None, \n                    duration: int=None, response_code: str =None, http_method: str=None, \n                    properties: Dict[str, object]=None, measurements: Dict[str, object]=None, \n                    request_id: str=None):\n        \"\"\"\n        Sends a single request that was captured for the application.\n        :param name: The name for this request. All requests with the same name will be grouped together.\n        :param url: The actual URL for this request (to show in individual request instances).\n        :param success: True if the request ended in success, False otherwise.\n        :param start_time: the start time of the request. The value should look the same as the one returned by :func:`datetime.isoformat()` (defaults to: None)\n        :param duration: the number of milliseconds that this request lasted. (defaults to: None)\n        :param response_code: the response code that this request returned. (defaults to: None)\n        :param http_method: the HTTP method that triggered this request. (defaults to: None)\n        :param properties: the set of custom properties the client wants attached to this data item. (defaults to: None)\n        :param measurements: the set of custom measurements the client wants to attach to this data item. (defaults to: None)\n        :param request_id: the id for this request. If None, a new uuid will be generated. (defaults to: None)\n        \"\"\"\n        self._client.track_request(name, url, success, start_time, duration, response_code, http_method, properties,\n                                    measurements, request_id)","method_path":"https:\/\/github.com\/Microsoft\/botbuilder-python\/blob\/274663dd91c811bae6ac4488915ba5880771b0a7\/libraries\/botbuilder-applicationinsights\/botbuilder\/applicationinsights\/application_insights_telemetry_client.py#L97-L115"}
{"repo_name":"Microsoft\/botbuilder-python","method_name":"ApplicationInsightsTelemetryClient.track_dependency","method_code":"def track_dependency(self, name:str, data:str, type:str=None, target:str=None, duration:int=None, \n                        success:bool=None, result_code:str=None, properties:Dict[str, object]=None, \n                        measurements:Dict[str, object]=None, dependency_id:str=None):\n        \"\"\"\"\"\"\n        self._client.track_dependency(name, data, type, target, duration, success, result_code, properties,\n                                        measurements, dependency_id)","method_summary":"Sends a single dependency telemetry that was captured for the application.","original_method_code":"def track_dependency(self, name:str, data:str, type:str=None, target:str=None, duration:int=None, \n                        success:bool=None, result_code:str=None, properties:Dict[str, object]=None, \n                        measurements:Dict[str, object]=None, dependency_id:str=None):\n        \"\"\"\n        Sends a single dependency telemetry that was captured for the application.\n        :param name: the name of the command initiated with this dependency call. Low cardinality value. Examples are stored procedure name and URL path template.\n        :param data: the command initiated by this dependency call. Examples are SQL statement and HTTP URL with all query parameters.\n        :param type: the dependency type name. Low cardinality value for logical grouping of dependencies and interpretation of other fields like commandName and resultCode. Examples are SQL, Azure table, and HTTP. (default to: None)\n        :param target: the target site of a dependency call. Examples are server name, host address. (default to: None)\n        :param duration: the number of milliseconds that this dependency call lasted. (defaults to: None)\n        :param success: true if the dependency call ended in success, false otherwise. (defaults to: None)\n        :param result_code: the result code of a dependency call. Examples are SQL error code and HTTP status code. (defaults to: None)\n        :param properties: the set of custom properties the client wants attached to this data item. (defaults to: None)\n        :param measurements: the set of custom measurements the client wants to attach to this data item. (defaults to: None)\n        :param id: the id for this dependency call. If None, a new uuid will be generated. (defaults to: None)\n        \"\"\"\n        self._client.track_dependency(name, data, type, target, duration, success, result_code, properties,\n                                        measurements, dependency_id)","method_path":"https:\/\/github.com\/Microsoft\/botbuilder-python\/blob\/274663dd91c811bae6ac4488915ba5880771b0a7\/libraries\/botbuilder-applicationinsights\/botbuilder\/applicationinsights\/application_insights_telemetry_client.py#L117-L134"}
{"repo_name":"Qiskit\/qiskit-terra","method_name":"Instruction.params","method_code":"def params(self):\n        \"\"\"\"\"\"\n        \n        if self._definition and not self._params:\n            self._params = []\n            for sub_instr, _, _ in self._definition:\n                self._params.extend(sub_instr.params)  \n            return self._params\n        else:\n            return self._params","method_summary":"return instruction params","original_method_code":"def params(self):\n        \"\"\"return instruction params\"\"\"\n        # if params already defined don't attempt to get them from definition\n        if self._definition and not self._params:\n            self._params = []\n            for sub_instr, _, _ in self._definition:\n                self._params.extend(sub_instr.params)  # recursive call\n            return self._params\n        else:\n            return self._params","method_path":"https:\/\/github.com\/Qiskit\/qiskit-terra\/blob\/d4f58d903bc96341b816f7c35df936d6421267d1\/qiskit\/circuit\/instruction.py#L112-L121"}
{"repo_name":"Qiskit\/qiskit-terra","method_name":"Instruction.assemble","method_code":"def assemble(self):\n        \"\"\"\"\"\"\n        instruction = QasmQobjInstruction(name=self.name)\n        \n        if self.params:\n            params = [\n                x.evalf() if hasattr(x, 'evalf') else x for x in self.params\n            ]\n            params = [\n                sympy.matrix2numpy(x, dtype=complex) if isinstance(\n                    x, sympy.Matrix) else x for x in params\n            ]\n            instruction.params = params\n        \n        if self.num_qubits:\n            instruction.qubits = list(range(self.num_qubits))\n        if self.num_clbits:\n            instruction.memory = list(range(self.num_clbits))\n        \n        \n        \n        if self.control:\n            instruction._control = self.control\n        return instruction","method_summary":"Assemble a QasmQobjInstruction","original_method_code":"def assemble(self):\n        \"\"\"Assemble a QasmQobjInstruction\"\"\"\n        instruction = QasmQobjInstruction(name=self.name)\n        # Evaluate parameters\n        if self.params:\n            params = [\n                x.evalf() if hasattr(x, 'evalf') else x for x in self.params\n            ]\n            params = [\n                sympy.matrix2numpy(x, dtype=complex) if isinstance(\n                    x, sympy.Matrix) else x for x in params\n            ]\n            instruction.params = params\n        # Add placeholder for qarg and carg params\n        if self.num_qubits:\n            instruction.qubits = list(range(self.num_qubits))\n        if self.num_clbits:\n            instruction.memory = list(range(self.num_clbits))\n        # Add control parameters for assembler. This is needed to convert\n        # to a qobj conditional instruction at assemble time and after\n        # conversion will be deleted by the assembler.\n        if self.control:\n            instruction._control = self.control\n        return instruction","method_path":"https:\/\/github.com\/Qiskit\/qiskit-terra\/blob\/d4f58d903bc96341b816f7c35df936d6421267d1\/qiskit\/circuit\/instruction.py#L169-L192"}
{"repo_name":"Qiskit\/qiskit-terra","method_name":"Instruction.mirror","method_code":"def mirror(self):\n        \"\"\"\"\"\"\n        if not self._definition:\n            return self.copy()\n\n        reverse_inst = self.copy(name=self.name + '_mirror')\n        reverse_inst.definition = []\n        for inst, qargs, cargs in reversed(self._definition):\n            reverse_inst._definition.append((inst.mirror(), qargs, cargs))\n        return reverse_inst","method_summary":"For a composite instruction, reverse the order of sub-gates. This is done by recursively mirroring all sub-instructions. It does not invert any gate.","original_method_code":"def mirror(self):\n        \"\"\"For a composite instruction, reverse the order of sub-gates.\n\n        This is done by recursively mirroring all sub-instructions.\n        It does not invert any gate.\n\n        Returns:\n            Instruction: a fresh gate with sub-gates reversed\n        \"\"\"\n        if not self._definition:\n            return self.copy()\n\n        reverse_inst = self.copy(name=self.name + '_mirror')\n        reverse_inst.definition = []\n        for inst, qargs, cargs in reversed(self._definition):\n            reverse_inst._definition.append((inst.mirror(), qargs, cargs))\n        return reverse_inst","method_path":"https:\/\/github.com\/Qiskit\/qiskit-terra\/blob\/d4f58d903bc96341b816f7c35df936d6421267d1\/qiskit\/circuit\/instruction.py#L194-L210"}
{"repo_name":"Qiskit\/qiskit-terra","method_name":"Instruction.c_if","method_code":"def c_if(self, classical, val):\n        \"\"\"\"\"\"\n        if not isinstance(classical, ClassicalRegister):\n            raise QiskitError(\"c_if must be used with a classical register\")\n        if val < 0:\n            raise QiskitError(\"control value should be non-negative\")\n        self.control = (classical, val)\n        return self","method_summary":"Add classical control on register classical and value val.","original_method_code":"def c_if(self, classical, val):\n        \"\"\"Add classical control on register classical and value val.\"\"\"\n        if not isinstance(classical, ClassicalRegister):\n            raise QiskitError(\"c_if must be used with a classical register\")\n        if val < 0:\n            raise QiskitError(\"control value should be non-negative\")\n        self.control = (classical, val)\n        return self","method_path":"https:\/\/github.com\/Qiskit\/qiskit-terra\/blob\/d4f58d903bc96341b816f7c35df936d6421267d1\/qiskit\/circuit\/instruction.py#L236-L243"}
{"repo_name":"Qiskit\/qiskit-terra","method_name":"Instruction.copy","method_code":"def copy(self, name=None):\n        \"\"\"\"\"\"\n        cpy = copy.copy(self)\n        if name:\n            cpy.name = name\n        return cpy","method_summary":"shallow copy of the instruction.","original_method_code":"def copy(self, name=None):\n        \"\"\"\n        shallow copy of the instruction.\n\n        Args:\n          name (str): name to be given to the copied circuit,\n            if None then the name stays the same\n\n        Returns:\n          Instruction: a shallow copy of the current instruction, with the name\n            updated if it was provided\n        \"\"\"\n        cpy = copy.copy(self)\n        if name:\n            cpy.name = name\n        return cpy","method_path":"https:\/\/github.com\/Qiskit\/qiskit-terra\/blob\/d4f58d903bc96341b816f7c35df936d6421267d1\/qiskit\/circuit\/instruction.py#L245-L260"}
{"repo_name":"Qiskit\/qiskit-terra","method_name":"Instruction._qasmif","method_code":"def _qasmif(self, string):\n        \"\"\"\"\"\"\n        if self.control is None:\n            return string\n        return \"if(%s==%d) \" % (self.control[0].name, self.control[1]) + string","method_summary":"Print an if statement if needed.","original_method_code":"def _qasmif(self, string):\n        \"\"\"Print an if statement if needed.\"\"\"\n        if self.control is None:\n            return string\n        return \"if(%s==%d) \" % (self.control[0].name, self.control[1]) + string","method_path":"https:\/\/github.com\/Qiskit\/qiskit-terra\/blob\/d4f58d903bc96341b816f7c35df936d6421267d1\/qiskit\/circuit\/instruction.py#L262-L266"}
{"repo_name":"Qiskit\/qiskit-terra","method_name":"Instruction.qasm","method_code":"def qasm(self):\n        \"\"\"\"\"\"\n        name_param = self.name\n        if self.params:\n            name_param = \"%s(%s)\" % (name_param, \",\".join(\n                [str(i) for i in self.params]))\n\n        return self._qasmif(name_param)","method_summary":"Return a default OpenQASM string for the instruction. Derived instructions may override this to print in a different format (e.g. measure q[0] -> c[0];).","original_method_code":"def qasm(self):\n        \"\"\"Return a default OpenQASM string for the instruction.\n\n        Derived instructions may override this to print in a\n        different format (e.g. measure q[0] -> c[0];).\n        \"\"\"\n        name_param = self.name\n        if self.params:\n            name_param = \"%s(%s)\" % (name_param, \",\".join(\n                [str(i) for i in self.params]))\n\n        return self._qasmif(name_param)","method_path":"https:\/\/github.com\/Qiskit\/qiskit-terra\/blob\/d4f58d903bc96341b816f7c35df936d6421267d1\/qiskit\/circuit\/instruction.py#L268-L279"}
{"repo_name":"Qiskit\/qiskit-terra","method_name":"PassManager._join_options","method_code":"def _join_options(self, passset_options):\n        \"\"\"\"\"\"\n        default = {'ignore_preserves': False,  \n                   'ignore_requires': False,  \n                   'max_iteration': 1000}  \n\n        passmanager_level = {k: v for k, v in self.passmanager_options.items() if v is not None}\n        passset_level = {k: v for k, v in passset_options.items() if v is not None}\n        return {**default, **passmanager_level, **passset_level}","method_summary":"Set the options of each passset, based on precedence","original_method_code":"def _join_options(self, passset_options):\n        \"\"\"Set the options of each passset, based on precedence rules:\n        passset options (set via ``PassManager.append()``) override\n        passmanager options (set via ``PassManager.__init__()``), which override Default.\n        .\n        \"\"\"\n        default = {'ignore_preserves': False,  # Ignore preserves for this pass\n                   'ignore_requires': False,  # Ignore requires for this pass\n                   'max_iteration': 1000}  # Maximum allowed iteration on this pass\n\n        passmanager_level = {k: v for k, v in self.passmanager_options.items() if v is not None}\n        passset_level = {k: v for k, v in passset_options.items() if v is not None}\n        return {**default, **passmanager_level, **passset_level}","method_path":"https:\/\/github.com\/Qiskit\/qiskit-terra\/blob\/d4f58d903bc96341b816f7c35df936d6421267d1\/qiskit\/transpiler\/passmanager.py#L59-L71"}
{"repo_name":"Qiskit\/qiskit-terra","method_name":"PassManager.run","method_code":"def run(self, circuit):\n        \"\"\"\"\"\"\n        name = circuit.name\n        dag = circuit_to_dag(circuit)\n        del circuit\n        for passset in self.working_list:\n            for pass_ in passset:\n                dag = self._do_pass(pass_, dag, passset.options)\n        circuit = dag_to_circuit(dag)\n        circuit.name = name\n        return circuit","method_summary":"Run all the passes on a QuantumCircuit","original_method_code":"def run(self, circuit):\n        \"\"\"Run all the passes on a QuantumCircuit\n\n        Args:\n            circuit (QuantumCircuit): circuit to transform via all the registered passes\n\n        Returns:\n            QuantumCircuit: Transformed circuit.\n        \"\"\"\n        name = circuit.name\n        dag = circuit_to_dag(circuit)\n        del circuit\n        for passset in self.working_list:\n            for pass_ in passset:\n                dag = self._do_pass(pass_, dag, passset.options)\n        circuit = dag_to_circuit(dag)\n        circuit.name = name\n        return circuit","method_path":"https:\/\/github.com\/Qiskit\/qiskit-terra\/blob\/d4f58d903bc96341b816f7c35df936d6421267d1\/qiskit\/transpiler\/passmanager.py#L118-L135"}
{"repo_name":"Qiskit\/qiskit-terra","method_name":"PassManager._do_pass","method_code":"def _do_pass(self, pass_, dag, options):\n        \"\"\"\"\"\"\n\n        \n        if not options[\"ignore_requires\"]:\n            for required_pass in pass_.requires:\n                dag = self._do_pass(required_pass, dag, options)\n\n        \n        if pass_ not in self.valid_passes:\n            if pass_.is_transformation_pass:\n                pass_.property_set = self.fenced_property_set\n                new_dag = pass_.run(dag)\n                if not isinstance(new_dag, DAGCircuit):\n                    raise TranspilerError(\"Transformation passes should return a transformed dag.\"\n                                          \"The pass %s is returning a %s\" % (type(pass_).__name__,\n                                                                             type(new_dag)))\n                dag = new_dag\n            elif pass_.is_analysis_pass:\n                pass_.property_set = self.property_set\n                pass_.run(FencedDAGCircuit(dag))\n            else:\n                raise TranspilerError(\"I dont know how to handle this type of pass\")\n\n            \n            self._update_valid_passes(pass_, options['ignore_preserves'])\n\n        return dag","method_summary":"Do a pass and its \"requires\".","original_method_code":"def _do_pass(self, pass_, dag, options):\n        \"\"\"Do a pass and its \"requires\".\n\n        Args:\n            pass_ (BasePass): Pass to do.\n            dag (DAGCircuit): The dag on which the pass is ran.\n            options (dict): PassManager options.\n        Returns:\n            DAGCircuit: The transformed dag in case of a transformation pass.\n            The same input dag in case of an analysis pass.\n        Raises:\n            TranspilerError: If the pass is not a proper pass instance.\n        \"\"\"\n\n        # First, do the requires of pass_\n        if not options[\"ignore_requires\"]:\n            for required_pass in pass_.requires:\n                dag = self._do_pass(required_pass, dag, options)\n\n        # Run the pass itself, if not already run\n        if pass_ not in self.valid_passes:\n            if pass_.is_transformation_pass:\n                pass_.property_set = self.fenced_property_set\n                new_dag = pass_.run(dag)\n                if not isinstance(new_dag, DAGCircuit):\n                    raise TranspilerError(\"Transformation passes should return a transformed dag.\"\n                                          \"The pass %s is returning a %s\" % (type(pass_).__name__,\n                                                                             type(new_dag)))\n                dag = new_dag\n            elif pass_.is_analysis_pass:\n                pass_.property_set = self.property_set\n                pass_.run(FencedDAGCircuit(dag))\n            else:\n                raise TranspilerError(\"I dont know how to handle this type of pass\")\n\n            # update the valid_passes property\n            self._update_valid_passes(pass_, options['ignore_preserves'])\n\n        return dag","method_path":"https:\/\/github.com\/Qiskit\/qiskit-terra\/blob\/d4f58d903bc96341b816f7c35df936d6421267d1\/qiskit\/transpiler\/passmanager.py#L137-L175"}
{"repo_name":"yandex\/yandex-tank","method_name":"ComponentFactory.get_load_plan","method_code":"def get_load_plan(self):\n        \"\"\"\"\"\"\n        if self.rps_schedule and self.instances_schedule:\n            raise StepperConfigurationError(\n                'Both rps and instances schedules specified. You must specify only one of them'\n            )\n        elif self.rps_schedule:\n            info.status.publish('loadscheme', self.rps_schedule)\n            return lp.create(self.rps_schedule)\n        elif self.instances_schedule:\n            info.status.publish('loadscheme', self.instances_schedule)\n            return ip.create(self.instances_schedule)\n        else:\n            self.instances_schedule = []\n            info.status.publish('loadscheme', self.instances_schedule)\n            return ip.create(self.instances_schedule)","method_summary":"return load plan (timestamps generator)","original_method_code":"def get_load_plan(self):\n        \"\"\"\n        return load plan (timestamps generator)\n        \"\"\"\n        if self.rps_schedule and self.instances_schedule:\n            raise StepperConfigurationError(\n                'Both rps and instances schedules specified. You must specify only one of them'\n            )\n        elif self.rps_schedule:\n            info.status.publish('loadscheme', self.rps_schedule)\n            return lp.create(self.rps_schedule)\n        elif self.instances_schedule:\n            info.status.publish('loadscheme', self.instances_schedule)\n            return ip.create(self.instances_schedule)\n        else:\n            self.instances_schedule = []\n            info.status.publish('loadscheme', self.instances_schedule)\n            return ip.create(self.instances_schedule)","method_path":"https:\/\/github.com\/yandex\/yandex-tank\/blob\/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b\/yandextank\/stepper\/config.py#L56-L73"}
{"repo_name":"yandex\/yandex-tank","method_name":"ComponentFactory.get_ammo_generator","method_code":"def get_ammo_generator(self):\n        \"\"\"\"\"\"\n        af_readers = {\n            'phantom': missile.AmmoFileReader,\n            'slowlog': missile.SlowLogReader,\n            'line': missile.LineReader,\n            'uri': missile.UriReader,\n            'uripost': missile.UriPostReader,\n            'access': missile.AccessLogReader,\n            'caseline': missile.CaseLineReader,\n        }\n        if self.uris and self.ammo_file:\n            raise StepperConfigurationError(\n                'Both uris and ammo file specified. You must specify only one of them'\n            )\n        elif self.uris:\n            ammo_gen = missile.UriStyleGenerator(\n                self.uris, self.headers, http_ver=self.http_ver)\n        elif self.ammo_file:\n            if self.ammo_type in af_readers:\n                if self.ammo_type == 'phantom':\n                    opener = resource.get_opener(self.ammo_file)\n                    with opener(self.use_cache) as ammo:\n                        try:\n                            if not ammo.next()[0].isdigit():\n                                self.ammo_type = 'uri'\n                                self.log.info(\n                                    \"Setting ammo_type 'uri' because ammo is not started with digit and you did not specify ammo format\"\n                                )\n                            else:\n                                self.log.info(\n                                    \"Default ammo type ('phantom') used, use 'phantom.ammo_type' option to override it\"\n                                )\n                        except StopIteration:\n                            self.log.exception(\n                                \"Couldn't read first line of ammo file\")\n                            raise AmmoFileError(\n                                \"Couldn't read first line of ammo file\")\n            else:\n                raise NotImplementedError(\n                    'No such ammo type implemented: \"%s\"' % self.ammo_type)\n            ammo_gen = af_readers[self.ammo_type](\n                self.ammo_file, headers=self.headers, http_ver=self.http_ver, use_cache=self.use_cache)\n        else:\n            raise StepperConfigurationError(\n                'Ammo not found. Specify uris or ammo file')\n        self.log.info(\"Using %s ammo reader\" % type(ammo_gen).__name__)\n        return ammo_gen","method_summary":"return ammo generator","original_method_code":"def get_ammo_generator(self):\n        \"\"\"\n        return ammo generator\n        \"\"\"\n        af_readers = {\n            'phantom': missile.AmmoFileReader,\n            'slowlog': missile.SlowLogReader,\n            'line': missile.LineReader,\n            'uri': missile.UriReader,\n            'uripost': missile.UriPostReader,\n            'access': missile.AccessLogReader,\n            'caseline': missile.CaseLineReader,\n        }\n        if self.uris and self.ammo_file:\n            raise StepperConfigurationError(\n                'Both uris and ammo file specified. You must specify only one of them'\n            )\n        elif self.uris:\n            ammo_gen = missile.UriStyleGenerator(\n                self.uris, self.headers, http_ver=self.http_ver)\n        elif self.ammo_file:\n            if self.ammo_type in af_readers:\n                if self.ammo_type == 'phantom':\n                    opener = resource.get_opener(self.ammo_file)\n                    with opener(self.use_cache) as ammo:\n                        try:\n                            if not ammo.next()[0].isdigit():\n                                self.ammo_type = 'uri'\n                                self.log.info(\n                                    \"Setting ammo_type 'uri' because ammo is not started with digit and you did not specify ammo format\"\n                                )\n                            else:\n                                self.log.info(\n                                    \"Default ammo type ('phantom') used, use 'phantom.ammo_type' option to override it\"\n                                )\n                        except StopIteration:\n                            self.log.exception(\n                                \"Couldn't read first line of ammo file\")\n                            raise AmmoFileError(\n                                \"Couldn't read first line of ammo file\")\n            else:\n                raise NotImplementedError(\n                    'No such ammo type implemented: \"%s\"' % self.ammo_type)\n            ammo_gen = af_readers[self.ammo_type](\n                self.ammo_file, headers=self.headers, http_ver=self.http_ver, use_cache=self.use_cache)\n        else:\n            raise StepperConfigurationError(\n                'Ammo not found. Specify uris or ammo file')\n        self.log.info(\"Using %s ammo reader\" % type(ammo_gen).__name__)\n        return ammo_gen","method_path":"https:\/\/github.com\/yandex\/yandex-tank\/blob\/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b\/yandextank\/stepper\/config.py#L75-L124"}
{"repo_name":"yandex\/yandex-tank","method_name":"_exc_to_net","method_code":"def _exc_to_net(param1, success):\n    \"\"\"\"\"\"\n    if len(param1) <= 3:\n        \n        \n        \n        if success:\n            return 0\n        else:\n            return 314\n\n    exc = param1.split(' ')[-1]\n    if exc in KNOWN_EXC.keys():\n        return KNOWN_EXC[exc]\n    else:\n        logger.warning(\n            \"Unknown Java exception, consider adding it to dictionary: %s\",\n            param1)\n        return 41","method_summary":"translate http code to net code. if accertion failed, set net code to 314","original_method_code":"def _exc_to_net(param1, success):\n    \"\"\" translate http code to net code. if accertion failed, set net code to 314 \"\"\"\n    if len(param1) <= 3:\n        # FIXME: we're unable to use better logic here, because we should support non-http codes\n        # but, we should look for core.util.HTTP or some other common logic\n        # here\n        if success:\n            return 0\n        else:\n            return 314\n\n    exc = param1.split(' ')[-1]\n    if exc in KNOWN_EXC.keys():\n        return KNOWN_EXC[exc]\n    else:\n        logger.warning(\n            \"Unknown Java exception, consider adding it to dictionary: %s\",\n            param1)\n        return 41","method_path":"https:\/\/github.com\/yandex\/yandex-tank\/blob\/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b\/yandextank\/plugins\/JMeter\/reader.py#L35-L53"}
{"repo_name":"yandex\/yandex-tank","method_name":"_exc_to_http","method_code":"def _exc_to_http(param1):\n    \"\"\"\"\"\"\n    if len(param1) <= 3:\n        try:\n            int(param1)\n        except BaseException:\n            logger.error(\n                \"JMeter wrote some strange data into codes column: %s\", param1)\n        else:\n            return int(param1)\n\n    exc = param1.split(' ')[-1]\n    if exc in KNOWN_EXC.keys():\n        return 0\n    else:\n        logger.warning(\"Unknown Java exception. %s\", param1)\n        return 0","method_summary":"translate exception str to http code","original_method_code":"def _exc_to_http(param1):\n    \"\"\" translate exception str to http code\"\"\"\n    if len(param1) <= 3:\n        try:\n            int(param1)\n        except BaseException:\n            logger.error(\n                \"JMeter wrote some strange data into codes column: %s\", param1)\n        else:\n            return int(param1)\n\n    exc = param1.split(' ')[-1]\n    if exc in KNOWN_EXC.keys():\n        return 0\n    else:\n        logger.warning(\"Unknown Java exception. %s\", param1)\n        return 0","method_path":"https:\/\/github.com\/yandex\/yandex-tank\/blob\/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b\/yandextank\/plugins\/JMeter\/reader.py#L56-L72"}
{"repo_name":"yandex\/yandex-tank","method_name":"PhantomConfig.read_config","method_code":"def read_config(self):\n        \"\"\"\"\"\"\n        self.threads = self.cfg[\"threads\"] or str(int(multiprocessing.cpu_count() \/ 2) + 1)\n        self.phantom_modules_path = self.cfg[\"phantom_modules_path\"]\n        self.additional_libs = ' '.join(self.cfg[\"additional_libs\"])\n        self.answ_log_level = self.cfg[\"writelog\"]\n        if self.answ_log_level.lower() in ['0', 'false']:\n            self.answ_log_level = 'none'\n        elif self.answ_log_level.lower() in ['1', 'true']:\n            self.answ_log_level = 'all'\n        self.timeout = parse_duration(self.cfg[\"timeout\"])\n        if self.timeout > 120000:\n            logger.warning(\n                \"You've set timeout over 2 minutes.\"\n                \" Are you a functional tester?\")\n        self.answ_log = self.core.mkstemp(\".log\", \"answ_\")\n        self.core.add_artifact_file(self.answ_log)\n        self.core.add_artifact_file(self.phout_file)\n        self.core.add_artifact_file(self.stat_log)\n        self.phantom_log = self.core.mkstemp(\".log\", \"phantom_\")\n        self.core.add_artifact_file(self.phantom_log)\n\n        main_stream = StreamConfig(\n            self.core,\n            len(self.streams), self.phout_file, self.answ_log,\n            self.answ_log_level, self.timeout, self.cfg, True)\n        self.streams.append(main_stream)\n\n        for section in self.multi():\n            self.streams.append(\n                StreamConfig(\n                    self.core,\n                    len(self.streams), self.phout_file, self.answ_log,\n                    self.answ_log_level, self.timeout, section))\n\n        for stream in self.streams:\n            stream.read_config()\n\n        if any(stream.ssl for stream in self.streams):\n            self.additional_libs += ' ssl io_benchmark_method_stream_transport_ssl'","method_summary":"Read phantom tool specific options","original_method_code":"def read_config(self):\n        \"\"\"        Read phantom tool specific options        \"\"\"\n        self.threads = self.cfg[\"threads\"] or str(int(multiprocessing.cpu_count() \/ 2) + 1)\n        self.phantom_modules_path = self.cfg[\"phantom_modules_path\"]\n        self.additional_libs = ' '.join(self.cfg[\"additional_libs\"])\n        self.answ_log_level = self.cfg[\"writelog\"]\n        if self.answ_log_level.lower() in ['0', 'false']:\n            self.answ_log_level = 'none'\n        elif self.answ_log_level.lower() in ['1', 'true']:\n            self.answ_log_level = 'all'\n        self.timeout = parse_duration(self.cfg[\"timeout\"])\n        if self.timeout > 120000:\n            logger.warning(\n                \"You've set timeout over 2 minutes.\"\n                \" Are you a functional tester?\")\n        self.answ_log = self.core.mkstemp(\".log\", \"answ_\")\n        self.core.add_artifact_file(self.answ_log)\n        self.core.add_artifact_file(self.phout_file)\n        self.core.add_artifact_file(self.stat_log)\n        self.phantom_log = self.core.mkstemp(\".log\", \"phantom_\")\n        self.core.add_artifact_file(self.phantom_log)\n\n        main_stream = StreamConfig(\n            self.core,\n            len(self.streams), self.phout_file, self.answ_log,\n            self.answ_log_level, self.timeout, self.cfg, True)\n        self.streams.append(main_stream)\n\n        for section in self.multi():\n            self.streams.append(\n                StreamConfig(\n                    self.core,\n                    len(self.streams), self.phout_file, self.answ_log,\n                    self.answ_log_level, self.timeout, section))\n\n        for stream in self.streams:\n            stream.read_config()\n\n        if any(stream.ssl for stream in self.streams):\n            self.additional_libs += ' ssl io_benchmark_method_stream_transport_ssl'","method_path":"https:\/\/github.com\/yandex\/yandex-tank\/blob\/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b\/yandextank\/plugins\/Phantom\/utils.py#L59-L98"}
{"repo_name":"yandex\/yandex-tank","method_name":"PhantomConfig.compose_config","method_code":"def compose_config(self):\n        \"\"\"\"\"\"\n        streams_config = ''\n        stat_benchmarks = ''\n        for stream in self.streams:\n            streams_config += stream.compose_config()\n            if not stream.is_main:\n                stat_benchmarks += \" \" + \"benchmark_io%s\" % stream.sequence_no\n\n        kwargs = {}\n        kwargs['threads'] = self.threads\n        kwargs['phantom_log'] = self.phantom_log\n        kwargs['stat_log'] = self.stat_log\n        kwargs['benchmarks_block'] = streams_config\n        kwargs['stat_benchmarks'] = stat_benchmarks\n        kwargs['additional_libs'] = self.additional_libs\n        kwargs['phantom_modules_path'] = self.phantom_modules_path\n        filename = self.core.mkstemp(\".conf\", \"phantom_\")\n        self.core.add_artifact_file(filename)\n        logger.debug(\"Generating phantom config: %s\", filename)\n        template_str = resource_string(__name__, \"config\/phantom.conf.tpl\")\n        tpl = string.Template(template_str)\n        config = tpl.substitute(kwargs)\n\n        with open(filename, 'w') as conffile:\n            conffile.write(config)\n        return filename","method_summary":"Generate phantom tool run config","original_method_code":"def compose_config(self):\n        \"\"\"        Generate phantom tool run config        \"\"\"\n        streams_config = ''\n        stat_benchmarks = ''\n        for stream in self.streams:\n            streams_config += stream.compose_config()\n            if not stream.is_main:\n                stat_benchmarks += \" \" + \"benchmark_io%s\" % stream.sequence_no\n\n        kwargs = {}\n        kwargs['threads'] = self.threads\n        kwargs['phantom_log'] = self.phantom_log\n        kwargs['stat_log'] = self.stat_log\n        kwargs['benchmarks_block'] = streams_config\n        kwargs['stat_benchmarks'] = stat_benchmarks\n        kwargs['additional_libs'] = self.additional_libs\n        kwargs['phantom_modules_path'] = self.phantom_modules_path\n        filename = self.core.mkstemp(\".conf\", \"phantom_\")\n        self.core.add_artifact_file(filename)\n        logger.debug(\"Generating phantom config: %s\", filename)\n        template_str = resource_string(__name__, \"config\/phantom.conf.tpl\")\n        tpl = string.Template(template_str)\n        config = tpl.substitute(kwargs)\n\n        with open(filename, 'w') as conffile:\n            conffile.write(config)\n        return filename","method_path":"https:\/\/github.com\/yandex\/yandex-tank\/blob\/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b\/yandextank\/plugins\/Phantom\/utils.py#L109-L135"}
{"repo_name":"yandex\/yandex-tank","method_name":"PhantomConfig.get_info","method_code":"def get_info(self):\n        \"\"\"\"\"\"\n        result = copy.copy(self.streams[0])\n        result.stat_log = self.stat_log\n        result.steps = []\n        result.ammo_file = ''\n        result.rps_schedule = None\n        result.ammo_count = 0\n        result.duration = 0\n\n        result.instances = 0\n        result.loadscheme = []\n        result.loop_count = 0\n\n        for stream in self.streams:\n            sec_no = 0\n            logger.debug(\"Steps: %s\", stream.stepper_wrapper.steps)\n            for item in stream.stepper_wrapper.steps:\n                for x in range(0, item[1]):\n                    if len(result.steps) > sec_no:\n                        result.steps[sec_no][0] += item[0]\n                    else:\n                        result.steps.append([item[0], 1])\n                    sec_no += 1\n\n            if result.rps_schedule:\n                result.rps_schedule = []\n            else:\n                result.rps_schedule = stream.stepper_wrapper.loadscheme\n            if result.loadscheme:\n                result.loadscheme = ''\n            else:\n                \n                \n                \n                result.loadscheme = ''\n\n            if result.loop_count:\n                result.loop_count = u'0'\n            else:\n                result.loop_count = stream.stepper_wrapper.loop_count\n\n            result.ammo_file += '{} '.format(stream.stepper_wrapper.ammo_file)\n            result.ammo_count += stream.stepper_wrapper.ammo_count\n            result.duration = max(\n                result.duration, stream.stepper_wrapper.duration)\n            result.instances += stream.instances\n\n        if not result.ammo_count:\n            raise ValueError(\"Total ammo count cannot be zero\")\n        return result","method_summary":"get merged info about phantom conf","original_method_code":"def get_info(self):\n        \"\"\" get merged info about phantom conf \"\"\"\n        result = copy.copy(self.streams[0])\n        result.stat_log = self.stat_log\n        result.steps = []\n        result.ammo_file = ''\n        result.rps_schedule = None\n        result.ammo_count = 0\n        result.duration = 0\n\n        result.instances = 0\n        result.loadscheme = []\n        result.loop_count = 0\n\n        for stream in self.streams:\n            sec_no = 0\n            logger.debug(\"Steps: %s\", stream.stepper_wrapper.steps)\n            for item in stream.stepper_wrapper.steps:\n                for x in range(0, item[1]):\n                    if len(result.steps) > sec_no:\n                        result.steps[sec_no][0] += item[0]\n                    else:\n                        result.steps.append([item[0], 1])\n                    sec_no += 1\n\n            if result.rps_schedule:\n                result.rps_schedule = []\n            else:\n                result.rps_schedule = stream.stepper_wrapper.loadscheme\n            if result.loadscheme:\n                result.loadscheme = ''\n            else:\n                # FIXME: add formatted load scheme for server:\n                # <step_size,step_type,first_rps,last_rps,original_step_params>\n                # as a string\n                result.loadscheme = ''\n\n            if result.loop_count:\n                result.loop_count = u'0'\n            else:\n                result.loop_count = stream.stepper_wrapper.loop_count\n\n            result.ammo_file += '{} '.format(stream.stepper_wrapper.ammo_file)\n            result.ammo_count += stream.stepper_wrapper.ammo_count\n            result.duration = max(\n                result.duration, stream.stepper_wrapper.duration)\n            result.instances += stream.instances\n\n        if not result.ammo_count:\n            raise ValueError(\"Total ammo count cannot be zero\")\n        return result","method_path":"https:\/\/github.com\/yandex\/yandex-tank\/blob\/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b\/yandextank\/plugins\/Phantom\/utils.py#L142-L192"}
{"repo_name":"yandex\/yandex-tank","method_name":"StreamConfig.read_config","method_code":"def read_config(self):\n        \"\"\"\"\"\"\n        \n        self.ssl = self.get_option(\"ssl\")\n        self.tank_type = self.get_option(\"tank_type\")\n        \n        \n        \n        self.gatling = ' '.join(self.get_option('gatling_ip').split(\"\\n\"))\n        self.method_prefix = self.get_option(\"method_prefix\")\n        self.method_options = self.get_option(\"method_options\")\n        self.source_log_prefix = self.get_option(\"source_log_prefix\")\n\n        self.phantom_http_line = self.get_option(\"phantom_http_line\")\n        self.phantom_http_field_num = self.get_option(\"phantom_http_field_num\")\n        self.phantom_http_field = self.get_option(\"phantom_http_field\")\n        self.phantom_http_entity = self.get_option(\"phantom_http_entity\")\n\n        self.address = self.get_option('address')\n        do_test_connect = self.get_option(\"connection_test\")\n        explicit_port = self.get_option('port', '')\n        self.ipv6, self.resolved_ip, self.port, self.address = self.address_wizard.resolve(\n            self.address, do_test_connect, explicit_port)\n\n        logger.info(\n            \"Resolved %s into %s:%s\", self.address, self.resolved_ip, self.port)\n\n        self.client_cipher_suites = self.get_option(\"client_cipher_suites\", \"\")\n        self.client_certificate = self.get_option(\"client_certificate\", \"\")\n        self.client_key = self.get_option(\"client_key\", \"\")\n        self.stepper_wrapper.read_config()","method_summary":"reads config","original_method_code":"def read_config(self):\n        \"\"\" reads config \"\"\"\n        # multi-options\n        self.ssl = self.get_option(\"ssl\")\n        self.tank_type = self.get_option(\"tank_type\")\n        # TODO: refactor. Maybe we should decide how to interact with\n        # StepperWrapper here.\n        # self.instances = self.get_option('instances')\n        self.gatling = ' '.join(self.get_option('gatling_ip').split(\"\\n\"))\n        self.method_prefix = self.get_option(\"method_prefix\")\n        self.method_options = self.get_option(\"method_options\")\n        self.source_log_prefix = self.get_option(\"source_log_prefix\")\n\n        self.phantom_http_line = self.get_option(\"phantom_http_line\")\n        self.phantom_http_field_num = self.get_option(\"phantom_http_field_num\")\n        self.phantom_http_field = self.get_option(\"phantom_http_field\")\n        self.phantom_http_entity = self.get_option(\"phantom_http_entity\")\n\n        self.address = self.get_option('address')\n        do_test_connect = self.get_option(\"connection_test\")\n        explicit_port = self.get_option('port', '')\n        self.ipv6, self.resolved_ip, self.port, self.address = self.address_wizard.resolve(\n            self.address, do_test_connect, explicit_port)\n\n        logger.info(\n            \"Resolved %s into %s:%s\", self.address, self.resolved_ip, self.port)\n\n        self.client_cipher_suites = self.get_option(\"client_cipher_suites\", \"\")\n        self.client_certificate = self.get_option(\"client_certificate\", \"\")\n        self.client_key = self.get_option(\"client_key\", \"\")\n        self.stepper_wrapper.read_config()","method_path":"https:\/\/github.com\/yandex\/yandex-tank\/blob\/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b\/yandextank\/plugins\/Phantom\/utils.py#L253-L283"}
{"repo_name":"yandex\/yandex-tank","method_name":"StreamConfig.compose_config","method_code":"def compose_config(self):\n        \"\"\"\"\"\"\n        \n        self.stepper_wrapper.prepare_stepper()\n        self.stpd = self.stepper_wrapper.stpd\n        if self.stepper_wrapper.instances:\n            self.instances = self.stepper_wrapper.instances\n\n        if not self.stpd:\n            raise RuntimeError(\"Cannot proceed with no STPD file\")\n\n        kwargs = {}\n        kwargs['sequence_no'] = self.sequence_no\n        if self.ssl:\n            _auth_section = ''\n            _ciphers = ''\n            ssl_template = \"transport_t ssl_transport = transport_ssl_t {\\n\" \\\n                           \"                timeout = 1s\\n\" \\\n                           \"                %s\\n\" \\\n                           \"                %s}\\n\" \\\n                           \"                transport = ssl_transport\"\n\n            if self.client_certificate or self.client_key:\n                _auth_section = 'auth_t def_auth = auth_t { key = \"%s\" cert = \"%s\"} auth = def_auth' \\\n                                % (self.client_key, self.client_certificate)\n            if self.client_cipher_suites:\n                _ciphers = 'ciphers = \"%s\"' % self.client_cipher_suites\n            kwargs['ssl_transport'] = ssl_template % (_auth_section, _ciphers)\n        else:\n            kwargs['ssl_transport'] = \"\"\n        kwargs['method_stream'] = self.method_prefix + \\\n            \"_ipv6_t\" if self.ipv6 else self.method_prefix + \"_ipv4_t\"\n        kwargs['phout'] = self.phout_file\n        kwargs['answ_log'] = self.answ_log\n        kwargs['answ_log_level'] = self.answ_log_level\n        kwargs['comment_answ'] = \"# \" if self.answ_log_level == 'none' else ''\n        kwargs['stpd'] = self.stpd\n        kwargs['source_log_prefix'] = self.source_log_prefix\n        kwargs['method_options'] = self.method_options\n        if self.tank_type:\n            kwargs[\n                'proto'] = \"proto=http_proto%s\" % self.sequence_no if self.tank_type == 'http' else \"proto=none_proto\"\n            kwargs['comment_proto'] = \"\"\n        else:\n            kwargs['proto'] = \"\"\n            kwargs['comment_proto'] = \"#\"\n\n        if self.gatling:\n            kwargs['bind'] = 'bind={ ' + self.gatling + ' }'\n        else:\n            kwargs['bind'] = ''\n        kwargs['ip'] = self.resolved_ip\n        kwargs['port'] = self.port\n        kwargs['timeout'] = self.timeout\n        kwargs['instances'] = self.instances\n        tune = ''\n        if self.phantom_http_entity:\n            tune += \"entity = \" + self.phantom_http_entity + \"\\n\"\n        if self.phantom_http_field:\n            tune += \"field = \" + self.phantom_http_field + \"\\n\"\n        if self.phantom_http_field_num:\n            tune += \"field_num = {}\\n\".format(self.phantom_http_field_num)\n        if self.phantom_http_line:\n            tune += \"line = \" + self.phantom_http_line + \"\\n\"\n        if tune:\n            kwargs['reply_limits'] = 'reply_limits = {\\n' + tune + \"}\"\n        else:\n            kwargs['reply_limits'] = ''\n\n        if self.is_main:\n            fname = 'phantom_benchmark_main.tpl'\n        else:\n            fname = 'phantom_benchmark_additional.tpl'\n        template_str = resource_string(\n            __name__, \"config\/\" + fname)\n        tpl = string.Template(template_str)\n        config = tpl.substitute(kwargs)\n\n        return config","method_summary":"compose benchmark block","original_method_code":"def compose_config(self):\n        \"\"\" compose benchmark block \"\"\"\n        # step file\n        self.stepper_wrapper.prepare_stepper()\n        self.stpd = self.stepper_wrapper.stpd\n        if self.stepper_wrapper.instances:\n            self.instances = self.stepper_wrapper.instances\n\n        if not self.stpd:\n            raise RuntimeError(\"Cannot proceed with no STPD file\")\n\n        kwargs = {}\n        kwargs['sequence_no'] = self.sequence_no\n        if self.ssl:\n            _auth_section = ''\n            _ciphers = ''\n            ssl_template = \"transport_t ssl_transport = transport_ssl_t {\\n\" \\\n                           \"                timeout = 1s\\n\" \\\n                           \"                %s\\n\" \\\n                           \"                %s}\\n\" \\\n                           \"                transport = ssl_transport\"\n\n            if self.client_certificate or self.client_key:\n                _auth_section = 'auth_t def_auth = auth_t { key = \"%s\" cert = \"%s\"} auth = def_auth' \\\n                                % (self.client_key, self.client_certificate)\n            if self.client_cipher_suites:\n                _ciphers = 'ciphers = \"%s\"' % self.client_cipher_suites\n            kwargs['ssl_transport'] = ssl_template % (_auth_section, _ciphers)\n        else:\n            kwargs['ssl_transport'] = \"\"\n        kwargs['method_stream'] = self.method_prefix + \\\n            \"_ipv6_t\" if self.ipv6 else self.method_prefix + \"_ipv4_t\"\n        kwargs['phout'] = self.phout_file\n        kwargs['answ_log'] = self.answ_log\n        kwargs['answ_log_level'] = self.answ_log_level\n        kwargs['comment_answ'] = \"# \" if self.answ_log_level == 'none' else ''\n        kwargs['stpd'] = self.stpd\n        kwargs['source_log_prefix'] = self.source_log_prefix\n        kwargs['method_options'] = self.method_options\n        if self.tank_type:\n            kwargs[\n                'proto'] = \"proto=http_proto%s\" % self.sequence_no if self.tank_type == 'http' else \"proto=none_proto\"\n            kwargs['comment_proto'] = \"\"\n        else:\n            kwargs['proto'] = \"\"\n            kwargs['comment_proto'] = \"#\"\n\n        if self.gatling:\n            kwargs['bind'] = 'bind={ ' + self.gatling + ' }'\n        else:\n            kwargs['bind'] = ''\n        kwargs['ip'] = self.resolved_ip\n        kwargs['port'] = self.port\n        kwargs['timeout'] = self.timeout\n        kwargs['instances'] = self.instances\n        tune = ''\n        if self.phantom_http_entity:\n            tune += \"entity = \" + self.phantom_http_entity + \"\\n\"\n        if self.phantom_http_field:\n            tune += \"field = \" + self.phantom_http_field + \"\\n\"\n        if self.phantom_http_field_num:\n            tune += \"field_num = {}\\n\".format(self.phantom_http_field_num)\n        if self.phantom_http_line:\n            tune += \"line = \" + self.phantom_http_line + \"\\n\"\n        if tune:\n            kwargs['reply_limits'] = 'reply_limits = {\\n' + tune + \"}\"\n        else:\n            kwargs['reply_limits'] = ''\n\n        if self.is_main:\n            fname = 'phantom_benchmark_main.tpl'\n        else:\n            fname = 'phantom_benchmark_additional.tpl'\n        template_str = resource_string(\n            __name__, \"config\/\" + fname)\n        tpl = string.Template(template_str)\n        config = tpl.substitute(kwargs)\n\n        return config","method_path":"https:\/\/github.com\/yandex\/yandex-tank\/blob\/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b\/yandextank\/plugins\/Phantom\/utils.py#L285-L363"}
{"repo_name":"yandex\/yandex-tank","method_name":"Plugin.patch_config","method_code":"def patch_config(self, config):\n        \"\"\"\"\"\"\n        \n        if config.get(\"monitoring\"):\n            if config[\"monitoring\"].get(\"expvar\"):\n                self.expvar = config[\"monitoring\"][\"expvar\"].get(\"enabled\")\n                if config[\"monitoring\"][\"expvar\"].get(\"port\"):\n                    self.expvar_port = config[\"monitoring\"][\"expvar\"].get(\"port\")\n                else:\n                    self.expvar_port = self.DEFAULT_EXPVAR_PORT\n        \n        else:\n            config[\"monitoring\"] = {\n                \"expvar\": {\n                    \"enabled\": True,\n                }\n            }\n            self.expvar = True\n            self.expvar_port = self.DEFAULT_EXPVAR_PORT\n\n        \n        \n        for pool in config['pools']:\n            if pool.get('ammo', {}).get('file', ''):\n                self.ammofile = pool['ammo']['file']\n                pool['ammo']['file'] = resource_manager.resource_filename(\n                    self.ammofile\n                )\n            if not pool.get('result') or 'phout' not in pool.get('result', {}).get('type', ''):\n                logger.warning('Seems like pandora result file not specified... adding defaults')\n                pool['result'] = dict(\n                    destination=self.DEFAULT_REPORT_FILE,\n                    type='phout',\n                )\n        return config","method_summary":"download remote resources, replace links with local filenames add result file section","original_method_code":"def patch_config(self, config):\n        \"\"\"\n        download remote resources, replace links with local filenames\n        add result file section\n        :param dict config: pandora config\n        \"\"\"\n        # get expvar parameters\n        if config.get(\"monitoring\"):\n            if config[\"monitoring\"].get(\"expvar\"):\n                self.expvar = config[\"monitoring\"][\"expvar\"].get(\"enabled\")\n                if config[\"monitoring\"][\"expvar\"].get(\"port\"):\n                    self.expvar_port = config[\"monitoring\"][\"expvar\"].get(\"port\")\n                else:\n                    self.expvar_port = self.DEFAULT_EXPVAR_PORT\n        # or set if expvar not exists\n        else:\n            config[\"monitoring\"] = {\n                \"expvar\": {\n                    \"enabled\": True,\n                }\n            }\n            self.expvar = True\n            self.expvar_port = self.DEFAULT_EXPVAR_PORT\n\n        # FIXME this is broken for custom ammo providers due to interface incompatibility\n        # FIXME refactor pandora plx\n        for pool in config['pools']:\n            if pool.get('ammo', {}).get('file', ''):\n                self.ammofile = pool['ammo']['file']\n                pool['ammo']['file'] = resource_manager.resource_filename(\n                    self.ammofile\n                )\n            if not pool.get('result') or 'phout' not in pool.get('result', {}).get('type', ''):\n                logger.warning('Seems like pandora result file not specified... adding defaults')\n                pool['result'] = dict(\n                    destination=self.DEFAULT_REPORT_FILE,\n                    type='phout',\n                )\n        return config","method_path":"https:\/\/github.com\/yandex\/yandex-tank\/blob\/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b\/yandextank\/plugins\/Pandora\/plugin.py#L94-L132"}
{"repo_name":"dagster-io\/dagster","method_name":"MultipleResults.from_dict","method_code":"def from_dict(result_dict):\n        ''''''\n        check.dict_param(result_dict, 'result_dict', key_type=str)\n        results = []\n        for name, value in result_dict.items():\n            results.append(Result(value, name))\n        return MultipleResults(*results)","method_summary":"Create a new ``MultipleResults`` object from a dictionary. Keys of the dictionary are unpacked into result names.","original_method_code":"def from_dict(result_dict):\n        '''Create a new ``MultipleResults`` object from a dictionary.\n        \n        Keys of the dictionary are unpacked into result names.\n        \n        Args:\n            result_dict (dict) - The dictionary to unpack.\n        \n        Returns:\n            (:py:class:`MultipleResults <dagster.MultipleResults>`) A new ``MultipleResults`` object\n\n        '''\n        check.dict_param(result_dict, 'result_dict', key_type=str)\n        results = []\n        for name, value in result_dict.items():\n            results.append(Result(value, name))\n        return MultipleResults(*results)","method_path":"https:\/\/github.com\/dagster-io\/dagster\/blob\/4119f8c773089de64831b1dfb9e168e353d401dc\/python_modules\/dagster\/dagster\/core\/definitions\/decorators.py#L64-L80"}
{"repo_name":"dagster-io\/dagster","method_name":"gunzipper","method_code":"def gunzipper(gzip_file):\n    ''''''\n    \n\n    path_prefix = os.path.dirname(gzip_file)\n    output_folder = os.path.join(path_prefix, 'raw\/2019\/01\/01')\n    outfile = os.path.join(output_folder, 'data.json')\n\n    if not safe_isfile(outfile):\n        mkdir_p(output_folder)\n\n        with gzip.open(gzip_file, 'rb') as f_in, open(outfile, 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n\n    return [path_prefix]","method_summary":"gunzips \/path\/to\/foo.gz to \/path\/to\/raw\/2019\/01\/01\/data.json","original_method_code":"def gunzipper(gzip_file):\n    '''gunzips \/path\/to\/foo.gz to \/path\/to\/raw\/2019\/01\/01\/data.json\n    '''\n    # TODO: take date as an input\n\n    path_prefix = os.path.dirname(gzip_file)\n    output_folder = os.path.join(path_prefix, 'raw\/2019\/01\/01')\n    outfile = os.path.join(output_folder, 'data.json')\n\n    if not safe_isfile(outfile):\n        mkdir_p(output_folder)\n\n        with gzip.open(gzip_file, 'rb') as f_in, open(outfile, 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n\n    return [path_prefix]","method_path":"https:\/\/github.com\/dagster-io\/dagster\/blob\/4119f8c773089de64831b1dfb9e168e353d401dc\/examples\/event-pipeline-demo\/event_pipeline_demo\/pipelines.py#L31-L46"}
{"repo_name":"dagster-io\/dagster","method_name":"_check_key_value_types","method_code":"def _check_key_value_types(obj, key_type, value_type, key_check=isinstance, value_check=isinstance):\n    ''''''\n    if not isinstance(obj, dict):\n        raise_with_traceback(_type_mismatch_error(obj, dict))\n\n    if key_type is str:\n        key_type = string_types\n\n    if value_type is str:\n        value_type = string_types\n\n    for key, value in obj.items():\n        if key_type and not key_check(key, key_type):\n            raise_with_traceback(\n                CheckError(\n                    'Key in dictionary mismatches type. Expected {key_type}. Got {obj_repr}'.format(\n                        key_type=repr(key_type), obj_repr=repr(key)\n                    )\n                )\n            )\n        if value_type and not value_check(value, value_type):\n            raise_with_traceback(\n                CheckError(\n                    'Value in dictionary mismatches expected type for key {key}. Expected value '\n                    'of type {vtype}. Got value {value} of type {obj_type}.'.format(\n                        vtype=repr(value_type), obj_type=type(value), key=key, value=value\n                    )\n                )\n            )\n    return obj","method_summary":"Ensures argument obj is a dictionary, and enforces that the keys\/values conform to the types specified by key_type, value_type.","original_method_code":"def _check_key_value_types(obj, key_type, value_type, key_check=isinstance, value_check=isinstance):\n    '''Ensures argument obj is a dictionary, and enforces that the keys\/values conform to the types\n    specified by key_type, value_type.\n    '''\n    if not isinstance(obj, dict):\n        raise_with_traceback(_type_mismatch_error(obj, dict))\n\n    if key_type is str:\n        key_type = string_types\n\n    if value_type is str:\n        value_type = string_types\n\n    for key, value in obj.items():\n        if key_type and not key_check(key, key_type):\n            raise_with_traceback(\n                CheckError(\n                    'Key in dictionary mismatches type. Expected {key_type}. Got {obj_repr}'.format(\n                        key_type=repr(key_type), obj_repr=repr(key)\n                    )\n                )\n            )\n        if value_type and not value_check(value, value_type):\n            raise_with_traceback(\n                CheckError(\n                    'Value in dictionary mismatches expected type for key {key}. Expected value '\n                    'of type {vtype}. Got value {value} of type {obj_type}.'.format(\n                        vtype=repr(value_type), obj_type=type(value), key=key, value=value\n                    )\n                )\n            )\n    return obj","method_path":"https:\/\/github.com\/dagster-io\/dagster\/blob\/4119f8c773089de64831b1dfb9e168e353d401dc\/python_modules\/dagster\/dagster\/check\/__init__.py#L328-L359"}
{"repo_name":"dagster-io\/dagster","method_name":"dict_param","method_code":"def dict_param(obj, param_name, key_type=None, value_type=None):\n    ''''''\n    if not isinstance(obj, dict):\n        raise_with_traceback(_param_type_mismatch_exception(obj, dict, param_name))\n\n    if not (key_type or value_type):\n        return obj\n\n    return _check_key_value_types(obj, key_type, value_type)","method_summary":"Ensures argument obj is a native Python dictionary, raises an exception if not, and otherwise returns obj.","original_method_code":"def dict_param(obj, param_name, key_type=None, value_type=None):\n    '''Ensures argument obj is a native Python dictionary, raises an exception if not, and otherwise\n    returns obj.\n    '''\n    if not isinstance(obj, dict):\n        raise_with_traceback(_param_type_mismatch_exception(obj, dict, param_name))\n\n    if not (key_type or value_type):\n        return obj\n\n    return _check_key_value_types(obj, key_type, value_type)","method_path":"https:\/\/github.com\/dagster-io\/dagster\/blob\/4119f8c773089de64831b1dfb9e168e353d401dc\/python_modules\/dagster\/dagster\/check\/__init__.py#L362-L372"}
{"repo_name":"dagster-io\/dagster","method_name":"opt_dict_param","method_code":"def opt_dict_param(obj, param_name, key_type=None, value_type=None, value_class=None):\n    ''''''\n    if obj is not None and not isinstance(obj, dict):\n        raise_with_traceback(_param_type_mismatch_exception(obj, dict, param_name))\n\n    if not obj:\n        return {}\n\n    if value_class:\n        return _check_key_value_types(obj, key_type, value_type=value_class, value_check=issubclass)\n    return _check_key_value_types(obj, key_type, value_type)","method_summary":"Ensures argument obj is either a dictionary or None; if the latter, instantiates an empty dictionary.","original_method_code":"def opt_dict_param(obj, param_name, key_type=None, value_type=None, value_class=None):\n    '''Ensures argument obj is either a dictionary or None; if the latter, instantiates an empty\n    dictionary.\n    '''\n    if obj is not None and not isinstance(obj, dict):\n        raise_with_traceback(_param_type_mismatch_exception(obj, dict, param_name))\n\n    if not obj:\n        return {}\n\n    if value_class:\n        return _check_key_value_types(obj, key_type, value_type=value_class, value_check=issubclass)\n    return _check_key_value_types(obj, key_type, value_type)","method_path":"https:\/\/github.com\/dagster-io\/dagster\/blob\/4119f8c773089de64831b1dfb9e168e353d401dc\/python_modules\/dagster\/dagster\/check\/__init__.py#L375-L387"}
{"repo_name":"dagster-io\/dagster","method_name":"construct_event_logger","method_code":"def construct_event_logger(event_record_callback):\n    ''''''\n    check.callable_param(event_record_callback, 'event_record_callback')\n\n    return construct_single_handler_logger(\n        'event-logger',\n        DEBUG,\n        StructuredLoggerHandler(\n            lambda logger_message: event_record_callback(construct_event_record(logger_message))\n        ),\n    )","method_summary":"Callback receives a stream of event_records","original_method_code":"def construct_event_logger(event_record_callback):\n    '''\n    Callback receives a stream of event_records\n    '''\n    check.callable_param(event_record_callback, 'event_record_callback')\n\n    return construct_single_handler_logger(\n        'event-logger',\n        DEBUG,\n        StructuredLoggerHandler(\n            lambda logger_message: event_record_callback(construct_event_record(logger_message))\n        ),\n    )","method_path":"https:\/\/github.com\/dagster-io\/dagster\/blob\/4119f8c773089de64831b1dfb9e168e353d401dc\/python_modules\/dagster\/dagster\/core\/events\/logging.py#L134-L146"}
{"repo_name":"dagster-io\/dagster","method_name":"construct_json_event_logger","method_code":"def construct_json_event_logger(json_path):\n    ''''''\n    check.str_param(json_path, 'json_path')\n    return construct_single_handler_logger(\n        \"json-event-record-logger\",\n        DEBUG,\n        JsonEventLoggerHandler(\n            json_path,\n            lambda record: construct_event_record(\n                StructuredLoggerMessage(\n                    name=record.name,\n                    message=record.msg,\n                    level=record.levelno,\n                    meta=record.dagster_meta,\n                    record=record,\n                )\n            ),\n        ),\n    )","method_summary":"Record a stream of event records to json","original_method_code":"def construct_json_event_logger(json_path):\n    '''Record a stream of event records to json'''\n    check.str_param(json_path, 'json_path')\n    return construct_single_handler_logger(\n        \"json-event-record-logger\",\n        DEBUG,\n        JsonEventLoggerHandler(\n            json_path,\n            lambda record: construct_event_record(\n                StructuredLoggerMessage(\n                    name=record.name,\n                    message=record.msg,\n                    level=record.levelno,\n                    meta=record.dagster_meta,\n                    record=record,\n                )\n            ),\n        ),\n    )","method_path":"https:\/\/github.com\/dagster-io\/dagster\/blob\/4119f8c773089de64831b1dfb9e168e353d401dc\/python_modules\/dagster\/dagster\/core\/events\/logging.py#L149-L167"}
{"repo_name":"dagster-io\/dagster","method_name":"replace_parameters","method_code":"def replace_parameters(context, nb, parameters):\n    \n    \n    \n\n    ''''''\n\n    \n    nb = copy.deepcopy(nb)\n\n    \n    param_content = DagsterTranslator.codify(parameters)\n    \n    \n    \n    newcell = nbformat.v4.new_code_cell(source=param_content)\n    newcell.metadata['tags'] = ['injected-parameters']\n\n    param_cell_index = _find_first_tagged_cell_index(nb, 'parameters')\n    injected_cell_index = _find_first_tagged_cell_index(nb, 'injected-parameters')\n    if injected_cell_index >= 0:\n        \n        before = nb.cells[:injected_cell_index]\n        after = nb.cells[injected_cell_index + 1 :]\n        check.int_value_param(param_cell_index, -1, 'param_cell_index')\n        \n    elif param_cell_index >= 0:\n        \n        before = nb.cells[:param_cell_index]\n        after = nb.cells[param_cell_index + 1 :]\n    else:\n        \n        context.log.debug(\n            (\n                'Warning notebook has no parameters cell, '\n                'so first cell must import dagstermill and call dm.register_repo()'\n            )\n        )\n        before = nb.cells[:1]\n        after = nb.cells[1:]\n\n    nb.cells = before + [newcell] + after\n    nb.metadata.papermill['parameters'] = parameters\n\n    return nb","method_summary":"Assigned parameters into the appropiate place in the input notebook","original_method_code":"def replace_parameters(context, nb, parameters):\n    # Uma: This is a copy-paste from papermill papermill\/execute.py:104 (execute_parameters).\n    # Typically, papermill injects the injected-parameters cell *below* the parameters cell\n    # but we want to *replace* the parameters cell, which is what this function does.\n\n    '''Assigned parameters into the appropiate place in the input notebook\n    Args:\n        nb (NotebookNode): Executable notebook object\n        parameters (dict): Arbitrary keyword arguments to pass to the notebook parameters.\n    '''\n\n    # Copy the nb object to avoid polluting the input\n    nb = copy.deepcopy(nb)\n\n    # Generate parameter content based on the kernel_name\n    param_content = DagsterTranslator.codify(parameters)\n    # papermill method choosed translator based on kernel_name and language,\n    # but we just call the DagsterTranslator\n    # translate_parameters(kernel_name, language, parameters)\n    newcell = nbformat.v4.new_code_cell(source=param_content)\n    newcell.metadata['tags'] = ['injected-parameters']\n\n    param_cell_index = _find_first_tagged_cell_index(nb, 'parameters')\n    injected_cell_index = _find_first_tagged_cell_index(nb, 'injected-parameters')\n    if injected_cell_index >= 0:\n        # Replace the injected cell with a new version\n        before = nb.cells[:injected_cell_index]\n        after = nb.cells[injected_cell_index + 1 :]\n        check.int_value_param(param_cell_index, -1, 'param_cell_index')\n        # We should have blown away the parameters cell if there is an injected-parameters cell\n    elif param_cell_index >= 0:\n        # Replace the parameter cell with the injected-parameters cell\n        before = nb.cells[:param_cell_index]\n        after = nb.cells[param_cell_index + 1 :]\n    else:\n        # Inject to the top of the notebook, presumably first cell includes dagstermill import\n        context.log.debug(\n            (\n                'Warning notebook has no parameters cell, '\n                'so first cell must import dagstermill and call dm.register_repo()'\n            )\n        )\n        before = nb.cells[:1]\n        after = nb.cells[1:]\n\n    nb.cells = before + [newcell] + after\n    nb.metadata.papermill['parameters'] = parameters\n\n    return nb","method_path":"https:\/\/github.com\/dagster-io\/dagster\/blob\/4119f8c773089de64831b1dfb9e168e353d401dc\/python_modules\/dagstermill\/dagstermill\/__init__.py#L478-L526"}
{"repo_name":"dagster-io\/dagster","method_name":"nonce_solid","method_code":"def nonce_solid(name, n_inputs, n_outputs):\n    \"\"\"\"\"\"\n\n    @solid(\n        name=name,\n        inputs=[\n            InputDefinition(name='input_{}'.format(i)) for i in range(n_inputs)\n        ],\n        outputs=[\n            OutputDefinition(name='output_{}'.format(i))\n            for i in range(n_outputs)\n        ],\n    )\n    def solid_fn(context, **_kwargs):\n        for i in range(200):\n            time.sleep(0.02)\n            if i % 1000 == 420:\n                context.log.error(\n                    'Error message seq={i} from solid {name}'.format(\n                        i=i, name=name\n                    )\n                )\n            elif i % 100 == 0:\n                context.log.warning(\n                    'Warning message seq={i} from solid {name}'.format(\n                        i=i, name=name\n                    )\n                )\n            elif i % 10 == 0:\n                context.log.info(\n                    'Info message seq={i} from solid {name}'.format(\n                        i=i, name=name\n                    )\n                )\n            else:\n                context.log.debug(\n                    'Debug message seq={i} from solid {name}'.format(\n                        i=i, name=name\n                    )\n                )\n        return MultipleResults.from_dict(\n            {'output_{}'.format(i): 'foo' for i in range(n_outputs)}\n        )\n\n    return solid_fn","method_summary":"Creates a solid with the given number of (meaningless) inputs and outputs. Config controls the behavior of the nonce solid.","original_method_code":"def nonce_solid(name, n_inputs, n_outputs):\n    \"\"\"Creates a solid with the given number of (meaningless) inputs and outputs.\n\n    Config controls the behavior of the nonce solid.\"\"\"\n\n    @solid(\n        name=name,\n        inputs=[\n            InputDefinition(name='input_{}'.format(i)) for i in range(n_inputs)\n        ],\n        outputs=[\n            OutputDefinition(name='output_{}'.format(i))\n            for i in range(n_outputs)\n        ],\n    )\n    def solid_fn(context, **_kwargs):\n        for i in range(200):\n            time.sleep(0.02)\n            if i % 1000 == 420:\n                context.log.error(\n                    'Error message seq={i} from solid {name}'.format(\n                        i=i, name=name\n                    )\n                )\n            elif i % 100 == 0:\n                context.log.warning(\n                    'Warning message seq={i} from solid {name}'.format(\n                        i=i, name=name\n                    )\n                )\n            elif i % 10 == 0:\n                context.log.info(\n                    'Info message seq={i} from solid {name}'.format(\n                        i=i, name=name\n                    )\n                )\n            else:\n                context.log.debug(\n                    'Debug message seq={i} from solid {name}'.format(\n                        i=i, name=name\n                    )\n                )\n        return MultipleResults.from_dict(\n            {'output_{}'.format(i): 'foo' for i in range(n_outputs)}\n        )\n\n    return solid_fn","method_path":"https:\/\/github.com\/dagster-io\/dagster\/blob\/4119f8c773089de64831b1dfb9e168e353d401dc\/examples\/toys\/log_spew.py#L14-L60"}
{"repo_name":"dagster-io\/dagster","method_name":"format_config_for_graphql","method_code":"def format_config_for_graphql(config):\n    ''''''\n\n    def _format_config_subdict(config, current_indent=0):\n        check.dict_param(config, 'config', key_type=str)\n\n        printer = IndentingStringIoPrinter(indent_level=2, current_indent=current_indent)\n        printer.line('{')\n\n        n_elements = len(config)\n        for i, key in enumerate(sorted(config, key=lambda x: x[0])):\n            value = config[key]\n            with printer.with_indent():\n                formatted_value = (\n                    _format_config_item(value, current_indent=printer.current_indent)\n                    .lstrip(' ')\n                    .rstrip('\\n')\n                )\n                printer.line(\n                    '{key}: {formatted_value}{comma}'.format(\n                        key=key,\n                        formatted_value=formatted_value,\n                        comma=',' if i != n_elements - 1 else '',\n                    )\n                )\n        printer.line('}')\n\n        return printer.read()\n\n    def _format_config_sublist(config, current_indent=0):\n        printer = IndentingStringIoPrinter(indent_level=2, current_indent=current_indent)\n        printer.line('[')\n\n        n_elements = len(config)\n        for i, value in enumerate(config):\n            with printer.with_indent():\n                formatted_value = (\n                    _format_config_item(value, current_indent=printer.current_indent)\n                    .lstrip(' ')\n                    .rstrip('\\n')\n                )\n                printer.line(\n                    '{formatted_value}{comma}'.format(\n                        formatted_value=formatted_value, comma=',' if i != n_elements - 1 else ''\n                    )\n                )\n        printer.line(']')\n\n        return printer.read()\n\n    def _format_config_item(config, current_indent=0):\n        printer = IndentingStringIoPrinter(indent_level=2, current_indent=current_indent)\n\n        if isinstance(config, dict):\n            return _format_config_subdict(config, printer.current_indent)\n        elif isinstance(config, list):\n            return _format_config_sublist(config, printer.current_indent)\n        elif isinstance(config, bool):\n            return repr(config).lower()\n        else:\n            return repr(config).replace('\\'', '\"')\n\n    check.dict_param(config, 'config', key_type=str)\n    if not isinstance(config, dict):\n        check.failed('Expected a dict to format as config, got: {item}'.format(item=repr(config)))\n\n    return _format_config_subdict(config)","method_summary":"This recursive descent thing formats a config dict for GraphQL.","original_method_code":"def format_config_for_graphql(config):\n    '''This recursive descent thing formats a config dict for GraphQL.'''\n\n    def _format_config_subdict(config, current_indent=0):\n        check.dict_param(config, 'config', key_type=str)\n\n        printer = IndentingStringIoPrinter(indent_level=2, current_indent=current_indent)\n        printer.line('{')\n\n        n_elements = len(config)\n        for i, key in enumerate(sorted(config, key=lambda x: x[0])):\n            value = config[key]\n            with printer.with_indent():\n                formatted_value = (\n                    _format_config_item(value, current_indent=printer.current_indent)\n                    .lstrip(' ')\n                    .rstrip('\\n')\n                )\n                printer.line(\n                    '{key}: {formatted_value}{comma}'.format(\n                        key=key,\n                        formatted_value=formatted_value,\n                        comma=',' if i != n_elements - 1 else '',\n                    )\n                )\n        printer.line('}')\n\n        return printer.read()\n\n    def _format_config_sublist(config, current_indent=0):\n        printer = IndentingStringIoPrinter(indent_level=2, current_indent=current_indent)\n        printer.line('[')\n\n        n_elements = len(config)\n        for i, value in enumerate(config):\n            with printer.with_indent():\n                formatted_value = (\n                    _format_config_item(value, current_indent=printer.current_indent)\n                    .lstrip(' ')\n                    .rstrip('\\n')\n                )\n                printer.line(\n                    '{formatted_value}{comma}'.format(\n                        formatted_value=formatted_value, comma=',' if i != n_elements - 1 else ''\n                    )\n                )\n        printer.line(']')\n\n        return printer.read()\n\n    def _format_config_item(config, current_indent=0):\n        printer = IndentingStringIoPrinter(indent_level=2, current_indent=current_indent)\n\n        if isinstance(config, dict):\n            return _format_config_subdict(config, printer.current_indent)\n        elif isinstance(config, list):\n            return _format_config_sublist(config, printer.current_indent)\n        elif isinstance(config, bool):\n            return repr(config).lower()\n        else:\n            return repr(config).replace('\\'', '\"')\n\n    check.dict_param(config, 'config', key_type=str)\n    if not isinstance(config, dict):\n        check.failed('Expected a dict to format as config, got: {item}'.format(item=repr(config)))\n\n    return _format_config_subdict(config)","method_path":"https:\/\/github.com\/dagster-io\/dagster\/blob\/4119f8c773089de64831b1dfb9e168e353d401dc\/python_modules\/dagster-airflow\/dagster_airflow\/format.py#L5-L71"}
{"repo_name":"SectorLabs\/django-postgres-extra","method_name":"PostgresQuerySet.update","method_code":"def update(self, **fields):\n        \"\"\"\"\"\"\n\n        \n        self._for_write = True\n        if django.VERSION >= (2, 0):\n            query = self.query.chain(UpdateQuery)\n        else:\n            query = self.query.clone(UpdateQuery)\n        query._annotations = None\n        query.add_update_values(fields)\n\n        \n        connection = django.db.connections[self.db]\n        compiler = PostgresReturningUpdateCompiler(query, connection, self.db)\n\n        \n        with transaction.atomic(using=self.db, savepoint=False):\n            rows = compiler.execute_sql(CURSOR)\n        self._result_cache = None\n\n        \n        for row in rows:\n            signals.update.send(self.model, pk=row[0])\n\n        \n        \n        return len(rows)","method_summary":"Updates all rows that match the filter.","original_method_code":"def update(self, **fields):\n        \"\"\"Updates all rows that match the filter.\"\"\"\n\n        # build up the query to execute\n        self._for_write = True\n        if django.VERSION >= (2, 0):\n            query = self.query.chain(UpdateQuery)\n        else:\n            query = self.query.clone(UpdateQuery)\n        query._annotations = None\n        query.add_update_values(fields)\n\n        # build the compiler for for the query\n        connection = django.db.connections[self.db]\n        compiler = PostgresReturningUpdateCompiler(query, connection, self.db)\n\n        # execute the query\n        with transaction.atomic(using=self.db, savepoint=False):\n            rows = compiler.execute_sql(CURSOR)\n        self._result_cache = None\n\n        # send out a signal for each row\n        for row in rows:\n            signals.update.send(self.model, pk=row[0])\n\n        # the original update(..) returns the amount of rows\n        # affected, let's do the same\n        return len(rows)","method_path":"https:\/\/github.com\/SectorLabs\/django-postgres-extra\/blob\/eef2ed5504d225858d4e4f5d77a838082ca6053e\/psqlextra\/manager\/manager.py#L91-L118"}
{"repo_name":"SectorLabs\/django-postgres-extra","method_name":"PostgresQuerySet.on_conflict","method_code":"def on_conflict(self, fields: List[Union[str, Tuple[str]]], action, index_predicate: str=None):\n        \"\"\"\"\"\"\n\n        self.conflict_target = fields\n        self.conflict_action = action\n        self.index_predicate = index_predicate\n\n        return self","method_summary":"Sets the action to take when conflicts arise when attempting to insert\/create a new row.","original_method_code":"def on_conflict(self, fields: List[Union[str, Tuple[str]]], action, index_predicate: str=None):\n        \"\"\"Sets the action to take when conflicts arise when attempting\n        to insert\/create a new row.\n\n        Arguments:\n            fields:\n                The fields the conflicts can occur in.\n\n            action:\n                The action to take when the conflict occurs.\n\n            index_predicate:\n                The index predicate to satisfy an arbiter partial index (i.e. what partial index to use for checking\n                conflicts)\n        \"\"\"\n\n        self.conflict_target = fields\n        self.conflict_action = action\n        self.index_predicate = index_predicate\n\n        return self","method_path":"https:\/\/github.com\/SectorLabs\/django-postgres-extra\/blob\/eef2ed5504d225858d4e4f5d77a838082ca6053e\/psqlextra\/manager\/manager.py#L120-L140"}
{"repo_name":"SectorLabs\/django-postgres-extra","method_name":"PostgresQuerySet.bulk_insert","method_code":"def bulk_insert(self, rows, return_model=False):\n        \"\"\"\"\"\"\n\n        if self.conflict_target or self.conflict_action:\n            compiler = self._build_insert_compiler(rows)\n            objs = compiler.execute_sql(return_id=True)\n            if return_model:\n                return [self.model(**dict(r, **k)) for r, k in zip(rows, objs)]\n            else:\n                return [dict(r, **k) for r, k in zip(rows, objs)]\n\n        \n        return super().bulk_create([self.model(**fields) for fields in rows])","method_summary":"Creates multiple new records in the database. This allows specifying custom conflict behavior using .on_conflict(). If no special behavior was specified, this uses the normal Django create(","original_method_code":"def bulk_insert(self, rows, return_model=False):\n        \"\"\"Creates multiple new records in the database.\n\n        This allows specifying custom conflict behavior using .on_conflict().\n        If no special behavior was specified, this uses the normal Django create(..)\n\n        Arguments:\n            rows:\n                An array of dictionaries, where each dictionary\n                describes the fields to insert.\n\n            return_model (default: False):\n                If model instances should be returned rather than\n                just dicts.\n\n        Returns:\n            A list of either the dicts of the rows inserted, including the pk or\n            the models of the rows inserted with defaults for any fields not specified\n        \"\"\"\n\n        if self.conflict_target or self.conflict_action:\n            compiler = self._build_insert_compiler(rows)\n            objs = compiler.execute_sql(return_id=True)\n            if return_model:\n                return [self.model(**dict(r, **k)) for r, k in zip(rows, objs)]\n            else:\n                return [dict(r, **k) for r, k in zip(rows, objs)]\n\n        # no special action required, use the standard Django bulk_create(..)\n        return super().bulk_create([self.model(**fields) for fields in rows])","method_path":"https:\/\/github.com\/SectorLabs\/django-postgres-extra\/blob\/eef2ed5504d225858d4e4f5d77a838082ca6053e\/psqlextra\/manager\/manager.py#L142-L171"}
{"repo_name":"SectorLabs\/django-postgres-extra","method_name":"PostgresQuerySet.insert","method_code":"def insert(self, **fields):\n        \"\"\"\"\"\"\n\n        if self.conflict_target or self.conflict_action:\n            compiler = self._build_insert_compiler([fields])\n            rows = compiler.execute_sql(return_id=True)\n\n            pk_field_name = self.model._meta.pk.name\n            return rows[0][pk_field_name]\n\n        \n        return super().create(**fields).pk","method_summary":"Creates a new record in the database. This allows specifying custom conflict behavior using .on_conflict(). If no special behavior was specified, this uses the normal Django create(","original_method_code":"def insert(self, **fields):\n        \"\"\"Creates a new record in the database.\n\n        This allows specifying custom conflict behavior using .on_conflict().\n        If no special behavior was specified, this uses the normal Django create(..)\n\n        Arguments:\n            fields:\n                The fields of the row to create.\n\n        Returns:\n            The primary key of the record that was created.\n        \"\"\"\n\n        if self.conflict_target or self.conflict_action:\n            compiler = self._build_insert_compiler([fields])\n            rows = compiler.execute_sql(return_id=True)\n\n            pk_field_name = self.model._meta.pk.name\n            return rows[0][pk_field_name]\n\n        # no special action required, use the standard Django create(..)\n        return super().create(**fields).pk","method_path":"https:\/\/github.com\/SectorLabs\/django-postgres-extra\/blob\/eef2ed5504d225858d4e4f5d77a838082ca6053e\/psqlextra\/manager\/manager.py#L173-L195"}
{"repo_name":"SectorLabs\/django-postgres-extra","method_name":"PostgresQuerySet.upsert","method_code":"def upsert(self, conflict_target: List, fields: Dict, index_predicate: str=None) -> int:\n        \"\"\"\"\"\"\n\n        self.on_conflict(conflict_target, ConflictAction.UPDATE, index_predicate)\n        return self.insert(**fields)","method_summary":"Creates a new record or updates the existing one with the specified data.","original_method_code":"def upsert(self, conflict_target: List, fields: Dict, index_predicate: str=None) -> int:\n        \"\"\"Creates a new record or updates the existing one\n        with the specified data.\n\n        Arguments:\n            conflict_target:\n                Fields to pass into the ON CONFLICT clause.\n\n            fields:\n                Fields to insert\/update.\n\n            index_predicate:\n                The index predicate to satisfy an arbiter partial index (i.e. what partial index to use for checking\n                conflicts)\n\n        Returns:\n            The primary key of the row that was created\/updated.\n        \"\"\"\n\n        self.on_conflict(conflict_target, ConflictAction.UPDATE, index_predicate)\n        return self.insert(**fields)","method_path":"https:\/\/github.com\/SectorLabs\/django-postgres-extra\/blob\/eef2ed5504d225858d4e4f5d77a838082ca6053e\/psqlextra\/manager\/manager.py#L238-L258"}
{"repo_name":"SectorLabs\/django-postgres-extra","method_name":"PostgresQuerySet.upsert_and_get","method_code":"def upsert_and_get(self, conflict_target: List, fields: Dict, index_predicate: str=None):\n        \"\"\"\"\"\"\n\n        self.on_conflict(conflict_target, ConflictAction.UPDATE, index_predicate)\n        return self.insert_and_get(**fields)","method_summary":"Creates a new record or updates the existing one with the specified data and then gets the row.","original_method_code":"def upsert_and_get(self, conflict_target: List, fields: Dict, index_predicate: str=None):\n        \"\"\"Creates a new record or updates the existing one\n        with the specified data and then gets the row.\n\n        Arguments:\n            conflict_target:\n                Fields to pass into the ON CONFLICT clause.\n\n            fields:\n                Fields to insert\/update.\n\n            index_predicate:\n                The index predicate to satisfy an arbiter partial index (i.e. what partial index to use for checking\n                conflicts)\n\n        Returns:\n            The model instance representing the row\n            that was created\/updated.\n        \"\"\"\n\n        self.on_conflict(conflict_target, ConflictAction.UPDATE, index_predicate)\n        return self.insert_and_get(**fields)","method_path":"https:\/\/github.com\/SectorLabs\/django-postgres-extra\/blob\/eef2ed5504d225858d4e4f5d77a838082ca6053e\/psqlextra\/manager\/manager.py#L260-L281"}
{"repo_name":"SectorLabs\/django-postgres-extra","method_name":"PostgresQuerySet.bulk_upsert","method_code":"def bulk_upsert(self, conflict_target: List, rows: List[Dict], index_predicate: str=None):\n        \"\"\"\"\"\"\n\n        if not rows or len(rows) <= 0:\n            return\n\n        self.on_conflict(conflict_target, ConflictAction.UPDATE, index_predicate)\n        return self.bulk_insert(rows)","method_summary":"Creates a set of new records or updates the existing ones with the specified data.","original_method_code":"def bulk_upsert(self, conflict_target: List, rows: List[Dict], index_predicate: str=None):\n        \"\"\"Creates a set of new records or updates the existing\n        ones with the specified data.\n\n        Arguments:\n            conflict_target:\n                Fields to pass into the ON CONFLICT clause.\n\n            rows:\n                Rows to upsert.\n\n            index_predicate:\n                The index predicate to satisfy an arbiter partial index (i.e. what partial index to use for checking\n                conflicts)\n        \"\"\"\n\n        if not rows or len(rows) <= 0:\n            return\n\n        self.on_conflict(conflict_target, ConflictAction.UPDATE, index_predicate)\n        return self.bulk_insert(rows)","method_path":"https:\/\/github.com\/SectorLabs\/django-postgres-extra\/blob\/eef2ed5504d225858d4e4f5d77a838082ca6053e\/psqlextra\/manager\/manager.py#L283-L303"}
{"repo_name":"SectorLabs\/django-postgres-extra","method_name":"PostgresQuerySet._build_insert_compiler","method_code":"def _build_insert_compiler(self, rows: List[Dict]):\n        \"\"\"\"\"\"\n\n        \n        \n        \n        \n        \n        objs = []\n        field_count = len(rows[0])\n        for index, row in enumerate(rows):\n            if field_count != len(row):\n                raise SuspiciousOperation((\n                    'In bulk upserts, you cannot have rows with different field '\n                    'configurations. Row {0} has a different field config than '\n                    'the first row.'\n                ).format(index))\n\n            objs.append(self.model(**row))\n\n        \n        self._for_write = True\n\n        \n        insert_fields, update_fields = self._get_upsert_fields(rows[0])\n\n        \n        query = PostgresInsertQuery(self.model)\n        query.conflict_action = self.conflict_action\n        query.conflict_target = self.conflict_target\n        query.index_predicate = self.index_predicate\n        query.values(objs, insert_fields, update_fields)\n\n        \n        \n        connection = django.db.connections[self.db]\n        compiler = PostgresInsertCompiler(query, connection, self.db)\n\n        return compiler","method_summary":"Builds the SQL compiler for a insert query.","original_method_code":"def _build_insert_compiler(self, rows: List[Dict]):\n        \"\"\"Builds the SQL compiler for a insert query.\n\n        Arguments:\n            rows:\n                A list of dictionaries, where each entry\n                describes a record to insert.\n\n        Returns:\n            The SQL compiler for the insert.\n        \"\"\"\n\n        # create model objects, we also have to detect cases\n        # such as:\n        #   [dict(first_name='swen'), dict(fist_name='swen', last_name='kooij')]\n        # we need to be certain that each row specifies the exact same\n        # amount of fields\/columns\n        objs = []\n        field_count = len(rows[0])\n        for index, row in enumerate(rows):\n            if field_count != len(row):\n                raise SuspiciousOperation((\n                    'In bulk upserts, you cannot have rows with different field '\n                    'configurations. Row {0} has a different field config than '\n                    'the first row.'\n                ).format(index))\n\n            objs.append(self.model(**row))\n\n        # indicate this query is going to perform write\n        self._for_write = True\n\n        # get the fields to be used during update\/insert\n        insert_fields, update_fields = self._get_upsert_fields(rows[0])\n\n        # build a normal insert query\n        query = PostgresInsertQuery(self.model)\n        query.conflict_action = self.conflict_action\n        query.conflict_target = self.conflict_target\n        query.index_predicate = self.index_predicate\n        query.values(objs, insert_fields, update_fields)\n\n        # use the postgresql insert query compiler to transform the insert\n        # into an special postgresql insert\n        connection = django.db.connections[self.db]\n        compiler = PostgresInsertCompiler(query, connection, self.db)\n\n        return compiler","method_path":"https:\/\/github.com\/SectorLabs\/django-postgres-extra\/blob\/eef2ed5504d225858d4e4f5d77a838082ca6053e\/psqlextra\/manager\/manager.py#L305-L352"}
{"repo_name":"SectorLabs\/django-postgres-extra","method_name":"PostgresQuerySet._is_magical_field","method_code":"def _is_magical_field(self, model_instance, field, is_insert: bool):\n        \"\"\"\"\"\"\n\n        \n        old_value = getattr(model_instance, field.name, None)\n        field.pre_save(model_instance, is_insert)\n        new_value = getattr(model_instance, field.name, None)\n\n        return old_value != new_value","method_summary":"Verifies whether this field is gonna modify something on its own. \"Magical\" means that a field modifies the field value during the pre_save.","original_method_code":"def _is_magical_field(self, model_instance, field, is_insert: bool):\n        \"\"\"Verifies whether this field is gonna modify something\n        on its own.\n\n        \"Magical\" means that a field modifies the field value\n        during the pre_save.\n\n        Arguments:\n            model_instance:\n                The model instance the field is defined on.\n\n            field:\n                The field to get of whether the field is\n                magical.\n\n            is_insert:\n                Pretend whether this is an insert?\n\n        Returns:\n            True when this field modifies something.\n        \"\"\"\n\n        # does this field modify someting upon insert?\n        old_value = getattr(model_instance, field.name, None)\n        field.pre_save(model_instance, is_insert)\n        new_value = getattr(model_instance, field.name, None)\n\n        return old_value != new_value","method_path":"https:\/\/github.com\/SectorLabs\/django-postgres-extra\/blob\/eef2ed5504d225858d4e4f5d77a838082ca6053e\/psqlextra\/manager\/manager.py#L354-L381"}
{"repo_name":"SectorLabs\/django-postgres-extra","method_name":"PostgresManager.on_conflict","method_code":"def on_conflict(self, fields: List[Union[str, Tuple[str]]], action, index_predicate: str=None):\n        \"\"\"\"\"\"\n        return self.get_queryset().on_conflict(fields, action, index_predicate)","method_summary":"Sets the action to take when conflicts arise when attempting to insert\/create a new row.","original_method_code":"def on_conflict(self, fields: List[Union[str, Tuple[str]]], action, index_predicate: str=None):\n        \"\"\"Sets the action to take when conflicts arise when attempting\n        to insert\/create a new row.\n\n        Arguments:\n            fields:\n                The fields the conflicts can occur in.\n\n            action:\n                The action to take when the conflict occurs.\n\n            index_predicate:\n                The index predicate to satisfy an arbiter partial index.\n        \"\"\"\n        return self.get_queryset().on_conflict(fields, action, index_predicate)","method_path":"https:\/\/github.com\/SectorLabs\/django-postgres-extra\/blob\/eef2ed5504d225858d4e4f5d77a838082ca6053e\/psqlextra\/manager\/manager.py#L491-L505"}
{"repo_name":"chaoss\/grimoirelab-perceval","method_name":"Mattermost.fetch_items","method_code":"def fetch_items(self, category, **kwargs):\n        \"\"\"\"\"\"\n        from_date = kwargs['from_date']\n\n        logger.info(\"Fetching messages of '%s' - '%s' channel from %s\",\n                    self.url, self.channel, str(from_date))\n\n        fetching = True\n        page = 0\n        nposts = 0\n\n        \n        since = int(from_date.timestamp() * 1000)\n\n        while fetching:\n            raw_posts = self.client.posts(self.channel, page=page)\n\n            posts_before = nposts\n\n            for post in self._parse_posts(raw_posts):\n                if post['update_at'] < since:\n                    fetching = False\n                    break\n\n                \n                user_id = post['user_id']\n                user = self._get_or_fetch_user(user_id)\n                post['user_data'] = user\n\n                yield post\n                nposts += 1\n\n            if fetching:\n                \n                if posts_before == nposts:\n                    fetching = False\n                else:\n                    page += 1\n\n        logger.info(\"Fetch process completed: %s posts fetched\", nposts)","method_summary":"Fetch the messages.","original_method_code":"def fetch_items(self, category, **kwargs):\n        \"\"\"Fetch the messages.\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"\n        from_date = kwargs['from_date']\n\n        logger.info(\"Fetching messages of '%s' - '%s' channel from %s\",\n                    self.url, self.channel, str(from_date))\n\n        fetching = True\n        page = 0\n        nposts = 0\n\n        # Convert timestamp to integer for comparing\n        since = int(from_date.timestamp() * 1000)\n\n        while fetching:\n            raw_posts = self.client.posts(self.channel, page=page)\n\n            posts_before = nposts\n\n            for post in self._parse_posts(raw_posts):\n                if post['update_at'] < since:\n                    fetching = False\n                    break\n\n                # Fetch user data\n                user_id = post['user_id']\n                user = self._get_or_fetch_user(user_id)\n                post['user_data'] = user\n\n                yield post\n                nposts += 1\n\n            if fetching:\n                # If no new posts were fetched; stop the process\n                if posts_before == nposts:\n                    fetching = False\n                else:\n                    page += 1\n\n        logger.info(\"Fetch process completed: %s posts fetched\", nposts)","method_path":"https:\/\/github.com\/chaoss\/grimoirelab-perceval\/blob\/41c908605e88b7ebc3a536c643fa0f212eaf9e0e\/perceval\/backends\/core\/mattermost.py#L116-L161"}
{"repo_name":"chaoss\/grimoirelab-perceval","method_name":"Mattermost._init_client","method_code":"def _init_client(self, from_archive=False):\n        \"\"\"\"\"\"\n\n        return MattermostClient(self.url, self.api_token,\n                                max_items=self.max_items,\n                                sleep_for_rate=self.sleep_for_rate,\n                                min_rate_to_sleep=self.min_rate_to_sleep,\n                                sleep_time=self.sleep_time,\n                                archive=self.archive, from_archive=from_archive)","method_summary":"Init client","original_method_code":"def _init_client(self, from_archive=False):\n        \"\"\"Init client\"\"\"\n\n        return MattermostClient(self.url, self.api_token,\n                                max_items=self.max_items,\n                                sleep_for_rate=self.sleep_for_rate,\n                                min_rate_to_sleep=self.min_rate_to_sleep,\n                                sleep_time=self.sleep_time,\n                                archive=self.archive, from_archive=from_archive)","method_path":"https:\/\/github.com\/chaoss\/grimoirelab-perceval\/blob\/41c908605e88b7ebc3a536c643fa0f212eaf9e0e\/perceval\/backends\/core\/mattermost.py#L224-L232"}
{"repo_name":"chaoss\/grimoirelab-perceval","method_name":"Mattermost._parse_posts","method_code":"def _parse_posts(self, raw_posts):\n        \"\"\"\"\"\"\n\n        parsed_posts = self.parse_json(raw_posts)\n\n        \n        \n        for post_id in parsed_posts['order']:\n            yield parsed_posts['posts'][post_id]","method_summary":"Parse posts and returns in order.","original_method_code":"def _parse_posts(self, raw_posts):\n        \"\"\"Parse posts and returns in order.\"\"\"\n\n        parsed_posts = self.parse_json(raw_posts)\n\n        # Posts are not sorted. The order is provided by\n        # 'order' key.\n        for post_id in parsed_posts['order']:\n            yield parsed_posts['posts'][post_id]","method_path":"https:\/\/github.com\/chaoss\/grimoirelab-perceval\/blob\/41c908605e88b7ebc3a536c643fa0f212eaf9e0e\/perceval\/backends\/core\/mattermost.py#L234-L242"}
{"repo_name":"chaoss\/grimoirelab-perceval","method_name":"MattermostClient.posts","method_code":"def posts(self, channel, page=None):\n        \"\"\"\"\"\"\n\n        entrypoint = self.RCHANNELS + '\/' + channel + '\/' + self.RPOSTS\n\n        params = {\n            self.PPER_PAGE: self.max_items\n        }\n\n        if page is not None:\n            params[self.PPAGE] = page\n\n        response = self._fetch(entrypoint, params)\n\n        return response","method_summary":"Fetch the history of a channel.","original_method_code":"def posts(self, channel, page=None):\n        \"\"\"Fetch the history of a channel.\"\"\"\n\n        entrypoint = self.RCHANNELS + '\/' + channel + '\/' + self.RPOSTS\n\n        params = {\n            self.PPER_PAGE: self.max_items\n        }\n\n        if page is not None:\n            params[self.PPAGE] = page\n\n        response = self._fetch(entrypoint, params)\n\n        return response","method_path":"https:\/\/github.com\/chaoss\/grimoirelab-perceval\/blob\/41c908605e88b7ebc3a536c643fa0f212eaf9e0e\/perceval\/backends\/core\/mattermost.py#L297-L311"}
{"repo_name":"chaoss\/grimoirelab-perceval","method_name":"MattermostClient.user","method_code":"def user(self, user):\n        \"\"\"\"\"\"\n\n        entrypoint = self.RUSERS + '\/' + user\n        response = self._fetch(entrypoint, None)\n\n        return response","method_summary":"Fetch user data.","original_method_code":"def user(self, user):\n        \"\"\"Fetch user data.\"\"\"\n\n        entrypoint = self.RUSERS + '\/' + user\n        response = self._fetch(entrypoint, None)\n\n        return response","method_path":"https:\/\/github.com\/chaoss\/grimoirelab-perceval\/blob\/41c908605e88b7ebc3a536c643fa0f212eaf9e0e\/perceval\/backends\/core\/mattermost.py#L313-L319"}
{"repo_name":"chaoss\/grimoirelab-perceval","method_name":"MattermostClient._fetch","method_code":"def _fetch(self, entry_point, params):\n        \"\"\"\"\"\"\n        url = self.API_URL % {'base_url': self.base_url, 'entrypoint': entry_point}\n\n        logger.debug(\"Mattermost client requests: %s params: %s\",\n                     entry_point, str(params))\n\n        r = self.fetch(url, payload=params)\n\n        return r.text","method_summary":"Fetch a resource.","original_method_code":"def _fetch(self, entry_point, params):\n        \"\"\"Fetch a resource.\n\n        :param entrypoint: entrypoint to access\n        :param params: dict with the HTTP parameters needed to access the\n            given entry point\n        \"\"\"\n        url = self.API_URL % {'base_url': self.base_url, 'entrypoint': entry_point}\n\n        logger.debug(\"Mattermost client requests: %s params: %s\",\n                     entry_point, str(params))\n\n        r = self.fetch(url, payload=params)\n\n        return r.text","method_path":"https:\/\/github.com\/chaoss\/grimoirelab-perceval\/blob\/41c908605e88b7ebc3a536c643fa0f212eaf9e0e\/perceval\/backends\/core\/mattermost.py#L358-L372"}
{"repo_name":"chaoss\/grimoirelab-perceval","method_name":"PipermailCommand._pre_init","method_code":"def _pre_init(self):\n        \"\"\"\"\"\"\n\n        if not self.parsed_args.mboxes_path:\n            base_path = os.path.expanduser('~\/.perceval\/mailinglists\/')\n            dirpath = os.path.join(base_path, self.parsed_args.url)\n        else:\n            dirpath = self.parsed_args.mboxes_path\n\n        setattr(self.parsed_args, 'dirpath', dirpath)","method_summary":"Initialize mailing lists directory path","original_method_code":"def _pre_init(self):\n        \"\"\"Initialize mailing lists directory path\"\"\"\n\n        if not self.parsed_args.mboxes_path:\n            base_path = os.path.expanduser('~\/.perceval\/mailinglists\/')\n            dirpath = os.path.join(base_path, self.parsed_args.url)\n        else:\n            dirpath = self.parsed_args.mboxes_path\n\n        setattr(self.parsed_args, 'dirpath', dirpath)","method_path":"https:\/\/github.com\/chaoss\/grimoirelab-perceval\/blob\/41c908605e88b7ebc3a536c643fa0f212eaf9e0e\/perceval\/backends\/core\/pipermail.py#L136-L145"}
{"repo_name":"chaoss\/grimoirelab-perceval","method_name":"PipermailList.mboxes","method_code":"def mboxes(self):\n        \"\"\"\"\"\"\n        archives = []\n\n        for mbox in super().mboxes:\n            dt = self._parse_date_from_filepath(mbox.filepath)\n            archives.append((dt, mbox))\n\n        archives.sort(key=lambda x: x[0])\n\n        return [a[1] for a in archives]","method_summary":"Get the mboxes managed by this mailing list.","original_method_code":"def mboxes(self):\n        \"\"\"Get the mboxes managed by this mailing list.\n\n        Returns the archives sorted by date in ascending order.\n\n        :returns: a list of `.MBoxArchive` objects\n        \"\"\"\n        archives = []\n\n        for mbox in super().mboxes:\n            dt = self._parse_date_from_filepath(mbox.filepath)\n            archives.append((dt, mbox))\n\n        archives.sort(key=lambda x: x[0])\n\n        return [a[1] for a in archives]","method_path":"https:\/\/github.com\/chaoss\/grimoirelab-perceval\/blob\/41c908605e88b7ebc3a536c643fa0f212eaf9e0e\/perceval\/backends\/core\/pipermail.py#L240-L255"}
{"repo_name":"chaoss\/grimoirelab-perceval","method_name":"RSS.fetch","method_code":"def fetch(self, category=CATEGORY_ENTRY):\n        \"\"\"\"\"\"\n        kwargs = {}\n        items = super().fetch(category, **kwargs)\n\n        return items","method_summary":"Fetch the entries from the url. The method retrieves all entries from a RSS url","original_method_code":"def fetch(self, category=CATEGORY_ENTRY):\n        \"\"\"Fetch the entries from the url.\n\n        The method retrieves all entries from a RSS url\n\n        :param category: the category of items to fetch\n\n        :returns: a generator of entries\n        \"\"\"\n        kwargs = {}\n        items = super().fetch(category, **kwargs)\n\n        return items","method_path":"https:\/\/github.com\/chaoss\/grimoirelab-perceval\/blob\/41c908605e88b7ebc3a536c643fa0f212eaf9e0e\/perceval\/backends\/core\/rss.py#L61-L73"}
{"repo_name":"chaoss\/grimoirelab-perceval","method_name":"RSS.fetch_items","method_code":"def fetch_items(self, category, **kwargs):\n        \"\"\"\"\"\"\n        logger.info(\"Looking for rss entries at feed '%s'\", self.url)\n\n        nentries = 0  \n\n        raw_entries = self.client.get_entries()\n        entries = self.parse_feed(raw_entries)['entries']\n        for item in entries:\n            yield item\n            nentries += 1\n\n        logger.info(\"Total number of entries: %i\", nentries)","method_summary":"Fetch the entries","original_method_code":"def fetch_items(self, category, **kwargs):\n        \"\"\"Fetch the entries\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"\n        logger.info(\"Looking for rss entries at feed '%s'\", self.url)\n\n        nentries = 0  # number of entries\n\n        raw_entries = self.client.get_entries()\n        entries = self.parse_feed(raw_entries)['entries']\n        for item in entries:\n            yield item\n            nentries += 1\n\n        logger.info(\"Total number of entries: %i\", nentries)","method_path":"https:\/\/github.com\/chaoss\/grimoirelab-perceval\/blob\/41c908605e88b7ebc3a536c643fa0f212eaf9e0e\/perceval\/backends\/core\/rss.py#L75-L93"}
{"repo_name":"celiao\/tmdbsimple","method_name":"Genres.movie_list","method_code":"def movie_list(self, **kwargs):\n        \"\"\"\"\"\"\n        path = self._get_path('movie_list')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_summary":"Get the list of Movie genres.","original_method_code":"def movie_list(self, **kwargs):\n        \"\"\"\n        Get the list of Movie genres.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_path('movie_list')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_path":"https:\/\/github.com\/celiao\/tmdbsimple\/blob\/ff17893110c99771d6398a62c35d36dd9735f4b9\/tmdbsimple\/genres.py#L34-L48"}
{"repo_name":"celiao\/tmdbsimple","method_name":"Genres.tv_list","method_code":"def tv_list(self, **kwargs):\n        \"\"\"\"\"\"\n        path = self._get_path('tv_list')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_summary":"Get the list of TV genres.","original_method_code":"def tv_list(self, **kwargs):\n        \"\"\"\n        Get the list of TV genres.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_path('tv_list')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_path":"https:\/\/github.com\/celiao\/tmdbsimple\/blob\/ff17893110c99771d6398a62c35d36dd9735f4b9\/tmdbsimple\/genres.py#L50-L64"}
{"repo_name":"celiao\/tmdbsimple","method_name":"Genres.movies","method_code":"def movies(self, **kwargs):\n        \"\"\"\"\"\"\n        path = self._get_id_path('movies')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_summary":"Get the list of movies for a particular genre by id. By default, only movies with 10 or more votes are included.","original_method_code":"def movies(self, **kwargs):\n        \"\"\"\n        Get the list of movies for a particular genre by id. By default, only\n        movies with 10 or more votes are included.\n\n        Args:\n            page: (optional) Minimum 1, maximum 1000.\n            language: (optional) ISO 639-1 code.\n            include_all_movies: (optional) Toggle the inclusion of all movies \n                                and not just those with 10 or more ratings. \n                                Expected value is: True or False.\n            include_adult: (optional) Toggle the inclusion of adult titles.\n                           Expected value is: True or False.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('movies')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_path":"https:\/\/github.com\/celiao\/tmdbsimple\/blob\/ff17893110c99771d6398a62c35d36dd9735f4b9\/tmdbsimple\/genres.py#L66-L87"}
{"repo_name":"celiao\/tmdbsimple","method_name":"Movies.info","method_code":"def info(self, **kwargs):\n        \"\"\"\"\"\"\n        path = self._get_id_path('info')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_summary":"Get the basic movie information for a specific movie id.","original_method_code":"def info(self, **kwargs):\n        \"\"\"\n        Get the basic movie information for a specific movie id.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            append_to_response: (optional) Comma separated, any movie method.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('info')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_path":"https:\/\/github.com\/celiao\/tmdbsimple\/blob\/ff17893110c99771d6398a62c35d36dd9735f4b9\/tmdbsimple\/movies.py#L53-L68"}
{"repo_name":"celiao\/tmdbsimple","method_name":"Movies.alternative_titles","method_code":"def alternative_titles(self, **kwargs):\n        \"\"\"\"\"\"\n        path = self._get_id_path('alternative_titles')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_summary":"Get the alternative titles for a specific movie id.","original_method_code":"def alternative_titles(self, **kwargs):\n        \"\"\"\n        Get the alternative titles for a specific movie id.\n\n        Args:\n            country: (optional) ISO 3166-1 code.\n            append_to_response: (optional) Comma separated, any movie method.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('alternative_titles')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_path":"https:\/\/github.com\/celiao\/tmdbsimple\/blob\/ff17893110c99771d6398a62c35d36dd9735f4b9\/tmdbsimple\/movies.py#L70-L85"}
{"repo_name":"celiao\/tmdbsimple","method_name":"Movies.credits","method_code":"def credits(self, **kwargs):\n        \"\"\"\"\"\"\n        path = self._get_id_path('credits')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_summary":"Get the cast and crew information for a specific movie id.","original_method_code":"def credits(self, **kwargs):\n        \"\"\"\n        Get the cast and crew information for a specific movie id.\n\n        Args:\n            append_to_response: (optional) Comma separated, any movie method.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('credits')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_path":"https:\/\/github.com\/celiao\/tmdbsimple\/blob\/ff17893110c99771d6398a62c35d36dd9735f4b9\/tmdbsimple\/movies.py#L87-L101"}
{"repo_name":"celiao\/tmdbsimple","method_name":"Movies.external_ids","method_code":"def external_ids(self, **kwargs):\n        \"\"\"\"\"\"\n        path = self._get_id_path('external_ids')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_summary":"Get the external ids for a specific movie id.","original_method_code":"def external_ids(self, **kwargs):\n        \"\"\"\n        Get the external ids for a specific movie id.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            append_to_response: (optional) Comma separated, any movie method.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('external_ids')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_path":"https:\/\/github.com\/celiao\/tmdbsimple\/blob\/ff17893110c99771d6398a62c35d36dd9735f4b9\/tmdbsimple\/movies.py#L103-L118"}
{"repo_name":"celiao\/tmdbsimple","method_name":"Movies.images","method_code":"def images(self, **kwargs):\n        \"\"\"\"\"\"\n        path = self._get_id_path('images')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_summary":"Get the images (posters and backdrops) for a specific movie id.","original_method_code":"def images(self, **kwargs):\n        \"\"\"\n        Get the images (posters and backdrops) for a specific movie id.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            append_to_response: (optional) Comma separated, any movie method.\n            include_image_language: (optional) Comma separated, a valid\n                                    ISO 69-1.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('images')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_path":"https:\/\/github.com\/celiao\/tmdbsimple\/blob\/ff17893110c99771d6398a62c35d36dd9735f4b9\/tmdbsimple\/movies.py#L120-L137"}
{"repo_name":"celiao\/tmdbsimple","method_name":"Movies.keywords","method_code":"def keywords(self):\n        \"\"\"\"\"\"\n        path = self._get_id_path('keywords')\n\n        response = self._GET(path)\n        self._set_attrs_to_values(response)\n        return response","method_summary":"Get the plot keywords for a specific movie id.","original_method_code":"def keywords(self):\n        \"\"\"\n        Get the plot keywords for a specific movie id.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('keywords')\n\n        response = self._GET(path)\n        self._set_attrs_to_values(response)\n        return response","method_path":"https:\/\/github.com\/celiao\/tmdbsimple\/blob\/ff17893110c99771d6398a62c35d36dd9735f4b9\/tmdbsimple\/movies.py#L139-L150"}
{"repo_name":"celiao\/tmdbsimple","method_name":"Movies.recommendations","method_code":"def recommendations(self, **kwargs):\n        \"\"\"\"\"\"\n        path = self._get_id_path('recommendations')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_summary":"Get a list of recommended movies for a movie.","original_method_code":"def recommendations(self, **kwargs):\n        \"\"\"\n        Get a list of recommended movies for a movie.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            page: (optional) Minimum value of 1.  Expected value is an integer.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('recommendations')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response","method_path":"https:\/\/github.com\/celiao\/tmdbsimple\/blob\/ff17893110c99771d6398a62c35d36dd9735f4b9\/tmdbsimple\/movies.py#L152-L167"}
{"repo_name":"bloomreach\/s4cmd","method_name":"log_calls","method_code":"def log_calls(func):\n  ''''''\n  def wrapper(*args, **kargs):\n    callStr = \"%s(%s)\" % (func.__name__, \", \".join([repr(p) for p in args] + [\"%s=%s\" % (k, repr(v)) for (k, v) in list(kargs.items())]))\n    debug(\">> %s\", callStr)\n    ret = func(*args, **kargs)\n    debug(\"<< %s: %s\", callStr, repr(ret))\n    return ret\n  return wrapper","method_summary":"Decorator to log function calls.","original_method_code":"def log_calls(func):\n  '''Decorator to log function calls.'''\n  def wrapper(*args, **kargs):\n    callStr = \"%s(%s)\" % (func.__name__, \", \".join([repr(p) for p in args] + [\"%s=%s\" % (k, repr(v)) for (k, v) in list(kargs.items())]))\n    debug(\">> %s\", callStr)\n    ret = func(*args, **kargs)\n    debug(\"<< %s: %s\", callStr, repr(ret))\n    return ret\n  return wrapper","method_path":"https:\/\/github.com\/bloomreach\/s4cmd\/blob\/bb51075bf43703e7cd95aa39288cf7732ec13a6d\/s4cmd.py#L124-L132"}
{"repo_name":"bloomreach\/s4cmd","method_name":"synchronized","method_code":"def synchronized(func):\n  ''''''\n  func.__lock__ = threading.Lock()\n  def synced_func(*args, **kargs):\n    with func.__lock__:\n      return func(*args, **kargs)\n  return synced_func","method_summary":"Decorator to synchronize function.","original_method_code":"def synchronized(func):\n  '''Decorator to synchronize function.'''\n  func.__lock__ = threading.Lock()\n  def synced_func(*args, **kargs):\n    with func.__lock__:\n      return func(*args, **kargs)\n  return synced_func","method_path":"https:\/\/github.com\/bloomreach\/s4cmd\/blob\/bb51075bf43703e7cd95aa39288cf7732ec13a6d\/s4cmd.py#L138-L144"}
{"repo_name":"bloomreach\/s4cmd","method_name":"progress","method_code":"def progress(msg, *args):\n  ''''''\n  \n  if not (sys.stdout.isatty() and sys.stderr.isatty()):\n    return\n\n  text = (msg % args)\n  if progress.prev_message:\n    sys.stderr.write(' ' * len(progress.prev_message) + '\\r')\n  sys.stderr.write(text + '\\r')\n  progress.prev_message = text","method_summary":"Show current progress message to stderr. This function will remember the previous message so that next time, it will clear the previous message before showing next one.","original_method_code":"def progress(msg, *args):\n  '''Show current progress message to stderr.\n     This function will remember the previous message so that next time,\n     it will clear the previous message before showing next one.\n  '''\n  # Don't show any progress if the output is directed to a file.\n  if not (sys.stdout.isatty() and sys.stderr.isatty()):\n    return\n\n  text = (msg % args)\n  if progress.prev_message:\n    sys.stderr.write(' ' * len(progress.prev_message) + '\\r')\n  sys.stderr.write(text + '\\r')\n  progress.prev_message = text","method_path":"https:\/\/github.com\/bloomreach\/s4cmd\/blob\/bb51075bf43703e7cd95aa39288cf7732ec13a6d\/s4cmd.py#L151-L164"}
{"repo_name":"bloomreach\/s4cmd","method_name":"message","method_code":"def message(msg, *args):\n  ''''''\n  clear_progress()\n  text = (msg % args)\n  sys.stdout.write(text + '\\n')","method_summary":"Program message output.","original_method_code":"def message(msg, *args):\n  '''Program message output.'''\n  clear_progress()\n  text = (msg % args)\n  sys.stdout.write(text + '\\n')","method_path":"https:\/\/github.com\/bloomreach\/s4cmd\/blob\/bb51075bf43703e7cd95aa39288cf7732ec13a6d\/s4cmd.py#L169-L173"}
{"repo_name":"bloomreach\/s4cmd","method_name":"fail","method_code":"def fail(message, exc_info=None, status=1, stacktrace=False):\n  ''''''\n  text = message\n  if exc_info:\n    text += str(exc_info)\n  error(text)\n  if stacktrace:\n    error(traceback.format_exc())\n  clean_tempfiles()\n  if __name__ == '__main__':\n    sys.exit(status)\n  else:\n    raise RuntimeError(status)","method_summary":"Utility function to handle runtime failures gracefully. Show concise information if possible, then terminate program.","original_method_code":"def fail(message, exc_info=None, status=1, stacktrace=False):\n  '''Utility function to handle runtime failures gracefully.\n     Show concise information if possible, then terminate program.\n  '''\n  text = message\n  if exc_info:\n    text += str(exc_info)\n  error(text)\n  if stacktrace:\n    error(traceback.format_exc())\n  clean_tempfiles()\n  if __name__ == '__main__':\n    sys.exit(status)\n  else:\n    raise RuntimeError(status)","method_path":"https:\/\/github.com\/bloomreach\/s4cmd\/blob\/bb51075bf43703e7cd95aa39288cf7732ec13a6d\/s4cmd.py#L175-L189"}
{"repo_name":"bloomreach\/s4cmd","method_name":"tempfile_get","method_code":"def tempfile_get(target):\n  ''''''\n  fn = '%s-%s.tmp' % (target, ''.join(random.Random().sample(\"0123456789abcdefghijklmnopqrstuvwxyz\", 15)))\n  TEMP_FILES.add(fn)\n  return fn","method_summary":"Get a temp filename for atomic download.","original_method_code":"def tempfile_get(target):\n  '''Get a temp filename for atomic download.'''\n  fn = '%s-%s.tmp' % (target, ''.join(random.Random().sample(\"0123456789abcdefghijklmnopqrstuvwxyz\", 15)))\n  TEMP_FILES.add(fn)\n  return fn","method_path":"https:\/\/github.com\/bloomreach\/s4cmd\/blob\/bb51075bf43703e7cd95aa39288cf7732ec13a6d\/s4cmd.py#L192-L196"}
{"repo_name":"bloomreach\/s4cmd","method_name":"tempfile_set","method_code":"def tempfile_set(tempfile, target):\n  ''''''\n  if target:\n    os.rename(tempfile, target)\n  else:\n    os.unlink(tempfile)\n\n  if target in TEMP_FILES:\n    TEMP_FILES.remove(tempfile)","method_summary":"Atomically rename and clean tempfile","original_method_code":"def tempfile_set(tempfile, target):\n  '''Atomically rename and clean tempfile'''\n  if target:\n    os.rename(tempfile, target)\n  else:\n    os.unlink(tempfile)\n\n  if target in TEMP_FILES:\n    TEMP_FILES.remove(tempfile)","method_path":"https:\/\/github.com\/bloomreach\/s4cmd\/blob\/bb51075bf43703e7cd95aa39288cf7732ec13a6d\/s4cmd.py#L199-L207"}
{"repo_name":"bloomreach\/s4cmd","method_name":"clean_tempfiles","method_code":"def clean_tempfiles():\n  ''''''\n  for fn in TEMP_FILES:\n    if os.path.exists(fn):\n      os.unlink(fn)","method_summary":"Clean up temp files","original_method_code":"def clean_tempfiles():\n  '''Clean up temp files'''\n  for fn in TEMP_FILES:\n    if os.path.exists(fn):\n      os.unlink(fn)","method_path":"https:\/\/github.com\/bloomreach\/s4cmd\/blob\/bb51075bf43703e7cd95aa39288cf7732ec13a6d\/s4cmd.py#L209-L213"}
{"repo_name":"bloomreach\/s4cmd","method_name":"S4cmdLoggingClass.get_loggers","method_code":"def get_loggers(self):\n    ''''''\n\n    return self.log.debug, self.log.info, self.log.warn, self.log.error","method_summary":"Return a list of the logger","original_method_code":"def get_loggers(self):\n    '''Return a list of the logger methods: (debug, info, warn, error)'''\n\n    return self.log.debug, self.log.info, self.log.warn, self.log.error","method_path":"https:\/\/github.com\/bloomreach\/s4cmd\/blob\/bb51075bf43703e7cd95aa39288cf7732ec13a6d\/s4cmd.py#L111-L114"}
{"repo_name":"bloomreach\/s4cmd","method_name":"S3URL.get_fixed_path","method_code":"def get_fixed_path(self):\n    ''''''\n    pi = self.path.split(PATH_SEP)\n    fi = []\n    for p in pi:\n      if '*' in p or '?' in p:\n        break\n      fi.append(p)\n    return PATH_SEP.join(fi)","method_summary":"Get the fixed part of the path without wildcard","original_method_code":"def get_fixed_path(self):\n    '''Get the fixed part of the path without wildcard'''\n    pi = self.path.split(PATH_SEP)\n    fi = []\n    for p in pi:\n      if '*' in p or '?' in p:\n        break\n      fi.append(p)\n    return PATH_SEP.join(fi)","method_path":"https:\/\/github.com\/bloomreach\/s4cmd\/blob\/bb51075bf43703e7cd95aa39288cf7732ec13a6d\/s4cmd.py#L233-L241"}
{"repo_name":"PyCQA\/pylint","method_name":"is_inside_lambda","method_code":"def is_inside_lambda(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"\"\"\"\n    parent = node.parent\n    while parent is not None:\n        if isinstance(parent, astroid.Lambda):\n            return True\n        parent = parent.parent\n    return False","method_summary":"Return true if given node is inside lambda","original_method_code":"def is_inside_lambda(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Return true if given node is inside lambda\"\"\"\n    parent = node.parent\n    while parent is not None:\n        if isinstance(parent, astroid.Lambda):\n            return True\n        parent = parent.parent\n    return False","method_path":"https:\/\/github.com\/PyCQA\/pylint\/blob\/2bf5c61a3ff6ae90613b81679de42c0f19aea600\/pylint\/checkers\/utils.py#L218-L225"}
{"repo_name":"PyCQA\/pylint","method_name":"get_all_elements","method_code":"def get_all_elements(\n    node: astroid.node_classes.NodeNG\n) -> Iterable[astroid.node_classes.NodeNG]:\n    \"\"\"\"\"\"\n    if isinstance(node, (astroid.Tuple, astroid.List)):\n        for child in node.elts:\n            for e in get_all_elements(child):\n                yield e\n    else:\n        yield node","method_summary":"Recursively returns all atoms in nested lists and tuples.","original_method_code":"def get_all_elements(\n    node: astroid.node_classes.NodeNG\n) -> Iterable[astroid.node_classes.NodeNG]:\n    \"\"\"Recursively returns all atoms in nested lists and tuples.\"\"\"\n    if isinstance(node, (astroid.Tuple, astroid.List)):\n        for child in node.elts:\n            for e in get_all_elements(child):\n                yield e\n    else:\n        yield node","method_path":"https:\/\/github.com\/PyCQA\/pylint\/blob\/2bf5c61a3ff6ae90613b81679de42c0f19aea600\/pylint\/checkers\/utils.py#L228-L237"}
{"repo_name":"PyCQA\/pylint","method_name":"clobber_in_except","method_code":"def clobber_in_except(\n    node: astroid.node_classes.NodeNG\n) -> Tuple[bool, Tuple[str, str]]:\n    \"\"\"\"\"\"\n    if isinstance(node, astroid.AssignAttr):\n        return True, (node.attrname, \"object %r\" % (node.expr.as_string(),))\n    if isinstance(node, astroid.AssignName):\n        name = node.name\n        if is_builtin(name):\n            return (True, (name, \"builtins\"))\n\n        stmts = node.lookup(name)[1]\n        if stmts and not isinstance(\n            stmts[0].assign_type(),\n            (astroid.Assign, astroid.AugAssign, astroid.ExceptHandler),\n        ):\n            return True, (name, \"outer scope (line %s)\" % stmts[0].fromlineno)\n    return False, None","method_summary":"Checks if an assignment node in an except handler clobbers an existing variable.","original_method_code":"def clobber_in_except(\n    node: astroid.node_classes.NodeNG\n) -> Tuple[bool, Tuple[str, str]]:\n    \"\"\"Checks if an assignment node in an except handler clobbers an existing\n    variable.\n\n    Returns (True, args for W0623) if assignment clobbers an existing variable,\n    (False, None) otherwise.\n    \"\"\"\n    if isinstance(node, astroid.AssignAttr):\n        return True, (node.attrname, \"object %r\" % (node.expr.as_string(),))\n    if isinstance(node, astroid.AssignName):\n        name = node.name\n        if is_builtin(name):\n            return (True, (name, \"builtins\"))\n\n        stmts = node.lookup(name)[1]\n        if stmts and not isinstance(\n            stmts[0].assign_type(),\n            (astroid.Assign, astroid.AugAssign, astroid.ExceptHandler),\n        ):\n            return True, (name, \"outer scope (line %s)\" % stmts[0].fromlineno)\n    return False, None","method_path":"https:\/\/github.com\/PyCQA\/pylint\/blob\/2bf5c61a3ff6ae90613b81679de42c0f19aea600\/pylint\/checkers\/utils.py#L240-L262"}
{"repo_name":"PyCQA\/pylint","method_name":"is_super","method_code":"def is_super(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"\"\"\"\n    if getattr(node, \"name\", None) == \"super\" and node.root().name == BUILTINS_NAME:\n        return True\n    return False","method_summary":"return True if the node is referencing the \"super\" builtin function","original_method_code":"def is_super(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"return True if the node is referencing the \"super\" builtin function\n    \"\"\"\n    if getattr(node, \"name\", None) == \"super\" and node.root().name == BUILTINS_NAME:\n        return True\n    return False","method_path":"https:\/\/github.com\/PyCQA\/pylint\/blob\/2bf5c61a3ff6ae90613b81679de42c0f19aea600\/pylint\/checkers\/utils.py#L265-L270"}
{"repo_name":"PyCQA\/pylint","method_name":"is_error","method_code":"def is_error(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"\"\"\"\n    for child_node in node.get_children():\n        if isinstance(child_node, astroid.Raise):\n            return True\n    return False","method_summary":"return true if the function does nothing but raising an exception","original_method_code":"def is_error(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"return true if the function does nothing but raising an exception\"\"\"\n    for child_node in node.get_children():\n        if isinstance(child_node, astroid.Raise):\n            return True\n    return False","method_path":"https:\/\/github.com\/PyCQA\/pylint\/blob\/2bf5c61a3ff6ae90613b81679de42c0f19aea600\/pylint\/checkers\/utils.py#L273-L278"}
{"repo_name":"PyCQA\/pylint","method_name":"is_default_argument","method_code":"def is_default_argument(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"\"\"\"\n    parent = node.scope()\n    if isinstance(parent, (astroid.FunctionDef, astroid.Lambda)):\n        for default_node in parent.args.defaults:\n            for default_name_node in default_node.nodes_of_class(astroid.Name):\n                if default_name_node is node:\n                    return True\n    return False","method_summary":"return true if the given Name node is used in function or lambda default argument's value","original_method_code":"def is_default_argument(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"return true if the given Name node is used in function or lambda\n    default argument's value\n    \"\"\"\n    parent = node.scope()\n    if isinstance(parent, (astroid.FunctionDef, astroid.Lambda)):\n        for default_node in parent.args.defaults:\n            for default_name_node in default_node.nodes_of_class(astroid.Name):\n                if default_name_node is node:\n                    return True\n    return False","method_path":"https:\/\/github.com\/PyCQA\/pylint\/blob\/2bf5c61a3ff6ae90613b81679de42c0f19aea600\/pylint\/checkers\/utils.py#L370-L380"}
{"repo_name":"PyCQA\/pylint","method_name":"is_func_decorator","method_code":"def is_func_decorator(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"\"\"\"\n    parent = node.parent\n    while parent is not None:\n        if isinstance(parent, astroid.Decorators):\n            return True\n        if parent.is_statement or isinstance(\n            parent,\n            (astroid.Lambda, scoped_nodes.ComprehensionScope, scoped_nodes.ListComp),\n        ):\n            break\n        parent = parent.parent\n    return False","method_summary":"return true if the name is used in function decorator","original_method_code":"def is_func_decorator(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"return true if the name is used in function decorator\"\"\"\n    parent = node.parent\n    while parent is not None:\n        if isinstance(parent, astroid.Decorators):\n            return True\n        if parent.is_statement or isinstance(\n            parent,\n            (astroid.Lambda, scoped_nodes.ComprehensionScope, scoped_nodes.ListComp),\n        ):\n            break\n        parent = parent.parent\n    return False","method_path":"https:\/\/github.com\/PyCQA\/pylint\/blob\/2bf5c61a3ff6ae90613b81679de42c0f19aea600\/pylint\/checkers\/utils.py#L383-L395"}
{"repo_name":"PyCQA\/pylint","method_name":"is_ancestor_name","method_code":"def is_ancestor_name(\n    frame: astroid.node_classes.NodeNG, node: astroid.node_classes.NodeNG\n) -> bool:\n    \"\"\"\"\"\"\n    try:\n        bases = frame.bases\n    except AttributeError:\n        return False\n    for base in bases:\n        if node in base.nodes_of_class(astroid.Name):\n            return True\n    return False","method_summary":"return True if `frame` is an astroid.Class node with `node` in the subtree of its bases attribute","original_method_code":"def is_ancestor_name(\n    frame: astroid.node_classes.NodeNG, node: astroid.node_classes.NodeNG\n) -> bool:\n    \"\"\"return True if `frame` is an astroid.Class node with `node` in the\n    subtree of its bases attribute\n    \"\"\"\n    try:\n        bases = frame.bases\n    except AttributeError:\n        return False\n    for base in bases:\n        if node in base.nodes_of_class(astroid.Name):\n            return True\n    return False","method_path":"https:\/\/github.com\/PyCQA\/pylint\/blob\/2bf5c61a3ff6ae90613b81679de42c0f19aea600\/pylint\/checkers\/utils.py#L398-L411"}
{"repo_name":"PyCQA\/pylint","method_name":"assign_parent","method_code":"def assign_parent(node: astroid.node_classes.NodeNG) -> astroid.node_classes.NodeNG:\n    \"\"\"\"\"\"\n    while node and isinstance(node, (astroid.AssignName, astroid.Tuple, astroid.List)):\n        node = node.parent\n    return node","method_summary":"return the higher parent which is not an AssignName, Tuple or List node","original_method_code":"def assign_parent(node: astroid.node_classes.NodeNG) -> astroid.node_classes.NodeNG:\n    \"\"\"return the higher parent which is not an AssignName, Tuple or List node\n    \"\"\"\n    while node and isinstance(node, (astroid.AssignName, astroid.Tuple, astroid.List)):\n        node = node.parent\n    return node","method_path":"https:\/\/github.com\/PyCQA\/pylint\/blob\/2bf5c61a3ff6ae90613b81679de42c0f19aea600\/pylint\/checkers\/utils.py#L414-L419"}
{"repo_name":"PyCQA\/pylint","method_name":"overrides_a_method","method_code":"def overrides_a_method(class_node: astroid.node_classes.NodeNG, name: str) -> bool:\n    \"\"\"\"\"\"\n    for ancestor in class_node.ancestors():\n        if name in ancestor and isinstance(ancestor[name], astroid.FunctionDef):\n            return True\n    return False","method_summary":"return True if <name> is a method overridden from an ancestor","original_method_code":"def overrides_a_method(class_node: astroid.node_classes.NodeNG, name: str) -> bool:\n    \"\"\"return True if <name> is a method overridden from an ancestor\"\"\"\n    for ancestor in class_node.ancestors():\n        if name in ancestor and isinstance(ancestor[name], astroid.FunctionDef):\n            return True\n    return False","method_path":"https:\/\/github.com\/PyCQA\/pylint\/blob\/2bf5c61a3ff6ae90613b81679de42c0f19aea600\/pylint\/checkers\/utils.py#L422-L427"}
{"repo_name":"lepture\/flask-oauthlib","method_name":"bind_cache_grant","method_code":"def bind_cache_grant(app, provider, current_user, config_prefix='OAUTH2'):\n    \"\"\"\"\"\"\n    cache = Cache(app, config_prefix)\n\n    @provider.grantsetter\n    def create_grant(client_id, code, request, *args, **kwargs):\n        \"\"\"\"\"\"\n        grant = Grant(\n            cache,\n            client_id=client_id,\n            code=code['code'],\n            redirect_uri=request.redirect_uri,\n            scopes=request.scopes,\n            user=current_user(),\n        )\n        log.debug(\"Set Grant Token with key %s\" % grant.key)\n        cache.set(grant.key, dict(grant))\n\n    @provider.grantgetter\n    def get(client_id, code):\n        \"\"\"\"\"\"\n        grant = Grant(cache, client_id=client_id, code=code)\n        ret = cache.get(grant.key)\n        if not ret:\n            log.debug(\"Grant Token not found with key %s\" % grant.key)\n            return None\n        log.debug(\"Grant Token found with key %s\" % grant.key)\n        for k, v in ret.items():\n            setattr(grant, k, v)\n        return grant","method_summary":"Configures an :class:`OAuth2Provider` instance to use various caching systems to get and set the grant token. This removes the need to register :func:`grantgetter` and :func:`grantsetter` yourself.","original_method_code":"def bind_cache_grant(app, provider, current_user, config_prefix='OAUTH2'):\n    \"\"\"Configures an :class:`OAuth2Provider` instance to use various caching\n    systems to get and set the grant token. This removes the need to\n    register :func:`grantgetter` and :func:`grantsetter` yourself.\n\n    :param app: Flask application instance\n    :param provider: :class:`OAuth2Provider` instance\n    :param current_user: function that returns an :class:`User` object\n    :param config_prefix: prefix for config\n\n    A usage example::\n\n        oauth = OAuth2Provider(app)\n        app.config.update({'OAUTH2_CACHE_TYPE': 'redis'})\n\n        bind_cache_grant(app, oauth, current_user)\n\n    You can define which cache system you would like to use by setting the\n    following configuration option::\n\n        OAUTH2_CACHE_TYPE = 'null' \/\/ memcache, simple, redis, filesystem\n\n    For more information on the supported cache systems please visit:\n    `Cache <http:\/\/werkzeug.pocoo.org\/docs\/contrib\/cache\/>`_\n    \"\"\"\n    cache = Cache(app, config_prefix)\n\n    @provider.grantsetter\n    def create_grant(client_id, code, request, *args, **kwargs):\n        \"\"\"Sets the grant token with the configured cache system\"\"\"\n        grant = Grant(\n            cache,\n            client_id=client_id,\n            code=code['code'],\n            redirect_uri=request.redirect_uri,\n            scopes=request.scopes,\n            user=current_user(),\n        )\n        log.debug(\"Set Grant Token with key %s\" % grant.key)\n        cache.set(grant.key, dict(grant))\n\n    @provider.grantgetter\n    def get(client_id, code):\n        \"\"\"Gets the grant token with the configured cache system\"\"\"\n        grant = Grant(cache, client_id=client_id, code=code)\n        ret = cache.get(grant.key)\n        if not ret:\n            log.debug(\"Grant Token not found with key %s\" % grant.key)\n            return None\n        log.debug(\"Grant Token found with key %s\" % grant.key)\n        for k, v in ret.items():\n            setattr(grant, k, v)\n        return grant","method_path":"https:\/\/github.com\/lepture\/flask-oauthlib\/blob\/9e6f152a5bb360e7496210da21561c3e6d41b0e1\/flask_oauthlib\/contrib\/oauth2.py#L66-L118"}
{"repo_name":"lepture\/flask-oauthlib","method_name":"bind_sqlalchemy","method_code":"def bind_sqlalchemy(provider, session, user=None, client=None,\n                    token=None, grant=None, current_user=None):\n    \"\"\"\"\"\"\n    if user:\n        user_binding = UserBinding(user, session)\n        provider.usergetter(user_binding.get)\n\n    if client:\n        client_binding = ClientBinding(client, session)\n        provider.clientgetter(client_binding.get)\n\n    if token:\n        token_binding = TokenBinding(token, session, current_user)\n        provider.tokengetter(token_binding.get)\n        provider.tokensetter(token_binding.set)\n\n    if grant:\n        if not current_user:\n            raise ValueError(('`current_user` is required'\n                              'for Grant Binding'))\n        grant_binding = GrantBinding(grant, session, current_user)\n        provider.grantgetter(grant_binding.get)\n        provider.grantsetter(grant_binding.set)","method_summary":"Configures the given :class:`OAuth2Provider` instance with the required getters and setters for persistence with SQLAlchemy. An example of using all","original_method_code":"def bind_sqlalchemy(provider, session, user=None, client=None,\n                    token=None, grant=None, current_user=None):\n    \"\"\"Configures the given :class:`OAuth2Provider` instance with the\n    required getters and setters for persistence with SQLAlchemy.\n\n    An example of using all models::\n\n        oauth = OAuth2Provider(app)\n\n        bind_sqlalchemy(oauth, session, user=User, client=Client,\n                        token=Token, grant=Grant, current_user=current_user)\n\n    You can omit any model if you wish to register the functions yourself.\n    It is also possible to override the functions by registering them\n    afterwards::\n\n        oauth = OAuth2Provider(app)\n\n        bind_sqlalchemy(oauth, session, user=User, client=Client, token=Token)\n\n        @oauth.grantgetter\n        def get_grant(client_id, code):\n            pass\n\n        @oauth.grantsetter\n        def set_grant(client_id, code, request, *args, **kwargs):\n            pass\n\n        # register tokensetter with oauth but keeping the tokengetter\n        # registered by `SQLAlchemyBinding`\n        # You would only do this for the token and grant since user and client\n        # only have getters\n        @oauth.tokensetter\n        def set_token(token, request, *args, **kwargs):\n            pass\n\n    Note that current_user is only required if you're using SQLAlchemy\n    for grant caching. If you're using another caching system with\n    GrantCacheBinding instead, omit current_user.\n\n    :param provider: :class:`OAuth2Provider` instance\n    :param session: A :class:`Session` object\n    :param user: :class:`User` model\n    :param client: :class:`Client` model\n    :param token: :class:`Token` model\n    :param grant: :class:`Grant` model\n    :param current_user: function that returns a :class:`User` object\n    \"\"\"\n    if user:\n        user_binding = UserBinding(user, session)\n        provider.usergetter(user_binding.get)\n\n    if client:\n        client_binding = ClientBinding(client, session)\n        provider.clientgetter(client_binding.get)\n\n    if token:\n        token_binding = TokenBinding(token, session, current_user)\n        provider.tokengetter(token_binding.get)\n        provider.tokensetter(token_binding.set)\n\n    if grant:\n        if not current_user:\n            raise ValueError(('`current_user` is required'\n                              'for Grant Binding'))\n        grant_binding = GrantBinding(grant, session, current_user)\n        provider.grantgetter(grant_binding.get)\n        provider.grantsetter(grant_binding.set)","method_path":"https:\/\/github.com\/lepture\/flask-oauthlib\/blob\/9e6f152a5bb360e7496210da21561c3e6d41b0e1\/flask_oauthlib\/contrib\/oauth2.py#L121-L188"}
{"repo_name":"lepture\/flask-oauthlib","method_name":"Grant.delete","method_code":"def delete(self):\n        \"\"\"\"\"\"\n        log.debug(\n            \"Deleting grant %s for client %s\" % (self.code, self.client_id)\n        )\n        self._cache.delete(self.key)\n        return None","method_summary":"Removes itself from the cache","original_method_code":"def delete(self):\n        \"\"\"Removes itself from the cache\n\n        Note: This is required by the oauthlib\n        \"\"\"\n        log.debug(\n            \"Deleting grant %s for client %s\" % (self.code, self.client_id)\n        )\n        self._cache.delete(self.key)\n        return None","method_path":"https:\/\/github.com\/lepture\/flask-oauthlib\/blob\/9e6f152a5bb360e7496210da21561c3e6d41b0e1\/flask_oauthlib\/contrib\/oauth2.py#L43-L52"}
{"repo_name":"lepture\/flask-oauthlib","method_name":"BaseBinding.query","method_code":"def query(self):\n        \"\"\"\"\"\"\n        if hasattr(self.model, 'query'):\n            return self.model.query\n        else:\n            return self.session.query(self.model)","method_summary":"Determines which method of getting the query object for use","original_method_code":"def query(self):\n        \"\"\"Determines which method of getting the query object for use\"\"\"\n        if hasattr(self.model, 'query'):\n            return self.model.query\n        else:\n            return self.session.query(self.model)","method_path":"https:\/\/github.com\/lepture\/flask-oauthlib\/blob\/9e6f152a5bb360e7496210da21561c3e6d41b0e1\/flask_oauthlib\/contrib\/oauth2.py#L203-L208"}
{"repo_name":"lepture\/flask-oauthlib","method_name":"TokenBinding.get","method_code":"def get(self, access_token=None, refresh_token=None):\n        \"\"\"\"\"\"\n        if access_token:\n            return self.query.filter_by(access_token=access_token).first()\n        elif refresh_token:\n            return self.query.filter_by(refresh_token=refresh_token).first()\n        return None","method_summary":"returns a Token object with the given access token or refresh token","original_method_code":"def get(self, access_token=None, refresh_token=None):\n        \"\"\"returns a Token object with the given access token or refresh token\n\n        :param access_token: User's access token\n        :param refresh_token: User's refresh token\n        \"\"\"\n        if access_token:\n            return self.query.filter_by(access_token=access_token).first()\n        elif refresh_token:\n            return self.query.filter_by(refresh_token=refresh_token).first()\n        return None","method_path":"https:\/\/github.com\/lepture\/flask-oauthlib\/blob\/9e6f152a5bb360e7496210da21561c3e6d41b0e1\/flask_oauthlib\/contrib\/oauth2.py#L247-L257"}
{"repo_name":"lepture\/flask-oauthlib","method_name":"TokenBinding.set","method_code":"def set(self, token, request, *args, **kwargs):\n        \"\"\"\"\"\"\n        if hasattr(request, 'user') and request.user:\n            user = request.user\n        elif self.current_user:\n            \n            user = self.current_user()\n\n        client = request.client\n\n        tokens = self.query.filter_by(\n            client_id=client.client_id,\n            user_id=user.id).all()\n        if tokens:\n            for tk in tokens:\n                self.session.delete(tk)\n            self.session.commit()\n\n        expires_in = token.get('expires_in')\n        expires = datetime.utcnow() + timedelta(seconds=expires_in)\n\n        tok = self.model(**token)\n        tok.expires = expires\n        tok.client_id = client.client_id\n        tok.user_id = user.id\n\n        self.session.add(tok)\n        self.session.commit()\n        return tok","method_summary":"Creates a Token object and removes all expired tokens that belong to the user","original_method_code":"def set(self, token, request, *args, **kwargs):\n        \"\"\"Creates a Token object and removes all expired tokens that belong\n        to the user\n\n        :param token: token object\n        :param request: OAuthlib request object\n        \"\"\"\n        if hasattr(request, 'user') and request.user:\n            user = request.user\n        elif self.current_user:\n            # for implicit token\n            user = self.current_user()\n\n        client = request.client\n\n        tokens = self.query.filter_by(\n            client_id=client.client_id,\n            user_id=user.id).all()\n        if tokens:\n            for tk in tokens:\n                self.session.delete(tk)\n            self.session.commit()\n\n        expires_in = token.get('expires_in')\n        expires = datetime.utcnow() + timedelta(seconds=expires_in)\n\n        tok = self.model(**token)\n        tok.expires = expires\n        tok.client_id = client.client_id\n        tok.user_id = user.id\n\n        self.session.add(tok)\n        self.session.commit()\n        return tok","method_path":"https:\/\/github.com\/lepture\/flask-oauthlib\/blob\/9e6f152a5bb360e7496210da21561c3e6d41b0e1\/flask_oauthlib\/contrib\/oauth2.py#L259-L292"}
{"repo_name":"lepture\/flask-oauthlib","method_name":"GrantBinding.set","method_code":"def set(self, client_id, code, request, *args, **kwargs):\n        \"\"\"\"\"\"\n        expires = datetime.utcnow() + timedelta(seconds=100)\n        grant = self.model(\n            client_id=request.client.client_id,\n            code=code['code'],\n            redirect_uri=request.redirect_uri,\n            scope=' '.join(request.scopes),\n            user=self.current_user(),\n            expires=expires\n        )\n        self.session.add(grant)\n\n        self.session.commit()","method_summary":"Creates Grant object with the given params","original_method_code":"def set(self, client_id, code, request, *args, **kwargs):\n        \"\"\"Creates Grant object with the given params\n\n        :param client_id: ID of the client\n        :param code:\n        :param request: OAuthlib request object\n        \"\"\"\n        expires = datetime.utcnow() + timedelta(seconds=100)\n        grant = self.model(\n            client_id=request.client.client_id,\n            code=code['code'],\n            redirect_uri=request.redirect_uri,\n            scope=' '.join(request.scopes),\n            user=self.current_user(),\n            expires=expires\n        )\n        self.session.add(grant)\n\n        self.session.commit()","method_path":"https:\/\/github.com\/lepture\/flask-oauthlib\/blob\/9e6f152a5bb360e7496210da21561c3e6d41b0e1\/flask_oauthlib\/contrib\/oauth2.py#L304-L322"}
{"repo_name":"lepture\/flask-oauthlib","method_name":"GrantBinding.get","method_code":"def get(self, client_id, code):\n        \"\"\"\"\"\"\n        return self.query.filter_by(client_id=client_id, code=code).first()","method_summary":"Get the Grant object with the given client ID and code","original_method_code":"def get(self, client_id, code):\n        \"\"\"Get the Grant object with the given client ID and code\n\n        :param client_id: ID of the client\n        :param code:\n        \"\"\"\n        return self.query.filter_by(client_id=client_id, code=code).first()","method_path":"https:\/\/github.com\/lepture\/flask-oauthlib\/blob\/9e6f152a5bb360e7496210da21561c3e6d41b0e1\/flask_oauthlib\/contrib\/oauth2.py#L324-L330"}
{"repo_name":"lepture\/flask-oauthlib","method_name":"parse_response","method_code":"def parse_response(resp, content, strict=False, content_type=None):\n    \"\"\"\"\"\"\n    if not content_type:\n        content_type = resp.headers.get('content-type', 'application\/json')\n    ct, options = parse_options_header(content_type)\n\n    if ct in ('application\/json', 'text\/javascript'):\n        if not content:\n            return {}\n        return json.loads(content)\n\n    if ct in ('application\/xml', 'text\/xml'):\n        return get_etree().fromstring(content)\n\n    if ct != 'application\/x-www-form-urlencoded' and strict:\n        return content\n    charset = options.get('charset', 'utf-8')\n    return url_decode(content, charset=charset).to_dict()","method_summary":"Parse the response returned by :meth:`OAuthRemoteApp.http_request`.","original_method_code":"def parse_response(resp, content, strict=False, content_type=None):\n    \"\"\"Parse the response returned by :meth:`OAuthRemoteApp.http_request`.\n\n    :param resp: response of http_request\n    :param content: content of the response\n    :param strict: strict mode for form urlencoded content\n    :param content_type: assign a content type manually\n    \"\"\"\n    if not content_type:\n        content_type = resp.headers.get('content-type', 'application\/json')\n    ct, options = parse_options_header(content_type)\n\n    if ct in ('application\/json', 'text\/javascript'):\n        if not content:\n            return {}\n        return json.loads(content)\n\n    if ct in ('application\/xml', 'text\/xml'):\n        return get_etree().fromstring(content)\n\n    if ct != 'application\/x-www-form-urlencoded' and strict:\n        return content\n    charset = options.get('charset', 'utf-8')\n    return url_decode(content, charset=charset).to_dict()","method_path":"https:\/\/github.com\/lepture\/flask-oauthlib\/blob\/9e6f152a5bb360e7496210da21561c3e6d41b0e1\/flask_oauthlib\/client.py#L113-L136"}
{"repo_name":"lepture\/flask-oauthlib","method_name":"prepare_request","method_code":"def prepare_request(uri, headers=None, data=None, method=None):\n    \"\"\"\"\"\"\n    if headers is None:\n        headers = {}\n\n    if data and not method:\n        method = 'POST'\n    elif not method:\n        method = 'GET'\n\n    if method == 'GET' and data:\n        uri = add_params_to_uri(uri, data)\n        data = None\n\n    return uri, headers, data, method","method_summary":"Make request parameters right.","original_method_code":"def prepare_request(uri, headers=None, data=None, method=None):\n    \"\"\"Make request parameters right.\"\"\"\n    if headers is None:\n        headers = {}\n\n    if data and not method:\n        method = 'POST'\n    elif not method:\n        method = 'GET'\n\n    if method == 'GET' and data:\n        uri = add_params_to_uri(uri, data)\n        data = None\n\n    return uri, headers, data, method","method_path":"https:\/\/github.com\/lepture\/flask-oauthlib\/blob\/9e6f152a5bb360e7496210da21561c3e6d41b0e1\/flask_oauthlib\/client.py#L139-L153"}
{"repo_name":"pyca\/pyopenssl","method_name":"Checker_X509_get_pubkey.check_exception","method_code":"def check_exception(self):\n        \"\"\"\"\"\"\n        for i in xrange(self.iterations):\n            cert = X509()\n            try:\n                cert.get_pubkey()\n            except Error:\n                pass","method_summary":"Call the method repeatedly such that it will raise an exception.","original_method_code":"def check_exception(self):\n        \"\"\"\n        Call the method repeatedly such that it will raise an exception.\n        \"\"\"\n        for i in xrange(self.iterations):\n            cert = X509()\n            try:\n                cert.get_pubkey()\n            except Error:\n                pass","method_path":"https:\/\/github.com\/pyca\/pyopenssl\/blob\/1fbe064c50fd030948141d7d630673761525b0d0\/leakcheck\/crypto.py#L24-L33"}
{"repo_name":"pyca\/pyopenssl","method_name":"Checker_X509_get_pubkey.check_success","method_code":"def check_success(self):\n        \"\"\"\"\"\"\n        small = xrange(3)\n        for i in xrange(self.iterations):\n            key = PKey()\n            key.generate_key(TYPE_DSA, 256)\n            for i in small:\n                cert = X509()\n                cert.set_pubkey(key)\n                for i in small:\n                    cert.get_pubkey()","method_summary":"Call the method repeatedly such that it will return a PKey object.","original_method_code":"def check_success(self):\n        \"\"\"\n        Call the method repeatedly such that it will return a PKey object.\n        \"\"\"\n        small = xrange(3)\n        for i in xrange(self.iterations):\n            key = PKey()\n            key.generate_key(TYPE_DSA, 256)\n            for i in small:\n                cert = X509()\n                cert.set_pubkey(key)\n                for i in small:\n                    cert.get_pubkey()","method_path":"https:\/\/github.com\/pyca\/pyopenssl\/blob\/1fbe064c50fd030948141d7d630673761525b0d0\/leakcheck\/crypto.py#L36-L48"}
{"repo_name":"pyca\/pyopenssl","method_name":"Checker_load_privatekey.check_load_privatekey_callback","method_code":"def check_load_privatekey_callback(self):\n        \"\"\"\"\"\"\n        for i in xrange(self.iterations * 10):\n            load_privatekey(\n                FILETYPE_PEM, self.ENCRYPTED_PEM, lambda *args: \"hello, secret\")","method_summary":"Call the function with an encrypted PEM and a passphrase callback.","original_method_code":"def check_load_privatekey_callback(self):\n        \"\"\"\n        Call the function with an encrypted PEM and a passphrase callback.\n        \"\"\"\n        for i in xrange(self.iterations * 10):\n            load_privatekey(\n                FILETYPE_PEM, self.ENCRYPTED_PEM, lambda *args: \"hello, secret\")","method_path":"https:\/\/github.com\/pyca\/pyopenssl\/blob\/1fbe064c50fd030948141d7d630673761525b0d0\/leakcheck\/crypto.py#L70-L76"}
{"repo_name":"pyca\/pyopenssl","method_name":"Checker_load_privatekey.check_load_privatekey_callback_incorrect","method_code":"def check_load_privatekey_callback_incorrect(self):\n        \"\"\"\"\"\"\n        for i in xrange(self.iterations * 10):\n            try:\n                load_privatekey(\n                    FILETYPE_PEM, self.ENCRYPTED_PEM,\n                    lambda *args: \"hello, public\")\n            except Error:\n                pass","method_summary":"Call the function with an encrypted PEM and a passphrase callback which returns the wrong passphrase.","original_method_code":"def check_load_privatekey_callback_incorrect(self):\n        \"\"\"\n        Call the function with an encrypted PEM and a passphrase callback which\n        returns the wrong passphrase.\n        \"\"\"\n        for i in xrange(self.iterations * 10):\n            try:\n                load_privatekey(\n                    FILETYPE_PEM, self.ENCRYPTED_PEM,\n                    lambda *args: \"hello, public\")\n            except Error:\n                pass","method_path":"https:\/\/github.com\/pyca\/pyopenssl\/blob\/1fbe064c50fd030948141d7d630673761525b0d0\/leakcheck\/crypto.py#L79-L90"}
{"repo_name":"pyca\/pyopenssl","method_name":"Checker_load_privatekey.check_load_privatekey_callback_wrong_type","method_code":"def check_load_privatekey_callback_wrong_type(self):\n        \"\"\"\"\"\"\n        for i in xrange(self.iterations * 10):\n            try:\n                load_privatekey(\n                    FILETYPE_PEM, self.ENCRYPTED_PEM,\n                    lambda *args: {})\n            except ValueError:\n                pass","method_summary":"Call the function with an encrypted PEM and a passphrase callback which returns a non-string.","original_method_code":"def check_load_privatekey_callback_wrong_type(self):\n        \"\"\"\n        Call the function with an encrypted PEM and a passphrase callback which\n        returns a non-string.\n        \"\"\"\n        for i in xrange(self.iterations * 10):\n            try:\n                load_privatekey(\n                    FILETYPE_PEM, self.ENCRYPTED_PEM,\n                    lambda *args: {})\n            except ValueError:\n                pass","method_path":"https:\/\/github.com\/pyca\/pyopenssl\/blob\/1fbe064c50fd030948141d7d630673761525b0d0\/leakcheck\/crypto.py#L93-L104"}
{"repo_name":"pyca\/pyopenssl","method_name":"Checker_CRL.check_get_revoked","method_code":"def check_get_revoked(self):\n        \"\"\"\"\"\"\n        crl = CRL()\n        for i in xrange(100):\n            crl.add_revoked(Revoked())\n        for i in xrange(self.iterations):\n            crl.get_revoked()","method_summary":"Create a CRL object with 100 Revoked objects, then call the get_revoked method repeatedly.","original_method_code":"def check_get_revoked(self):\n        \"\"\"\n        Create a CRL object with 100 Revoked objects, then call the\n        get_revoked method repeatedly.\n        \"\"\"\n        crl = CRL()\n        for i in xrange(100):\n            crl.add_revoked(Revoked())\n        for i in xrange(self.iterations):\n            crl.get_revoked()","method_path":"https:\/\/github.com\/pyca\/pyopenssl\/blob\/1fbe064c50fd030948141d7d630673761525b0d0\/leakcheck\/crypto.py#L120-L129"}
{"repo_name":"pyca\/pyopenssl","method_name":"Checker_X509_REVOKED_dup.check_X509_REVOKED_dup","method_code":"def check_X509_REVOKED_dup(self):\n        \"\"\"\"\"\"\n        for i in xrange(self.iterations * 100):\n            revoked_copy = _X509_REVOKED_dup(Revoked()._revoked)\n            _lib.X509_REVOKED_free(revoked_copy)","method_summary":"Copy an empty Revoked object repeatedly. The copy is not garbage collected, therefore it needs to be manually freed.","original_method_code":"def check_X509_REVOKED_dup(self):\n        \"\"\"\n        Copy an empty Revoked object repeatedly. The copy is not garbage\n        collected, therefore it needs to be manually freed.\n        \"\"\"\n        for i in xrange(self.iterations * 100):\n            revoked_copy = _X509_REVOKED_dup(Revoked()._revoked)\n            _lib.X509_REVOKED_free(revoked_copy)","method_path":"https:\/\/github.com\/pyca\/pyopenssl\/blob\/1fbe064c50fd030948141d7d630673761525b0d0\/leakcheck\/crypto.py#L137-L144"}
{"repo_name":"pyca\/pyopenssl","method_name":"Checker_EllipticCurve.check_to_EC_KEY","method_code":"def check_to_EC_KEY(self):\n        \"\"\"\"\"\"\n        curves = get_elliptic_curves()\n        if curves:\n            curve = next(iter(curves))\n            for i in xrange(self.iterations * 1000):\n                curve._to_EC_KEY()","method_summary":"Repeatedly create an EC_KEY","original_method_code":"def check_to_EC_KEY(self):\n        \"\"\"\n        Repeatedly create an EC_KEY* from an :py:obj:`_EllipticCurve`.  The\n        structure should be automatically garbage collected.\n        \"\"\"\n        curves = get_elliptic_curves()\n        if curves:\n            curve = next(iter(curves))\n            for i in xrange(self.iterations * 1000):\n                curve._to_EC_KEY()","method_path":"https:\/\/github.com\/pyca\/pyopenssl\/blob\/1fbe064c50fd030948141d7d630673761525b0d0\/leakcheck\/crypto.py#L152-L161"}
{"repo_name":"pyca\/pyopenssl","method_name":"main","method_code":"def main():\n    \"\"\"\"\"\"\n    if len(argv) < 2:\n        print('Usage: %s <hostname>' % (argv[0],))\n        return 1\n\n    client = socket()\n\n    print('Connecting...', end=\"\")\n    stdout.flush()\n    client.connect(('127.0.0.1', 8443))\n    print('connected', client.getpeername())\n\n    client_ssl = Connection(Context(TLSv1_METHOD), client)\n    client_ssl.set_connect_state()\n    client_ssl.set_tlsext_host_name(argv[1])\n    client_ssl.do_handshake()\n    print('Server subject is', client_ssl.get_peer_certificate().get_subject())\n    client_ssl.close()","method_summary":"Connect to an SNI-enabled server and request a specific hostname, specified by argv[1], of it.","original_method_code":"def main():\n    \"\"\"\n    Connect to an SNI-enabled server and request a specific hostname, specified\n    by argv[1], of it.\n    \"\"\"\n    if len(argv) < 2:\n        print('Usage: %s <hostname>' % (argv[0],))\n        return 1\n\n    client = socket()\n\n    print('Connecting...', end=\"\")\n    stdout.flush()\n    client.connect(('127.0.0.1', 8443))\n    print('connected', client.getpeername())\n\n    client_ssl = Connection(Context(TLSv1_METHOD), client)\n    client_ssl.set_connect_state()\n    client_ssl.set_tlsext_host_name(argv[1])\n    client_ssl.do_handshake()\n    print('Server subject is', client_ssl.get_peer_certificate().get_subject())\n    client_ssl.close()","method_path":"https:\/\/github.com\/pyca\/pyopenssl\/blob\/1fbe064c50fd030948141d7d630673761525b0d0\/examples\/sni\/client.py#L12-L33"}
{"repo_name":"pyca\/pyopenssl","method_name":"createKeyPair","method_code":"def createKeyPair(type, bits):\n    \"\"\"\"\"\"\n    pkey = crypto.PKey()\n    pkey.generate_key(type, bits)\n    return pkey","method_summary":"Create a public\/private key pair.","original_method_code":"def createKeyPair(type, bits):\n    \"\"\"\n    Create a public\/private key pair.\n\n    Arguments: type - Key type, must be one of TYPE_RSA and TYPE_DSA\n               bits - Number of bits to use in the key\n    Returns:   The public\/private key pair in a PKey object\n    \"\"\"\n    pkey = crypto.PKey()\n    pkey.generate_key(type, bits)\n    return pkey","method_path":"https:\/\/github.com\/pyca\/pyopenssl\/blob\/1fbe064c50fd030948141d7d630673761525b0d0\/examples\/certgen.py#L17-L27"}
{"repo_name":"zqfang\/GSEApy","method_name":"calc_pvalues","method_code":"def calc_pvalues(query, gene_sets, background=20000, **kwargs):\n    \"\"\"\"\"\"\n\n    \n    k = len(query) \n    query = set(query)\n    vals = []\n    \n    \n    if isinstance(background, set): \n        bg = len(background) \n        \n        query = query.intersection(background)\n    elif isinstance(background, int):\n        bg = background\n    else:\n        raise ValueError(\"background should be set or int object\")\n    \n    subsets = sorted(gene_sets.keys())\n    for s in subsets:\n        category = gene_sets.get(s)\n        m = len(category)\n        hits = query.intersection(set(category))\n        x = len(hits)\n        if x < 1 : continue\n        \n        \n        vals.append((s, hypergeom.sf(x-1, bg, m, k), x, m, hits))\n\n    return zip(*vals)","method_summary":"calculate pvalues for all categories in the graph","original_method_code":"def calc_pvalues(query, gene_sets, background=20000, **kwargs):\n    \"\"\" calculate pvalues for all categories in the graph\n\n    :param set query: set of identifiers for which the p value is calculated\n    :param dict gene_sets: gmt file dict after background was set\n    :param set background: total number of genes in your annotated database.\n    :returns: pvalues\n              x: overlapped gene number\n              n: length of gene_set which belongs to each terms\n              hits: overlapped gene names.\n\n\n    For 2*2 contingency table: \n    =============================================================================\n                         |   in  query  |  not in query |    row total\n    =>      in gene_set  |        a     |       b       |       a+b  \n    =>  not in gene_set  |        c     |       d       |       c+d  \n           column total                                 | a+b+c+d = anno database\n    =============================================================================\n    background genes number = a + b + c + d.\n\n    Then, in R\n        x=a     the number of white balls drawn without replacement \n                from an urn which contains both black and white balls.\n        m=a+b   the number of white balls in the urn    \n        n=c+d   the number of black balls in the urn    \n        k=a+c   the number of balls drawn from the urn  \n   \n    In Scipy:\n    for args in scipy.hypergeom.sf(k, M, n, N, loc=0):\n        M: the total number of objects, \n        n: the total number of Type I objects. \n        k: the random variate represents the number of Type I objects in N drawn \n           without replacement from the total population.\n    \n    Therefore, these two functions are the same when using parameters from 2*2 table:\n    R:     >   phyper(x-1, m, n, k, lower.tail=FALSE)\n    Scipy: >>> hypergeom.sf(x-1, m+n, m, k)\n     \n    \"\"\"\n\n    # number of genes in your query data\n    k = len(query) \n    query = set(query)\n    vals = []\n    # background should be all genes in annotated database\n    # such as go, kegg et.al.\n    if isinstance(background, set): \n        bg = len(background) # total number in your annotated database \n        # filter genes that not found in annotated database\n        query = query.intersection(background)\n    elif isinstance(background, int):\n        bg = background\n    else:\n        raise ValueError(\"background should be set or int object\")\n    # pval\n    subsets = sorted(gene_sets.keys())\n    for s in subsets:\n        category = gene_sets.get(s)\n        m = len(category)\n        hits = query.intersection(set(category))\n        x = len(hits)\n        if x < 1 : continue\n        # pVal = hypergeom.sf(hitCount-1,popTotal,bgHits,queryTotal) \n        # p(X >= hitCounts)\n        vals.append((s, hypergeom.sf(x-1, bg, m, k), x, m, hits))\n\n    return zip(*vals)","method_path":"https:\/\/github.com\/zqfang\/GSEApy\/blob\/673e9ec1391e3b14d3e8a4353117151fd2cb9345\/gseapy\/stats.py#L10-L77"}
{"repo_name":"zqfang\/GSEApy","method_name":"fdrcorrection","method_code":"def fdrcorrection(pvals, alpha=0.05):\n    \"\"\"\"\"\"\n    \n    pvals = np.asarray(pvals)\n    pvals_sortind = np.argsort(pvals)\n    pvals_sorted = np.take(pvals, pvals_sortind)\n\n    ecdffactor = _ecdf(pvals_sorted)\n    reject = pvals_sorted <= ecdffactor*alpha\n    if reject.any():\n        rejectmax = max(np.nonzero(reject)[0])\n        reject[:rejectmax] = True\n    pvals_corrected_raw = pvals_sorted \/ ecdffactor\n    pvals_corrected = np.minimum.accumulate(pvals_corrected_raw[::-1])[::-1]\n    del pvals_corrected_raw\n    pvals_corrected[pvals_corrected>1] = 1\n    pvals_corrected_ = np.empty_like(pvals_corrected)\n    pvals_corrected_[pvals_sortind] = pvals_corrected\n    del pvals_corrected\n    reject_ = np.empty_like(reject)\n    reject_[pvals_sortind] = reject\n    return reject_, pvals_corrected_","method_summary":"benjamini hocheberg fdr correction. inspired by statsmodels","original_method_code":"def fdrcorrection(pvals, alpha=0.05):\n    \"\"\" benjamini hocheberg fdr correction. inspired by statsmodels \n    \"\"\"\n    # Implement copy from GOATools.\n    pvals = np.asarray(pvals)\n    pvals_sortind = np.argsort(pvals)\n    pvals_sorted = np.take(pvals, pvals_sortind)\n\n    ecdffactor = _ecdf(pvals_sorted)\n    reject = pvals_sorted <= ecdffactor*alpha\n    if reject.any():\n        rejectmax = max(np.nonzero(reject)[0])\n        reject[:rejectmax] = True\n    pvals_corrected_raw = pvals_sorted \/ ecdffactor\n    pvals_corrected = np.minimum.accumulate(pvals_corrected_raw[::-1])[::-1]\n    del pvals_corrected_raw\n    pvals_corrected[pvals_corrected>1] = 1\n    pvals_corrected_ = np.empty_like(pvals_corrected)\n    pvals_corrected_[pvals_sortind] = pvals_corrected\n    del pvals_corrected\n    reject_ = np.empty_like(reject)\n    reject_[pvals_sortind] = reject\n    return reject_, pvals_corrected_","method_path":"https:\/\/github.com\/zqfang\/GSEApy\/blob\/673e9ec1391e3b14d3e8a4353117151fd2cb9345\/gseapy\/stats.py#L85-L107"}
{"repo_name":"zqfang\/GSEApy","method_name":"zscore","method_code":"def zscore(data2d, axis=0):\n    \"\"\"\"\"\"\n    if axis is None:\n        \n        \n        return data2d\n    assert axis in [0,1]\n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    z_scored = data2d.apply(lambda x: (x-x.mean())\/x.std(ddof=1), \n                            axis=operator.xor(1, axis))\n    return z_scored","method_summary":"Standardize the mean and variance of the data axis","original_method_code":"def zscore(data2d, axis=0):\n    \"\"\"Standardize the mean and variance of the data axis Parameters.\n\n    :param data2d: DataFrame to normalize.\n    :param axis: int, Which axis to normalize across. If 0, normalize across rows,\n                  if 1, normalize across columns. If None, don't change data\n                  \n    :Returns: Normalized DataFrame. Normalized data with a mean of 0 and variance of 1\n              across the specified axis.\n\n    \"\"\"\n    if axis is None:\n        # normalized to mean and std using entire matrix\n        # z_scored = (data2d - data2d.values.mean()) \/ data2d.values.std(ddof=1)\n        return data2d\n    assert axis in [0,1]\n    # if axis == 1:\n    #     z_scored = data2d\n    # else:\n    #     z_scored = data2d.T\n\n    # z_scored = (z_scored - z_scored.mean()) \/ z_scored.std(ddof=1)\n    \n    # if axis == 1:\n    #     return z_scored\n    # else:\n    #     return z_scored.T\n    z_scored = data2d.apply(lambda x: (x-x.mean())\/x.std(ddof=1), \n                            axis=operator.xor(1, axis))\n    return z_scored","method_path":"https:\/\/github.com\/zqfang\/GSEApy\/blob\/673e9ec1391e3b14d3e8a4353117151fd2cb9345\/gseapy\/plot.py#L28-L57"}
{"repo_name":"zqfang\/GSEApy","method_name":"heatmap","method_code":"def heatmap(df, z_score=None, title='', figsize=(5,5), cmap='RdBu_r', \n            xticklabels=True, yticklabels=True, ofname=None, **kwargs):\n    \"\"\"\"\"\"\n    df = zscore(df, axis=z_score)\n    df = df.iloc[::-1]\n    \n    ny, nx = df.shape\n    xticks = np.arange(0, nx, 1) + .5\n    yticks = np.arange(0, ny, 1) + .5\n\n    \n    if hasattr(sys, 'ps1') and (ofname is None): \n        fig = plt.figure(figsize=figsize)\n    else:\n        fig = Figure(figsize=figsize)\n        canvas = FigureCanvas(fig)\n    ax = fig.add_subplot(111)\n    vmin = np.percentile(df.min(), 2)\n    vmax =  np.percentile(df.max(), 98)\n    matrix = ax.pcolormesh(df.values, cmap=cmap, vmin=vmin, vmax=vmax)\n    ax.set_ylim([0,len(df)])\n    ax.set(xticks=xticks, yticks=yticks)\n    ax.set_xticklabels(df.columns.values if xticklabels else '', fontsize=14, rotation=90)\n    ax.set_yticklabels(df.index.values if yticklabels else '',  fontsize=14)\n    ax.set_title(\"%s\\nHeatmap of the Analyzed Geneset\"%title, fontsize=20)\n    ax.tick_params(axis='both', which='both', bottom=False, top=False,\n                   right=False, left=False)\n    \n    \n    cbar = colorbar(matrix)\n    cbar.ax.tick_params(axis='both', which='both', bottom=False, top=False,\n                        right=False, left=False)\n    for side in [\"top\", \"right\", \"left\", \"bottom\"]:\n        ax.spines[side].set_visible(False)\n        cbar.ax.spines[side].set_visible(False)\n    \n\n    if ofname is not None: \n        \n        fig.savefig(ofname, bbox_inches='tight', dpi=300)\n    return","method_summary":"Visualize the dataframe.","original_method_code":"def heatmap(df, z_score=None, title='', figsize=(5,5), cmap='RdBu_r', \n            xticklabels=True, yticklabels=True, ofname=None, **kwargs):\n    \"\"\"Visualize the dataframe.\n\n    :param df: DataFrame from expression table.\n    :param z_score: z_score axis{0, 1}. If None, don't normalize data.\n    :param title: gene set name.\n    :param outdir: path to save heatmap.\n    :param figsize: heatmap figsize.\n    :param cmap: matplotlib colormap.\n    :param ofname: output file name. If None, don't save figure \n\n    \"\"\"\n    df = zscore(df, axis=z_score)\n    df = df.iloc[::-1]\n    # Get the positions and used label for the ticks\n    ny, nx = df.shape\n    xticks = np.arange(0, nx, 1) + .5\n    yticks = np.arange(0, ny, 1) + .5\n\n    # If working on commandline, don't show figure\n    if hasattr(sys, 'ps1') and (ofname is None): \n        fig = plt.figure(figsize=figsize)\n    else:\n        fig = Figure(figsize=figsize)\n        canvas = FigureCanvas(fig)\n    ax = fig.add_subplot(111)\n    vmin = np.percentile(df.min(), 2)\n    vmax =  np.percentile(df.max(), 98)\n    matrix = ax.pcolormesh(df.values, cmap=cmap, vmin=vmin, vmax=vmax)\n    ax.set_ylim([0,len(df)])\n    ax.set(xticks=xticks, yticks=yticks)\n    ax.set_xticklabels(df.columns.values if xticklabels else '', fontsize=14, rotation=90)\n    ax.set_yticklabels(df.index.values if yticklabels else '',  fontsize=14)\n    ax.set_title(\"%s\\nHeatmap of the Analyzed Geneset\"%title, fontsize=20)\n    ax.tick_params(axis='both', which='both', bottom=False, top=False,\n                   right=False, left=False)\n    # cax=fig.add_axes([0.93,0.25,0.05,0.20])\n    # cbar = fig.colorbar(matrix, cax=cax)\n    cbar = colorbar(matrix)\n    cbar.ax.tick_params(axis='both', which='both', bottom=False, top=False,\n                        right=False, left=False)\n    for side in [\"top\", \"right\", \"left\", \"bottom\"]:\n        ax.spines[side].set_visible(False)\n        cbar.ax.spines[side].set_visible(False)\n    # cbar.ax.set_title('',loc='left')\n\n    if ofname is not None: \n        # canvas.print_figure(ofname, bbox_inches='tight', dpi=300)\n        fig.savefig(ofname, bbox_inches='tight', dpi=300)\n    return","method_path":"https:\/\/github.com\/zqfang\/GSEApy\/blob\/673e9ec1391e3b14d3e8a4353117151fd2cb9345\/gseapy\/plot.py#L67-L117"}
{"repo_name":"zqfang\/GSEApy","method_name":"gseaplot","method_code":"def gseaplot(rank_metric, term, hits_indices, nes, pval, fdr, RES,\n              pheno_pos='', pheno_neg='', figsize=(6,5.5), \n              cmap='seismic', ofname=None, **kwargs):\n    \"\"\"\"\"\"\n    \n    \n    norm = _MidpointNormalize(midpoint=0)\n\n    \n    x = np.arange(len(rank_metric))\n    rankings = rank_metric.values\n    \n    phenoP_label = pheno_pos + ' (Positively Correlated)'\n    phenoN_label = pheno_neg + ' (Negatively Correlated)'\n    zero_score_ind = np.abs(rankings).argmin()\n    z_score_label = 'Zero score at ' + str(zero_score_ind)\n    nes_label = 'NES: '+ \"{:.3f}\".format(float(nes))\n    pval_label = 'Pval: '+ \"{:.3f}\".format(float(pval))\n    fdr_label = 'FDR: '+ \"{:.3f}\".format(float(fdr))\n    im_matrix = np.tile(rankings, (2,1))\n\n    \n    plt.rcParams.update({'pdf.fonttype':42,'ps.fonttype':42})\n    \n    \n\n    \n    gs = plt.GridSpec(16,1)\n    if hasattr(sys, 'ps1') and (ofname is None):\n        \n        fig = plt.figure(figsize=figsize)\n    else:\n        \n        fig = Figure(figsize=figsize)\n        canvas = FigureCanvas(fig)\n    \n    ax1 =  fig.add_subplot(gs[11:])\n    module = 'tmp' if ofname is  None else ofname.split(\".\")[-2]\n    if module == 'ssgsea':\n        nes_label = 'ES: '+ \"{:.3f}\".format(float(nes))\n        pval_label='Pval: '\n        fdr_label='FDR: '\n        ax1.fill_between(x, y1=np.log(rankings), y2=0, color='#C9D3DB')\n        ax1.set_ylabel(\"log ranked metric\", fontsize=14)\n    else:\n        ax1.fill_between(x, y1=rankings, y2=0, color='#C9D3DB')\n        ax1.set_ylabel(\"Ranked list metric\", fontsize=14)\n    ax1.text(.05, .9, phenoP_label, color='red',\n             horizontalalignment='left', verticalalignment='top',\n             transform=ax1.transAxes)\n    ax1.text(.95, .05, phenoN_label, color='Blue',\n             horizontalalignment='right', verticalalignment='bottom',\n             transform=ax1.transAxes)\n    \n    trans1 = transforms.blended_transform_factory(ax1.transData, ax1.transAxes)\n    if module != 'ssgsea':\n        ax1.vlines(zero_score_ind, 0, 1, linewidth=.5, transform=trans1, linestyles='--', color='grey')\n        ax1.text(zero_score_ind, 0.5, z_score_label,\n                 horizontalalignment='center',\n                 verticalalignment='center',\n                 transform=trans1)\n    ax1.set_xlabel(\"Rank in Ordered Dataset\", fontsize=14)\n    ax1.spines['top'].set_visible(False)\n    ax1.tick_params(axis='both', which='both', top=False, right=False, left=False)\n    ax1.locator_params(axis='y', nbins=5)\n    ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda tick_loc,tick_num :  '{:.1f}'.format(tick_loc) ))\n\n    \n    \n\n    \n    ax2 = fig.add_subplot(gs[8:10], sharex=ax1)\n\n    \n    trans2 = transforms.blended_transform_factory(ax2.transData, ax2.transAxes)\n    ax2.vlines(hits_indices, 0, 1,linewidth=.5,transform=trans2)\n    ax2.spines['bottom'].set_visible(False)\n    ax2.tick_params(axis='both', which='both', bottom=False, top=False,\n                    labelbottom=False, right=False, left=False, labelleft=False)\n    \n    ax3 =  fig.add_subplot(gs[10], sharex=ax1)\n    ax3.imshow(im_matrix, aspect='auto', norm=norm, cmap=cmap, interpolation='none') \n    ax3.spines['bottom'].set_visible(False)\n    ax3.tick_params(axis='both', which='both', bottom=False, top=False,\n                    labelbottom=False, right=False, left=False,labelleft=False)\n\n    \n    ax4 = fig.add_subplot(gs[:8], sharex=ax1)\n    ax4.plot(x, RES, linewidth=4, color ='#88C544')\n    ax4.text(.1, .1, fdr_label, transform=ax4.transAxes)\n    ax4.text(.1, .2, pval_label, transform=ax4.transAxes)\n    ax4.text(.1, .3, nes_label, transform=ax4.transAxes)\n\n    \n    trans4 = transforms.blended_transform_factory(ax4.transAxes, ax4.transData)\n    ax4.hlines(0, 0, 1, linewidth=.5, transform=trans4, color='grey')\n    ax4.set_ylabel(\"Enrichment score (ES)\", fontsize=14)\n    ax4.set_xlim(min(x), max(x))\n    ax4.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False)\n    ax4.locator_params(axis='y', nbins=5)\n    \n    ax4.yaxis.set_major_formatter(plt.FuncFormatter(lambda tick_loc,tick_num :  '{:.1f}'.format(tick_loc)) )\n\n    \n    fig.suptitle(term, fontsize=16, fontweight='bold')\n    fig.subplots_adjust(hspace=0)\n    \n    if ofname is not None: \n        \n        fig.savefig(ofname, bbox_inches='tight', dpi=300)\n    return","method_summary":"This is the main function for reproducing the gsea plot.","original_method_code":"def gseaplot(rank_metric, term, hits_indices, nes, pval, fdr, RES,\n              pheno_pos='', pheno_neg='', figsize=(6,5.5), \n              cmap='seismic', ofname=None, **kwargs):\n    \"\"\"This is the main function for reproducing the gsea plot.\n\n    :param rank_metric: pd.Series for rankings, rank_metric.values.\n    :param term: gene_set name\n    :param hits_indices: hits indices of rank_metric.index presented in gene set S.\n    :param nes: Normalized enrichment scores.\n    :param pval: nominal p-value.\n    :param fdr: false discovery rate.\n    :param RES: running enrichment scores.\n    :param pheno_pos: phenotype label, positive correlated.\n    :param pheno_neg: phenotype label, negative correlated.\n    :param figsize: matplotlib figsize.\n    :param ofname: output file name. If None, don't save figure \n\n    \"\"\"\n    # plt.style.use('classic')\n    # center color map at midpoint = 0\n    norm = _MidpointNormalize(midpoint=0)\n\n    #dataFrame of ranked matrix scores\n    x = np.arange(len(rank_metric))\n    rankings = rank_metric.values\n    # figsize = (6,6)\n    phenoP_label = pheno_pos + ' (Positively Correlated)'\n    phenoN_label = pheno_neg + ' (Negatively Correlated)'\n    zero_score_ind = np.abs(rankings).argmin()\n    z_score_label = 'Zero score at ' + str(zero_score_ind)\n    nes_label = 'NES: '+ \"{:.3f}\".format(float(nes))\n    pval_label = 'Pval: '+ \"{:.3f}\".format(float(pval))\n    fdr_label = 'FDR: '+ \"{:.3f}\".format(float(fdr))\n    im_matrix = np.tile(rankings, (2,1))\n\n    # output truetype\n    plt.rcParams.update({'pdf.fonttype':42,'ps.fonttype':42})\n    # in most case, we will have many plots, so do not display plots\n    # It's also usefull to run this script on command line.\n\n    # GSEA Plots\n    gs = plt.GridSpec(16,1)\n    if hasattr(sys, 'ps1') and (ofname is None):\n        # working inside python console, show figure\n        fig = plt.figure(figsize=figsize)\n    else:\n        # If working on commandline, don't show figure\n        fig = Figure(figsize=figsize)\n        canvas = FigureCanvas(fig)\n    # Ranked Metric Scores Plot\n    ax1 =  fig.add_subplot(gs[11:])\n    module = 'tmp' if ofname is  None else ofname.split(\".\")[-2]\n    if module == 'ssgsea':\n        nes_label = 'ES: '+ \"{:.3f}\".format(float(nes))\n        pval_label='Pval: '\n        fdr_label='FDR: '\n        ax1.fill_between(x, y1=np.log(rankings), y2=0, color='#C9D3DB')\n        ax1.set_ylabel(\"log ranked metric\", fontsize=14)\n    else:\n        ax1.fill_between(x, y1=rankings, y2=0, color='#C9D3DB')\n        ax1.set_ylabel(\"Ranked list metric\", fontsize=14)\n    ax1.text(.05, .9, phenoP_label, color='red',\n             horizontalalignment='left', verticalalignment='top',\n             transform=ax1.transAxes)\n    ax1.text(.95, .05, phenoN_label, color='Blue',\n             horizontalalignment='right', verticalalignment='bottom',\n             transform=ax1.transAxes)\n    # the x coords of this transformation are data, and the y coord are axes\n    trans1 = transforms.blended_transform_factory(ax1.transData, ax1.transAxes)\n    if module != 'ssgsea':\n        ax1.vlines(zero_score_ind, 0, 1, linewidth=.5, transform=trans1, linestyles='--', color='grey')\n        ax1.text(zero_score_ind, 0.5, z_score_label,\n                 horizontalalignment='center',\n                 verticalalignment='center',\n                 transform=trans1)\n    ax1.set_xlabel(\"Rank in Ordered Dataset\", fontsize=14)\n    ax1.spines['top'].set_visible(False)\n    ax1.tick_params(axis='both', which='both', top=False, right=False, left=False)\n    ax1.locator_params(axis='y', nbins=5)\n    ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda tick_loc,tick_num :  '{:.1f}'.format(tick_loc) ))\n\n    # use round method to control float number\n    # ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda tick_loc,tick_num :  round(tick_loc, 1) ))\n\n    # gene hits\n    ax2 = fig.add_subplot(gs[8:10], sharex=ax1)\n\n    # the x coords of this transformation are data, and the y coord are axes\n    trans2 = transforms.blended_transform_factory(ax2.transData, ax2.transAxes)\n    ax2.vlines(hits_indices, 0, 1,linewidth=.5,transform=trans2)\n    ax2.spines['bottom'].set_visible(False)\n    ax2.tick_params(axis='both', which='both', bottom=False, top=False,\n                    labelbottom=False, right=False, left=False, labelleft=False)\n    # colormap\n    ax3 =  fig.add_subplot(gs[10], sharex=ax1)\n    ax3.imshow(im_matrix, aspect='auto', norm=norm, cmap=cmap, interpolation='none') # cm.coolwarm\n    ax3.spines['bottom'].set_visible(False)\n    ax3.tick_params(axis='both', which='both', bottom=False, top=False,\n                    labelbottom=False, right=False, left=False,labelleft=False)\n\n    # Enrichment score plot\n    ax4 = fig.add_subplot(gs[:8], sharex=ax1)\n    ax4.plot(x, RES, linewidth=4, color ='#88C544')\n    ax4.text(.1, .1, fdr_label, transform=ax4.transAxes)\n    ax4.text(.1, .2, pval_label, transform=ax4.transAxes)\n    ax4.text(.1, .3, nes_label, transform=ax4.transAxes)\n\n    # the y coords of this transformation are data, and the x coord are axes\n    trans4 = transforms.blended_transform_factory(ax4.transAxes, ax4.transData)\n    ax4.hlines(0, 0, 1, linewidth=.5, transform=trans4, color='grey')\n    ax4.set_ylabel(\"Enrichment score (ES)\", fontsize=14)\n    ax4.set_xlim(min(x), max(x))\n    ax4.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False)\n    ax4.locator_params(axis='y', nbins=5)\n    # FuncFormatter need two argument, I don't know why. this lambda function used to format yaxis tick labels.\n    ax4.yaxis.set_major_formatter(plt.FuncFormatter(lambda tick_loc,tick_num :  '{:.1f}'.format(tick_loc)) )\n\n    # fig adjustment\n    fig.suptitle(term, fontsize=16, fontweight='bold')\n    fig.subplots_adjust(hspace=0)\n    # fig.tight_layout()\n    if ofname is not None: \n        # canvas.print_figure(ofname, bbox_inches='tight', dpi=300)\n        fig.savefig(ofname, bbox_inches='tight', dpi=300)\n    return","method_path":"https:\/\/github.com\/zqfang\/GSEApy\/blob\/673e9ec1391e3b14d3e8a4353117151fd2cb9345\/gseapy\/plot.py#L119-L243"}
{"repo_name":"zqfang\/GSEApy","method_name":"dotplot","method_code":"def dotplot(df, column='Adjusted P-value', title='', cutoff=0.05, top_term=10, \n            sizes=None, norm=None, legend=True, figsize=(6, 5.5), \n            cmap='RdBu_r', ofname=None, **kwargs):\n    \"\"\"\"\"\"\n\n\n    colname = column    \n    \n    if colname in ['Adjusted P-value', 'P-value']:\n        \n        can_be_coerced = df[colname].map(isfloat)\n        if np.sum(~can_be_coerced) > 0:\n            raise ValueError('some value in %s could not be typecast to `float`'%colname)\n        else:\n            df.loc[:, colname] = df[colname].map(float)\n        df = df[df[colname] <= cutoff]\n        if len(df) < 1: \n            msg = \"Warning: No enrich terms when cutoff = %s\"%cutoff\n            return msg\n        df = df.assign(logAP=lambda x: - x[colname].apply(np.log10))\n        colname='logAP'\n    df = df.sort_values(by=colname).iloc[-top_term:,:]\n    \n    temp = df['Overlap'].str.split(\"\/\", expand=True).astype(int)\n    df = df.assign(Hits=temp.iloc[:,0], Background=temp.iloc[:,1])\n    df = df.assign(Hits_ratio=lambda x:x.Hits \/ x.Background)\n    \n    x = df.loc[:, colname].values\n    combined_score = df['Combined Score'].round().astype('int')\n    \n    y = [i for i in range(0,len(df))]\n    ylabels = df['Term'].values\n    \n    \n    \n    \n    \n    levels = numbers = np.sort(df.Hits.unique())\n    if norm is None:\n        norm = Normalize()\n    elif isinstance(norm, tuple):\n        norm = Normalize(*norm)\n    elif not isinstance(norm, Normalize):\n        err = (\"``size_norm`` must be None, tuple, \"\n                \"or Normalize object.\")\n        raise ValueError(err)\n    min_width, max_width = np.r_[20, 100] * plt.rcParams[\"lines.linewidth\"]\n    norm.clip = True\n    if not norm.scaled():\n        norm(np.asarray(numbers))\n    size_limits = norm.vmin, norm.vmax\n    scl = norm(numbers)\n    widths = np.asarray(min_width + scl * (max_width - min_width))\n    if scl.mask.any():\n        widths[scl.mask] = 0\n    sizes = dict(zip(levels, widths))\n    df['sizes'] = df.Hits.map(sizes)\n    area = df['sizes'].values\n\n    \n    if hasattr(sys, 'ps1') and (ofname is None):\n        \n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        \n        fig = Figure(figsize=figsize)\n        canvas = FigureCanvas(fig)\n        ax = fig.add_subplot(111)\n    vmin = np.percentile(combined_score.min(), 2)\n    vmax =  np.percentile(combined_score.max(), 98)\n    sc = ax.scatter(x=x, y=y, s=area, edgecolors='face', c=combined_score,\n                    cmap=cmap, vmin=vmin, vmax=vmax)\n\n    if column in ['Adjusted P-value', 'P-value']:\n        xlabel = \"-log$_{10}$(%s)\"%column\n    else:\n        xlabel = column \n    ax.set_xlabel(xlabel, fontsize=14, fontweight='bold')\n    ax.yaxis.set_major_locator(plt.FixedLocator(y))\n    ax.yaxis.set_major_formatter(plt.FixedFormatter(ylabels))\n    ax.set_yticklabels(ylabels, fontsize=16)\n    \n    \n    ax.grid()\n    \n    cax=fig.add_axes([0.95,0.20,0.03,0.22])\n    cbar = fig.colorbar(sc, cax=cax,)\n    cbar.ax.tick_params(right=True)\n    cbar.ax.set_title('Combined\\nScore',loc='left', fontsize=12)\n\n    \n    if len(df) >= 3:\n        \n        idx = [area.argmax(), np.abs(area - area.mean()).argmin(), area.argmin()]\n        idx = unique(idx)\n    else:\n        idx = df.index.values\n    label = df.iloc[idx, df.columns.get_loc('Hits')]\n    \n    if legend:\n        handles, _ = ax.get_legend_handles_labels()\n        legend_markers = []\n        for ix in idx: \n            legend_markers.append(ax.scatter([],[], s=area[ix], c='b'))\n        \n        ax.legend(legend_markers, label, title='Hits')\n    ax.set_title(title, fontsize=20, fontweight='bold')\n    \n    if ofname is not None: \n        \n        fig.savefig(ofname, bbox_inches='tight', dpi=300)\n        return\n    return ax","method_summary":"Visualize enrichr results.","original_method_code":"def dotplot(df, column='Adjusted P-value', title='', cutoff=0.05, top_term=10, \n            sizes=None, norm=None, legend=True, figsize=(6, 5.5), \n            cmap='RdBu_r', ofname=None, **kwargs):\n    \"\"\"Visualize enrichr results.\n\n    :param df: GSEApy DataFrame results.\n    :param column: which column of DataFrame to show. Default: Adjusted P-value\n    :param title: figure title\n    :param cutoff: p-adjust cut-off.\n    :param top_term: number of enriched terms to show.\n    :param ascending: bool, the order of y axis.\n    :param sizes: tuple, (min, max) scatter size. Not functional for now\n    :param norm: maplotlib.colors.Normalize object.\n    :param legend: bool, whether to show legend.\n    :param figsize: tuple, figure size. \n    :param cmap: matplotlib colormap\n    :param ofname: output file name. If None, don't save figure \n\n    \"\"\"\n\n\n    colname = column    \n    # sorting the dataframe for better visualization\n    if colname in ['Adjusted P-value', 'P-value']:\n        # check if any values in `df[colname]` can't be coerced to floats\n        can_be_coerced = df[colname].map(isfloat)\n        if np.sum(~can_be_coerced) > 0:\n            raise ValueError('some value in %s could not be typecast to `float`'%colname)\n        else:\n            df.loc[:, colname] = df[colname].map(float)\n        df = df[df[colname] <= cutoff]\n        if len(df) < 1: \n            msg = \"Warning: No enrich terms when cutoff = %s\"%cutoff\n            return msg\n        df = df.assign(logAP=lambda x: - x[colname].apply(np.log10))\n        colname='logAP'\n    df = df.sort_values(by=colname).iloc[-top_term:,:]\n    # \n    temp = df['Overlap'].str.split(\"\/\", expand=True).astype(int)\n    df = df.assign(Hits=temp.iloc[:,0], Background=temp.iloc[:,1])\n    df = df.assign(Hits_ratio=lambda x:x.Hits \/ x.Background)\n    # x axis values\n    x = df.loc[:, colname].values\n    combined_score = df['Combined Score'].round().astype('int')\n    # y axis index and values\n    y = [i for i in range(0,len(df))]\n    ylabels = df['Term'].values\n    # Normalise to [0,1]\n    # b = (df['Count']  - df['Count'].min())\/ np.ptp(df['Count'])\n    # area = 100 * b\n    \n    # control the size of scatter and legend marker\n    levels = numbers = np.sort(df.Hits.unique())\n    if norm is None:\n        norm = Normalize()\n    elif isinstance(norm, tuple):\n        norm = Normalize(*norm)\n    elif not isinstance(norm, Normalize):\n        err = (\"``size_norm`` must be None, tuple, \"\n                \"or Normalize object.\")\n        raise ValueError(err)\n    min_width, max_width = np.r_[20, 100] * plt.rcParams[\"lines.linewidth\"]\n    norm.clip = True\n    if not norm.scaled():\n        norm(np.asarray(numbers))\n    size_limits = norm.vmin, norm.vmax\n    scl = norm(numbers)\n    widths = np.asarray(min_width + scl * (max_width - min_width))\n    if scl.mask.any():\n        widths[scl.mask] = 0\n    sizes = dict(zip(levels, widths))\n    df['sizes'] = df.Hits.map(sizes)\n    area = df['sizes'].values\n\n    # creat scatter plot\n    if hasattr(sys, 'ps1') and (ofname is None):\n        # working inside python console, show figure\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        # If working on commandline, don't show figure\n        fig = Figure(figsize=figsize)\n        canvas = FigureCanvas(fig)\n        ax = fig.add_subplot(111)\n    vmin = np.percentile(combined_score.min(), 2)\n    vmax =  np.percentile(combined_score.max(), 98)\n    sc = ax.scatter(x=x, y=y, s=area, edgecolors='face', c=combined_score,\n                    cmap=cmap, vmin=vmin, vmax=vmax)\n\n    if column in ['Adjusted P-value', 'P-value']:\n        xlabel = \"-log$_{10}$(%s)\"%column\n    else:\n        xlabel = column \n    ax.set_xlabel(xlabel, fontsize=14, fontweight='bold')\n    ax.yaxis.set_major_locator(plt.FixedLocator(y))\n    ax.yaxis.set_major_formatter(plt.FixedFormatter(ylabels))\n    ax.set_yticklabels(ylabels, fontsize=16)\n    \n    # ax.set_ylim([-1, len(df)])\n    ax.grid()\n    # colorbar\n    cax=fig.add_axes([0.95,0.20,0.03,0.22])\n    cbar = fig.colorbar(sc, cax=cax,)\n    cbar.ax.tick_params(right=True)\n    cbar.ax.set_title('Combined\\nScore',loc='left', fontsize=12)\n\n    # for terms less than 3\n    if len(df) >= 3:\n        # find the index of the closest value to the median\n        idx = [area.argmax(), np.abs(area - area.mean()).argmin(), area.argmin()]\n        idx = unique(idx)\n    else:\n        idx = df.index.values\n    label = df.iloc[idx, df.columns.get_loc('Hits')]\n    \n    if legend:\n        handles, _ = ax.get_legend_handles_labels()\n        legend_markers = []\n        for ix in idx: \n            legend_markers.append(ax.scatter([],[], s=area[ix], c='b'))\n        # artist = ax.scatter([], [], s=size_levels,) \n        ax.legend(legend_markers, label, title='Hits')\n    ax.set_title(title, fontsize=20, fontweight='bold')\n    \n    if ofname is not None: \n        # canvas.print_figure(ofname, bbox_inches='tight', dpi=300)\n        fig.savefig(ofname, bbox_inches='tight', dpi=300)\n        return\n    return ax","method_path":"https:\/\/github.com\/zqfang\/GSEApy\/blob\/673e9ec1391e3b14d3e8a4353117151fd2cb9345\/gseapy\/plot.py#L253-L380"}
{"repo_name":"zqfang\/GSEApy","method_name":"barplot","method_code":"def barplot(df, column='Adjusted P-value', title=\"\", cutoff=0.05, top_term=10,\n            figsize=(6.5,6), color='salmon', ofname=None, **kwargs):\n    \"\"\"\"\"\"\n\n    colname = column   \n    if colname in ['Adjusted P-value', 'P-value']: \n        df = df[df[colname] <= cutoff]\n        if len(df) < 1: \n            msg = \"Warning: No enrich terms using library %s when cutoff = %s\"%(title, cutoff)\n            return msg\n        df = df.assign(logAP = lambda x: - x[colname].apply(np.log10))\n        colname = 'logAP' \n    dd = df.sort_values(by=colname).iloc[-top_term:,:]\n    \n    \n    if hasattr(sys, 'ps1') and (ofname is None):\n        \n        fig = plt.figure(figsize=figsize)\n    else:\n        \n        fig = Figure(figsize=figsize)\n        canvas = FigureCanvas(fig)\n    ax = fig.add_subplot(111)\n    bar = dd.plot.barh(x='Term', y=colname, color=color, \n                       alpha=0.75, fontsize=16, ax=ax)\n    \n    if column in ['Adjusted P-value', 'P-value']:\n        xlabel = \"-log$_{10}$(%s)\"%column\n    else:\n        xlabel = column \n    bar.set_xlabel(xlabel, fontsize=16, fontweight='bold')\n    bar.set_ylabel(\"\")\n    bar.set_title(title, fontsize=24, fontweight='bold')\n    bar.xaxis.set_major_locator(MaxNLocator(integer=True))\n    bar.legend_.remove()\n    adjust_spines(ax, spines=['left','bottom'])\n\n    if ofname is not None: \n        \n        fig.savefig(ofname, bbox_inches='tight', dpi=300)\n        return\n    return ax","method_summary":"Visualize enrichr results.","original_method_code":"def barplot(df, column='Adjusted P-value', title=\"\", cutoff=0.05, top_term=10,\n            figsize=(6.5,6), color='salmon', ofname=None, **kwargs):\n    \"\"\"Visualize enrichr results.\n\n    :param df: GSEApy DataFrame results.\n    :param column: which column of DataFrame to show. Default: Adjusted P-value\n    :param title: figure title.\n    :param cutoff: cut-off of the cloumn you've chosen.\n    :param top_term: number of top enriched terms to show.\n    :param figsize: tuple, matplotlib figsize.\n    :param color: color for bars.\n    :param ofname: output file name. If None, don't save figure    \n    \n    \"\"\"\n\n    colname = column   \n    if colname in ['Adjusted P-value', 'P-value']: \n        df = df[df[colname] <= cutoff]\n        if len(df) < 1: \n            msg = \"Warning: No enrich terms using library %s when cutoff = %s\"%(title, cutoff)\n            return msg\n        df = df.assign(logAP = lambda x: - x[colname].apply(np.log10))\n        colname = 'logAP' \n    dd = df.sort_values(by=colname).iloc[-top_term:,:]\n    # dd = d.head(top_term).sort_values('logAP')\n    # create bar plot\n    if hasattr(sys, 'ps1') and (ofname is None):\n        # working inside python console, show (True) figure\n        fig = plt.figure(figsize=figsize)\n    else:\n        # If working on commandline, don't show figure\n        fig = Figure(figsize=figsize)\n        canvas = FigureCanvas(fig)\n    ax = fig.add_subplot(111)\n    bar = dd.plot.barh(x='Term', y=colname, color=color, \n                       alpha=0.75, fontsize=16, ax=ax)\n    \n    if column in ['Adjusted P-value', 'P-value']:\n        xlabel = \"-log$_{10}$(%s)\"%column\n    else:\n        xlabel = column \n    bar.set_xlabel(xlabel, fontsize=16, fontweight='bold')\n    bar.set_ylabel(\"\")\n    bar.set_title(title, fontsize=24, fontweight='bold')\n    bar.xaxis.set_major_locator(MaxNLocator(integer=True))\n    bar.legend_.remove()\n    adjust_spines(ax, spines=['left','bottom'])\n\n    if ofname is not None: \n        # canvas.print_figure(ofname, bbox_inches='tight', dpi=300)\n        fig.savefig(ofname, bbox_inches='tight', dpi=300)\n        return\n    return ax","method_path":"https:\/\/github.com\/zqfang\/GSEApy\/blob\/673e9ec1391e3b14d3e8a4353117151fd2cb9345\/gseapy\/plot.py#L382-L434"}
{"repo_name":"zqfang\/GSEApy","method_name":"adjust_spines","method_code":"def adjust_spines(ax, spines):\n    \"\"\"\"\"\"\n    for loc, spine in ax.spines.items():\n        if loc in spines:\n            \n            \n            continue\n        else:\n            spine.set_color('none')  \n\n    \n    if 'left' in spines:\n        ax.yaxis.set_ticks_position('left')\n    else:\n        \n        ax.yaxis.set_ticks([])\n\n    if 'bottom' in spines:\n        ax.xaxis.set_ticks_position('bottom')\n    else:\n        \n        ax.xaxis.set_ticks([])","method_summary":"function for removing spines and ticks.","original_method_code":"def adjust_spines(ax, spines):\n    \"\"\"function for removing spines and ticks.\n\n    :param ax: axes object\n    :param spines: a list of spines names to keep. e.g [left, right, top, bottom]\n                    if spines = []. remove all spines and ticks.\n\n    \"\"\"\n    for loc, spine in ax.spines.items():\n        if loc in spines:\n            # spine.set_position(('outward', 10))  # outward by 10 points\n            # spine.set_smart_bounds(True)\n            continue\n        else:\n            spine.set_color('none')  # don't draw spine\n\n    # turn off ticks where there is no spine\n    if 'left' in spines:\n        ax.yaxis.set_ticks_position('left')\n    else:\n        # no yaxis ticks\n        ax.yaxis.set_ticks([])\n\n    if 'bottom' in spines:\n        ax.xaxis.set_ticks_position('bottom')\n    else:\n        # no xaxis ticks\n        ax.xaxis.set_ticks([])","method_path":"https:\/\/github.com\/zqfang\/GSEApy\/blob\/673e9ec1391e3b14d3e8a4353117151fd2cb9345\/gseapy\/plot.py#L436-L463"}
{"repo_name":"zqfang\/GSEApy","method_name":"main","method_code":"def main():\n    \"\"\"\"\"\"\n\n    \n    argparser = prepare_argparser()\n    args = argparser.parse_args()\n    subcommand = args.subcommand_name\n\n    if subcommand == \"replot\":\n        \n        from .gsea import Replot\n        rep = Replot(indir=args.indir, outdir=args.outdir, weighted_score_type=args.weight,\n                     figsize=args.figsize, graph_num=args.graph,\n                     format=args.format, verbose=args.verbose)\n        rep.run()\n\n\n    elif subcommand == \"gsea\":\n        \n        from .gsea import GSEA\n\n        gs = GSEA(args.data, args.gmt, args.cls, args.outdir,\n                  args.mins, args.maxs, args.n, args.weight,\n                  args.type, args.method, args.ascending, args.threads,\n                  args.figsize, args.format, args.graph, args.noplot, args.seed, args.verbose)\n        gs.run()\n    elif subcommand == \"prerank\":\n        from .gsea import Prerank\n\n        pre = Prerank(args.rnk, args.gmt, args.outdir, args.label[0], args.label[1],\n                      args.mins, args.maxs, args.n, args.weight, args.ascending, args.threads,\n                      args.figsize, args.format, args.graph, args.noplot, args.seed, args.verbose)\n        pre.run()\n\n    elif subcommand == \"ssgsea\":\n        from .gsea import SingleSampleGSEA\n        ss = SingleSampleGSEA(data=args.data, gene_sets=args.gmt, outdir=args.outdir,\n                              sample_norm_method=args.norm,\n                              min_size=args.mins, max_size=args.maxs, permutation_num=args.n,\n                              weighted_score_type=args.weight, scale=args.scale,\n                              ascending=args.ascending, processes=args.threads,\n                              figsize=args.figsize, format=args.format, graph_num=args.graph,\n                              no_plot=args.noplot, seed=args.seed, verbose=args.verbose)\n        ss.run()\n\n    elif subcommand == \"enrichr\":\n        \n        from .enrichr import Enrichr\n        enr = Enrichr(gene_list=args.gene_list, descriptions=args.descrip,\n                      gene_sets=args.library, organism=args.organism,\n                      outdir=args.outdir, format=args.format, cutoff=args.thresh, \n                      background=args.bg, figsize=args.figsize,\n                      top_term=args.term, no_plot=args.noplot, verbose=args.verbose)\n        enr.run()\n    elif subcommand == \"biomart\":\n        from .parser import Biomart\n        \n        name, value = args.filter\n        if os.path.isfile(value):\n            with open(value, 'r') as val:\n                lines = val.readlines()\n            value = [ l.strip() for l in lines]\n        \n        bm = Biomart(host=args.host, verbose=args.verbose)\n        bm.query(dataset=args.bg, attributes=args.attrs.split(\",\"), \n                 filters={name : value}, filename=args.ofile)\n    else:\n        argparser.print_help()\n        sys.exit(0)","method_summary":"The Main function\/pipeline for GSEApy.","original_method_code":"def main():\n    \"\"\"The Main function\/pipeline for GSEApy.\"\"\"\n\n    # Parse options...\n    argparser = prepare_argparser()\n    args = argparser.parse_args()\n    subcommand = args.subcommand_name\n\n    if subcommand == \"replot\":\n        # reproduce plots using GSEAPY\n        from .gsea import Replot\n        rep = Replot(indir=args.indir, outdir=args.outdir, weighted_score_type=args.weight,\n                     figsize=args.figsize, graph_num=args.graph,\n                     format=args.format, verbose=args.verbose)\n        rep.run()\n\n\n    elif subcommand == \"gsea\":\n        # compute using GSEAPY\n        from .gsea import GSEA\n\n        gs = GSEA(args.data, args.gmt, args.cls, args.outdir,\n                  args.mins, args.maxs, args.n, args.weight,\n                  args.type, args.method, args.ascending, args.threads,\n                  args.figsize, args.format, args.graph, args.noplot, args.seed, args.verbose)\n        gs.run()\n    elif subcommand == \"prerank\":\n        from .gsea import Prerank\n\n        pre = Prerank(args.rnk, args.gmt, args.outdir, args.label[0], args.label[1],\n                      args.mins, args.maxs, args.n, args.weight, args.ascending, args.threads,\n                      args.figsize, args.format, args.graph, args.noplot, args.seed, args.verbose)\n        pre.run()\n\n    elif subcommand == \"ssgsea\":\n        from .gsea import SingleSampleGSEA\n        ss = SingleSampleGSEA(data=args.data, gene_sets=args.gmt, outdir=args.outdir,\n                              sample_norm_method=args.norm,\n                              min_size=args.mins, max_size=args.maxs, permutation_num=args.n,\n                              weighted_score_type=args.weight, scale=args.scale,\n                              ascending=args.ascending, processes=args.threads,\n                              figsize=args.figsize, format=args.format, graph_num=args.graph,\n                              no_plot=args.noplot, seed=args.seed, verbose=args.verbose)\n        ss.run()\n\n    elif subcommand == \"enrichr\":\n        # calling enrichr API\n        from .enrichr import Enrichr\n        enr = Enrichr(gene_list=args.gene_list, descriptions=args.descrip,\n                      gene_sets=args.library, organism=args.organism,\n                      outdir=args.outdir, format=args.format, cutoff=args.thresh, \n                      background=args.bg, figsize=args.figsize,\n                      top_term=args.term, no_plot=args.noplot, verbose=args.verbose)\n        enr.run()\n    elif subcommand == \"biomart\":\n        from .parser import Biomart\n        # read input file or a argument\n        name, value = args.filter\n        if os.path.isfile(value):\n            with open(value, 'r') as val:\n                lines = val.readlines()\n            value = [ l.strip() for l in lines]\n        # run query\n        bm = Biomart(host=args.host, verbose=args.verbose)\n        bm.query(dataset=args.bg, attributes=args.attrs.split(\",\"), \n                 filters={name : value}, filename=args.ofile)\n    else:\n        argparser.print_help()\n        sys.exit(0)","method_path":"https:\/\/github.com\/zqfang\/GSEApy\/blob\/673e9ec1391e3b14d3e8a4353117151fd2cb9345\/gseapy\/__main__.py#L16-L84"}
{"repo_name":"zqfang\/GSEApy","method_name":"prepare_argparser","method_code":"def prepare_argparser():\n    \"\"\"\"\"\"\n    description = \"%(prog)s -- Gene Set Enrichment Analysis in Python\"\n    epilog = \"For command line options of each command, type: %(prog)s COMMAND -h\"\n\n    \n    argparser = ap.ArgumentParser(description=description, epilog=epilog)\n    argparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \"+ __version__)\n    subparsers = argparser.add_subparsers(dest='subcommand_name') \n\n    \n    add_gsea_parser(subparsers)\n    \n    add_prerank_parser(subparsers)\n    \n    add_singlesample_parser(subparsers)\n    \n    add_plot_parser(subparsers)\n    \n    add_enrichr_parser(subparsers)\n    \n    add_biomart_parser(subparsers)\n\n    return argparser","method_summary":"Prepare argparser object. New options will be added in this function first.","original_method_code":"def prepare_argparser():\n    \"\"\"Prepare argparser object. New options will be added in this function first.\"\"\"\n    description = \"%(prog)s -- Gene Set Enrichment Analysis in Python\"\n    epilog = \"For command line options of each command, type: %(prog)s COMMAND -h\"\n\n    # top-level parser\n    argparser = ap.ArgumentParser(description=description, epilog=epilog)\n    argparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \"+ __version__)\n    subparsers = argparser.add_subparsers(dest='subcommand_name') #help=\"sub-command help\")\n\n    # command for 'gsea'\n    add_gsea_parser(subparsers)\n    # command for 'prerank'\n    add_prerank_parser(subparsers)\n    # command for 'ssgsea'\n    add_singlesample_parser(subparsers)\n    # command for 'plot'\n    add_plot_parser(subparsers)\n    # command for 'enrichr'\n    add_enrichr_parser(subparsers)\n    # command for 'biomart'\n    add_biomart_parser(subparsers)\n\n    return argparser","method_path":"https:\/\/github.com\/zqfang\/GSEApy\/blob\/673e9ec1391e3b14d3e8a4353117151fd2cb9345\/gseapy\/__main__.py#L87-L110"}
{"repo_name":"3DLIRIOUS\/MeshLabXML","method_name":"cube","method_code":"def cube(script, size=1.0, center=False, color=None):\n    \"\"\"\"\"\"\n\n    \"\"\"\"\"\"\n    size = util.make_list(size, 3)\n    if script.ml_version == '1.3.4BETA':\n        filter_name = 'Box'\n    else:\n        filter_name = 'Box\/Cube'\n    filter_xml = ''.join([\n        '  <filter name=\"{}\">\\n'.format(filter_name),\n        '    <Param name=\"size\" ',\n        'value=\"1.0\" ',\n        'description=\"Scale factor\" ',\n        'type=\"RichFloat\" ',\n        '\/>\\n',\n        '  <\/filter>\\n'])\n    util.write_filter(script, filter_xml)\n    if isinstance(script, FilterScript):\n        script.add_layer('Cube', change_layer=True)\n    transform.scale(script, value=size)\n    \n    if not center:\n        transform.translate(script, value=[size[0]\/2, size[1]\/2, size[2]\/2])\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_summary":"Create a cube primitive Note that this is made of 6 quads, not triangles","original_method_code":"def cube(script, size=1.0, center=False, color=None):\n    \"\"\"Create a cube primitive\n\n    Note that this is made of 6 quads, not triangles\n    \"\"\"\n\n    \"\"\"# Convert size to list if it isn't already\n    if not isinstance(size, list):\n        size = list(size)\n    # If a single value was supplied use it for all 3 axes\n    if len(size) == 1:\n        size = [size[0], size[0], size[0]]\"\"\"\n    size = util.make_list(size, 3)\n    if script.ml_version == '1.3.4BETA':\n        filter_name = 'Box'\n    else:\n        filter_name = 'Box\/Cube'\n    filter_xml = ''.join([\n        '  <filter name=\"{}\">\\n'.format(filter_name),\n        '    <Param name=\"size\" ',\n        'value=\"1.0\" ',\n        'description=\"Scale factor\" ',\n        'type=\"RichFloat\" ',\n        '\/>\\n',\n        '  <\/filter>\\n'])\n    util.write_filter(script, filter_xml)\n    if isinstance(script, FilterScript):\n        script.add_layer('Cube', change_layer=True)\n    transform.scale(script, value=size)\n    # Box is centered on origin at creation\n    if not center:\n        transform.translate(script, value=[size[0]\/2, size[1]\/2, size[2]\/2])\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_path":"https:\/\/github.com\/3DLIRIOUS\/MeshLabXML\/blob\/177cce21e92baca500f56a932d66bd9a33257af8\/meshlabxml\/create.py#L13-L47"}
{"repo_name":"3DLIRIOUS\/MeshLabXML","method_name":"torus","method_code":"def torus(script, major_radius=3.0, minor_radius=1.0, inner_diameter=None,\n          outer_diameter=None, major_segments=48, minor_segments=12,\n          color=None):\n    \"\"\"\"\"\"\n    if inner_diameter is not None and outer_diameter is not None:\n        major_radius = (inner_diameter + outer_diameter) \/ 4\n        minor_radius = major_radius - inner_diameter \/ 2\n        \n        \n    filter_xml = ''.join([\n        '  <filter name=\"Torus\">\\n',\n        '    <Param name=\"hRadius\" ',\n        'value=\"%s\" ' % major_radius,\n        'description=\"Horizontal Radius\" ',\n        'type=\"RichFloat\" ',\n        '\/>\\n',\n        '    <Param name=\"vRadius\" ',\n        'value=\"%s\" ' % minor_radius,\n        'description=\"Vertical Radius\" ',\n        'type=\"RichFloat\" ',\n        '\/>\\n',\n        '    <Param name=\"hSubdiv\" ',\n        'value=\"%d\" ' % major_segments,\n        'description=\"Horizontal Subdivision\" ',\n        'type=\"RichInt\" ',\n        '\/>\\n',\n        '    <Param name=\"vSubdiv\" ',\n        'value=\"%d\" ' % minor_segments,\n        'description=\"Vertical Subdivision\" ',\n        'type=\"RichInt\" ',\n        '\/>\\n',\n        '  <\/filter>\\n'])\n    util.write_filter(script, filter_xml)\n    if isinstance(script, FilterScript):\n        script.add_layer('Torus', change_layer=True)\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_summary":"Create a torus mesh","original_method_code":"def torus(script, major_radius=3.0, minor_radius=1.0, inner_diameter=None,\n          outer_diameter=None, major_segments=48, minor_segments=12,\n          color=None):\n    \"\"\"Create a torus mesh\n\n    Args:\n        major_radius (float, (optional)): radius from the origin to the\n            center of the cross sections\n        minor_radius (float, (optional)): radius of the torus cross\n            section\n        inner_diameter (float, (optional)): inner diameter of torus. If\n            both inner_diameter and outer_diameter are provided then\n            these will override major_radius and minor_radius.,\n        outer_diameter (float, (optional)): outer diameter of torus. If\n            both inner_diameter and outer_diameter are provided then\n            these will override major_radius and minor_radius.\n        major_segments (int (optional)): number of segments for the main\n            ring of the torus\n        minor_segments (int (optional)): number of segments for the minor\n            ring of the torus\n        color (str (optional)): color name to apply vertex colors to the\n            newly created mesh\n\n    Returns:\n        None\n\n    \"\"\"\n    if inner_diameter is not None and outer_diameter is not None:\n        major_radius = (inner_diameter + outer_diameter) \/ 4\n        minor_radius = major_radius - inner_diameter \/ 2\n        # Ref: inner_diameter = 2 * (major_radius - minor_radius)\n        # Ref: outer_diameter = 2 * (major_radius + minor_radius)\n    filter_xml = ''.join([\n        '  <filter name=\"Torus\">\\n',\n        '    <Param name=\"hRadius\" ',\n        'value=\"%s\" ' % major_radius,\n        'description=\"Horizontal Radius\" ',\n        'type=\"RichFloat\" ',\n        '\/>\\n',\n        '    <Param name=\"vRadius\" ',\n        'value=\"%s\" ' % minor_radius,\n        'description=\"Vertical Radius\" ',\n        'type=\"RichFloat\" ',\n        '\/>\\n',\n        '    <Param name=\"hSubdiv\" ',\n        'value=\"%d\" ' % major_segments,\n        'description=\"Horizontal Subdivision\" ',\n        'type=\"RichInt\" ',\n        '\/>\\n',\n        '    <Param name=\"vSubdiv\" ',\n        'value=\"%d\" ' % minor_segments,\n        'description=\"Vertical Subdivision\" ',\n        'type=\"RichInt\" ',\n        '\/>\\n',\n        '  <\/filter>\\n'])\n    util.write_filter(script, filter_xml)\n    if isinstance(script, FilterScript):\n        script.add_layer('Torus', change_layer=True)\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_path":"https:\/\/github.com\/3DLIRIOUS\/MeshLabXML\/blob\/177cce21e92baca500f56a932d66bd9a33257af8\/meshlabxml\/create.py#L196-L256"}
{"repo_name":"3DLIRIOUS\/MeshLabXML","method_name":"cube_open_hires_old","method_code":"def cube_open_hires_old(script, size=1.0, x_segments=1, y_segments=1, z_segments=1,\n                    center=False, color=None):\n    \"\"\"\"\"\"\n    \"\"\"\"\"\"\n    size = util.make_list(size, 3)\n\n    \n    grid(script, [size[0], size[2]],\n         x_segments=x_segments,\n         y_segments=z_segments)\n    transform.rotate(script, 'x', 90)\n    \n    layers.duplicate(script)\n    \n    transform.rotate(script, 'z', 180)\n    transform.translate(script, [size[0], size[1], 0])\n\n    \n    grid(script, [size[2], size[1]],\n         x_segments=z_segments,\n         y_segments=y_segments)\n    transform.rotate(script, 'y', -90)\n    \n    \n    layers.duplicate(script)\n    \n    transform.rotate(script, 'z', 180)\n    transform.translate(script, [size[0], size[1], 0])\n\n    layers.join(script)\n    clean.merge_vert(script, threshold=0.00002)\n    \n    if center:\n        transform.translate(script, [-size[0] \/ 2, -size[1] \/ 2, -size[2] \/ 2])\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_summary":"Creates a square open tube, e.g. a box with no top or bottom. Useful if you want to wrap it around and join the open ends together, forming a torus.","original_method_code":"def cube_open_hires_old(script, size=1.0, x_segments=1, y_segments=1, z_segments=1,\n                    center=False, color=None):\n    \"\"\" Creates a square open tube, e.g. a box with no top or bottom.\n\n    Useful if you want to wrap it around and join the open ends together, forming a torus.\n    \"\"\"\n    \"\"\"# Convert size to list if it isn't already\n    if not isinstance(size, list):\n        size = list(size)\n    # If a single value was supplied use it for all 3 axes\n    if len(size) == 1:\n        size = [size[0], size[0], size[0]]\"\"\"\n    size = util.make_list(size, 3)\n\n    # X sides\n    grid(script, [size[0], size[2]],\n         x_segments=x_segments,\n         y_segments=z_segments)\n    transform.rotate(script, 'x', 90)\n    #transform.translate(script, [0, 0, -size[2]])\n    layers.duplicate(script)\n    # Rotate to correct normals\n    transform.rotate(script, 'z', 180)\n    transform.translate(script, [size[0], size[1], 0])\n\n    # Y sides\n    grid(script, [size[2], size[1]],\n         x_segments=z_segments,\n         y_segments=y_segments)\n    transform.rotate(script, 'y', -90)\n    #transform.rotate(script, 'z', 90)\n    #transform.translate(script, [0, 0, -size[2]])\n    layers.duplicate(script)\n    # Rotate to correct normals\n    transform.rotate(script, 'z', 180)\n    transform.translate(script, [size[0], size[1], 0])\n\n    layers.join(script)\n    clean.merge_vert(script, threshold=0.00002)\n    # normals.fix(script)\n    if center:\n        transform.translate(script, [-size[0] \/ 2, -size[1] \/ 2, -size[2] \/ 2])\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_path":"https:\/\/github.com\/3DLIRIOUS\/MeshLabXML\/blob\/177cce21e92baca500f56a932d66bd9a33257af8\/meshlabxml\/create.py#L408-L452"}
{"repo_name":"3DLIRIOUS\/MeshLabXML","method_name":"cube_open_hires","method_code":"def cube_open_hires(script, size=1.0, x_segments=1, y_segments=1, z_segments=1,\n                    center=False, color=None):\n    \"\"\"\"\"\"\n    \"\"\"\"\"\"\n    size = util.make_list(size, 3)\n\n    \n    grid(script, [2*(x_segments + y_segments), z_segments],\n         x_segments=2*(x_segments + y_segments),\n         y_segments=z_segments)\n    transform.rotate(script, 'x', 90)\n    \n    if script.ml_version == '1.3.4BETA': \n        transform.vert_function(script,\n            x_func='if(x>{x_size}, {x_size}, x)'.format(x_size=x_segments),\n            y_func='if(x>{x_size}, (x-{x_size}), y)'.format(x_size=x_segments),\n            z_func='z')\n        transform.vert_function(script,\n            x_func='if(y>{y_size}, ({y_size}-y+{x_size}), x)'.format(x_size=x_segments, y_size=y_segments),\n            y_func='if(y>{y_size}, {y_size}, y)'.format(y_size=y_segments),\n            z_func='z')\n        transform.vert_function(script,\n            x_func='if(x<0, 0, x)',\n            y_func='if(x<0, ({y_size}+x), y)'.format(y_size=y_segments),\n            z_func='z')\n    else: \n        transform.vert_function(script,\n            x_func='(x>{x_size} ? {x_size} : x)'.format(x_size=x_segments),\n            y_func='(x>{x_size} ? (x-{x_size}) : y)'.format(x_size=x_segments),\n            z_func='z')\n        transform.vert_function(script,\n            x_func='(y>{y_size} ? ({y_size}-y+{x_size}) : x)'.format(x_size=x_segments, y_size=y_segments),\n            y_func='(y>{y_size} ? {y_size} : y)'.format(y_size=y_segments),\n            z_func='z')\n        transform.vert_function(script,\n            x_func='(x<0 ? 0 : x)',\n            y_func='(x<0 ? ({y_size}+x) : y)'.format(y_size=y_segments),\n            z_func='z')\n    clean.merge_vert(script, threshold=0.00002)\n    transform.scale(script, [size[0]\/x_segments, size[1]\/y_segments, size[2]\/z_segments])\n    if center:\n        transform.translate(script, [-size[0] \/ 2, -size[1] \/ 2, -size[2] \/ 2])\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_summary":"Creates a square open tube, e.g. a box with no top or bottom. Useful if you want to wrap it around and join the open ends together, forming a torus.","original_method_code":"def cube_open_hires(script, size=1.0, x_segments=1, y_segments=1, z_segments=1,\n                    center=False, color=None):\n    \"\"\" Creates a square open tube, e.g. a box with no top or bottom.\n\n    Useful if you want to wrap it around and join the open ends together, forming a torus.\n    \"\"\"\n    \"\"\"# Convert size to list if it isn't already\n    if not isinstance(size, list):\n        size = list(size)\n    # If a single value was supplied use it for all 3 axes\n    if len(size) == 1:\n        size = [size[0], size[0], size[0]]\"\"\"\n    size = util.make_list(size, 3)\n\n    # Make big grid and bend\n    grid(script, [2*(x_segments + y_segments), z_segments],\n         x_segments=2*(x_segments + y_segments),\n         y_segments=z_segments)\n    transform.rotate(script, 'x', 90)\n    # Bend 3 times into a rectangular tube\n    if script.ml_version == '1.3.4BETA': # muparser version: 1.3.2\n        transform.vert_function(script,\n            x_func='if(x>{x_size}, {x_size}, x)'.format(x_size=x_segments),\n            y_func='if(x>{x_size}, (x-{x_size}), y)'.format(x_size=x_segments),\n            z_func='z')\n        transform.vert_function(script,\n            x_func='if(y>{y_size}, ({y_size}-y+{x_size}), x)'.format(x_size=x_segments, y_size=y_segments),\n            y_func='if(y>{y_size}, {y_size}, y)'.format(y_size=y_segments),\n            z_func='z')\n        transform.vert_function(script,\n            x_func='if(x<0, 0, x)',\n            y_func='if(x<0, ({y_size}+x), y)'.format(y_size=y_segments),\n            z_func='z')\n    else: # muparser version: 2.2.5\n        transform.vert_function(script,\n            x_func='(x>{x_size} ? {x_size} : x)'.format(x_size=x_segments),\n            y_func='(x>{x_size} ? (x-{x_size}) : y)'.format(x_size=x_segments),\n            z_func='z')\n        transform.vert_function(script,\n            x_func='(y>{y_size} ? ({y_size}-y+{x_size}) : x)'.format(x_size=x_segments, y_size=y_segments),\n            y_func='(y>{y_size} ? {y_size} : y)'.format(y_size=y_segments),\n            z_func='z')\n        transform.vert_function(script,\n            x_func='(x<0 ? 0 : x)',\n            y_func='(x<0 ? ({y_size}+x) : y)'.format(y_size=y_segments),\n            z_func='z')\n    clean.merge_vert(script, threshold=0.00002)\n    transform.scale(script, [size[0]\/x_segments, size[1]\/y_segments, size[2]\/z_segments])\n    if center:\n        transform.translate(script, [-size[0] \/ 2, -size[1] \/ 2, -size[2] \/ 2])\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_path":"https:\/\/github.com\/3DLIRIOUS\/MeshLabXML\/blob\/177cce21e92baca500f56a932d66bd9a33257af8\/meshlabxml\/create.py#L455-L507"}
{"repo_name":"3DLIRIOUS\/MeshLabXML","method_name":"plane_hires_edges","method_code":"def plane_hires_edges(script, size=1.0, x_segments=1, y_segments=1,\n                      center=False, color=None):\n    \"\"\"\"\"\"\n    size = util.make_list(size, 2)\n\n    grid(script, size=[x_segments + y_segments - 1, 1],\n         x_segments=(x_segments + y_segments - 1), y_segments=1)\n    if ml_script1.ml_version == '1.3.4BETA':\n        and_val = 'and'\n    else:\n        and_val = '&&'\n\n    if script.ml_version == '1.3.4BETA': \n        \n        transform.vert_function(\n            script,\n            x_func='if((y>0) and (x<%s),0,x)' % (y_segments),\n            y_func='if((y>0) and (x<%s),(x+1)*%s,y)' % (\n                y_segments, size[1] \/ y_segments))\n        \n        transform.vert_function(\n            script,\n            x_func='if((y>0) and (x>=%s),(x-%s+1)*%s,x)' % (\n                y_segments, y_segments, size[0] \/ x_segments),\n            y_func='if((y>0) and (x>=%s),%s,y)' % (y_segments, size[1]))\n        \n        transform.vert_function(\n            script,\n            x_func='if((y<.00001) and (x>%s),%s,x)' % (\n                x_segments, size[0]),\n            y_func='if((y<.00001) and (x>%s),(x-%s)*%s,y)' % (\n                x_segments, x_segments, size[1] \/ y_segments))\n        \n        transform.vert_function(\n            script,\n            x_func='if((y<.00001) and (x<=%s) and (x>0),(x)*%s,x)' % (\n                x_segments, size[0] \/ x_segments),\n            y_func='if((y<.00001) and (x<=%s) and (x>0),0,y)' % (x_segments))\n    else: \n        \n        transform.vert_function(\n            script,\n            x_func='((y>0) && (x<{yseg}) ? 0 : x)'.format(yseg=y_segments),\n            y_func='((y>0) && (x<%s) ? (x+1)*%s : y)' % (\n                y_segments, size[1] \/ y_segments))\n        \n        transform.vert_function(\n            script,\n            x_func='((y>0) && (x>=%s) ? (x-%s+1)*%s : x)' % (\n                y_segments, y_segments, size[0] \/ x_segments),\n            y_func='((y>0) && (x>=%s) ? %s : y)' % (y_segments, size[1]))\n        \n        transform.vert_function(\n            script,\n            x_func='((y<.00001) && (x>%s) ? %s : x)' % (\n                x_segments, size[0]),\n            y_func='((y<.00001) && (x>%s) ? (x-%s)*%s : y)' % (\n                x_segments, x_segments, size[1] \/ y_segments))\n        \n        transform.vert_function(\n            script,\n            x_func='((y<.00001) && (x<=%s) && (x>0) ? (x)*%s : x)' % (\n                x_segments, size[0] \/ x_segments),\n            y_func='((y<.00001) && (x<=%s) && (x>0) ? 0 : y)' % (x_segments))\n    if center:\n        transform.translate(script, [-size[0] \/ 2, -size[1] \/ 2])\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_summary":"Creates a plane with a specified number of vertices on it sides, but no vertices on the interior. Currently used to create a simpler bottom for cube_hires.","original_method_code":"def plane_hires_edges(script, size=1.0, x_segments=1, y_segments=1,\n                      center=False, color=None):\n    \"\"\" Creates a plane with a specified number of vertices\n    on it sides, but no vertices on the interior.\n\n    Currently used to create a simpler bottom for cube_hires.\n\n    \"\"\"\n    size = util.make_list(size, 2)\n\n    grid(script, size=[x_segments + y_segments - 1, 1],\n         x_segments=(x_segments + y_segments - 1), y_segments=1)\n    if ml_script1.ml_version == '1.3.4BETA':\n        and_val = 'and'\n    else:\n        and_val = '&&'\n\n    if script.ml_version == '1.3.4BETA': # muparser version: 1.3.2\n        # Deform left side\n        transform.vert_function(\n            script,\n            x_func='if((y>0) and (x<%s),0,x)' % (y_segments),\n            y_func='if((y>0) and (x<%s),(x+1)*%s,y)' % (\n                y_segments, size[1] \/ y_segments))\n        # Deform top\n        transform.vert_function(\n            script,\n            x_func='if((y>0) and (x>=%s),(x-%s+1)*%s,x)' % (\n                y_segments, y_segments, size[0] \/ x_segments),\n            y_func='if((y>0) and (x>=%s),%s,y)' % (y_segments, size[1]))\n        # Deform right side\n        transform.vert_function(\n            script,\n            x_func='if((y<.00001) and (x>%s),%s,x)' % (\n                x_segments, size[0]),\n            y_func='if((y<.00001) and (x>%s),(x-%s)*%s,y)' % (\n                x_segments, x_segments, size[1] \/ y_segments))\n        # Deform bottom\n        transform.vert_function(\n            script,\n            x_func='if((y<.00001) and (x<=%s) and (x>0),(x)*%s,x)' % (\n                x_segments, size[0] \/ x_segments),\n            y_func='if((y<.00001) and (x<=%s) and (x>0),0,y)' % (x_segments))\n    else: # muparser version: 2.2.5\n        # Deform left side\n        transform.vert_function(\n            script,\n            x_func='((y>0) && (x<{yseg}) ? 0 : x)'.format(yseg=y_segments),\n            y_func='((y>0) && (x<%s) ? (x+1)*%s : y)' % (\n                y_segments, size[1] \/ y_segments))\n        # Deform top\n        transform.vert_function(\n            script,\n            x_func='((y>0) && (x>=%s) ? (x-%s+1)*%s : x)' % (\n                y_segments, y_segments, size[0] \/ x_segments),\n            y_func='((y>0) && (x>=%s) ? %s : y)' % (y_segments, size[1]))\n        # Deform right side\n        transform.vert_function(\n            script,\n            x_func='((y<.00001) && (x>%s) ? %s : x)' % (\n                x_segments, size[0]),\n            y_func='((y<.00001) && (x>%s) ? (x-%s)*%s : y)' % (\n                x_segments, x_segments, size[1] \/ y_segments))\n        # Deform bottom\n        transform.vert_function(\n            script,\n            x_func='((y<.00001) && (x<=%s) && (x>0) ? (x)*%s : x)' % (\n                x_segments, size[0] \/ x_segments),\n            y_func='((y<.00001) && (x<=%s) && (x>0) ? 0 : y)' % (x_segments))\n    if center:\n        transform.translate(script, [-size[0] \/ 2, -size[1] \/ 2])\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_path":"https:\/\/github.com\/3DLIRIOUS\/MeshLabXML\/blob\/177cce21e92baca500f56a932d66bd9a33257af8\/meshlabxml\/create.py#L510-L583"}
{"repo_name":"3DLIRIOUS\/MeshLabXML","method_name":"annulus_hires","method_code":"def annulus_hires(script, radius=None, radius1=None, radius2=None,\n                  diameter=None, diameter1=None, diameter2=None,\n                  cir_segments=48, rad_segments=1, color=None):\n    \"\"\"\"\"\"\n    if radius is not None and diameter is None:\n        if radius1 is None and diameter1 is None:\n            radius1 = radius\n        if radius2 is None and diameter2 is None:\n            radius2 = 0\n    if diameter is not None:\n        if radius1 is None and diameter1 is None:\n            radius1 = diameter \/ 2\n        if radius2 is None and diameter2 is None:\n            radius2 = 0\n    if diameter1 is not None:\n        radius1 = diameter1 \/ 2\n    if diameter2 is not None:\n        radius2 = diameter2 \/ 2\n    if radius1 is None:\n        radius1 = 1\n    if radius2 is None:\n        radius2 = 0\n    ring = (radius1 - radius2) \/ rad_segments\n\n    for i in range(0, rad_segments):\n        annulus(script,\n                radius1=radius1 - i * ring,\n                radius2=radius1 - (i + 1) * ring,\n                cir_segments=cir_segments)\n    layers.join(script, merge_vert=True)\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_summary":"Create a cylinder with user defined number of segments","original_method_code":"def annulus_hires(script, radius=None, radius1=None, radius2=None,\n                  diameter=None, diameter1=None, diameter2=None,\n                  cir_segments=48, rad_segments=1, color=None):\n    \"\"\"Create a cylinder with user defined number of segments\n\n    \"\"\"\n    if radius is not None and diameter is None:\n        if radius1 is None and diameter1 is None:\n            radius1 = radius\n        if radius2 is None and diameter2 is None:\n            radius2 = 0\n    if diameter is not None:\n        if radius1 is None and diameter1 is None:\n            radius1 = diameter \/ 2\n        if radius2 is None and diameter2 is None:\n            radius2 = 0\n    if diameter1 is not None:\n        radius1 = diameter1 \/ 2\n    if diameter2 is not None:\n        radius2 = diameter2 \/ 2\n    if radius1 is None:\n        radius1 = 1\n    if radius2 is None:\n        radius2 = 0\n    ring = (radius1 - radius2) \/ rad_segments\n\n    for i in range(0, rad_segments):\n        annulus(script,\n                radius1=radius1 - i * ring,\n                radius2=radius1 - (i + 1) * ring,\n                cir_segments=cir_segments)\n    layers.join(script, merge_vert=True)\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_path":"https:\/\/github.com\/3DLIRIOUS\/MeshLabXML\/blob\/177cce21e92baca500f56a932d66bd9a33257af8\/meshlabxml\/create.py#L651-L685"}
{"repo_name":"3DLIRIOUS\/MeshLabXML","method_name":"tube_hires","method_code":"def tube_hires(script, height=1.0, radius=None, radius1=None, radius2=None,\n               diameter=None, diameter1=None, diameter2=None, cir_segments=32,\n               rad_segments=1, height_segments=1, center=False,\n               simple_bottom=False, color=None):\n    \"\"\"\"\"\"\n\n    \n    \n    \n    if radius is not None and diameter is None:\n        if radius1 is None and diameter1 is None:\n            radius1 = radius\n        if radius2 is None and diameter2 is None:\n            radius2 = 0\n    if diameter is not None:\n        if radius1 is None and diameter1 is None:\n            radius1 = diameter \/ 2\n        if radius2 is None and diameter2 is None:\n            radius2 = 0\n    if diameter1 is not None:\n        radius1 = diameter1 \/ 2\n    if diameter2 is not None:\n        radius2 = diameter2 \/ 2\n    if radius1 is None:\n        radius1 = 1\n    if radius2 is None:\n        radius2 = 0\n\n    \n    annulus_hires(script,\n                  radius1=radius1,\n                  radius2=radius2,\n                  cir_segments=cir_segments,\n                  rad_segments=rad_segments)\n    transform.translate(script, [0, 0, height])\n\n    \n    if simple_bottom:\n        annulus(script,\n                radius1=radius1,\n                radius2=radius2,\n                cir_segments=cir_segments)\n    else:\n        layers.duplicate(script)\n        transform.translate(script, [0, 0, -height])\n    \n    transform.rotate(script, 'x', 180)\n\n    \n    cylinder_open_hires(script, height, radius1,\n                        cir_segments=cir_segments,\n                        height_segments=height_segments)\n\n    \n    if radius2 != 0:\n        cylinder_open_hires(script, height, radius2,\n                            cir_segments=cir_segments,\n                            height_segments=height_segments,\n                            invert_normals=True)\n\n    \n    layers.join(script)\n    \n    clean.merge_vert(script, threshold=0.00002)\n    if center:\n        transform.translate(script, [0, 0, -height \/ 2])\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_summary":"Create a cylinder with user defined number of segments","original_method_code":"def tube_hires(script, height=1.0, radius=None, radius1=None, radius2=None,\n               diameter=None, diameter1=None, diameter2=None, cir_segments=32,\n               rad_segments=1, height_segments=1, center=False,\n               simple_bottom=False, color=None):\n    \"\"\"Create a cylinder with user defined number of segments\n\n    \"\"\"\n\n    # TODO: add option to round the top of the cylinder, i.e. deform spherically\n    # TODO: add warnings if values are ignored, e.g. if you specify both radius\n    # and diameter.\n    if radius is not None and diameter is None:\n        if radius1 is None and diameter1 is None:\n            radius1 = radius\n        if radius2 is None and diameter2 is None:\n            radius2 = 0\n    if diameter is not None:\n        if radius1 is None and diameter1 is None:\n            radius1 = diameter \/ 2\n        if radius2 is None and diameter2 is None:\n            radius2 = 0\n    if diameter1 is not None:\n        radius1 = diameter1 \/ 2\n    if diameter2 is not None:\n        radius2 = diameter2 \/ 2\n    if radius1 is None:\n        radius1 = 1\n    if radius2 is None:\n        radius2 = 0\n\n    # Create top\n    annulus_hires(script,\n                  radius1=radius1,\n                  radius2=radius2,\n                  cir_segments=cir_segments,\n                  rad_segments=rad_segments)\n    transform.translate(script, [0, 0, height])\n\n    # Create bottom\n    if simple_bottom:\n        annulus(script,\n                radius1=radius1,\n                radius2=radius2,\n                cir_segments=cir_segments)\n    else:\n        layers.duplicate(script)\n        transform.translate(script, [0, 0, -height])\n    # Rotate to correct normals\n    transform.rotate(script, 'x', 180)\n\n    # Create outer tube\n    cylinder_open_hires(script, height, radius1,\n                        cir_segments=cir_segments,\n                        height_segments=height_segments)\n\n    # Create inner tube\n    if radius2 != 0:\n        cylinder_open_hires(script, height, radius2,\n                            cir_segments=cir_segments,\n                            height_segments=height_segments,\n                            invert_normals=True)\n\n    # Join everything together\n    layers.join(script)\n    # Need some tolerance on merge_vert due to rounding errors\n    clean.merge_vert(script, threshold=0.00002)\n    if center:\n        transform.translate(script, [0, 0, -height \/ 2])\n    if color is not None:\n        vert_color.function(script, color=color)\n    return None","method_path":"https:\/\/github.com\/3DLIRIOUS\/MeshLabXML\/blob\/177cce21e92baca500f56a932d66bd9a33257af8\/meshlabxml\/create.py#L688-L758"}
{"repo_name":"3DLIRIOUS\/MeshLabXML","method_name":"color_values","method_code":"def color_values(color):\n    \"\"\"\"\"\"\n    \n    this_dir = os.path.dirname(\n        os.path.realpath(\n            inspect.getsourcefile(\n                lambda: 0)))\n    color_name_file = os.path.join(this_dir, 'color_names.txt')\n    found = False\n    for line in open(color_name_file, 'r'):\n        line = line.rstrip()\n        if color.lower() == line.split()[0]:\n            \n            red = line.split()[2]\n            green = line.split()[3]\n            blue = line.split()[4]\n            found = True\n            break\n    if not found:\n        print('Color name \"%s\" not found, using default (white)' % color)\n        red = 255\n        green = 255\n        blue = 255\n    return red, green, blue","method_summary":"Read color_names.txt and find the red, green, and blue values for a named color.","original_method_code":"def color_values(color):\n    \"\"\"Read color_names.txt and find the red, green, and blue values\n        for a named color.\n    \"\"\"\n    # Get the directory where this script file is located:\n    this_dir = os.path.dirname(\n        os.path.realpath(\n            inspect.getsourcefile(\n                lambda: 0)))\n    color_name_file = os.path.join(this_dir, 'color_names.txt')\n    found = False\n    for line in open(color_name_file, 'r'):\n        line = line.rstrip()\n        if color.lower() == line.split()[0]:\n            #hex_color = line.split()[1]\n            red = line.split()[2]\n            green = line.split()[3]\n            blue = line.split()[4]\n            found = True\n            break\n    if not found:\n        print('Color name \"%s\" not found, using default (white)' % color)\n        red = 255\n        green = 255\n        blue = 255\n    return red, green, blue","method_path":"https:\/\/github.com\/3DLIRIOUS\/MeshLabXML\/blob\/177cce21e92baca500f56a932d66bd9a33257af8\/meshlabxml\/util.py#L41-L66"}
{"repo_name":"3DLIRIOUS\/MeshLabXML","method_name":"check_list","method_code":"def check_list(var, num_terms):\n    \"\"\"\"\"\"\n    if not isinstance(var, list):\n        if isinstance(var, tuple):\n            var = list(var)\n        else:\n            var = [var]\n        for _ in range(1, num_terms):\n            var.append(var[0])\n    if len(var) != num_terms:\n        print(\n            '\"%s\" has the wrong number of terms; it needs %s. Exiting ...' %\n            (var, num_terms))\n        sys.exit(1)\n    return var","method_summary":"Check if a variable is a list and is the correct length. If variable is not a list it will make it a list of the correct length with all terms identical.","original_method_code":"def check_list(var, num_terms):\n    \"\"\" Check if a variable is a list and is the correct length.\n\n    If variable is not a list it will make it a list of the correct length with\n    all terms identical.\n    \"\"\"\n    if not isinstance(var, list):\n        if isinstance(var, tuple):\n            var = list(var)\n        else:\n            var = [var]\n        for _ in range(1, num_terms):\n            var.append(var[0])\n    if len(var) != num_terms:\n        print(\n            '\"%s\" has the wrong number of terms; it needs %s. Exiting ...' %\n            (var, num_terms))\n        sys.exit(1)\n    return var","method_path":"https:\/\/github.com\/3DLIRIOUS\/MeshLabXML\/blob\/177cce21e92baca500f56a932d66bd9a33257af8\/meshlabxml\/util.py#L69-L87"}
{"repo_name":"3DLIRIOUS\/MeshLabXML","method_name":"make_list","method_code":"def make_list(var, num_terms=1):\n    \"\"\"\"\"\"\n    if not isinstance(var, list):\n        if isinstance(var, tuple):\n            var = list(var)\n        else:\n            var = [var]\n    \n            for _ in range(1, num_terms):\n                var.append(var[0])\n    return var","method_summary":"Make a variable a list if it is not already If variable is not a list it will make it a list of the correct length with all terms identical.","original_method_code":"def make_list(var, num_terms=1):\n    \"\"\" Make a variable a list if it is not already\n\n    If variable is not a list it will make it a list of the correct length with\n    all terms identical.\n    \"\"\"\n    if not isinstance(var, list):\n        if isinstance(var, tuple):\n            var = list(var)\n        else:\n            var = [var]\n    #if len(var) == 1:\n            for _ in range(1, num_terms):\n                var.append(var[0])\n    return var","method_path":"https:\/\/github.com\/3DLIRIOUS\/MeshLabXML\/blob\/177cce21e92baca500f56a932d66bd9a33257af8\/meshlabxml\/util.py#L90-L104"}
{"repo_name":"neurosynth\/neurosynth","method_name":"Decoder.decode","method_code":"def decode(self, images, save=None, round=4, names=None, **kwargs):\n        \"\"\"\"\"\"\n\n        if isinstance(images, string_types):\n            images = [images]\n\n        if isinstance(images, list):\n            imgs_to_decode = imageutils.load_imgs(images, self.masker)\n        else:\n            imgs_to_decode = images\n\n        methods = {\n            'pearson': self._pearson_correlation,\n            'dot': self._dot_product,\n            'roi': self._roi_association\n        }\n\n        result = np.around(\n            methods[self.method](imgs_to_decode, **kwargs), round)\n\n        \n\n        if names is None:\n            if type(images).__module__ == np.__name__:\n                names = ['image_%d' % i for i in range(images.shape[1])]\n            elif self.method == 'roi':\n                names = ['cluster_%d' % i for i in range(result.shape[1])]\n            else:\n                names = images\n\n        result = pd.DataFrame(result, columns=names, index=self.feature_names)\n\n        if save is not None:\n            result.to_csv(save, index_label='Feature')\n        return result","method_summary":"Decodes a set of images.","original_method_code":"def decode(self, images, save=None, round=4, names=None, **kwargs):\n        \"\"\" Decodes a set of images.\n\n        Args:\n          images: The images to decode. Can be:\n            - A single String specifying the filename of the image to decode\n            - A list of filenames\n            - A single NumPy array containing the image data\n          save: Optional filename to save results to. If None (default), returns\n            all results as an array.\n          round: Optional integer indicating number of decimals to round result\n            to. Defaults to 4.\n          names: Optional list of names corresponding to the images in filenames.\n            If passed, must be of same length and in same order as filenames.\n            By default, the columns in the output will be named using the image\n            filenames.\n\n        Returns:\n          An n_features x n_files numpy array, where each feature is a row and\n          each image is a column. The meaning of the values depends on the\n          decoding method used. \"\"\"\n\n        if isinstance(images, string_types):\n            images = [images]\n\n        if isinstance(images, list):\n            imgs_to_decode = imageutils.load_imgs(images, self.masker)\n        else:\n            imgs_to_decode = images\n\n        methods = {\n            'pearson': self._pearson_correlation,\n            'dot': self._dot_product,\n            'roi': self._roi_association\n        }\n\n        result = np.around(\n            methods[self.method](imgs_to_decode, **kwargs), round)\n\n        # if save is not None:\n\n        if names is None:\n            if type(images).__module__ == np.__name__:\n                names = ['image_%d' % i for i in range(images.shape[1])]\n            elif self.method == 'roi':\n                names = ['cluster_%d' % i for i in range(result.shape[1])]\n            else:\n                names = images\n\n        result = pd.DataFrame(result, columns=names, index=self.feature_names)\n\n        if save is not None:\n            result.to_csv(save, index_label='Feature')\n        return result","method_path":"https:\/\/github.com\/neurosynth\/neurosynth\/blob\/948ce7edce15d7df693446e76834e0c23bfe8f11\/neurosynth\/analysis\/decode.py#L64-L117"}
{"repo_name":"neurosynth\/neurosynth","method_name":"Decoder.load_features","method_code":"def load_features(self, features, image_type=None, from_array=False,\n                      threshold=0.001):\n        \"\"\"\"\"\"\n        if from_array:\n            if isinstance(features, list):\n                features = features[0]\n            self._load_features_from_array(features)\n        elif path.exists(features[0]):\n            self._load_features_from_images(features)\n        else:\n            self._load_features_from_dataset(\n                features, image_type=image_type, threshold=threshold)","method_summary":"Load features from current Dataset instance or a list of files.","original_method_code":"def load_features(self, features, image_type=None, from_array=False,\n                      threshold=0.001):\n        \"\"\" Load features from current Dataset instance or a list of files.\n        Args:\n            features: List containing paths to, or names of, features to\n                extract. Each element in the list must be a string containing\n                either a path to an image, or the name of a feature (as named\n                in the current Dataset). Mixing of paths and feature names\n                within the list is not allowed.\n            image_type: Optional suffix indicating which kind of image to use\n                for analysis. Only used if features are taken from the Dataset;\n                if features is a list of filenames, image_type is ignored.\n            from_array: If True, the features argument is interpreted as a\n                string pointing to the location of a 2D ndarray on disk\n                containing feature data, where rows are voxels and columns are\n                individual features.\n            threshold: If features are taken from the dataset, this is the\n                threshold passed to the meta-analysis module to generate fresh\n                images.\n\n        \"\"\"\n        if from_array:\n            if isinstance(features, list):\n                features = features[0]\n            self._load_features_from_array(features)\n        elif path.exists(features[0]):\n            self._load_features_from_images(features)\n        else:\n            self._load_features_from_dataset(\n                features, image_type=image_type, threshold=threshold)","method_path":"https:\/\/github.com\/neurosynth\/neurosynth\/blob\/948ce7edce15d7df693446e76834e0c23bfe8f11\/neurosynth\/analysis\/decode.py#L123-L152"}
{"repo_name":"neurosynth\/neurosynth","method_name":"Decoder._load_features_from_array","method_code":"def _load_features_from_array(self, features):\n        \"\"\"\"\"\"\n        self.feature_images = np.load(features)\n        self.feature_names = range(self.feature_images.shape[1])","method_summary":"Load feature data from a 2D ndarray on disk.","original_method_code":"def _load_features_from_array(self, features):\n        \"\"\" Load feature data from a 2D ndarray on disk. \"\"\"\n        self.feature_images = np.load(features)\n        self.feature_names = range(self.feature_images.shape[1])","method_path":"https:\/\/github.com\/neurosynth\/neurosynth\/blob\/948ce7edce15d7df693446e76834e0c23bfe8f11\/neurosynth\/analysis\/decode.py#L154-L157"}
{"repo_name":"neurosynth\/neurosynth","method_name":"Decoder._load_features_from_dataset","method_code":"def _load_features_from_dataset(self, features=None, image_type=None,\n                                    threshold=0.001):\n        \"\"\"\"\"\"\n        self.feature_names = self.dataset.feature_table.feature_names\n        if features is not None:\n            self.feature_names = [f for f in features\n                                  if f in self.feature_names]\n        from neurosynth.analysis import meta\n        self.feature_images = meta.analyze_features(\n            self.dataset, self.feature_names, image_type=image_type,\n            threshold=threshold)\n        \n        if self.masker.layers:\n            in_mask = self.masker.get_mask(in_global_mask=True)\n            self.feature_images = self.feature_images[in_mask, :]","method_summary":"Load feature image data from the current Dataset instance. See load_features() for documentation.","original_method_code":"def _load_features_from_dataset(self, features=None, image_type=None,\n                                    threshold=0.001):\n        \"\"\" Load feature image data from the current Dataset instance. See\n        load_features() for documentation.\n        \"\"\"\n        self.feature_names = self.dataset.feature_table.feature_names\n        if features is not None:\n            self.feature_names = [f for f in features\n                                  if f in self.feature_names]\n        from neurosynth.analysis import meta\n        self.feature_images = meta.analyze_features(\n            self.dataset, self.feature_names, image_type=image_type,\n            threshold=threshold)\n        # Apply a mask if one was originally passed\n        if self.masker.layers:\n            in_mask = self.masker.get_mask(in_global_mask=True)\n            self.feature_images = self.feature_images[in_mask, :]","method_path":"https:\/\/github.com\/neurosynth\/neurosynth\/blob\/948ce7edce15d7df693446e76834e0c23bfe8f11\/neurosynth\/analysis\/decode.py#L159-L175"}
{"repo_name":"neurosynth\/neurosynth","method_name":"Decoder._load_features_from_images","method_code":"def _load_features_from_images(self, images, names=None):\n        \"\"\"\"\"\"\n        if names is not None and len(names) != len(images):\n            raise Exception(\n                \"Lists of feature names and images must be of same length!\")\n        self.feature_names = names if names is not None else images\n        self.feature_images = imageutils.load_imgs(images, self.masker)","method_summary":"Load feature image data from image files.","original_method_code":"def _load_features_from_images(self, images, names=None):\n        \"\"\" Load feature image data from image files.\n\n        Args:\n          images: A list of image filenames.\n          names: An optional list of strings to use as the feature names. Must\n            be in the same order as the images.\n        \"\"\"\n        if names is not None and len(names) != len(images):\n            raise Exception(\n                \"Lists of feature names and images must be of same length!\")\n        self.feature_names = names if names is not None else images\n        self.feature_images = imageutils.load_imgs(images, self.masker)","method_path":"https:\/\/github.com\/neurosynth\/neurosynth\/blob\/948ce7edce15d7df693446e76834e0c23bfe8f11\/neurosynth\/analysis\/decode.py#L177-L189"}
{"repo_name":"neurosynth\/neurosynth","method_name":"Decoder._pearson_correlation","method_code":"def _pearson_correlation(self, imgs_to_decode):\n        \"\"\"\"\"\"\n        x, y = imgs_to_decode.astype(float), self.feature_images.astype(float)\n        return self._xy_corr(x, y)","method_summary":"Decode images using Pearson's r. Computes the correlation between each input image and each feature image across voxels.","original_method_code":"def _pearson_correlation(self, imgs_to_decode):\n        \"\"\" Decode images using Pearson's r.\n\n        Computes the correlation between each input image and each feature\n        image across voxels.\n\n        Args:\n            imgs_to_decode: An ndarray of images to decode, with voxels in rows\n                and images in columns.\n\n        Returns:\n            An n_features x n_images 2D array, with each cell representing the\n            pearson correlation between the i'th feature and the j'th image\n            across all voxels.\n        \"\"\"\n        x, y = imgs_to_decode.astype(float), self.feature_images.astype(float)\n        return self._xy_corr(x, y)","method_path":"https:\/\/github.com\/neurosynth\/neurosynth\/blob\/948ce7edce15d7df693446e76834e0c23bfe8f11\/neurosynth\/analysis\/decode.py#L198-L214"}
{"repo_name":"neurosynth\/neurosynth","method_name":"Decoder._dot_product","method_code":"def _dot_product(self, imgs_to_decode):\n        \"\"\"\"\"\"\n        return np.dot(imgs_to_decode.T, self.feature_images).T","method_summary":"Decoding using the dot product.","original_method_code":"def _dot_product(self, imgs_to_decode):\n        \"\"\" Decoding using the dot product.\n        \"\"\"\n        return np.dot(imgs_to_decode.T, self.feature_images).T","method_path":"https:\/\/github.com\/neurosynth\/neurosynth\/blob\/948ce7edce15d7df693446e76834e0c23bfe8f11\/neurosynth\/analysis\/decode.py#L216-L219"}
{"repo_name":"neurosynth\/neurosynth","method_name":"feature_selection","method_code":"def feature_selection(feat_select, X, y):\n    \"\"\"\"\"\"\n    \n    if re.match('.*-best', feat_select) is not None:\n        n = int(feat_select.split('-')[0])\n\n        selector = SelectKBest(k=n)\n\n        import warnings\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', category=UserWarning)\n            features_selected = np.where(\n                selector.fit(X, y).get_support() is True)[0]\n\n    elif re.match('.*-randombest', feat_select) is not None:\n        n = int(feat_select.split('-')[0])\n\n        from random import shuffle\n        features = range(0, X.shape[1])\n        shuffle(features)\n\n        features_selected = features[:n]\n\n    return features_selected","method_summary":"Implements various kinds of feature selection","original_method_code":"def feature_selection(feat_select, X, y):\n    \"\"\"\" Implements various kinds of feature selection \"\"\"\n    # K-best\n    if re.match('.*-best', feat_select) is not None:\n        n = int(feat_select.split('-')[0])\n\n        selector = SelectKBest(k=n)\n\n        import warnings\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', category=UserWarning)\n            features_selected = np.where(\n                selector.fit(X, y).get_support() is True)[0]\n\n    elif re.match('.*-randombest', feat_select) is not None:\n        n = int(feat_select.split('-')[0])\n\n        from random import shuffle\n        features = range(0, X.shape[1])\n        shuffle(features)\n\n        features_selected = features[:n]\n\n    return features_selected","method_path":"https:\/\/github.com\/neurosynth\/neurosynth\/blob\/948ce7edce15d7df693446e76834e0c23bfe8f11\/neurosynth\/analysis\/classify.py#L10-L33"}
{"repo_name":"neurosynth\/neurosynth","method_name":"classify","method_code":"def classify(X, y, clf_method='ERF', classifier=None, output='summary_clf',\n             cross_val=None, class_weight=None, regularization=None,\n             param_grid=None, scoring='accuracy', refit_all=True,\n             feat_select=None):\n    \"\"\"\"\"\"\n\n    \n    clf = Classifier(clf_method, classifier, param_grid)\n\n    \n    if cross_val is not None:\n        score = clf.cross_val_fit(X, y, cross_val, scoring=scoring,\n                                  feat_select=feat_select,\n                                  class_weight=class_weight)\n    else:\n        \n        score = clf.fit(X, y, class_weight=class_weight).score(X, y)\n\n    \n    from collections import Counter\n\n    if output == 'clf':\n        return clf\n    else:\n        if output == 'summary':\n            output = {'score': score, 'n': dict(Counter(y))}\n        elif output == 'summary_clf':\n            output = {\n                'score': score,\n                'n': dict(Counter(y)),\n                'clf': clf,\n                'features_selected': clf.features_selected,\n                'predictions': clf.predictions\n            }\n\n        return output","method_summary":"Wrapper for scikit-learn classification functions Imlements various types of classification and cross validation","original_method_code":"def classify(X, y, clf_method='ERF', classifier=None, output='summary_clf',\n             cross_val=None, class_weight=None, regularization=None,\n             param_grid=None, scoring='accuracy', refit_all=True,\n             feat_select=None):\n    \"\"\" Wrapper for scikit-learn classification functions\n    Imlements various types of classification and cross validation \"\"\"\n\n    # Build classifier\n    clf = Classifier(clf_method, classifier, param_grid)\n\n    # Fit & test model with or without cross-validation\n    if cross_val is not None:\n        score = clf.cross_val_fit(X, y, cross_val, scoring=scoring,\n                                  feat_select=feat_select,\n                                  class_weight=class_weight)\n    else:\n        # Does not support scoring function\n        score = clf.fit(X, y, class_weight=class_weight).score(X, y)\n\n    # Return some stuff...\n    from collections import Counter\n\n    if output == 'clf':\n        return clf\n    else:\n        if output == 'summary':\n            output = {'score': score, 'n': dict(Counter(y))}\n        elif output == 'summary_clf':\n            output = {\n                'score': score,\n                'n': dict(Counter(y)),\n                'clf': clf,\n                'features_selected': clf.features_selected,\n                'predictions': clf.predictions\n            }\n\n        return output","method_path":"https:\/\/github.com\/neurosynth\/neurosynth\/blob\/948ce7edce15d7df693446e76834e0c23bfe8f11\/neurosynth\/analysis\/classify.py#L212-L248"}
{"repo_name":"neurosynth\/neurosynth","method_name":"Classifier.fit","method_code":"def fit(self, X, y, cv=None, class_weight='auto'):\n        \"\"\"\"\"\"\n\n        \n        \n        \n        \n\n        self.X = X\n        self.y = y\n\n        self.set_class_weight(class_weight=class_weight, y=y)\n\n        self.clf = self.clf.fit(X, y)\n\n        return self.clf","method_summary":"Fits X to outcomes y, using clf","original_method_code":"def fit(self, X, y, cv=None, class_weight='auto'):\n        \"\"\" Fits X to outcomes y, using clf \"\"\"\n\n        # Incorporate error checking such as :\n        # if isinstance(self.classifier, ScikitClassifier):\n        #     do one thingNone\n        # otherwiseNone.\n\n        self.X = X\n        self.y = y\n\n        self.set_class_weight(class_weight=class_weight, y=y)\n\n        self.clf = self.clf.fit(X, y)\n\n        return self.clf","method_path":"https:\/\/github.com\/neurosynth\/neurosynth\/blob\/948ce7edce15d7df693446e76834e0c23bfe8f11\/neurosynth\/analysis\/classify.py#L291-L306"}
{"repo_name":"mental32\/spotify.py","method_name":"HTTPClient.get_bearer_info","method_code":"async def get_bearer_info(self):\n        \"\"\"\"\"\"\n        if self.client_id is None:\n            raise SpotifyException(_GET_BEARER_ERR % 'client_id')\n\n        elif self.client_secret is None:\n            raise SpotifyException(_GET_BEARER_ERR % 'client_secret')\n\n        token = b64encode(':'.join((self.client_id, self.client_secret)).encode())\n\n        kwargs = {\n            'url': 'https:\/\/accounts.spotify.com\/api\/token',\n            'data': {'grant_type': 'client_credentials'},\n            'headers': {'Authorization': 'Basic ' + token.decode()}\n        }\n\n        async with self._session.post(**kwargs) as resp:\n            return json.loads(await resp.text(encoding='utf-8'))","method_summary":"Get the application bearer token from client_id and client_secret.","original_method_code":"async def get_bearer_info(self):\n        \"\"\"Get the application bearer token from client_id and client_secret.\"\"\"\n        if self.client_id is None:\n            raise SpotifyException(_GET_BEARER_ERR % 'client_id')\n\n        elif self.client_secret is None:\n            raise SpotifyException(_GET_BEARER_ERR % 'client_secret')\n\n        token = b64encode(':'.join((self.client_id, self.client_secret)).encode())\n\n        kwargs = {\n            'url': 'https:\/\/accounts.spotify.com\/api\/token',\n            'data': {'grant_type': 'client_credentials'},\n            'headers': {'Authorization': 'Basic ' + token.decode()}\n        }\n\n        async with self._session.post(**kwargs) as resp:\n            return json.loads(await resp.text(encoding='utf-8'))","method_path":"https:\/\/github.com\/mental32\/spotify.py\/blob\/bb296cac7c3dd289908906b7069bd80f43950515\/spotify\/http.py#L83-L100"}
{"repo_name":"mental32\/spotify.py","method_name":"HTTPClient.request","method_code":"async def request(self, route, **kwargs):\n        \"\"\"\"\"\"\n        if isinstance(route, tuple):\n            method, url = route\n        else:\n            method = route.method\n            url = route.url\n\n        if self.bearer_info is None:\n            self.bearer_info = bearer_info = await self.get_bearer_info()\n            access_token = bearer_info['access_token']\n        else:\n            access_token = self.bearer_info['access_token']\n\n        headers = {\n            'Authorization': 'Bearer ' + access_token,\n            'Content-Type': kwargs.get('content_type', 'application\/json'),\n            **kwargs.pop('headers', {})\n        }\n\n        for _ in range(self.RETRY_AMOUNT):\n            r = await self._session.request(method, url, headers=headers, **kwargs)\n            try:\n                status = r.status\n\n                try:\n                    data = json.loads(await r.text(encoding='utf-8'))\n                except json.decoder.JSONDecodeError:\n                    data = {}\n\n                if 300 > status >= 200:\n                    return data\n\n                if status == 401:\n                    self.bearer_info = bearer_info = await self.get_bearer_info()\n                    headers['Authorization'] = 'Bearer ' + bearer_info['access_token']\n                    continue\n\n                if status == 429:\n                    \n                    amount = r.headers.get('Retry-After')\n                    await asyncio.sleep(int(amount), loop=self.loop)\n                    continue\n\n                if status in (502, 503):\n                    \n                    continue\n\n                if status == 403:\n                    raise Forbidden(r, data)\n                elif status == 404:\n                    raise NotFound(r, data)\n            finally:\n                await r.release()\n        else:\n            raise HTTPException(r, data)","method_summary":"Make a request to the spotify API with the current bearer credentials.","original_method_code":"async def request(self, route, **kwargs):\n        \"\"\"Make a request to the spotify API with the current bearer credentials.\n\n        Parameters\n        ----------\n        route : Union[tuple[str, str], Route]\n            A tuple of the method and url or a :class:`Route` object.\n        kwargs : Any\n            keyword arguments to pass into :class:`aiohttp.ClientSession.request`\n        \"\"\"\n        if isinstance(route, tuple):\n            method, url = route\n        else:\n            method = route.method\n            url = route.url\n\n        if self.bearer_info is None:\n            self.bearer_info = bearer_info = await self.get_bearer_info()\n            access_token = bearer_info['access_token']\n        else:\n            access_token = self.bearer_info['access_token']\n\n        headers = {\n            'Authorization': 'Bearer ' + access_token,\n            'Content-Type': kwargs.get('content_type', 'application\/json'),\n            **kwargs.pop('headers', {})\n        }\n\n        for _ in range(self.RETRY_AMOUNT):\n            r = await self._session.request(method, url, headers=headers, **kwargs)\n            try:\n                status = r.status\n\n                try:\n                    data = json.loads(await r.text(encoding='utf-8'))\n                except json.decoder.JSONDecodeError:\n                    data = {}\n\n                if 300 > status >= 200:\n                    return data\n\n                if status == 401:\n                    self.bearer_info = bearer_info = await self.get_bearer_info()\n                    headers['Authorization'] = 'Bearer ' + bearer_info['access_token']\n                    continue\n\n                if status == 429:\n                    # we're being rate limited.\n                    amount = r.headers.get('Retry-After')\n                    await asyncio.sleep(int(amount), loop=self.loop)\n                    continue\n\n                if status in (502, 503):\n                    # unconditional retry\n                    continue\n\n                if status == 403:\n                    raise Forbidden(r, data)\n                elif status == 404:\n                    raise NotFound(r, data)\n            finally:\n                await r.release()\n        else:\n            raise HTTPException(r, data)","method_path":"https:\/\/github.com\/mental32\/spotify.py\/blob\/bb296cac7c3dd289908906b7069bd80f43950515\/spotify\/http.py#L102-L165"}
{"repo_name":"mental32\/spotify.py","method_name":"HTTPClient.album","method_code":"def album(self, spotify_id, market='US'):\n        \"\"\"\"\"\"\n        route = Route('GET', '\/albums\/{spotify_id}', spotify_id=spotify_id)\n        payload = {}\n\n        if market:\n            payload['market'] = market\n\n        return self.request(route, params=payload)","method_summary":"Get a spotify album by its ID.","original_method_code":"def album(self, spotify_id, market='US'):\n        \"\"\"Get a spotify album by its ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The spotify_id to search by.\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.\n\n        Returns\n        -------\n        album : Dict\n            The album object.\n        \"\"\"\n        route = Route('GET', '\/albums\/{spotify_id}', spotify_id=spotify_id)\n        payload = {}\n\n        if market:\n            payload['market'] = market\n\n        return self.request(route, params=payload)","method_path":"https:\/\/github.com\/mental32\/spotify.py\/blob\/bb296cac7c3dd289908906b7069bd80f43950515\/spotify\/http.py#L171-L192"}
{"repo_name":"mental32\/spotify.py","method_name":"HTTPClient.album_tracks","method_code":"def album_tracks(self, spotify_id, limit=20, offset=0, market='US'):\n        \"\"\"\"\"\"\n        route = Route('GET', '\/albums\/{spotify_id}\/tracks', spotify_id=spotify_id)\n        payload = {'limit': limit, 'offset': offset}\n\n        if market:\n            payload['market'] = market\n\n        return self.request(route, params=payload)","method_summary":"Get an albums tracks by an ID.","original_method_code":"def album_tracks(self, spotify_id, limit=20, offset=0, market='US'):\n        \"\"\"Get an albums tracks by an ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The spotify_id to search by.\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optiona[int]\n            The offset of which Spotify should start yielding from.\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.\n        \"\"\"\n        route = Route('GET', '\/albums\/{spotify_id}\/tracks', spotify_id=spotify_id)\n        payload = {'limit': limit, 'offset': offset}\n\n        if market:\n            payload['market'] = market\n\n        return self.request(route, params=payload)","method_path":"https:\/\/github.com\/mental32\/spotify.py\/blob\/bb296cac7c3dd289908906b7069bd80f43950515\/spotify\/http.py#L194-L214"}
{"repo_name":"mental32\/spotify.py","method_name":"HTTPClient.albums","method_code":"def albums(self, spotify_ids, market='US'):\n        \"\"\"\"\"\"\n        route = Route('GET', '\/albums\/')\n        payload = {'ids': spotify_ids}\n\n        if market:\n            payload['market'] = market\n\n        return self.request(route, params=payload)","method_summary":"Get a spotify album by its ID.","original_method_code":"def albums(self, spotify_ids, market='US'):\n        \"\"\"Get a spotify album by its ID.\n\n        Parameters\n        ----------\n        spotify_ids : List[str]\n            The spotify_ids to search by.\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.\n        \"\"\"\n        route = Route('GET', '\/albums\/')\n        payload = {'ids': spotify_ids}\n\n        if market:\n            payload['market'] = market\n\n        return self.request(route, params=payload)","method_path":"https:\/\/github.com\/mental32\/spotify.py\/blob\/bb296cac7c3dd289908906b7069bd80f43950515\/spotify\/http.py#L216-L232"}
{"repo_name":"mental32\/spotify.py","method_name":"HTTPClient.artist","method_code":"def artist(self, spotify_id):\n        \"\"\"\"\"\"\n        route = Route('GET', '\/artists\/{spotify_id}', spotify_id=spotify_id)\n        return self.request(route)","method_summary":"Get a spotify artist by their ID.","original_method_code":"def artist(self, spotify_id):\n        \"\"\"Get a spotify artist by their ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The spotify_id to search by.\n        \"\"\"\n        route = Route('GET', '\/artists\/{spotify_id}', spotify_id=spotify_id)\n        return self.request(route)","method_path":"https:\/\/github.com\/mental32\/spotify.py\/blob\/bb296cac7c3dd289908906b7069bd80f43950515\/spotify\/http.py#L234-L243"}
{"repo_name":"mental32\/spotify.py","method_name":"HTTPClient.artist_albums","method_code":"def artist_albums(self, spotify_id, include_groups=None, limit=20, offset=0, market='US'):\n        \"\"\"\"\"\"\n        route = Route('GET', '\/artists\/{spotify_id}\/albums', spotify_id=spotify_id)\n        payload = {'limit': limit, 'offset': offset}\n\n        if include_groups:\n            payload['include_groups'] = include_groups\n\n        if market:\n            payload['market'] = market\n\n        return self.request(route, params=payload)","method_summary":"Get an artists tracks by their ID.","original_method_code":"def artist_albums(self, spotify_id, include_groups=None, limit=20, offset=0, market='US'):\n        \"\"\"Get an artists tracks by their ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The spotify_id to search by.\n        include_groups : INCLUDE_GROUPS_TP\n            INCLUDE_GROUPS\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optiona[int]\n            The offset of which Spotify should start yielding from.\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.\n        \"\"\"\n        route = Route('GET', '\/artists\/{spotify_id}\/albums', spotify_id=spotify_id)\n        payload = {'limit': limit, 'offset': offset}\n\n        if include_groups:\n            payload['include_groups'] = include_groups\n\n        if market:\n            payload['market'] = market\n\n        return self.request(route, params=payload)","method_path":"https:\/\/github.com\/mental32\/spotify.py\/blob\/bb296cac7c3dd289908906b7069bd80f43950515\/spotify\/http.py#L245-L270"}
{"repo_name":"mental32\/spotify.py","method_name":"HTTPClient.artist_top_tracks","method_code":"def artist_top_tracks(self, spotify_id, country):\n        \"\"\"\"\"\"\n        route = Route('GET', '\/artists\/{spotify_id}\/top-tracks', spotify_id=spotify_id)\n        payload = {'country': country}\n        return self.request(route, params=payload)","method_summary":"Get an artists top tracks per country with their ID.","original_method_code":"def artist_top_tracks(self, spotify_id, country):\n        \"\"\"Get an artists top tracks per country with their ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The spotify_id to search by.\n        country : COUNTRY_TP\n            COUNTRY\n        \"\"\"\n        route = Route('GET', '\/artists\/{spotify_id}\/top-tracks', spotify_id=spotify_id)\n        payload = {'country': country}\n        return self.request(route, params=payload)","method_path":"https:\/\/github.com\/mental32\/spotify.py\/blob\/bb296cac7c3dd289908906b7069bd80f43950515\/spotify\/http.py#L272-L284"}
{"repo_name":"mental32\/spotify.py","method_name":"HTTPClient.artist_related_artists","method_code":"def artist_related_artists(self, spotify_id):\n        \"\"\"\"\"\"\n        route = Route('GET', '\/artists\/{spotify_id}\/related-artists', spotify_id=spotify_id)\n        return self.request(route)","method_summary":"Get related artists for an artist by their ID.","original_method_code":"def artist_related_artists(self, spotify_id):\n        \"\"\"Get related artists for an artist by their ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The spotify_id to search by.\n        \"\"\"\n        route = Route('GET', '\/artists\/{spotify_id}\/related-artists', spotify_id=spotify_id)\n        return self.request(route)","method_path":"https:\/\/github.com\/mental32\/spotify.py\/blob\/bb296cac7c3dd289908906b7069bd80f43950515\/spotify\/http.py#L286-L295"}
{"repo_name":"mental32\/spotify.py","method_name":"HTTPClient.artists","method_code":"def artists(self, spotify_ids):\n        \"\"\"\"\"\"\n        route = Route('GET', '\/artists')\n        payload = {'ids': spotify_ids}\n        return self.request(route, params=payload)","method_summary":"Get a spotify artists by their IDs.","original_method_code":"def artists(self, spotify_ids):\n        \"\"\"Get a spotify artists by their IDs.\n\n        Parameters\n        ----------\n        spotify_id : List[str]\n            The spotify_ids to search with.\n        \"\"\"\n        route = Route('GET', '\/artists')\n        payload = {'ids': spotify_ids}\n        return self.request(route, params=payload)","method_path":"https:\/\/github.com\/mental32\/spotify.py\/blob\/bb296cac7c3dd289908906b7069bd80f43950515\/spotify\/http.py#L297-L307"}
{"repo_name":"funilrys\/PyFunceble","method_name":"ExpirationDate.get","method_code":"def get(self):  \n        \"\"\"\"\"\"\n\n        \n        domain_validation = self.checker.is_domain_valid()\n        \n        ip_validation = self.checker.is_ip_valid()\n\n        if \"current_test_data\" in PyFunceble.INTERN:\n            \n\n            \n            PyFunceble.INTERN[\"current_test_data\"].update(\n                {\n                    \"domain_syntax_validation\": domain_validation,\n                    \"ip4_syntax_validation\": ip_validation,\n                }\n            )\n\n        if (\n            domain_validation\n            and not ip_validation\n            or domain_validation\n            or PyFunceble.CONFIGURATION[\"local\"]\n        ):\n            \n            \n            \n            \n            \n\n            \n            \n            \n            PyFunceble.INTERN.update(\n                {\"http_code\": HTTPCode().get(), \"referer\": Referer().get()}\n            )\n\n            if not PyFunceble.INTERN[\"referer\"]:\n                \n\n                \n                return PyFunceble.INTERN[\"referer\"]\n\n            \n\n            if PyFunceble.INTERN[\"referer\"] and not self.checker.is_subdomain():\n                \n                \n                \n\n                \n                \n                return self._extract()\n\n            \n\n            \n            Logs().whois(self.whois_record)\n\n            \n            return None\n\n        if (\n            ip_validation\n            and not domain_validation\n            or ip_validation\n            or PyFunceble.CONFIGURATION[\"local\"]\n        ):\n            \n            \n            \n            \n            \n\n            \n            PyFunceble.INTERN[\"http_code\"] = HTTPCode().get()\n\n            \n            Logs().whois(self.whois_record)\n\n            \n            return None\n\n        \n\n        \n        Logs().whois(self.whois_record)\n\n        \n        return False","method_summary":"Execute the logic behind the meaning of ExpirationDate + return the matched status.","original_method_code":"def get(self):  # pragma: no cover\n        \"\"\"\n        Execute the logic behind the meaning of ExpirationDate + return the matched status.\n\n        :return:\n            The status of the tested domain.\n            Can be one of the official status.\n        :rtype: str\n        \"\"\"\n\n        # We get the status of the domain validation.\n        domain_validation = self.checker.is_domain_valid()\n        # We get the status of the IPv4 validation.\n        ip_validation = self.checker.is_ip_valid()\n\n        if \"current_test_data\" in PyFunceble.INTERN:\n            # The end-user want more information whith his test.\n\n            # We update some index.\n            PyFunceble.INTERN[\"current_test_data\"].update(\n                {\n                    \"domain_syntax_validation\": domain_validation,\n                    \"ip4_syntax_validation\": ip_validation,\n                }\n            )\n\n        if (\n            domain_validation\n            and not ip_validation\n            or domain_validation\n            or PyFunceble.CONFIGURATION[\"local\"]\n        ):\n            # * The element is a valid domain.\n            # and\n            # * The element is not ahe valid IPv4.\n            # or\n            # * The element is a valid domain.\n\n            # * We get the HTTP status code of the currently tested element.\n            # and\n            # * We try to get the element status from the IANA database.\n            PyFunceble.INTERN.update(\n                {\"http_code\": HTTPCode().get(), \"referer\": Referer().get()}\n            )\n\n            if not PyFunceble.INTERN[\"referer\"]:\n                # We could not get the referer.\n\n                # We parse the referer status into the upstream call.\n                return PyFunceble.INTERN[\"referer\"]\n\n            # The WHOIS record status is not into our list of official status.\n\n            if PyFunceble.INTERN[\"referer\"] and not self.checker.is_subdomain():\n                # * The iana database comparison status is not None.\n                # and\n                # * The domain we are testing is not a subdomain.\n\n                # We try to extract the expiration date from the WHOIS record.\n                # And we return the matched status.\n                return self._extract()\n\n            # The iana database comparison status is None.\n\n            # We log our whois record if the debug mode is activated.\n            Logs().whois(self.whois_record)\n\n            # And we return None, we could not extract the expiration date.\n            return None\n\n        if (\n            ip_validation\n            and not domain_validation\n            or ip_validation\n            or PyFunceble.CONFIGURATION[\"local\"]\n        ):\n            # * The element is a valid IPv4.\n            # and\n            # * The element is not a valid domain.\n            # or\n            # * The element is a valid IPv4.\n\n            # We get the HTTP status code.\n            PyFunceble.INTERN[\"http_code\"] = HTTPCode().get()\n\n            # We log our whois record if the debug mode is activated.\n            Logs().whois(self.whois_record)\n\n            # And we return None, there is no expiration date to look for.\n            return None\n\n        # The validation was not passed.\n\n        # We log our whois record if the debug mode is activated.\n        Logs().whois(self.whois_record)\n\n        # And we return False, the domain could not pass the IP and domains syntax validation.\n        return False","method_path":"https:\/\/github.com\/funilrys\/PyFunceble\/blob\/cdf69cbde120199171f7158e1c33635753e6e2f5\/PyFunceble\/expiration_date.py#L94-L191"}
{"repo_name":"funilrys\/PyFunceble","method_name":"ExpirationDate._convert_or_shorten_month","method_code":"def _convert_or_shorten_month(cls, data):\n        \"\"\"\"\"\"\n\n        \n        short_month = {\n            \"jan\": [str(1), \"01\", \"Jan\", \"January\"],\n            \"feb\": [str(2), \"02\", \"Feb\", \"February\"],\n            \"mar\": [str(3), \"03\", \"Mar\", \"March\"],\n            \"apr\": [str(4), \"04\", \"Apr\", \"April\"],\n            \"may\": [str(5), \"05\", \"May\"],\n            \"jun\": [str(6), \"06\", \"Jun\", \"June\"],\n            \"jul\": [str(7), \"07\", \"Jul\", \"July\"],\n            \"aug\": [str(8), \"08\", \"Aug\", \"August\"],\n            \"sep\": [str(9), \"09\", \"Sep\", \"September\"],\n            \"oct\": [str(10), \"Oct\", \"October\"],\n            \"nov\": [str(11), \"Nov\", \"November\"],\n            \"dec\": [str(12), \"Dec\", \"December\"],\n        }\n\n        for month in short_month:\n            \n\n            if data in short_month[month]:\n                \n\n                \n                \n                return month\n\n        \n\n        \n        return data","method_summary":"Convert a given month into our unified format.","original_method_code":"def _convert_or_shorten_month(cls, data):\n        \"\"\"\n        Convert a given month into our unified format.\n\n        :param data: The month to convert or shorten.\n        :type data: str\n\n        :return: The unified month name.\n        :rtype: str\n        \"\"\"\n\n        # We map the different month and their possible representation.\n        short_month = {\n            \"jan\": [str(1), \"01\", \"Jan\", \"January\"],\n            \"feb\": [str(2), \"02\", \"Feb\", \"February\"],\n            \"mar\": [str(3), \"03\", \"Mar\", \"March\"],\n            \"apr\": [str(4), \"04\", \"Apr\", \"April\"],\n            \"may\": [str(5), \"05\", \"May\"],\n            \"jun\": [str(6), \"06\", \"Jun\", \"June\"],\n            \"jul\": [str(7), \"07\", \"Jul\", \"July\"],\n            \"aug\": [str(8), \"08\", \"Aug\", \"August\"],\n            \"sep\": [str(9), \"09\", \"Sep\", \"September\"],\n            \"oct\": [str(10), \"Oct\", \"October\"],\n            \"nov\": [str(11), \"Nov\", \"November\"],\n            \"dec\": [str(12), \"Dec\", \"December\"],\n        }\n\n        for month in short_month:\n            # We loop through our map.\n\n            if data in short_month[month]:\n                # If the parsed data (or month if you prefer) is into our map.\n\n                # We return the element (or key if you prefer) assigned to\n                # the month.\n                return month\n\n        # The element is not into our map.\n\n        # We return the parsed element (or month if you prefer).\n        return data","method_path":"https:\/\/github.com\/funilrys\/PyFunceble\/blob\/cdf69cbde120199171f7158e1c33635753e6e2f5\/PyFunceble\/expiration_date.py#L208-L248"}
{"repo_name":"funilrys\/PyFunceble","method_name":"ExpirationDate._cases_management","method_code":"def _cases_management(self, regex_number, matched_result):\n        \"\"\"\"\"\"\n\n        \n        \n        cases = {\n            \"first\": [[1, 2, 3, 10, 11, 22, 26, 27, 28, 29, 32, 34, 38], [0, 1, 2]],\n            \"second\": [[14, 15, 31, 33, 36, 37], [1, 0, 2]],\n            \"third\": [\n                [4, 5, 6, 7, 8, 9, 12, 13, 16, 17, 18, 19, 20, 21, 23, 24, 25, 30, 35],\n                [2, 1, 0],\n            ],\n        }\n\n        for case in cases:\n            \n\n            \n            case_data = cases[case]\n\n            if int(regex_number) in case_data[0]:\n                \n\n                \n                \n                \n                \n                return [\n                    self._convert_1_to_2_digits(matched_result[case_data[1][0]]),\n                    self._convert_or_shorten_month(matched_result[case_data[1][1]]),\n                    str(matched_result[case_data[1][2]]),\n                ]\n\n        \n\n        \n        return matched_result","method_summary":"A little internal helper of self.format. (Avoiding of nested loops)","original_method_code":"def _cases_management(self, regex_number, matched_result):\n        \"\"\"\n        A little internal helper of self.format. (Avoiding of nested loops)\n\n        .. note::\n            Please note that the second value of the case represent the groups\n            in order :code:`[day,month,year]`.\n\n            This means that a :code:`[2,1,0]` will be for example for a date\n            in format :code:`2017-01-02` where\n            :code:`01` is the month.\n\n        :param regex_number: The identifiant of the regex.\n        :type regex_number: int\n\n        :param matched_result: The matched result to format.\n        :type matched_result: list\n\n        :return:\n            A list representing the expiration date.\n            The list can be \"decoded\" like :code:`[day, month, year]`\n        :rtype: list|None\n        \"\"\"\n\n        # We map our regex numbers with with the right group order.\n        # Note: please report to the method note for more information about the mapping.\n        cases = {\n            \"first\": [[1, 2, 3, 10, 11, 22, 26, 27, 28, 29, 32, 34, 38], [0, 1, 2]],\n            \"second\": [[14, 15, 31, 33, 36, 37], [1, 0, 2]],\n            \"third\": [\n                [4, 5, 6, 7, 8, 9, 12, 13, 16, 17, 18, 19, 20, 21, 23, 24, 25, 30, 35],\n                [2, 1, 0],\n            ],\n        }\n\n        for case in cases:\n            # We loop through the cases.\n\n            # We get the case data.\n            case_data = cases[case]\n\n            if int(regex_number) in case_data[0]:\n                # The regex number is into the currently read case data.\n\n                # We return a list with the formatted elements.\n                # 1. We convert the day to 2 digits.\n                # 2. We convert the month to the unified format.\n                # 3. We return the year.\n                return [\n                    self._convert_1_to_2_digits(matched_result[case_data[1][0]]),\n                    self._convert_or_shorten_month(matched_result[case_data[1][1]]),\n                    str(matched_result[case_data[1][2]]),\n                ]\n\n        # The regex number is not already mapped.\n\n        # We return the parsed data.\n        return matched_result","method_path":"https:\/\/github.com\/funilrys\/PyFunceble\/blob\/cdf69cbde120199171f7158e1c33635753e6e2f5\/PyFunceble\/expiration_date.py#L250-L307"}
{"repo_name":"funilrys\/PyFunceble","method_name":"ExpirationDate._format","method_code":"def _format(self, date_to_convert=None):\n        \"\"\"\"\"\"\n\n        if not date_to_convert:  \n            \n\n            \n            date_to_convert = self.expiration_date\n\n        \n        \n        \n        regex_dates = {\n            \n            \"1\": r\"([0-9]{2})-([a-z]{3})-([0-9]{4})\",\n            \n            \"2\": r\"([0-9]{2})\\.([0-9]{2})\\.([0-9]{4})$\",\n            \n            \"3\": r\"([0-3][0-9])\\\/(0[1-9]|1[012])\\\/([0-9]{4})\",\n            \n            \"4\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})$\",\n            \n            \"5\": r\"([0-9]{4})\\.([0-9]{2})\\.([0-9]{2})$\",\n            \n            \"6\": r\"([0-9]{4})\\\/([0-9]{2})\\\/([0-9]{2})$\",\n            \n            \"7\": r\"([0-9]{4})\\.([0-9]{2})\\.([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\",\n            \n            \"8\": r\"([0-9]{4})([0-9]{2})([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\",\n            \n            \"9\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\",\n            \n            \"10\": r\"([0-9]{2})\\.([0-9]{2})\\.([0-9]{4})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\",\n            \n            \"11\": r\"([0-9]{2})-([A-Z]{1}[a-z]{2})-([0-9]{4})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\\s[A-Z]{1}.*\",  \n            \n            \"12\": r\"([0-9]{4})\\\/([0-9]{2})\\\/([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\\s\\(.*\\)\",\n            \n            \"13\": r\"([0-9]{4})\\\/([0-9]{2})\\\/([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}$\",\n            \n            \"14\": r\"[a-zA-Z]{3}\\s([a-zA-Z]{3})\\s([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\\s[A-Z]{3}\\s([0-9]{4})\",  \n            \n            \"15\": r\"[a-zA-Z]{3}\\s([a-zA-Z]{3})\\s([0-9]{2})\\s([0-9]{4})\",\n            \n            \"16\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}$\",\n            \n            \"17\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}[A-Z].*\",\n            \n            \"18\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}[+-][0-9]{4}\",\n            \n            \n            \"19\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}\\.[0-9].*[+-][0-9]{2}:[0-9]{2}\",  \n            \n            \"20\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}\\.[0-9]{6}$\",\n            \n            \"21\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}\\.[0-9].*[A-Z]\",\n            \n            \"22\": r\"([0-9]{2})-([0-9]{2})-([0-9]{4})\",\n            \n            \"23\": r\"([0-9]{4})\\.\\s([0-9]{2})\\.\\s([0-9]{2})\\.\",\n            \n            \"24\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}[+-][0-9]{2}:[0-9]{2}\",  \n            \n            \"25\": r\"(?=[0-9]{8})(?=([0-9]{4})([0-9]{2})([0-9]{2}))\",\n            \n            \"26\": r\"([0-9]{2})-([A-Z]{1}[a-z]{2})-([0-9]{4})$\",\n            \n            \"27\": r\"([0-9]{2})\\.([0-9]{1})\\.([0-9]{4})\",\n            \n            \"28\": r\"([0-9]{1,2})\\s([A-Z]{1}[a-z]{2})\\s([0-9]{4})\",\n            \n            \"29\": r\"([0-9]{2})-([A-Z]{1}[a-z]*)-([0-9]{4})\",\n            \n            \"30\": r\"([0-9]{4})-([A-Z]{1}[a-z]{2})-([0-9]{2})\\.\",\n            \n            \"31\": r\"[a-zA-Z]{3}\\s([a-zA-Z]{3})\\s([0-9]{1,2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\\s([0-9]{4})\",  \n            \n            \"32\": r\"()[a-zA-Z]{3}\\s([a-zA-Z]{3})\\s([0-9]{4})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\",\n            \n            \"33\": r\"([A-Z]{1}[a-z]*)\\s([0-9]{1,2})\\s([0-9]{4})\",\n            \n            \"34\": r\"([0-9]{1,2})\\.([0-9]{1,2})\\.([0-9]{4})\",\n            \n            \"35\": r\"([0-9]{4})([0-9]{2})([0-9]{2})[0-9]+\",\n            \n            \"36\": r\"(0[1-9]|1[012])\\\/([0-3][0-9])\\\/([0-9]{4})\",\n            \n            \"37\": r\"([A-Z]{1}[a-z].*)\\s\\s([0-9]{1,2})\\s([0-9]{4})\",\n            \n            \"38\": r\"([0-9]{1,})[a-z]{1,}\\s([A-Z].*)\\s(2[0-9]{3})\",\n        }\n\n        for regx in regex_dates:\n            \n\n            \n            \n            matched_result = Regex(\n                date_to_convert, regex_dates[regx], return_data=True, rematch=True\n            ).match()\n\n            if matched_result:\n                \n\n                \n                date = self._cases_management(regx, matched_result)\n\n                if date:\n                    \n\n                    \n                    return \"-\".join(date)\n\n        \n        return \"\"","method_summary":"Format the expiration date into an unified format (01-jan-1970).","original_method_code":"def _format(self, date_to_convert=None):\n        \"\"\"\n        Format the expiration date into an unified format (01-jan-1970).\n\n        :param date_to_convert:\n            The date to convert. In other words, the extracted date.\n        :type date_to_convert: str\n\n        :return: The formatted expiration date.\n        :rtype: str\n        \"\"\"\n\n        if not date_to_convert:  # pragma: no cover\n            # The date to conver is given.\n\n            # We initiate the date we are working with.\n            date_to_convert = self.expiration_date\n\n        # We map the different possible regex.\n        # The regex index represent a unique number which have to be reported\n        # to the self._case_management() method.\n        regex_dates = {\n            # Date in format: 02-jan-2017\n            \"1\": r\"([0-9]{2})-([a-z]{3})-([0-9]{4})\",\n            # Date in format: 02.01.2017 \/\/ Month: jan\n            \"2\": r\"([0-9]{2})\\.([0-9]{2})\\.([0-9]{4})$\",\n            # Date in format: 02\/01\/2017 \/\/ Month: jan\n            \"3\": r\"([0-3][0-9])\\\/(0[1-9]|1[012])\\\/([0-9]{4})\",\n            # Date in format: 2017-01-02 \/\/ Month: jan\n            \"4\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})$\",\n            # Date in format: 2017.01.02 \/\/ Month: jan\n            \"5\": r\"([0-9]{4})\\.([0-9]{2})\\.([0-9]{2})$\",\n            # Date in format: 2017\/01\/02 \/\/ Month: jan\n            \"6\": r\"([0-9]{4})\\\/([0-9]{2})\\\/([0-9]{2})$\",\n            # Date in format: 2017.01.02 15:00:00\n            \"7\": r\"([0-9]{4})\\.([0-9]{2})\\.([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\",\n            # Date in format: 20170102 15:00:00 \/\/ Month: jan\n            \"8\": r\"([0-9]{4})([0-9]{2})([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\",\n            # Date in format: 2017-01-02 15:00:00 \/\/ Month: jan\n            \"9\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\",\n            # Date in format: 02.01.2017 15:00:00 \/\/ Month: jan\n            \"10\": r\"([0-9]{2})\\.([0-9]{2})\\.([0-9]{4})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\",\n            # Date in format: 02-Jan-2017 15:00:00 UTC\n            \"11\": r\"([0-9]{2})-([A-Z]{1}[a-z]{2})-([0-9]{4})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\\s[A-Z]{1}.*\",  # pylint: disable=line-too-long\n            # Date in format: 2017\/01\/02 01:00:00 (+0900) \/\/ Month: jan\n            \"12\": r\"([0-9]{4})\\\/([0-9]{2})\\\/([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\\s\\(.*\\)\",\n            # Date in format: 2017\/01\/02 01:00:00 \/\/ Month: jan\n            \"13\": r\"([0-9]{4})\\\/([0-9]{2})\\\/([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}$\",\n            # Date in format: Mon Jan 02 15:00:00 GMT 2017\n            \"14\": r\"[a-zA-Z]{3}\\s([a-zA-Z]{3})\\s([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\\s[A-Z]{3}\\s([0-9]{4})\",  # pylint: disable=line-too-long\n            # Date in format: Mon Jan 02 2017\n            \"15\": r\"[a-zA-Z]{3}\\s([a-zA-Z]{3})\\s([0-9]{2})\\s([0-9]{4})\",\n            # Date in format: 2017-01-02T15:00:00 \/\/ Month: jan\n            \"16\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}$\",\n            # Date in format: 2017-01-02T15:00:00Z \/\/ Month: jan${'7}\n            \"17\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}[A-Z].*\",\n            # Date in format: 2017-01-02T15:00:00+0200 \/\/ Month: jan\n            \"18\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}[+-][0-9]{4}\",\n            # Date in format: 2017-01-02T15:00:00+0200.622265+03:00 \/\/\n            # Month: jan\n            \"19\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}\\.[0-9].*[+-][0-9]{2}:[0-9]{2}\",  # pylint: disable=line-too-long\n            # Date in format: 2017-01-02T15:00:00+0200.622265 \/\/ Month: jan\n            \"20\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}\\.[0-9]{6}$\",\n            # Date in format: 2017-01-02T23:59:59.0Z \/\/ Month: jan\n            \"21\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}\\.[0-9].*[A-Z]\",\n            # Date in format: 02-01-2017 \/\/ Month: jan\n            \"22\": r\"([0-9]{2})-([0-9]{2})-([0-9]{4})\",\n            # Date in format: 2017. 01. 02. \/\/ Month: jan\n            \"23\": r\"([0-9]{4})\\.\\s([0-9]{2})\\.\\s([0-9]{2})\\.\",\n            # Date in format: 2017-01-02T00:00:00+13:00 \/\/ Month: jan\n            \"24\": r\"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}[+-][0-9]{2}:[0-9]{2}\",  # pylint: disable=line-too-long\n            # Date in format: 20170102 \/\/ Month: jan\n            \"25\": r\"(?=[0-9]{8})(?=([0-9]{4})([0-9]{2})([0-9]{2}))\",\n            # Date in format: 02-Jan-2017\n            \"26\": r\"([0-9]{2})-([A-Z]{1}[a-z]{2})-([0-9]{4})$\",\n            # Date in format: 02.1.2017 \/\/ Month: jan\n            \"27\": r\"([0-9]{2})\\.([0-9]{1})\\.([0-9]{4})\",\n            # Date in format: 02 Jan 2017\n            \"28\": r\"([0-9]{1,2})\\s([A-Z]{1}[a-z]{2})\\s([0-9]{4})\",\n            # Date in format: 02-January-2017\n            \"29\": r\"([0-9]{2})-([A-Z]{1}[a-z]*)-([0-9]{4})\",\n            # Date in format: 2017-Jan-02.\n            \"30\": r\"([0-9]{4})-([A-Z]{1}[a-z]{2})-([0-9]{2})\\.\",\n            # Date in format: Mon Jan 02 15:00:00 2017\n            \"31\": r\"[a-zA-Z]{3}\\s([a-zA-Z]{3})\\s([0-9]{1,2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\\s([0-9]{4})\",  # pylint: disable=line-too-long\n            # Date in format: Mon Jan 2017 15:00:00\n            \"32\": r\"()[a-zA-Z]{3}\\s([a-zA-Z]{3})\\s([0-9]{4})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\",\n            # Date in format: January 02 2017-Jan-02\n            \"33\": r\"([A-Z]{1}[a-z]*)\\s([0-9]{1,2})\\s([0-9]{4})\",\n            # Date in format: 2.1.2017 \/\/ Month: jan\n            \"34\": r\"([0-9]{1,2})\\.([0-9]{1,2})\\.([0-9]{4})\",\n            # Date in format: 20170102000000 \/\/ Month: jan\n            \"35\": r\"([0-9]{4})([0-9]{2})([0-9]{2})[0-9]+\",\n            # Date in format: 01\/02\/2017 \/\/ Month: jan\n            \"36\": r\"(0[1-9]|1[012])\\\/([0-3][0-9])\\\/([0-9]{4})\",\n            # Date in format: January  2 2017\n            \"37\": r\"([A-Z]{1}[a-z].*)\\s\\s([0-9]{1,2})\\s([0-9]{4})\",\n            # Date in format: 2nd January 2017\n            \"38\": r\"([0-9]{1,})[a-z]{1,}\\s([A-Z].*)\\s(2[0-9]{3})\",\n        }\n\n        for regx in regex_dates:\n            # We loop through our map.\n\n            # We try to get the matched groups if the date to convert match the currently\n            # read regex.\n            matched_result = Regex(\n                date_to_convert, regex_dates[regx], return_data=True, rematch=True\n            ).match()\n\n            if matched_result:\n                # The matched result is not None or an empty list.\n\n                # We get the date.\n                date = self._cases_management(regx, matched_result)\n\n                if date:\n                    # The date is given.\n\n                    # We return the formatted date.\n                    return \"-\".join(date)\n\n        # We return an empty string as we were not eable to match the date format.\n        return \"\"","method_path":"https:\/\/github.com\/funilrys\/PyFunceble\/blob\/cdf69cbde120199171f7158e1c33635753e6e2f5\/PyFunceble\/expiration_date.py#L309-L432"}
{"repo_name":"funilrys\/PyFunceble","method_name":"ExpirationDate._extract","method_code":"def _extract(self):  \n        \"\"\"\"\"\"\n\n        \n        expiration_date_from_database = Whois().get_expiration_date()\n\n        if expiration_date_from_database:\n            \n            \n            \n\n            \n            \n            Generate(\n                PyFunceble.STATUS[\"official\"][\"up\"],\n                \"WHOIS\",\n                expiration_date_from_database,\n            ).status_file()\n\n            \n            return PyFunceble.STATUS[\"official\"][\"up\"]\n\n        \n        self.whois_record = Lookup().whois(PyFunceble.INTERN[\"referer\"])\n\n        \n        to_match = [\n            r\"expire:(.*)\",\n            r\"expire on:(.*)\",\n            r\"Expiry Date:(.*)\",\n            r\"free-date(.*)\",\n            r\"expires:(.*)\",\n            r\"Expiration date:(.*)\",\n            r\"Expiry date:(.*)\",\n            r\"Expire Date:(.*)\",\n            r\"renewal date:(.*)\",\n            r\"Expires:(.*)\",\n            r\"validity:(.*)\",\n            r\"Expiration Date             :(.*)\",\n            r\"Expiry :(.*)\",\n            r\"expires at:(.*)\",\n            r\"domain_datebilleduntil:(.*)\",\n            r\"Data de expira\u00e7\u00e3o \\\/ Expiration Date \\(dd\\\/mm\\\/yyyy\\):(.*)\",\n            r\"Fecha de expiraci\u00f3n \\(Expiration date\\):(.*)\",\n            r\"\\[Expires on\\](.*)\",\n            r\"Record expires on(.*)(\\(YYYY-MM-DD\\))\",\n            r\"status:      OK-UNTIL(.*)\",\n            r\"renewal:(.*)\",\n            r\"expires............:(.*)\",\n            r\"expire-date:(.*)\",\n            r\"Exp date:(.*)\",\n            r\"Valid-date(.*)\",\n            r\"Expires On:(.*)\",\n            r\"Fecha de vencimiento:(.*)\",\n            r\"Expiration:.........(.*)\",\n            r\"Fecha de Vencimiento:(.*)\",\n            r\"Registry Expiry Date:(.*)\",\n            r\"Expires on..............:(.*)\",\n            r\"Expiration Time:(.*)\",\n            r\"Expiration Date:(.*)\",\n            r\"Expired:(.*)\",\n            r\"Date d'expiration:(.*)\",\n            r\"expiration date:(.*)\",\n        ]\n\n        if self.whois_record:\n            \n\n            if \"current_test_data\" in PyFunceble.INTERN:\n                \n\n                \n                PyFunceble.INTERN[\"current_test_data\"][\n                    \"whois_record\"\n                ] = self.whois_record\n\n            for string in to_match:\n                \n\n                \n                expiration_date = Regex(\n                    self.whois_record, string, return_data=True, rematch=True, group=0\n                ).match()\n\n                if expiration_date:\n                    \n\n                    \n                    self.expiration_date = expiration_date[0].strip()\n\n                    \n                    \n                    regex_rumbers = r\"[0-9]\"\n\n                    if Regex(\n                        self.expiration_date, regex_rumbers, return_data=False\n                    ).match():\n                        \n\n                        \n                        self.expiration_date = self._format()\n\n                        if (\n                            self.expiration_date\n                            and not Regex(\n                                self.expiration_date,\n                                r\"[0-9]{2}\\-[a-z]{3}\\-2[0-9]{3}\",\n                                return_data=False,\n                            ).match()\n                        ):\n                            \n\n                            \n                            Logs().expiration_date(self.expiration_date)\n\n                            \n                            Logs().whois(self.whois_record)\n\n                        if \"current_test_data\" in PyFunceble.INTERN:\n                            \n\n                            \n                            PyFunceble.INTERN[\"current_test_data\"][\n                                \"expiration_date\"\n                            ] = self.expiration_date\n\n                        \n                        \n                        Generate(\n                            PyFunceble.STATUS[\"official\"][\"up\"],\n                            \"WHOIS\",\n                            self.expiration_date,\n                        ).status_file()\n\n                        \n                        Logs().whois(self.whois_record)\n\n                        \n                        Whois(expiration_date=self.expiration_date).add()\n\n                        \n                        return PyFunceble.STATUS[\"official\"][\"up\"]\n\n                    \n\n                    \n                    Logs().whois(self.whois_record)\n\n                    \n                    return None\n\n        \n\n        \n        return None","method_summary":"Extract the expiration date from the whois record.","original_method_code":"def _extract(self):  # pragma: no cover\n        \"\"\"\n        Extract the expiration date from the whois record.\n\n        :return: The status of the domain.\n        :rtype: str\n        \"\"\"\n\n        # We try to get the expiration date from the database.\n        expiration_date_from_database = Whois().get_expiration_date()\n\n        if expiration_date_from_database:\n            # The hash of the current whois record did not changed and the\n            # expiration date from the database is not empty not equal to\n            # None or False.\n\n            # We generate the files and print the status.\n            # It's an active element!\n            Generate(\n                PyFunceble.STATUS[\"official\"][\"up\"],\n                \"WHOIS\",\n                expiration_date_from_database,\n            ).status_file()\n\n            # We handle und return the official up status.\n            return PyFunceble.STATUS[\"official\"][\"up\"]\n\n        # We get the whois record.\n        self.whois_record = Lookup().whois(PyFunceble.INTERN[\"referer\"])\n\n        # We list the list of regex which will help us get an unformatted expiration date.\n        to_match = [\n            r\"expire:(.*)\",\n            r\"expire on:(.*)\",\n            r\"Expiry Date:(.*)\",\n            r\"free-date(.*)\",\n            r\"expires:(.*)\",\n            r\"Expiration date:(.*)\",\n            r\"Expiry date:(.*)\",\n            r\"Expire Date:(.*)\",\n            r\"renewal date:(.*)\",\n            r\"Expires:(.*)\",\n            r\"validity:(.*)\",\n            r\"Expiration Date             :(.*)\",\n            r\"Expiry :(.*)\",\n            r\"expires at:(.*)\",\n            r\"domain_datebilleduntil:(.*)\",\n            r\"Data de expira\u00e7\u00e3o \\\/ Expiration Date \\(dd\\\/mm\\\/yyyy\\):(.*)\",\n            r\"Fecha de expiraci\u00f3n \\(Expiration date\\):(.*)\",\n            r\"\\[Expires on\\](.*)\",\n            r\"Record expires on(.*)(\\(YYYY-MM-DD\\))\",\n            r\"status:      OK-UNTIL(.*)\",\n            r\"renewal:(.*)\",\n            r\"expires............:(.*)\",\n            r\"expire-date:(.*)\",\n            r\"Exp date:(.*)\",\n            r\"Valid-date(.*)\",\n            r\"Expires On:(.*)\",\n            r\"Fecha de vencimiento:(.*)\",\n            r\"Expiration:.........(.*)\",\n            r\"Fecha de Vencimiento:(.*)\",\n            r\"Registry Expiry Date:(.*)\",\n            r\"Expires on..............:(.*)\",\n            r\"Expiration Time:(.*)\",\n            r\"Expiration Date:(.*)\",\n            r\"Expired:(.*)\",\n            r\"Date d'expiration:(.*)\",\n            r\"expiration date:(.*)\",\n        ]\n\n        if self.whois_record:\n            # The whois record is not empty.\n\n            if \"current_test_data\" in PyFunceble.INTERN:\n                # The end-user want more information whith his test.\n\n                # We update the whois_record index.\n                PyFunceble.INTERN[\"current_test_data\"][\n                    \"whois_record\"\n                ] = self.whois_record\n\n            for string in to_match:\n                # We loop through the list of regex.\n\n                # We try tro extract the expiration date from the WHOIS record.\n                expiration_date = Regex(\n                    self.whois_record, string, return_data=True, rematch=True, group=0\n                ).match()\n\n                if expiration_date:\n                    # The expiration date could be extracted.\n\n                    # We get the extracted expiration date.\n                    self.expiration_date = expiration_date[0].strip()\n\n                    # We initate a regex which will help us know if a number\n                    # is present into the extracted expiration date.\n                    regex_rumbers = r\"[0-9]\"\n\n                    if Regex(\n                        self.expiration_date, regex_rumbers, return_data=False\n                    ).match():\n                        # The extracted expiration date has a number.\n\n                        # We format the extracted expiration date.\n                        self.expiration_date = self._format()\n\n                        if (\n                            self.expiration_date\n                            and not Regex(\n                                self.expiration_date,\n                                r\"[0-9]{2}\\-[a-z]{3}\\-2[0-9]{3}\",\n                                return_data=False,\n                            ).match()\n                        ):\n                            # The formatted expiration date does not match our unified format.\n\n                            # We log the problem.\n                            Logs().expiration_date(self.expiration_date)\n\n                            # We log the whois record.\n                            Logs().whois(self.whois_record)\n\n                        if \"current_test_data\" in PyFunceble.INTERN:\n                            # The end-user want more information whith his test.\n\n                            # We update the expiration_date index.\n                            PyFunceble.INTERN[\"current_test_data\"][\n                                \"expiration_date\"\n                            ] = self.expiration_date\n\n                        # We generate the files and print the status.\n                        # It's an active element!\n                        Generate(\n                            PyFunceble.STATUS[\"official\"][\"up\"],\n                            \"WHOIS\",\n                            self.expiration_date,\n                        ).status_file()\n\n                        # We log the whois record.\n                        Logs().whois(self.whois_record)\n\n                        # We save the whois record into the database.\n                        Whois(expiration_date=self.expiration_date).add()\n\n                        # We handle und return the official up status.\n                        return PyFunceble.STATUS[\"official\"][\"up\"]\n\n                    # The extracted expiration date does not have a number.\n\n                    # We log the whois record.\n                    Logs().whois(self.whois_record)\n\n                    # We return None, we could not get the expiration date.\n                    return None\n\n        # The whois record is empty.\n\n        # We return None, we could not get the whois record.\n        return None","method_path":"https:\/\/github.com\/funilrys\/PyFunceble\/blob\/cdf69cbde120199171f7158e1c33635753e6e2f5\/PyFunceble\/expiration_date.py#L434-L593"}
{"repo_name":"funilrys\/PyFunceble","method_name":"Production._update_code_urls","method_code":"def _update_code_urls(self):\n        \"\"\"\"\"\"\n\n        to_ignore = [\".gitignore\", \".keep\"]\n\n        for root, _, files in PyFunceble.walk(\n            PyFunceble.CURRENT_DIRECTORY\n            + PyFunceble.directory_separator\n            + \"PyFunceble\"\n            + PyFunceble.directory_separator\n        ):\n            \n\n            for file in files:\n                \n\n                if file not in to_ignore and \"__pycache__\" not in root:\n                    \n                    \n                    \n\n                    if root.endswith(PyFunceble.directory_separator):\n                        \n\n                        \n                        self._update_docs(root + file)\n                    else:\n                        \n\n                        \n                        \n                        self._update_docs(root + PyFunceble.directory_separator + file)\n\n        for root, _, files in PyFunceble.walk(\n            PyFunceble.CURRENT_DIRECTORY\n            + PyFunceble.directory_separator\n            + \"tests\"\n            + PyFunceble.directory_separator\n        ):\n            \n            for file in files:\n                \n\n                if file not in to_ignore and \"__pycache__\" not in root:\n                    \n                    \n                    \n\n                    if root.endswith(PyFunceble.directory_separator):\n                        \n\n                        \n                        self._update_docs(root + file)\n                    else:\n                        \n\n                        \n                        \n                        self._update_docs(root + PyFunceble.directory_separator + file)","method_summary":"Read the code and update all links.","original_method_code":"def _update_code_urls(self):\n        \"\"\"\n        Read the code and update all links.\n        \"\"\"\n\n        to_ignore = [\".gitignore\", \".keep\"]\n\n        for root, _, files in PyFunceble.walk(\n            PyFunceble.CURRENT_DIRECTORY\n            + PyFunceble.directory_separator\n            + \"PyFunceble\"\n            + PyFunceble.directory_separator\n        ):\n            # We loop through every directories and files in the `PyFunceble` directory.\n\n            for file in files:\n                # We loop through the list of files of the currently read directory.\n\n                if file not in to_ignore and \"__pycache__\" not in root:\n                    # * The filename is not into the list of file to ignore.\n                    # and\n                    # * The directory we are reading is not `__pycache__`.\n\n                    if root.endswith(PyFunceble.directory_separator):\n                        # The root directory ends with the directory separator.\n\n                        # We fix the path in the currently read file.\n                        self._update_docs(root + file)\n                    else:\n                        # The root directory does not ends with the directory separator.\n\n                        # We fix the path in the currently read file.\n                        # (after appending the directory separator between the root and file)\n                        self._update_docs(root + PyFunceble.directory_separator + file)\n\n        for root, _, files in PyFunceble.walk(\n            PyFunceble.CURRENT_DIRECTORY\n            + PyFunceble.directory_separator\n            + \"tests\"\n            + PyFunceble.directory_separator\n        ):\n            # We loop through every directories and files in the `tests` directory.\n            for file in files:\n                # We loop through the list of files of the currently read directory.\n\n                if file not in to_ignore and \"__pycache__\" not in root:\n                    # * The filename is not into the list of file to ignore.\n                    # and\n                    # * The directory we are reading is not `__pycache__`.\n\n                    if root.endswith(PyFunceble.directory_separator):\n                        # The root directory ends with the directory separator.\n\n                        # We fix the path in the currently read file.\n                        self._update_docs(root + file)\n                    else:\n                        # The root directory does not ends with the directory separator.\n\n                        # We fix the path in the currently read file.\n                        # (after appending the directory separator between the root and file)\n                        self._update_docs(root + PyFunceble.directory_separator + file)","method_path":"https:\/\/github.com\/funilrys\/PyFunceble\/blob\/cdf69cbde120199171f7158e1c33635753e6e2f5\/PyFunceble\/production.py#L248-L308"}
{"repo_name":"funilrys\/PyFunceble","method_name":"Production._is_version_greater","method_code":"def _is_version_greater(self):\n        \"\"\"\"\"\"\n\n        \n        checked = Version(True).check_versions(\n            self.current_version[0], self.version_yaml\n        )\n\n        if checked is not None and not checked:\n            \n\n            \n            return True\n\n        \n        return False","method_summary":"Check if the current version is greater as the older older one.","original_method_code":"def _is_version_greater(self):\n        \"\"\"\n        Check if the current version is greater as the older older one.\n        \"\"\"\n\n        # we compare the 2 versions.\n        checked = Version(True).check_versions(\n            self.current_version[0], self.version_yaml\n        )\n\n        if checked is not None and not checked:\n            # The current version is greater as the older one.\n\n            # We return True.\n            return True\n\n        # We return False\n        return False","method_path":"https:\/\/github.com\/funilrys\/PyFunceble\/blob\/cdf69cbde120199171f7158e1c33635753e6e2f5\/PyFunceble\/production.py#L320-L337"}
{"repo_name":"funilrys\/PyFunceble","method_name":"Production.is_dev_version","method_code":"def is_dev_version(cls):\n        \"\"\"\"\"\"\n\n        \n        \n        command = \"git branch\"\n\n        \n        command_result = Command(command).execute()\n\n        for branch in command_result.split(\"\\n\"):\n            \n\n            if branch.startswith(\"*\") and \"dev\" in branch:\n                \n\n                \n                return True\n\n        \n\n        \n        return False","method_summary":"Check if the current branch is `dev`.","original_method_code":"def is_dev_version(cls):\n        \"\"\"\n        Check if the current branch is `dev`.\n        \"\"\"\n\n        # We initiate the command we have to run in order to\n        # get the branch we are currently working with.\n        command = \"git branch\"\n\n        # We execute and get the command output.\n        command_result = Command(command).execute()\n\n        for branch in command_result.split(\"\\n\"):\n            # We loop through each line of the command output.\n\n            if branch.startswith(\"*\") and \"dev\" in branch:\n                # The current branch is `dev`.\n\n                # We return True.\n                return True\n\n        # The current branch is not `dev`.\n\n        # We return False.\n        return False","method_path":"https:\/\/github.com\/funilrys\/PyFunceble\/blob\/cdf69cbde120199171f7158e1c33635753e6e2f5\/PyFunceble\/production.py#L340-L364"}
{"repo_name":"funilrys\/PyFunceble","method_name":"Production._does_require_deprecation","method_code":"def _does_require_deprecation(self):\n        \"\"\"\"\"\"\n\n        for index, version_number in enumerate(self.current_version[0][:2]):\n            \n\n            if version_number > self.version_yaml[index]:\n                \n                \n\n                \n                return True\n\n        \n        return False","method_summary":"Check if we have to put the previous version into the deprecated list.","original_method_code":"def _does_require_deprecation(self):\n        \"\"\"\n        Check if we have to put the previous version into the deprecated list.\n        \"\"\"\n\n        for index, version_number in enumerate(self.current_version[0][:2]):\n            # We loop through the 2 last elements of the version.\n\n            if version_number > self.version_yaml[index]:\n                # The currently read version number is greater than the one we have in\n                # the version.yaml.\n\n                # We return True.\n                return True\n\n        # We return False, we do not need to deprecate anything.\n        return False","method_path":"https:\/\/github.com\/funilrys\/PyFunceble\/blob\/cdf69cbde120199171f7158e1c33635753e6e2f5\/PyFunceble\/production.py#L393-L409"}
{"repo_name":"funilrys\/PyFunceble","method_name":"Production._update_docs","method_code":"def _update_docs(self, file_to_update):\n        \"\"\"\"\"\"\n\n        if self.is_dev_version():\n            \n\n            \n            \n            regexes = {\n                \"\/%s\/\" % \"dev\": r\"\\\/%s\\\/\" % \"master\",\n                \"=%s\" % \"dev\": \"=%s\" % \"master\",\n            }\n        elif self.is_master_version():\n            \n\n            \n            regexes = {\n                \"\/%s\/\" % \"master\": r\"\\\/%s\\\/\" % \"dev\",\n                \"=%s\" % \"master\": \"=%s\" % \"dev\",\n            }\n        else:\n            \n\n            \n            \n            raise Exception(\"Please switch to `dev` or `master` branch.\")\n\n        \n        to_update = File(file_to_update).read()\n\n        for replacement, regex in regexes.items():\n            \n\n            \n            to_update = Regex(to_update, regex, replace_with=replacement).replace()\n\n        \n        \n        File(file_to_update).write(to_update, overwrite=True)","method_summary":"Update the given documentation file or :code:`README.rst` so that it always gives branch related URL and informations.","original_method_code":"def _update_docs(self, file_to_update):\n        \"\"\"\n        Update the given documentation file or :code:`README.rst` so that\n        it always gives branch related URL and informations.\n\n        .. note::\n            This only apply to :code:`dev` and :code:`master` branch.\n\n        :param file_to_update: The file to update.\n        :type file_to_update: str\n        \"\"\"\n\n        if self.is_dev_version():\n            # The current version is the dev version.\n\n            # We map what we have to replace.\n            # Format: {match:replacement}\n            regexes = {\n                \"\/%s\/\" % \"dev\": r\"\\\/%s\\\/\" % \"master\",\n                \"=%s\" % \"dev\": \"=%s\" % \"master\",\n            }\n        elif self.is_master_version():\n            # The current version is the master version.\n\n            # We map what we have to replace.\n            regexes = {\n                \"\/%s\/\" % \"master\": r\"\\\/%s\\\/\" % \"dev\",\n                \"=%s\" % \"master\": \"=%s\" % \"dev\",\n            }\n        else:\n            # The current version is not the master nor the dev version.\n\n            # We raise an exception as the branch we are currently is not meaned\n            # for production.\n            raise Exception(\"Please switch to `dev` or `master` branch.\")\n\n        # We get the content of the file to fix.\n        to_update = File(file_to_update).read()\n\n        for replacement, regex in regexes.items():\n            # We loop through reach element of the map.\n\n            # We process the replacement.\n            to_update = Regex(to_update, regex, replace_with=replacement).replace()\n\n        # We finally overwrite the file to fix with the filtered.\n        # content.\n        File(file_to_update).write(to_update, overwrite=True)","method_path":"https:\/\/github.com\/funilrys\/PyFunceble\/blob\/cdf69cbde120199171f7158e1c33635753e6e2f5\/PyFunceble\/production.py#L428-L475"}
{"repo_name":"OpenKMIP\/PyKMIP","method_name":"ObtainLeaseResponsePayload.read","method_code":"def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\"\"\"\n        super(ObtainLeaseResponsePayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream):\n            self._unique_identifier = primitives.TextString(\n                tag=enums.Tags.UNIQUE_IDENTIFIER\n            )\n            self._unique_identifier.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self.is_tag_next(enums.Tags.LEASE_TIME, local_stream):\n            self._lease_time = primitives.Interval(\n                tag=enums.Tags.LEASE_TIME\n            )\n            self._lease_time.read(local_stream, kmip_version=kmip_version)\n        if self.is_tag_next(enums.Tags.LAST_CHANGE_DATE, local_stream):\n            self._last_change_date = primitives.DateTime(\n                tag=enums.Tags.LAST_CHANGE_DATE\n            )\n            self._last_change_date.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.is_oversized(local_stream)","method_summary":"Read the data encoding the ObtainLease response payload and decode it into its constituent parts.","original_method_code":"def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the ObtainLease response payload and decode it\n        into its constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is missing from the\n                encoded payload.\n        \"\"\"\n        super(ObtainLeaseResponsePayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream):\n            self._unique_identifier = primitives.TextString(\n                tag=enums.Tags.UNIQUE_IDENTIFIER\n            )\n            self._unique_identifier.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self.is_tag_next(enums.Tags.LEASE_TIME, local_stream):\n            self._lease_time = primitives.Interval(\n                tag=enums.Tags.LEASE_TIME\n            )\n            self._lease_time.read(local_stream, kmip_version=kmip_version)\n        if self.is_tag_next(enums.Tags.LAST_CHANGE_DATE, local_stream):\n            self._last_change_date = primitives.DateTime(\n                tag=enums.Tags.LAST_CHANGE_DATE\n            )\n            self._last_change_date.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.is_oversized(local_stream)","method_path":"https:\/\/github.com\/OpenKMIP\/PyKMIP\/blob\/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e\/kmip\/core\/messages\/payloads\/obtain_lease.py#L254-L299"}
{"repo_name":"OpenKMIP\/PyKMIP","method_name":"ObtainLeaseResponsePayload.write","method_code":"def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\"\"\"\n        local_stream = utils.BytearrayStream()\n\n        if self._unique_identifier:\n            self._unique_identifier.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self._lease_time:\n            self._lease_time.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self._last_change_date:\n            self._last_change_date.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.length = local_stream.length()\n        super(ObtainLeaseResponsePayload, self).write(\n            output_stream,\n            kmip_version=kmip_version\n        )\n        output_stream.write(local_stream.buffer)","method_summary":"Write the data encoding the ObtainLease response payload to a stream.","original_method_code":"def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Write the data encoding the ObtainLease response payload to a stream.\n\n        Args:\n            output_stream (stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is not defined.\n        \"\"\"\n        local_stream = utils.BytearrayStream()\n\n        if self._unique_identifier:\n            self._unique_identifier.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self._lease_time:\n            self._lease_time.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self._last_change_date:\n            self._last_change_date.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.length = local_stream.length()\n        super(ObtainLeaseResponsePayload, self).write(\n            output_stream,\n            kmip_version=kmip_version\n        )\n        output_stream.write(local_stream.buffer)","method_path":"https:\/\/github.com\/OpenKMIP\/PyKMIP\/blob\/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e\/kmip\/core\/messages\/payloads\/obtain_lease.py#L301-L339"}
{"repo_name":"OpenKMIP\/PyKMIP","method_name":"CancelRequestPayload.write","method_code":"def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\"\"\"\n        local_stream = utils.BytearrayStream()\n\n        if self._asynchronous_correlation_value:\n            self._asynchronous_correlation_value.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.length = local_stream.length()\n        super(CancelRequestPayload, self).write(\n            output_stream,\n            kmip_version=kmip_version\n        )\n        output_stream.write(local_stream.buffer)","method_summary":"Write the data encoding the Cancel request payload to a stream.","original_method_code":"def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Write the data encoding the Cancel request payload to a stream.\n\n        Args:\n            output_stream (stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is not defined.\n        \"\"\"\n        local_stream = utils.BytearrayStream()\n\n        if self._asynchronous_correlation_value:\n            self._asynchronous_correlation_value.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.length = local_stream.length()\n        super(CancelRequestPayload, self).write(\n            output_stream,\n            kmip_version=kmip_version\n        )\n        output_stream.write(local_stream.buffer)","method_path":"https:\/\/github.com\/OpenKMIP\/PyKMIP\/blob\/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e\/kmip\/core\/messages\/payloads\/cancel.py#L103-L131"}
{"repo_name":"OpenKMIP\/PyKMIP","method_name":"CancelResponsePayload.read","method_code":"def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\"\"\"\n        super(CancelResponsePayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(\n                enums.Tags.ASYNCHRONOUS_CORRELATION_VALUE,\n                local_stream\n        ):\n            self._asynchronous_correlation_value = primitives.ByteString(\n                tag=enums.Tags.ASYNCHRONOUS_CORRELATION_VALUE\n            )\n            self._asynchronous_correlation_value.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self.is_tag_next(enums.Tags.CANCELLATION_RESULT, local_stream):\n            self._cancellation_result = primitives.Enumeration(\n                enums.CancellationResult,\n                tag=enums.Tags.CANCELLATION_RESULT\n            )\n            self._cancellation_result.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.is_oversized(local_stream)","method_summary":"Read the data encoding the Cancel response payload and decode it into its constituent parts.","original_method_code":"def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Cancel response payload and decode it into\n        its constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is missing from the\n                encoded payload.\n        \"\"\"\n        super(CancelResponsePayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(\n                enums.Tags.ASYNCHRONOUS_CORRELATION_VALUE,\n                local_stream\n        ):\n            self._asynchronous_correlation_value = primitives.ByteString(\n                tag=enums.Tags.ASYNCHRONOUS_CORRELATION_VALUE\n            )\n            self._asynchronous_correlation_value.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self.is_tag_next(enums.Tags.CANCELLATION_RESULT, local_stream):\n            self._cancellation_result = primitives.Enumeration(\n                enums.CancellationResult,\n                tag=enums.Tags.CANCELLATION_RESULT\n            )\n            self._cancellation_result.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.is_oversized(local_stream)","method_path":"https:\/\/github.com\/OpenKMIP\/PyKMIP\/blob\/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e\/kmip\/core\/messages\/payloads\/cancel.py#L237-L281"}
{"repo_name":"OpenKMIP\/PyKMIP","method_name":"Digest.read","method_code":"def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\"\"\"\n        super(Digest, self).read(istream, kmip_version=kmip_version)\n        tstream = BytearrayStream(istream.read(self.length))\n\n        self.hashing_algorithm.read(tstream, kmip_version=kmip_version)\n        self.digest_value.read(tstream, kmip_version=kmip_version)\n        self.key_format_type.read(tstream, kmip_version=kmip_version)\n\n        self.is_oversized(tstream)\n        self.validate()","method_summary":"Read the data encoding the Digest object and decode it into its constituent parts.","original_method_code":"def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Digest object and decode it into its\n        constituent parts.\n\n        Args:\n            istream (Stream): A data stream containing encoded object data,\n                supporting a read method; usually a BytearrayStream object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        super(Digest, self).read(istream, kmip_version=kmip_version)\n        tstream = BytearrayStream(istream.read(self.length))\n\n        self.hashing_algorithm.read(tstream, kmip_version=kmip_version)\n        self.digest_value.read(tstream, kmip_version=kmip_version)\n        self.key_format_type.read(tstream, kmip_version=kmip_version)\n\n        self.is_oversized(tstream)\n        self.validate()","method_path":"https:\/\/github.com\/OpenKMIP\/PyKMIP\/blob\/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e\/kmip\/core\/attributes.py#L899-L919"}
{"repo_name":"OpenKMIP\/PyKMIP","method_name":"Digest.write","method_code":"def write(self, ostream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\"\"\"\n        tstream = BytearrayStream()\n\n        self.hashing_algorithm.write(tstream, kmip_version=kmip_version)\n        self.digest_value.write(tstream, kmip_version=kmip_version)\n        self.key_format_type.write(tstream, kmip_version=kmip_version)\n\n        self.length = tstream.length()\n        super(Digest, self).write(ostream, kmip_version=kmip_version)\n        ostream.write(tstream.buffer)","method_summary":"Write the data encoding the Digest object to a stream.","original_method_code":"def write(self, ostream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Write the data encoding the Digest object to a stream.\n\n        Args:\n            ostream (Stream): A data stream in which to encode object data,\n                supporting a write method; usually a BytearrayStream object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        tstream = BytearrayStream()\n\n        self.hashing_algorithm.write(tstream, kmip_version=kmip_version)\n        self.digest_value.write(tstream, kmip_version=kmip_version)\n        self.key_format_type.write(tstream, kmip_version=kmip_version)\n\n        self.length = tstream.length()\n        super(Digest, self).write(ostream, kmip_version=kmip_version)\n        ostream.write(tstream.buffer)","method_path":"https:\/\/github.com\/OpenKMIP\/PyKMIP\/blob\/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e\/kmip\/core\/attributes.py#L921-L940"}
{"repo_name":"OpenKMIP\/PyKMIP","method_name":"Digest.create","method_code":"def create(cls,\n               hashing_algorithm=HashingAlgorithmEnum.SHA_256,\n               digest_value=b'',\n               key_format_type=KeyFormatTypeEnum.RAW):\n        \"\"\"\"\"\"\n        algorithm = HashingAlgorithm(hashing_algorithm)\n        value = DigestValue(bytearray(digest_value))\n        format_type = KeyFormatType(key_format_type)\n\n        return Digest(hashing_algorithm=algorithm,\n                      digest_value=value,\n                      key_format_type=format_type)","method_summary":"Construct a Digest object from provided digest values.","original_method_code":"def create(cls,\n               hashing_algorithm=HashingAlgorithmEnum.SHA_256,\n               digest_value=b'',\n               key_format_type=KeyFormatTypeEnum.RAW):\n        \"\"\"\n        Construct a Digest object from provided digest values.\n\n        Args:\n            hashing_algorithm (HashingAlgorithm): An enumeration representing\n                the hash algorithm used to compute the digest. Optional,\n                defaults to HashingAlgorithm.SHA_256.\n            digest_value (byte string): The bytes of the digest hash. Optional,\n                defaults to the empty byte string.\n            key_format_type (KeyFormatType): An enumeration representing the\n                format of the key corresponding to the digest. Optional,\n                defaults to KeyFormatType.RAW.\n\n        Returns:\n            Digest: The newly created Digest.\n\n        Example:\n            >>> x = Digest.create(HashingAlgorithm.MD5, b'\\x00',\n            ... KeyFormatType.RAW)\n            >>> x.hashing_algorithm\n            HashingAlgorithm(value=HashingAlgorithm.MD5)\n            >>> x.digest_value\n            DigestValue(value=bytearray(b'\\x00'))\n            >>> x.key_format_type\n            KeyFormatType(value=KeyFormatType.RAW)\n        \"\"\"\n        algorithm = HashingAlgorithm(hashing_algorithm)\n        value = DigestValue(bytearray(digest_value))\n        format_type = KeyFormatType(key_format_type)\n\n        return Digest(hashing_algorithm=algorithm,\n                      digest_value=value,\n                      key_format_type=format_type)","method_path":"https:\/\/github.com\/OpenKMIP\/PyKMIP\/blob\/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e\/kmip\/core\/attributes.py#L1003-L1039"}
{"repo_name":"OpenKMIP\/PyKMIP","method_name":"ApplicationSpecificInformation.read","method_code":"def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\"\"\"\n        super(ApplicationSpecificInformation, self).read(\n            istream,\n            kmip_version=kmip_version\n        )\n        tstream = BytearrayStream(istream.read(self.length))\n\n        self.application_namespace.read(tstream, kmip_version=kmip_version)\n        self.application_data.read(tstream, kmip_version=kmip_version)\n\n        self.is_oversized(tstream)\n        self.validate()","method_summary":"Read the data encoding the ApplicationSpecificInformation object and decode it into its constituent parts.","original_method_code":"def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the ApplicationSpecificInformation object and\n        decode it into its constituent parts.\n\n        Args:\n            istream (Stream): A data stream containing encoded object data,\n                supporting a read method; usually a BytearrayStream object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        super(ApplicationSpecificInformation, self).read(\n            istream,\n            kmip_version=kmip_version\n        )\n        tstream = BytearrayStream(istream.read(self.length))\n\n        self.application_namespace.read(tstream, kmip_version=kmip_version)\n        self.application_data.read(tstream, kmip_version=kmip_version)\n\n        self.is_oversized(tstream)\n        self.validate()","method_path":"https:\/\/github.com\/OpenKMIP\/PyKMIP\/blob\/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e\/kmip\/core\/attributes.py#L1154-L1176"}
{"repo_name":"OpenKMIP\/PyKMIP","method_name":"ApplicationSpecificInformation.write","method_code":"def write(self, ostream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\"\"\"\n        tstream = BytearrayStream()\n\n        self.application_namespace.write(tstream, kmip_version=kmip_version)\n        self.application_data.write(tstream, kmip_version=kmip_version)\n\n        self.length = tstream.length()\n        super(ApplicationSpecificInformation, self).write(\n            ostream,\n            kmip_version=kmip_version\n        )\n        ostream.write(tstream.buffer)","method_summary":"Write the data encoding the ApplicationSpecificInformation object to a stream.","original_method_code":"def write(self, ostream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Write the data encoding the ApplicationSpecificInformation object to a\n        stream.\n\n        Args:\n            ostream (Stream): A data stream in which to encode object data,\n                supporting a write method; usually a BytearrayStream object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        tstream = BytearrayStream()\n\n        self.application_namespace.write(tstream, kmip_version=kmip_version)\n        self.application_data.write(tstream, kmip_version=kmip_version)\n\n        self.length = tstream.length()\n        super(ApplicationSpecificInformation, self).write(\n            ostream,\n            kmip_version=kmip_version\n        )\n        ostream.write(tstream.buffer)","method_path":"https:\/\/github.com\/OpenKMIP\/PyKMIP\/blob\/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e\/kmip\/core\/attributes.py#L1178-L1200"}
{"repo_name":"OpenKMIP\/PyKMIP","method_name":"ApplicationSpecificInformation.create","method_code":"def create(cls, application_namespace, application_data):\n        \"\"\"\"\"\"\n        namespace = ApplicationNamespace(application_namespace)\n        data = ApplicationData(application_data)\n        return ApplicationSpecificInformation(\n            application_namespace=namespace, application_data=data)","method_summary":"Construct an ApplicationSpecificInformation object from provided data and namespace values.","original_method_code":"def create(cls, application_namespace, application_data):\n        \"\"\"\n        Construct an ApplicationSpecificInformation object from provided data\n        and namespace values.\n\n        Args:\n            application_namespace (str): The name of the application namespace.\n            application_data (str): Application data related to the namespace.\n\n        Returns:\n            ApplicationSpecificInformation: The newly created set of\n                application information.\n\n        Example:\n            >>> x = ApplicationSpecificInformation.create('namespace', 'data')\n            >>> x.application_namespace.value\n            'namespace'\n            >>> x.application_data.value\n            'data'\n        \"\"\"\n        namespace = ApplicationNamespace(application_namespace)\n        data = ApplicationData(application_data)\n        return ApplicationSpecificInformation(\n            application_namespace=namespace, application_data=data)","method_path":"https:\/\/github.com\/OpenKMIP\/PyKMIP\/blob\/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e\/kmip\/core\/attributes.py#L1257-L1280"}
{"repo_name":"iotaledger\/iota.lib.py","method_name":"Address._generate_checksum","method_code":"def _generate_checksum(self):\n        \n        \"\"\"\"\"\"\n        checksum_trits = []  \n\n        sponge = Kerl()\n        sponge.absorb(self.address.as_trits())\n        sponge.squeeze(checksum_trits)\n\n        checksum_length = AddressChecksum.LEN * TRITS_PER_TRYTE\n\n        return AddressChecksum.from_trits(checksum_trits[-checksum_length:])","method_summary":"Generates the correct checksum for this address.","original_method_code":"def _generate_checksum(self):\n        # type: () -> AddressChecksum\n        \"\"\"\n        Generates the correct checksum for this address.\n        \"\"\"\n        checksum_trits = []  # type: MutableSequence[int]\n\n        sponge = Kerl()\n        sponge.absorb(self.address.as_trits())\n        sponge.squeeze(checksum_trits)\n\n        checksum_length = AddressChecksum.LEN * TRITS_PER_TRYTE\n\n        return AddressChecksum.from_trits(checksum_trits[-checksum_length:])","method_path":"https:\/\/github.com\/iotaledger\/iota.lib.py\/blob\/97cdd1e241498446b46157b79b2a1ea2ec6d387a\/iota\/types.py#L858-L871"}
{"repo_name":"iotaledger\/iota.lib.py","method_name":"IotaCommandLineApp.execute","method_code":"def execute(self, api, **arguments):\n        \n        \"\"\"\"\"\"\n        raise NotImplementedError(\n            'Not implemented in {cls}.'.format(cls=type(self).__name__),\n        )","method_summary":"Executes the command and (optionally) returns an exit code (used by the shell to determine if the application exited cleanly).","original_method_code":"def execute(self, api, **arguments):\n        # type: (Iota, **Any) -> Optional[int]\n        \"\"\"\n        Executes the command and (optionally) returns an exit code (used by\n        the shell to determine if the application exited cleanly).\n\n        :param api:\n            The API object used to communicate with the node.\n\n        :param arguments:\n            Command-line arguments parsed by the argument parser.\n        \"\"\"\n        raise NotImplementedError(\n            'Not implemented in {cls}.'.format(cls=type(self).__name__),\n        )","method_path":"https:\/\/github.com\/iotaledger\/iota.lib.py\/blob\/97cdd1e241498446b46157b79b2a1ea2ec6d387a\/iota\/bin\/__init__.py#L41-L55"}
{"repo_name":"iotaledger\/iota.lib.py","method_name":"IotaCommandLineApp.run_from_argv","method_code":"def run_from_argv(self, argv=None):\n        \n        \"\"\"\"\"\"\n        exit_code = self.execute(**self.parse_argv(argv))\n\n        if exit_code is None:\n            exit_code = 0\n\n        return exit_code","method_summary":"Executes the command from a collection of arguments (e.g., :py:data`sys.argv`) and returns the exit code.","original_method_code":"def run_from_argv(self, argv=None):\n        # type: (Optional[tuple]) -> int\n        \"\"\"\n        Executes the command from a collection of arguments (e.g.,\n        :py:data`sys.argv`) and returns the exit code.\n\n        :param argv:\n            Arguments to pass to the argument parser.\n            If ``None``, defaults to ``sys.argv[1:]``.\n        \"\"\"\n        exit_code = self.execute(**self.parse_argv(argv))\n\n        if exit_code is None:\n            exit_code = 0\n\n        return exit_code","method_path":"https:\/\/github.com\/iotaledger\/iota.lib.py\/blob\/97cdd1e241498446b46157b79b2a1ea2ec6d387a\/iota\/bin\/__init__.py#L63-L78"}
{"repo_name":"iotaledger\/iota.lib.py","method_name":"IotaCommandLineApp.parse_argv","method_code":"def parse_argv(self, argv=None):\n        \n        \"\"\"\"\"\"\n        arguments = vars(self.create_argument_parser().parse_args(argv))\n\n        seed = None\n        if self.requires_seed:\n            seed_filepath = arguments.pop('seed_file')\n\n            seed = (\n                self.seed_from_filepath(seed_filepath)\n                if seed_filepath\n                else self.prompt_for_seed()\n            )\n\n        arguments['api'] = Iota(\n            adapter=arguments.pop('uri'),\n            seed=seed,\n            testnet=arguments.pop('testnet'),\n        )\n\n        return arguments","method_summary":"Parses arguments for the command.","original_method_code":"def parse_argv(self, argv=None):\n        # type: (Optional[tuple]) -> dict\n        \"\"\"\n        Parses arguments for the command.\n\n        :param argv:\n            Arguments to pass to the argument parser.\n            If ``None``, defaults to ``sys.argv[1:]``.\n        \"\"\"\n        arguments = vars(self.create_argument_parser().parse_args(argv))\n\n        seed = None\n        if self.requires_seed:\n            seed_filepath = arguments.pop('seed_file')\n\n            seed = (\n                self.seed_from_filepath(seed_filepath)\n                if seed_filepath\n                else self.prompt_for_seed()\n            )\n\n        arguments['api'] = Iota(\n            adapter=arguments.pop('uri'),\n            seed=seed,\n            testnet=arguments.pop('testnet'),\n        )\n\n        return arguments","method_path":"https:\/\/github.com\/iotaledger\/iota.lib.py\/blob\/97cdd1e241498446b46157b79b2a1ea2ec6d387a\/iota\/bin\/__init__.py#L80-L107"}
{"repo_name":"iotaledger\/iota.lib.py","method_name":"IotaCommandLineApp.prompt_for_seed","method_code":"def prompt_for_seed():\n        \n        \"\"\"\"\"\"\n        seed = secure_input(\n            'Enter seed and press return (typing will not be shown).\\n'\n            'If no seed is specified, a random one will be used instead.\\n'\n        )\n\n        if isinstance(seed, text_type):\n            seed = seed.encode('ascii')\n\n        return Seed(seed) if seed else Seed.random()","method_summary":"Prompts the user to enter their seed via stdin.","original_method_code":"def prompt_for_seed():\n        # type: () -> Seed\n        \"\"\"\n        Prompts the user to enter their seed via stdin.\n        \"\"\"\n        seed = secure_input(\n            'Enter seed and press return (typing will not be shown).\\n'\n            'If no seed is specified, a random one will be used instead.\\n'\n        )\n\n        if isinstance(seed, text_type):\n            seed = seed.encode('ascii')\n\n        return Seed(seed) if seed else Seed.random()","method_path":"https:\/\/github.com\/iotaledger\/iota.lib.py\/blob\/97cdd1e241498446b46157b79b2a1ea2ec6d387a\/iota\/bin\/__init__.py#L165-L178"}
{"repo_name":"iotaledger\/iota.lib.py","method_name":"KeyGenerator.get_key","method_code":"def get_key(self, index, iterations):\n        \n        \"\"\"\"\"\"\n        return (\n            self.get_keys(\n                start=index,\n                count=1,\n                step=1,\n                iterations=iterations,\n            )[0]\n        )","method_summary":"Generates a single key.","original_method_code":"def get_key(self, index, iterations):\n        # type: (int, int) -> PrivateKey\n        \"\"\"\n        Generates a single key.\n\n        :param index:\n            The key index.\n\n        :param iterations:\n            Number of transform iterations to apply to the key, also\n            known as security level.\n            Must be >= 1.\n\n            Increasing this value makes key generation slower, but more\n            resistant to brute-forcing.\n        \"\"\"\n        return (\n            self.get_keys(\n                start=index,\n                count=1,\n                step=1,\n                iterations=iterations,\n            )[0]\n        )","method_path":"https:\/\/github.com\/iotaledger\/iota.lib.py\/blob\/97cdd1e241498446b46157b79b2a1ea2ec6d387a\/iota\/crypto\/signing.py#L76-L99"}
{"repo_name":"iotaledger\/iota.lib.py","method_name":"KeyGenerator.get_key_for","method_code":"def get_key_for(self, address):\n        \"\"\"\"\"\"\n        return self.get_key(\n            index=address.key_index,\n            iterations=address.security_level,\n        )","method_summary":"Generates the key associated with the specified address. Note that this method will generate the wrong key if the input address was generated from a different key!","original_method_code":"def get_key_for(self, address):\n        \"\"\"\n        Generates the key associated with the specified address.\n\n        Note that this method will generate the wrong key if the input\n        address was generated from a different key!\n        \"\"\"\n        return self.get_key(\n            index=address.key_index,\n            iterations=address.security_level,\n        )","method_path":"https:\/\/github.com\/iotaledger\/iota.lib.py\/blob\/97cdd1e241498446b46157b79b2a1ea2ec6d387a\/iota\/crypto\/signing.py#L101-L111"}
{"repo_name":"iotaledger\/iota.lib.py","method_name":"KeyGenerator.create_iterator","method_code":"def create_iterator(self, start=0, step=1, security_level=1):\n        \n        \"\"\"\"\"\"\n        return KeyIterator(self.seed, start, step, security_level)","method_summary":"Creates a generator that can be used to progressively generate new keys.","original_method_code":"def create_iterator(self, start=0, step=1, security_level=1):\n        # type: (int, int, int) -> KeyIterator\n        \"\"\"\n        Creates a generator that can be used to progressively generate\n        new keys.\n\n        :param start:\n            Starting index.\n\n            Warning: This method may take awhile to reset if ``start``\n            is a large number!\n\n        :param step:\n            Number of indexes to advance after each key.\n\n            This value can be negative; the generator will exit if it\n            reaches an index < 0.\n\n            Warning: The generator may take awhile to advance between\n            iterations if ``step`` is a large number!\n\n        :param security_level:\n            Number of _transform iterations to apply to each key.\n            Must be >= 1.\n\n            Increasing this value makes key generation slower, but more\n            resistant to brute-forcing.\n        \"\"\"\n        return KeyIterator(self.seed, start, step, security_level)","method_path":"https:\/\/github.com\/iotaledger\/iota.lib.py\/blob\/97cdd1e241498446b46157b79b2a1ea2ec6d387a\/iota\/crypto\/signing.py#L191-L219"}
{"repo_name":"iotaledger\/iota.lib.py","method_name":"KeyIterator._create_sponge","method_code":"def _create_sponge(self, index):\n        \n        \"\"\"\"\"\"\n        seed = self.seed_as_trits[:]\n\n        sponge = Kerl()\n        sponge.absorb(add_trits(seed, trits_from_int(index)))\n\n        \n        \n        \n        \n        sponge.squeeze(seed)\n        sponge.reset()\n        sponge.absorb(seed)\n\n        return sponge","method_summary":"Prepares the hash sponge for the generator.","original_method_code":"def _create_sponge(self, index):\n        # type: (int) -> Kerl\n        \"\"\"\n        Prepares the hash sponge for the generator.\n        \"\"\"\n        seed = self.seed_as_trits[:]\n\n        sponge = Kerl()\n        sponge.absorb(add_trits(seed, trits_from_int(index)))\n\n        # Squeeze all of the trits out of the sponge and re-absorb them.\n        # Note that the sponge transforms several times per operation,\n        # so this sequence is not as redundant as it looks at first\n        # glance.\n        sponge.squeeze(seed)\n        sponge.reset()\n        sponge.absorb(seed)\n\n        return sponge","method_path":"https:\/\/github.com\/iotaledger\/iota.lib.py\/blob\/97cdd1e241498446b46157b79b2a1ea2ec6d387a\/iota\/crypto\/signing.py#L316-L334"}
{"repo_name":"iotaledger\/iota.lib.py","method_name":"Curl.absorb","method_code":"def absorb(self, trits, offset=0, length=None):\n        \n        \"\"\"\"\"\"\n        pad = ((len(trits) % HASH_LENGTH) or HASH_LENGTH)\n        trits += [0] * (HASH_LENGTH - pad)\n\n        if length is None:\n            length = len(trits)\n\n        if length < 1:\n            raise with_context(\n                exc=ValueError('Invalid length passed to ``absorb``.'),\n\n                context={\n                    'trits': trits,\n                    'offset': offset,\n                    'length': length,\n                },\n            )\n\n        \n        \n        while offset < length:\n            start = offset\n            stop = min(start + HASH_LENGTH, length)\n\n            \n            \n            \n            \n            \n            \n            self._state[0:stop - start] = trits[start:stop]\n\n            \n            self._transform()\n\n            \n            offset += HASH_LENGTH","method_summary":"Absorb trits into the sponge.","original_method_code":"def absorb(self, trits, offset=0, length=None):\n        # type: (Sequence[int], Optional[int], Optional[int]) -> None\n        \"\"\"\n        Absorb trits into the sponge.\n\n        :param trits:\n            Sequence of trits to absorb.\n\n        :param offset:\n            Starting offset in ``trits``.\n\n        :param length:\n            Number of trits to absorb.  Defaults to ``len(trits)``.\n        \"\"\"\n        pad = ((len(trits) % HASH_LENGTH) or HASH_LENGTH)\n        trits += [0] * (HASH_LENGTH - pad)\n\n        if length is None:\n            length = len(trits)\n\n        if length < 1:\n            raise with_context(\n                exc=ValueError('Invalid length passed to ``absorb``.'),\n\n                context={\n                    'trits': trits,\n                    'offset': offset,\n                    'length': length,\n                },\n            )\n\n        # Copy trits from ``trits`` into internal state, one hash at a\n        # time, transforming internal state in between hashes.\n        while offset < length:\n            start = offset\n            stop = min(start + HASH_LENGTH, length)\n\n            # Copy the next hash worth of trits to internal state.\n            #\n            # Note that we always copy the trits to the start of the\n            # state. ``self._state`` is 3 hashes long, but only the\n            # first hash is \"public\"; the other 2 are only accessible to\n            # :py:meth:`_transform`.\n            self._state[0:stop - start] = trits[start:stop]\n\n            # Transform.\n            self._transform()\n\n            # Move on to the next hash.\n            offset += HASH_LENGTH","method_path":"https:\/\/github.com\/iotaledger\/iota.lib.py\/blob\/97cdd1e241498446b46157b79b2a1ea2ec6d387a\/iota\/crypto\/pycurl.py#L64-L113"}
{"repo_name":"agile-geoscience\/striplog","method_name":"Lexicon.from_json_file","method_code":"def from_json_file(cls, filename):\n        \"\"\"\"\"\"\n        with open(filename, 'r') as fp:\n            return cls(json.load(fp))","method_summary":"Load a lexicon from a JSON file.","original_method_code":"def from_json_file(cls, filename):\n        \"\"\"\n        Load a lexicon from a JSON file.\n\n        Args:\n            filename (str): The path to a JSON dump.\n        \"\"\"\n        with open(filename, 'r') as fp:\n            return cls(json.load(fp))","method_path":"https:\/\/github.com\/agile-geoscience\/striplog\/blob\/8033b673a151f96c29802b43763e863519a3124c\/striplog\/lexicon.py#L72-L80"}
{"repo_name":"agile-geoscience\/striplog","method_name":"Lexicon.find_word_groups","method_code":"def find_word_groups(self, text, category, proximity=2):\n        \"\"\"\"\"\"\n        f = re.IGNORECASE\n        words = getattr(self, category)\n        regex = re.compile(r'(\\b' + r'\\b|\\b'.join(words) + r'\\b)', flags=f)\n        candidates = regex.finditer(text)\n\n        starts, ends = [], []\n        groups = []\n\n        for item in candidates:\n            starts.append(item.span()[0])\n            ends.append(item.span()[1])\n            groups.append(item.group().lower())\n\n        new_starts = []  \n        new_groups = []  \n\n        skip = False\n        for i, g in enumerate(groups):\n            if skip:\n                skip = False\n                continue\n            if (i < len(groups)-1) and (starts[i+1]-ends[i] <= proximity):\n                if g[-1] == '-':\n                    sep = ''  \n                else:\n                    sep = ' '\n                new_groups.append(g + sep + groups[i+1])\n                new_starts.append(starts[i])\n                skip = True\n            else:\n                if g not in new_groups:\n                    new_groups.append(g)\n                    new_starts.append(starts[i])\n                skip = False\n\n        return new_groups","method_summary":"Given a string and a category, finds and combines words into groups based on their proximity.","original_method_code":"def find_word_groups(self, text, category, proximity=2):\n        \"\"\"\n        Given a string and a category, finds and combines words into\n        groups based on their proximity.\n\n        Args:\n            text (str): Some text.\n            tokens (list): A list of regex strings.\n\n        Returns:\n            list. The combined strings it found.\n\n        Example:\n            COLOURS = [r\"red(?:dish)?\", r\"grey(?:ish)?\", r\"green(?:ish)?\"]\n            s = 'GREYISH-GREEN limestone with RED or GREY sandstone.'\n            find_word_groups(s, COLOURS) --> ['greyish green', 'red', 'grey']\n        \"\"\"\n        f = re.IGNORECASE\n        words = getattr(self, category)\n        regex = re.compile(r'(\\b' + r'\\b|\\b'.join(words) + r'\\b)', flags=f)\n        candidates = regex.finditer(text)\n\n        starts, ends = [], []\n        groups = []\n\n        for item in candidates:\n            starts.append(item.span()[0])\n            ends.append(item.span()[1])\n            groups.append(item.group().lower())\n\n        new_starts = []  # As a check only.\n        new_groups = []  # This is what I want.\n\n        skip = False\n        for i, g in enumerate(groups):\n            if skip:\n                skip = False\n                continue\n            if (i < len(groups)-1) and (starts[i+1]-ends[i] <= proximity):\n                if g[-1] == '-':\n                    sep = ''  # Don't insert spaces after hyphens.\n                else:\n                    sep = ' '\n                new_groups.append(g + sep + groups[i+1])\n                new_starts.append(starts[i])\n                skip = True\n            else:\n                if g not in new_groups:\n                    new_groups.append(g)\n                    new_starts.append(starts[i])\n                skip = False\n\n        return new_groups","method_path":"https:\/\/github.com\/agile-geoscience\/striplog\/blob\/8033b673a151f96c29802b43763e863519a3124c\/striplog\/lexicon.py#L82-L134"}
{"repo_name":"agile-geoscience\/striplog","method_name":"Lexicon.find_synonym","method_code":"def find_synonym(self, word):\n        \"\"\"\"\"\"\n        if word and self.synonyms:\n            \n            reverse_lookup = {}\n            for k, v in self.synonyms.items():\n                for i in v:\n                    reverse_lookup[i.lower()] = k.lower()\n\n            \n            if word.lower() in reverse_lookup:\n                return reverse_lookup[word.lower()]\n\n        return word","method_summary":"Given a string and a dict of synonyms, returns the 'preferred' word. Case insensitive.","original_method_code":"def find_synonym(self, word):\n        \"\"\"\n        Given a string and a dict of synonyms, returns the 'preferred'\n        word. Case insensitive.\n\n        Args:\n            word (str): A word.\n\n        Returns:\n            str: The preferred word, or the input word if not found.\n\n        Example:\n            >>> syn = {'snake': ['python', 'adder']}\n            >>> find_synonym('adder', syn)\n            'snake'\n            >>> find_synonym('rattler', syn)\n            'rattler'\n\n        TODO:\n            Make it handle case, returning the same case it received.\n        \"\"\"\n        if word and self.synonyms:\n            # Make the reverse look-up table.\n            reverse_lookup = {}\n            for k, v in self.synonyms.items():\n                for i in v:\n                    reverse_lookup[i.lower()] = k.lower()\n\n            # Now check words against this table.\n            if word.lower() in reverse_lookup:\n                return reverse_lookup[word.lower()]\n\n        return word","method_path":"https:\/\/github.com\/agile-geoscience\/striplog\/blob\/8033b673a151f96c29802b43763e863519a3124c\/striplog\/lexicon.py#L136-L168"}
{"repo_name":"agile-geoscience\/striplog","method_name":"Lexicon.expand_abbreviations","method_code":"def expand_abbreviations(self, text):\n        \"\"\"\"\"\"\n        if not self.abbreviations:\n            raise LexiconError(\"No abbreviations in lexicon.\")\n\n        def chunks(data, SIZE=25):\n            \"\"\"\"\"\"\n            it = iter(data)\n            for i in range(0, len(data), SIZE):\n                yield {k: data[k] for k in islice(it, SIZE)}\n\n        def cb(g):\n            \"\"\"\"\"\"\n            return self.abbreviations.get(g.group(0)) or g.group(0)\n\n        \n\n        \n        \n        text = re.sub(r'w\/', r'wi', text)\n\n        \n        for subdict in chunks(self.abbreviations):\n            regex = r'(\\b' + r'\\b)|(\\b'.join(subdict.keys()) + r'\\b)'\n            text = re.sub(regex, cb, text)\n\n        return text","method_summary":"Parse a piece of text and replace any abbreviations with their full word equivalents. Uses the lexicon.abbreviations dictionary to find abbreviations.","original_method_code":"def expand_abbreviations(self, text):\n        \"\"\"\n        Parse a piece of text and replace any abbreviations with their full\n        word equivalents. Uses the lexicon.abbreviations dictionary to find\n        abbreviations.\n\n        Args:\n            text (str): The text to parse.\n\n        Returns:\n            str: The text with abbreviations replaced.\n        \"\"\"\n        if not self.abbreviations:\n            raise LexiconError(\"No abbreviations in lexicon.\")\n\n        def chunks(data, SIZE=25):\n            \"\"\"\n            Regex only supports 100 groups for munging callbacks. So we have to\n            chunk the abbreviation dicitonary.\n            \"\"\"\n            it = iter(data)\n            for i in range(0, len(data), SIZE):\n                yield {k: data[k] for k in islice(it, SIZE)}\n\n        def cb(g):\n            \"\"\"Regex callback\"\"\"\n            return self.abbreviations.get(g.group(0)) or g.group(0)\n\n        # Special cases.\n\n        # TODO: We should handle these with a special set of\n        # replacements that are made before the others.\n        text = re.sub(r'w\/', r'wi', text)\n\n        # Main loop.\n        for subdict in chunks(self.abbreviations):\n            regex = r'(\\b' + r'\\b)|(\\b'.join(subdict.keys()) + r'\\b)'\n            text = re.sub(regex, cb, text)\n\n        return text","method_path":"https:\/\/github.com\/agile-geoscience\/striplog\/blob\/8033b673a151f96c29802b43763e863519a3124c\/striplog\/lexicon.py#L170-L209"}
{"repo_name":"agile-geoscience\/striplog","method_name":"Lexicon.get_component","method_code":"def get_component(self, text, required=False, first_only=True):\n        \"\"\"\"\"\"\n        component = {}\n\n        for i, (category, words) in enumerate(self.__dict__.items()):\n\n            \n            if category in SPECIAL:\n                \n                continue\n\n            groups = self.find_word_groups(text, category)\n\n            if groups and first_only:\n                groups = groups[:1]\n            elif groups:\n                \n                pass\n            else:\n                groups = [None]\n                if required:\n                    with warnings.catch_warnings():\n                        warnings.simplefilter(\"always\")\n                        w = \"No lithology in lexicon matching '{0}'\"\n                        warnings.warn(w.format(text))\n\n            filtered = [self.find_synonym(i) for i in groups]\n            if first_only:\n                component[category] = filtered[0]\n            else:\n                component[category] = filtered\n\n        return component","method_summary":"Takes a piece of text representing a lithologic description for one component, e.g. \"Red vf-f sandstone\" and turns it into a dictionary of attributes.","original_method_code":"def get_component(self, text, required=False, first_only=True):\n        \"\"\"\n        Takes a piece of text representing a lithologic description for one\n        component, e.g. \"Red vf-f sandstone\" and turns it into a dictionary\n        of attributes.\n\n        TODO:\n            Generalize this so that we can use any types of word, as specified\n            in the lexicon.\n        \"\"\"\n        component = {}\n\n        for i, (category, words) in enumerate(self.__dict__.items()):\n\n            # There is probably a more elegant way to do this.\n            if category in SPECIAL:\n                # There are special entries in the lexicon.\n                continue\n\n            groups = self.find_word_groups(text, category)\n\n            if groups and first_only:\n                groups = groups[:1]\n            elif groups:\n                # groups = groups\n                pass\n            else:\n                groups = [None]\n                if required:\n                    with warnings.catch_warnings():\n                        warnings.simplefilter(\"always\")\n                        w = \"No lithology in lexicon matching '{0}'\"\n                        warnings.warn(w.format(text))\n\n            filtered = [self.find_synonym(i) for i in groups]\n            if first_only:\n                component[category] = filtered[0]\n            else:\n                component[category] = filtered\n\n        return component","method_path":"https:\/\/github.com\/agile-geoscience\/striplog\/blob\/8033b673a151f96c29802b43763e863519a3124c\/striplog\/lexicon.py#L211-L251"}
{"repo_name":"agile-geoscience\/striplog","method_name":"Lexicon.split_description","method_code":"def split_description(self, text):\n        \"\"\"\"\"\"\n        \n        t = re.sub(r'(\\d) ?in\\. ', r'\\1 inch ', text)  \n        t = re.sub(r'(\\d) ?ft\\. ', r'\\1 feet ', t)  \n\n        \n        words = getattr(self, 'splitters')\n        try:\n            splitter = words[0].strip()\n        except:\n            splitter = 'with'\n        t = re.sub(r'\\,?\\;?\\.? ?((under)?(less than)? \\d+%) (?=\\w)', r' '+splitter+' \\1 ', t)\n\n        \n        f = re.IGNORECASE\n        pattern = re.compile(r'(?:' + r'|'.join(words) + r')', flags=f)\n        parts = filter(None, pattern.split(t))\n\n        return [i.strip() for i in parts]","method_summary":"Split a description into parts, each of which can be turned into a single component.","original_method_code":"def split_description(self, text):\n        \"\"\"\n        Split a description into parts, each of which can be turned into\n        a single component.\n        \"\"\"\n        # Protect some special sequences.\n        t = re.sub(r'(\\d) ?in\\. ', r'\\1 inch ', text)  # Protect.\n        t = re.sub(r'(\\d) ?ft\\. ', r'\\1 feet ', t)  # Protect.\n\n        # Transform all part delimiters to first splitter.\n        words = getattr(self, 'splitters')\n        try:\n            splitter = words[0].strip()\n        except:\n            splitter = 'with'\n        t = re.sub(r'\\,?\\;?\\.? ?((under)?(less than)? \\d+%) (?=\\w)', r' '+splitter+' \\1 ', t)\n\n        # Split.\n        f = re.IGNORECASE\n        pattern = re.compile(r'(?:' + r'|'.join(words) + r')', flags=f)\n        parts = filter(None, pattern.split(t))\n\n        return [i.strip() for i in parts]","method_path":"https:\/\/github.com\/agile-geoscience\/striplog\/blob\/8033b673a151f96c29802b43763e863519a3124c\/striplog\/lexicon.py#L253-L275"}
{"repo_name":"agile-geoscience\/striplog","method_name":"Lexicon.categories","method_code":"def categories(self):\n        \"\"\"\"\"\"\n        keys = [k for k in self.__dict__.keys() if k not in SPECIAL]\n        return keys","method_summary":"Lists the categories in the lexicon, except the optional categories.","original_method_code":"def categories(self):\n        \"\"\"\n        Lists the categories in the lexicon, except the\n        optional categories.\n\n        Returns:\n            list: A list of strings of category names.\n        \"\"\"\n        keys = [k for k in self.__dict__.keys() if k not in SPECIAL]\n        return keys","method_path":"https:\/\/github.com\/agile-geoscience\/striplog\/blob\/8033b673a151f96c29802b43763e863519a3124c\/striplog\/lexicon.py#L278-L287"}
{"repo_name":"agile-geoscience\/striplog","method_name":"Decor._repr_html_","method_code":"def _repr_html_(self):\n        \"\"\"\"\"\"\n        rows, c = '', ''\n        s = '<tr><td><strong>{k}<\/strong><\/td><td style=\"{stl}\">{v}<\/td><\/tr>'\n        for k, v in self.__dict__.items():\n\n            if k == '_colour':\n                k = 'colour'\n                c = utils.text_colour_for_hex(v)\n                style = 'color:{}; background-color:{}'.format(c, v)\n            else:\n                style = 'color:black; background-color:white'\n\n            if k == 'component':\n                try:\n                    v = v._repr_html_()\n                except AttributeError:\n                    v = v.__repr__()\n\n            rows += s.format(k=k, v=v, stl=style)\n        html = '<table>{}<\/table>'.format(rows)\n        return html","method_summary":"Jupyter Notebook magic repr function.","original_method_code":"def _repr_html_(self):\n        \"\"\"\n        Jupyter Notebook magic repr function.\n        \"\"\"\n        rows, c = '', ''\n        s = '<tr><td><strong>{k}<\/strong><\/td><td style=\"{stl}\">{v}<\/td><\/tr>'\n        for k, v in self.__dict__.items():\n\n            if k == '_colour':\n                k = 'colour'\n                c = utils.text_colour_for_hex(v)\n                style = 'color:{}; background-color:{}'.format(c, v)\n            else:\n                style = 'color:black; background-color:white'\n\n            if k == 'component':\n                try:\n                    v = v._repr_html_()\n                except AttributeError:\n                    v = v.__repr__()\n\n            rows += s.format(k=k, v=v, stl=style)\n        html = '<table>{}<\/table>'.format(rows)\n        return html","method_path":"https:\/\/github.com\/agile-geoscience\/striplog\/blob\/8033b673a151f96c29802b43763e863519a3124c\/striplog\/legend.py#L153-L176"}
{"repo_name":"agile-geoscience\/striplog","method_name":"Decor._repr_html_row_","method_code":"def _repr_html_row_(self, keys):\n        \"\"\"\"\"\"\n        tr, th, c = '', '', ''\n        r = '<td style=\"{stl}\">{v}<\/td>'\n        h = '<th>{k}<\/th>'\n        for k in keys:\n            v = self.__dict__.get(k)\n\n            if k == '_colour':\n                k = 'colour'\n                c = utils.text_colour_for_hex(v)\n                style = 'color:{}; background-color:{}'.format(c, v)\n            else:\n                style = 'color:black; background-color:white'\n\n            if k == 'component':\n                try:\n                    v = v._repr_html_()\n                except AttributeError:\n                    v = v.__repr__()\n\n            tr += r.format(v=v, stl=style)\n            th += h.format(k=k)\n\n        return th, tr","method_summary":"Jupyter Notebook magic repr function as a row \u2013 used by ``Legend._repr_html_()``.","original_method_code":"def _repr_html_row_(self, keys):\n        \"\"\"\n        Jupyter Notebook magic repr function as a row \u2013\u00a0used by\n        ``Legend._repr_html_()``.\n        \"\"\"\n        tr, th, c = '', '', ''\n        r = '<td style=\"{stl}\">{v}<\/td>'\n        h = '<th>{k}<\/th>'\n        for k in keys:\n            v = self.__dict__.get(k)\n\n            if k == '_colour':\n                k = 'colour'\n                c = utils.text_colour_for_hex(v)\n                style = 'color:{}; background-color:{}'.format(c, v)\n            else:\n                style = 'color:black; background-color:white'\n\n            if k == 'component':\n                try:\n                    v = v._repr_html_()\n                except AttributeError:\n                    v = v.__repr__()\n\n            tr += r.format(v=v, stl=style)\n            th += h.format(k=k)\n\n        return th, tr","method_path":"https:\/\/github.com\/agile-geoscience\/striplog\/blob\/8033b673a151f96c29802b43763e863519a3124c\/striplog\/legend.py#L178-L205"}
{"repo_name":"agile-geoscience\/striplog","method_name":"Decor.plot","method_code":"def plot(self, fmt=None, fig=None, ax=None):\n        \"\"\"\"\"\"\n\n        u = 4     \n        v = 0.25  \n\n        r = None\n\n        if (fig is None) and (ax is None):\n            fig = plt.figure(figsize=(u, 1))\n        else:\n            r = fig\n\n        if ax is None:\n            ax = fig.add_axes([0.1*v, 0.1, 0.8*v, 0.8])\n        else:\n            r = ax\n\n        rect1 = patches.Rectangle((0, 0),\n                                  u*v, u*v,\n                                  color=self.colour,\n                                  lw=1,\n                                  hatch=self.hatch,\n                                  ec='k')\n        ax.add_patch(rect1)\n        ax.text(1.0+0.1*v*u, u*v*0.5,\n                self.component.summary(fmt=fmt),\n                fontsize=max(u, 15),\n                verticalalignment='center',\n                horizontalalignment='left')\n        ax.set_xlim([0, u*v])\n        ax.set_ylim([0, u*v])\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        ax.invert_yaxis()\n\n        return r","method_summary":"Make a simple plot of the Decor.","original_method_code":"def plot(self, fmt=None, fig=None, ax=None):\n        \"\"\"\n        Make a simple plot of the Decor.\n\n        Args:\n            fmt (str): A Python format string for the component summaries.\n            fig (Pyplot figure): A figure, optional. Use either fig or ax, not\n                both.\n            ax (Pyplot axis): An axis, optional. Use either fig or ax, not\n                both.\n\n        Returns:\n            fig or ax or None. If you pass in an ax, you get it back. If you pass\n                in a fig, you get it. If you pass nothing, the function creates a\n                plot object as a side-effect.\n        \"\"\"\n\n        u = 4     # aspect ratio of decor plot\n        v = 0.25  # ratio of decor tile width\n\n        r = None\n\n        if (fig is None) and (ax is None):\n            fig = plt.figure(figsize=(u, 1))\n        else:\n            r = fig\n\n        if ax is None:\n            ax = fig.add_axes([0.1*v, 0.1, 0.8*v, 0.8])\n        else:\n            r = ax\n\n        rect1 = patches.Rectangle((0, 0),\n                                  u*v, u*v,\n                                  color=self.colour,\n                                  lw=1,\n                                  hatch=self.hatch,\n                                  ec='k')\n        ax.add_patch(rect1)\n        ax.text(1.0+0.1*v*u, u*v*0.5,\n                self.component.summary(fmt=fmt),\n                fontsize=max(u, 15),\n                verticalalignment='center',\n                horizontalalignment='left')\n        ax.set_xlim([0, u*v])\n        ax.set_ylim([0, u*v])\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        ax.invert_yaxis()\n\n        return r","method_path":"https:\/\/github.com\/agile-geoscience\/striplog\/blob\/8033b673a151f96c29802b43763e863519a3124c\/striplog\/legend.py#L271-L321"}
{"repo_name":"quantopian\/pgcontents","method_name":"PostgresContentsManager.purge_db","method_code":"def purge_db(self):\n        \"\"\"\"\"\"\n        with self.engine.begin() as db:\n            purge_user(db, self.user_id)","method_summary":"Clear all matching our user_id.","original_method_code":"def purge_db(self):\n        \"\"\"\n        Clear all matching our user_id.\n        \"\"\"\n        with self.engine.begin() as db:\n            purge_user(db, self.user_id)","method_path":"https:\/\/github.com\/quantopian\/pgcontents\/blob\/ed36268b7917332d16868208e1e565742a8753e1\/pgcontents\/pgmanager.py#L107-L112"}
{"repo_name":"quantopian\/pgcontents","method_name":"PostgresContentsManager.guess_type","method_code":"def guess_type(self, path, allow_directory=True):\n        \"\"\"\"\"\"\n        if path.endswith('.ipynb'):\n            return 'notebook'\n        elif allow_directory and self.dir_exists(path):\n            return 'directory'\n        else:\n            return 'file'","method_summary":"Guess the type of a file. If allow_directory is False, don't consider the possibility that the file is a directory.","original_method_code":"def guess_type(self, path, allow_directory=True):\n        \"\"\"\n        Guess the type of a file.\n\n        If allow_directory is False, don't consider the possibility that the\n        file is a directory.\n        \"\"\"\n        if path.endswith('.ipynb'):\n            return 'notebook'\n        elif allow_directory and self.dir_exists(path):\n            return 'directory'\n        else:\n            return 'file'","method_path":"https:\/\/github.com\/quantopian\/pgcontents\/blob\/ed36268b7917332d16868208e1e565742a8753e1\/pgcontents\/pgmanager.py#L115-L127"}
{"repo_name":"quantopian\/pgcontents","method_name":"PostgresContentsManager.get_file_id","method_code":"def get_file_id(self, path):\n        \"\"\"\"\"\"\n        with self.engine.begin() as db:\n            try:\n                file_id = get_file_id(db, self.user_id, path)\n            except NoSuchFile:\n                self.no_such_entity(path)\n\n        return file_id","method_summary":"Get the id of a file in the database. This function is specific to this implementation of ContentsManager and is not in the base class.","original_method_code":"def get_file_id(self, path):\n        \"\"\"\n        Get the id of a file in the database.  This function is specific to\n        this implementation of ContentsManager and is not in the base class.\n        \"\"\"\n        with self.engine.begin() as db:\n            try:\n                file_id = get_file_id(db, self.user_id, path)\n            except NoSuchFile:\n                self.no_such_entity(path)\n\n        return file_id","method_path":"https:\/\/github.com\/quantopian\/pgcontents\/blob\/ed36268b7917332d16868208e1e565742a8753e1\/pgcontents\/pgmanager.py#L166-L177"}
{"repo_name":"quantopian\/pgcontents","method_name":"PostgresContentsManager._get_notebook","method_code":"def _get_notebook(self, path, content, format):\n        \"\"\"\"\"\"\n        with self.engine.begin() as db:\n            try:\n                record = get_file(\n                    db,\n                    self.user_id,\n                    path,\n                    content,\n                    self.crypto.decrypt,\n                )\n            except NoSuchFile:\n                self.no_such_entity(path)\n\n        return self._notebook_model_from_db(record, content)","method_summary":"Get a notebook from the database.","original_method_code":"def _get_notebook(self, path, content, format):\n        \"\"\"\n        Get a notebook from the database.\n        \"\"\"\n        with self.engine.begin() as db:\n            try:\n                record = get_file(\n                    db,\n                    self.user_id,\n                    path,\n                    content,\n                    self.crypto.decrypt,\n                )\n            except NoSuchFile:\n                self.no_such_entity(path)\n\n        return self._notebook_model_from_db(record, content)","method_path":"https:\/\/github.com\/quantopian\/pgcontents\/blob\/ed36268b7917332d16868208e1e565742a8753e1\/pgcontents\/pgmanager.py#L179-L195"}
{"repo_name":"quantopian\/pgcontents","method_name":"PostgresContentsManager._notebook_model_from_db","method_code":"def _notebook_model_from_db(self, record, content):\n        \"\"\"\"\"\"\n        path = to_api_path(record['parent_name'] + record['name'])\n        model = base_model(path)\n        model['type'] = 'notebook'\n        model['last_modified'] = model['created'] = record['created_at']\n        if content:\n            content = reads_base64(record['content'])\n            self.mark_trusted_cells(content, path)\n            model['content'] = content\n            model['format'] = 'json'\n            self.validate_notebook_model(model)\n        return model","method_summary":"Build a notebook model from database record.","original_method_code":"def _notebook_model_from_db(self, record, content):\n        \"\"\"\n        Build a notebook model from database record.\n        \"\"\"\n        path = to_api_path(record['parent_name'] + record['name'])\n        model = base_model(path)\n        model['type'] = 'notebook'\n        model['last_modified'] = model['created'] = record['created_at']\n        if content:\n            content = reads_base64(record['content'])\n            self.mark_trusted_cells(content, path)\n            model['content'] = content\n            model['format'] = 'json'\n            self.validate_notebook_model(model)\n        return model","method_path":"https:\/\/github.com\/quantopian\/pgcontents\/blob\/ed36268b7917332d16868208e1e565742a8753e1\/pgcontents\/pgmanager.py#L197-L211"}
{"repo_name":"quantopian\/pgcontents","method_name":"PostgresContentsManager._get_directory","method_code":"def _get_directory(self, path, content, format):\n        \"\"\"\"\"\"\n        with self.engine.begin() as db:\n            try:\n                record = get_directory(\n                    db, self.user_id, path, content\n                )\n            except NoSuchDirectory:\n                if self.file_exists(path):\n                    \n                    \n                    self.do_400(\"Wrong type: %s\" % path)\n                else:\n                    self.no_such_entity(path)\n\n        return self._directory_model_from_db(record, content)","method_summary":"Get a directory from the database.","original_method_code":"def _get_directory(self, path, content, format):\n        \"\"\"\n        Get a directory from the database.\n        \"\"\"\n        with self.engine.begin() as db:\n            try:\n                record = get_directory(\n                    db, self.user_id, path, content\n                )\n            except NoSuchDirectory:\n                if self.file_exists(path):\n                    # TODO: It's awkward\/expensive to have to check this to\n                    # return a 400 instead of 404. Consider just 404ing.\n                    self.do_400(\"Wrong type: %s\" % path)\n                else:\n                    self.no_such_entity(path)\n\n        return self._directory_model_from_db(record, content)","method_path":"https:\/\/github.com\/quantopian\/pgcontents\/blob\/ed36268b7917332d16868208e1e565742a8753e1\/pgcontents\/pgmanager.py#L213-L230"}
{"repo_name":"quantopian\/pgcontents","method_name":"PostgresContentsManager._convert_file_records","method_code":"def _convert_file_records(self, file_records):\n        \"\"\"\"\"\"\n        for record in file_records:\n            type_ = self.guess_type(record['name'], allow_directory=False)\n            if type_ == 'notebook':\n                yield self._notebook_model_from_db(record, False)\n            elif type_ == 'file':\n                yield self._file_model_from_db(record, False, None)\n            else:\n                self.do_500(\"Unknown file type %s\" % type_)","method_summary":"Apply _notebook_model_from_db or _file_model_from_db to each entry in file_records, depending on the result of `guess_type`.","original_method_code":"def _convert_file_records(self, file_records):\n        \"\"\"\n        Apply _notebook_model_from_db or _file_model_from_db to each entry\n        in file_records, depending on the result of `guess_type`.\n        \"\"\"\n        for record in file_records:\n            type_ = self.guess_type(record['name'], allow_directory=False)\n            if type_ == 'notebook':\n                yield self._notebook_model_from_db(record, False)\n            elif type_ == 'file':\n                yield self._file_model_from_db(record, False, None)\n            else:\n                self.do_500(\"Unknown file type %s\" % type_)","method_path":"https:\/\/github.com\/quantopian\/pgcontents\/blob\/ed36268b7917332d16868208e1e565742a8753e1\/pgcontents\/pgmanager.py#L232-L244"}
{"repo_name":"quantopian\/pgcontents","method_name":"PostgresContentsManager._directory_model_from_db","method_code":"def _directory_model_from_db(self, record, content):\n        \"\"\"\"\"\"\n        model = base_directory_model(to_api_path(record['name']))\n        if content:\n            model['format'] = 'json'\n            model['content'] = list(\n                chain(\n                    self._convert_file_records(record['files']),\n                    (\n                        self._directory_model_from_db(subdir, False)\n                        for subdir in record['subdirs']\n                    ),\n                )\n            )\n        return model","method_summary":"Build a directory model from database directory record.","original_method_code":"def _directory_model_from_db(self, record, content):\n        \"\"\"\n        Build a directory model from database directory record.\n        \"\"\"\n        model = base_directory_model(to_api_path(record['name']))\n        if content:\n            model['format'] = 'json'\n            model['content'] = list(\n                chain(\n                    self._convert_file_records(record['files']),\n                    (\n                        self._directory_model_from_db(subdir, False)\n                        for subdir in record['subdirs']\n                    ),\n                )\n            )\n        return model","method_path":"https:\/\/github.com\/quantopian\/pgcontents\/blob\/ed36268b7917332d16868208e1e565742a8753e1\/pgcontents\/pgmanager.py#L246-L262"}
{"repo_name":"quantopian\/pgcontents","method_name":"PostgresContentsManager._file_model_from_db","method_code":"def _file_model_from_db(self, record, content, format):\n        \"\"\"\"\"\"\n        \n        path = to_api_path(record['parent_name'] + record['name'])\n        model = base_model(path)\n        model['type'] = 'file'\n        model['last_modified'] = model['created'] = record['created_at']\n        if content:\n            bcontent = record['content']\n            model['content'], model['format'], model['mimetype'] = from_b64(\n                path,\n                bcontent,\n                format,\n            )\n        return model","method_summary":"Build a file model from database record.","original_method_code":"def _file_model_from_db(self, record, content, format):\n        \"\"\"\n        Build a file model from database record.\n        \"\"\"\n        # TODO: Most of this is shared with _notebook_model_from_db.\n        path = to_api_path(record['parent_name'] + record['name'])\n        model = base_model(path)\n        model['type'] = 'file'\n        model['last_modified'] = model['created'] = record['created_at']\n        if content:\n            bcontent = record['content']\n            model['content'], model['format'], model['mimetype'] = from_b64(\n                path,\n                bcontent,\n                format,\n            )\n        return model","method_path":"https:\/\/github.com\/quantopian\/pgcontents\/blob\/ed36268b7917332d16868208e1e565742a8753e1\/pgcontents\/pgmanager.py#L264-L280"}
{"repo_name":"quantopian\/pgcontents","method_name":"PostgresContentsManager._save_notebook","method_code":"def _save_notebook(self, db, model, path):\n        \"\"\"\"\"\"\n        nb_contents = from_dict(model['content'])\n        self.check_and_sign(nb_contents, path)\n        save_file(\n            db,\n            self.user_id,\n            path,\n            writes_base64(nb_contents),\n            self.crypto.encrypt,\n            self.max_file_size_bytes,\n        )\n        \n        self.validate_notebook_model(model)\n        return model.get('message')","method_summary":"Save a notebook.","original_method_code":"def _save_notebook(self, db, model, path):\n        \"\"\"\n        Save a notebook.\n\n        Returns a validation message.\n        \"\"\"\n        nb_contents = from_dict(model['content'])\n        self.check_and_sign(nb_contents, path)\n        save_file(\n            db,\n            self.user_id,\n            path,\n            writes_base64(nb_contents),\n            self.crypto.encrypt,\n            self.max_file_size_bytes,\n        )\n        # It's awkward that this writes to the model instead of returning.\n        self.validate_notebook_model(model)\n        return model.get('message')","method_path":"https:\/\/github.com\/quantopian\/pgcontents\/blob\/ed36268b7917332d16868208e1e565742a8753e1\/pgcontents\/pgmanager.py#L301-L319"}
{"repo_name":"CiscoDevNet\/webexteamssdk","method_name":"TeamMembershipsAPI.create","method_code":"def create(self, teamId, personId=None, personEmail=None,\n               isModerator=False, **request_parameters):\n        \"\"\"\"\"\"\n        check_type(teamId, basestring, may_be_none=False)\n        check_type(personId, basestring)\n        check_type(personEmail, basestring)\n        check_type(isModerator, bool)\n\n        post_data = dict_from_items_with_values(\n            request_parameters,\n            teamId=teamId,\n            personId=personId,\n            personEmail=personEmail,\n            isModerator=isModerator,\n        )\n\n        \n        json_data = self._session.post(API_ENDPOINT, json=post_data)\n\n        \n        return self._object_factory(OBJECT_TYPE, json_data)","method_summary":"Add someone to a team by Person ID or email address. Add someone to a team by Person ID or email address; optionally making them a moderator.","original_method_code":"def create(self, teamId, personId=None, personEmail=None,\n               isModerator=False, **request_parameters):\n        \"\"\"Add someone to a team by Person ID or email address.\n\n        Add someone to a team by Person ID or email address; optionally making\n        them a moderator.\n\n        Args:\n            teamId(basestring): The team ID.\n            personId(basestring): The person ID.\n            personEmail(basestring): The email address of the person.\n            isModerator(bool): Set to True to make the person a team moderator.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            TeamMembership: A TeamMembership object with the details of the\n            created team membership.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"\n        check_type(teamId, basestring, may_be_none=False)\n        check_type(personId, basestring)\n        check_type(personEmail, basestring)\n        check_type(isModerator, bool)\n\n        post_data = dict_from_items_with_values(\n            request_parameters,\n            teamId=teamId,\n            personId=personId,\n            personEmail=personEmail,\n            isModerator=isModerator,\n        )\n\n        # API request\n        json_data = self._session.post(API_ENDPOINT, json=post_data)\n\n        # Return a team membership object created from the response JSON data\n        return self._object_factory(OBJECT_TYPE, json_data)","method_path":"https:\/\/github.com\/CiscoDevNet\/webexteamssdk\/blob\/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd\/webexteamssdk\/api\/team_memberships.py#L122-L163"}
{"repo_name":"CiscoDevNet\/webexteamssdk","method_name":"TeamMembershipsAPI.update","method_code":"def update(self, membershipId, isModerator=None, **request_parameters):\n        \"\"\"\"\"\"\n        check_type(membershipId, basestring, may_be_none=False)\n        check_type(isModerator, bool)\n\n        put_data = dict_from_items_with_values(\n            request_parameters,\n            isModerator=isModerator,\n        )\n\n        \n        json_data = self._session.put(API_ENDPOINT + '\/' + membershipId,\n                                      json=put_data)\n\n        \n        return self._object_factory(OBJECT_TYPE, json_data)","method_summary":"Update a team membership, by ID.","original_method_code":"def update(self, membershipId, isModerator=None, **request_parameters):\n        \"\"\"Update a team membership, by ID.\n\n        Args:\n            membershipId(basestring): The team membership ID.\n            isModerator(bool): Set to True to make the person a team moderator.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            TeamMembership: A TeamMembership object with the updated Webex\n            Teams team-membership details.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"\n        check_type(membershipId, basestring, may_be_none=False)\n        check_type(isModerator, bool)\n\n        put_data = dict_from_items_with_values(\n            request_parameters,\n            isModerator=isModerator,\n        )\n\n        # API request\n        json_data = self._session.put(API_ENDPOINT + '\/' + membershipId,\n                                      json=put_data)\n\n        # Return a team membership object created from the response JSON data\n        return self._object_factory(OBJECT_TYPE, json_data)","method_path":"https:\/\/github.com\/CiscoDevNet\/webexteamssdk\/blob\/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd\/webexteamssdk\/api\/team_memberships.py#L188-L219"}
{"repo_name":"CiscoDevNet\/webexteamssdk","method_name":"TeamMembershipsAPI.delete","method_code":"def delete(self, membershipId):\n        \"\"\"\"\"\"\n        check_type(membershipId, basestring, may_be_none=False)\n\n        \n        self._session.delete(API_ENDPOINT + '\/' + membershipId)","method_summary":"Delete a team membership, by ID.","original_method_code":"def delete(self, membershipId):\n        \"\"\"Delete a team membership, by ID.\n\n        Args:\n            membershipId(basestring): The team membership ID.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"\n        check_type(membershipId, basestring, may_be_none=False)\n\n        # API request\n        self._session.delete(API_ENDPOINT + '\/' + membershipId)","method_path":"https:\/\/github.com\/CiscoDevNet\/webexteamssdk\/blob\/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd\/webexteamssdk\/api\/team_memberships.py#L221-L235"}
{"repo_name":"CiscoDevNet\/webexteamssdk","method_name":"get_catfact","method_code":"def get_catfact():\n    \"\"\"\"\"\"\n    response = requests.get(CAT_FACTS_URL, verify=False)\n    response.raise_for_status()\n    json_data = response.json()\n    return json_data['fact']","method_summary":"Get a cat fact from catfact.ninja and return it as a string. Functions for Soundhound, Google, IBM Watson, or other APIs can be added to create the desired functionality into this bot.","original_method_code":"def get_catfact():\n    \"\"\"Get a cat fact from catfact.ninja and return it as a string.\n\n    Functions for Soundhound, Google, IBM Watson, or other APIs can be added\n    to create the desired functionality into this bot.\n\n    \"\"\"\n    response = requests.get(CAT_FACTS_URL, verify=False)\n    response.raise_for_status()\n    json_data = response.json()\n    return json_data['fact']","method_path":"https:\/\/github.com\/CiscoDevNet\/webexteamssdk\/blob\/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd\/examples\/bot-example-webpy.py#L83-L93"}
{"repo_name":"CiscoDevNet\/webexteamssdk","method_name":"webhook.POST","method_code":"def POST(self):\n        \"\"\"\"\"\"\n        \n        json_data = web.data()\n        print(\"\\nWEBHOOK POST RECEIVED:\")\n        print(json_data, \"\\n\")\n\n        \n        webhook_obj = Webhook(json_data)\n        \n        room = api.rooms.get(webhook_obj.data.roomId)\n        \n        message = api.messages.get(webhook_obj.data.id)\n        \n        person = api.people.get(message.personId)\n\n        print(\"NEW MESSAGE IN ROOM '{}'\".format(room.title))\n        print(\"FROM '{}'\".format(person.displayName))\n        print(\"MESSAGE '{}'\\n\".format(message.text))\n\n        \n        \n        \n        me = api.people.me()\n        if message.personId == me.id:\n            \n            return 'OK'\n        else:\n            \n            if \"\/CAT\" in message.text:\n                print(\"FOUND '\/CAT'\")\n                \n                cat_fact = get_catfact()\n                print(\"SENDING CAT FACT '{}'\".format(cat_fact))\n                \n                api.messages.create(room.id, text=cat_fact)\n        return 'OK'","method_summary":"Respond to inbound webhook JSON HTTP POSTs from Webex Teams.","original_method_code":"def POST(self):\n        \"\"\"Respond to inbound webhook JSON HTTP POSTs from Webex Teams.\"\"\"\n        # Get the POST data sent from Webex Teams\n        json_data = web.data()\n        print(\"\\nWEBHOOK POST RECEIVED:\")\n        print(json_data, \"\\n\")\n\n        # Create a Webhook object from the JSON data\n        webhook_obj = Webhook(json_data)\n        # Get the room details\n        room = api.rooms.get(webhook_obj.data.roomId)\n        # Get the message details\n        message = api.messages.get(webhook_obj.data.id)\n        # Get the sender's details\n        person = api.people.get(message.personId)\n\n        print(\"NEW MESSAGE IN ROOM '{}'\".format(room.title))\n        print(\"FROM '{}'\".format(person.displayName))\n        print(\"MESSAGE '{}'\\n\".format(message.text))\n\n        # This is a VERY IMPORTANT loop prevention control step.\n        # If you respond to all messages...  You will respond to the messages\n        # that the bot posts and thereby create a loop condition.\n        me = api.people.me()\n        if message.personId == me.id:\n            # Message was sent by me (bot); do not respond.\n            return 'OK'\n        else:\n            # Message was sent by someone else; parse message and respond.\n            if \"\/CAT\" in message.text:\n                print(\"FOUND '\/CAT'\")\n                # Get a cat fact\n                cat_fact = get_catfact()\n                print(\"SENDING CAT FACT '{}'\".format(cat_fact))\n                # Post the fact to the room where the request was received\n                api.messages.create(room.id, text=cat_fact)\n        return 'OK'","method_path":"https:\/\/github.com\/CiscoDevNet\/webexteamssdk\/blob\/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd\/examples\/bot-example-webpy.py#L97-L133"}
{"repo_name":"CiscoDevNet\/webexteamssdk","method_name":"MembershipsAPI.delete","method_code":"def delete(self, membershipId):\n        \"\"\"\"\"\"\n        check_type(membershipId, basestring)\n\n        \n        self._session.delete(API_ENDPOINT + '\/' + membershipId)","method_summary":"Delete a membership, by ID.","original_method_code":"def delete(self, membershipId):\n        \"\"\"Delete a membership, by ID.\n\n        Args:\n            membershipId(basestring): The membership ID.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"\n        check_type(membershipId, basestring)\n\n        # API request\n        self._session.delete(API_ENDPOINT + '\/' + membershipId)","method_path":"https:\/\/github.com\/CiscoDevNet\/webexteamssdk\/blob\/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd\/webexteamssdk\/api\/memberships.py#L237-L251"}
{"repo_name":"CiscoDevNet\/webexteamssdk","method_name":"to_unicode","method_code":"def to_unicode(string):\n    \"\"\"\"\"\"\n    assert isinstance(string, basestring)\n    if sys.version_info[0] >= 3:\n        if isinstance(string, bytes):\n            return string.decode('utf-8')\n        else:\n            return string\n    else:\n        if isinstance(string, str):\n            return string.decode('utf-8')\n        else:\n            return string","method_summary":"Convert a string (bytes, str or unicode) to unicode.","original_method_code":"def to_unicode(string):\n    \"\"\"Convert a string (bytes, str or unicode) to unicode.\"\"\"\n    assert isinstance(string, basestring)\n    if sys.version_info[0] >= 3:\n        if isinstance(string, bytes):\n            return string.decode('utf-8')\n        else:\n            return string\n    else:\n        if isinstance(string, str):\n            return string.decode('utf-8')\n        else:\n            return string","method_path":"https:\/\/github.com\/CiscoDevNet\/webexteamssdk\/blob\/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd\/webexteamssdk\/utils.py#L59-L71"}
{"repo_name":"CiscoDevNet\/webexteamssdk","method_name":"to_bytes","method_code":"def to_bytes(string):\n    \"\"\"\"\"\"\n    assert isinstance(string, basestring)\n    if sys.version_info[0] >= 3:\n        if isinstance(string, str):\n            return string.encode('utf-8')\n        else:\n            return string\n    else:\n        if isinstance(string, unicode):\n            return string.encode('utf-8')\n        else:\n            return string","method_summary":"Convert a string (bytes, str or unicode) to bytes.","original_method_code":"def to_bytes(string):\n    \"\"\"Convert a string (bytes, str or unicode) to bytes.\"\"\"\n    assert isinstance(string, basestring)\n    if sys.version_info[0] >= 3:\n        if isinstance(string, str):\n            return string.encode('utf-8')\n        else:\n            return string\n    else:\n        if isinstance(string, unicode):\n            return string.encode('utf-8')\n        else:\n            return string","method_path":"https:\/\/github.com\/CiscoDevNet\/webexteamssdk\/blob\/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd\/webexteamssdk\/utils.py#L74-L86"}
{"repo_name":"CiscoDevNet\/webexteamssdk","method_name":"validate_base_url","method_code":"def validate_base_url(base_url):\n    \"\"\"\"\"\"\n    parsed_url = urllib.parse.urlparse(base_url)\n    if parsed_url.scheme and parsed_url.netloc:\n        return parsed_url.geturl()\n    else:\n        error_message = \"base_url must contain a valid scheme (protocol \" \\\n                        \"specifier) and network location (hostname)\"\n        raise ValueError(error_message)","method_summary":"Verify that base_url specifies a protocol and network location.","original_method_code":"def validate_base_url(base_url):\n    \"\"\"Verify that base_url specifies a protocol and network location.\"\"\"\n    parsed_url = urllib.parse.urlparse(base_url)\n    if parsed_url.scheme and parsed_url.netloc:\n        return parsed_url.geturl()\n    else:\n        error_message = \"base_url must contain a valid scheme (protocol \" \\\n                        \"specifier) and network location (hostname)\"\n        raise ValueError(error_message)","method_path":"https:\/\/github.com\/CiscoDevNet\/webexteamssdk\/blob\/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd\/webexteamssdk\/utils.py#L89-L97"}
{"repo_name":"CiscoDevNet\/webexteamssdk","method_name":"is_web_url","method_code":"def is_web_url(string):\n    \"\"\"\"\"\"\n    assert isinstance(string, basestring)\n    parsed_url = urllib.parse.urlparse(string)\n    return (\n        (\n            parsed_url.scheme.lower() == 'http'\n            or parsed_url.scheme.lower() == 'https'\n        )\n        and parsed_url.netloc\n    )","method_summary":"Check to see if string is an validly-formatted web url.","original_method_code":"def is_web_url(string):\n    \"\"\"Check to see if string is an validly-formatted web url.\"\"\"\n    assert isinstance(string, basestring)\n    parsed_url = urllib.parse.urlparse(string)\n    return (\n        (\n            parsed_url.scheme.lower() == 'http'\n            or parsed_url.scheme.lower() == 'https'\n        )\n        and parsed_url.netloc\n    )","method_path":"https:\/\/github.com\/CiscoDevNet\/webexteamssdk\/blob\/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd\/webexteamssdk\/utils.py#L100-L110"}
{"repo_name":"ibm-watson-iot\/iot-python","method_name":"parseConfigFile","method_code":"def parseConfigFile(configFilePath):\n    \"\"\"\"\"\"\n\n    try:\n        with open(configFilePath) as f:\n            data = yaml.load(f)\n    except (OSError, IOError) as e:\n        \n        reason = \"Error reading device configuration file '%s' (%s)\" % (configFilePath, e)\n        raise ConfigurationException(reason)\n\n    if \"options\" in data and \"logLevel\" in data[\"options\"]:\n        if data[\"options\"][\"logLevel\"] not in [\"error\", \"warning\", \"info\", \"debug\"]:\n            raise ConfigurationException(\"Optional setting options.logLevel must be one of error, warning, info, debug\")\n        else:\n            \n            data[\"options\"][\"logLevel\"] = logging.getLevelName(data[\"options\"][\"logLevel\"].upper())\n\n    return data","method_summary":"Parse a yaml configuration file into a Python dictionary suitable for passing to the device client constructor as the `options` parameter #","original_method_code":"def parseConfigFile(configFilePath):\n    \"\"\"\n    Parse a yaml configuration file into a Python dictionary suitable for passing to the \n    device client constructor as the `options` parameter\n    \n    # Example Configuration File\n    \n    identity:\n      appId: myApp\n    auth:\n      key: a-23gh56-sdsdajhjnee\n      token: Ab$76s)asj8_s5\n    options:\n      domain: internetofthings.ibmcloud.com\n      logLevel: error|warning|info|debug\n      mqtt:\n        port: 8883\n        transport: tcp\n        cleanStart: false\n        sessionExpiry: 3600\n        keepAlive: 60\n        sharedSubscription: false\n        caFile: \/path\/to\/certificateAuthorityFile.pem\n      http:\n        verify: true    \n    \"\"\"\n\n    try:\n        with open(configFilePath) as f:\n            data = yaml.load(f)\n    except (OSError, IOError) as e:\n        # In 3.3, IOError became an alias for OSError, and FileNotFoundError is a subclass of OSError\n        reason = \"Error reading device configuration file '%s' (%s)\" % (configFilePath, e)\n        raise ConfigurationException(reason)\n\n    if \"options\" in data and \"logLevel\" in data[\"options\"]:\n        if data[\"options\"][\"logLevel\"] not in [\"error\", \"warning\", \"info\", \"debug\"]:\n            raise ConfigurationException(\"Optional setting options.logLevel must be one of error, warning, info, debug\")\n        else:\n            # Convert log levels from string to int (we need to upper case our strings from the config)\n            data[\"options\"][\"logLevel\"] = logging.getLevelName(data[\"options\"][\"logLevel\"].upper())\n\n    return data","method_path":"https:\/\/github.com\/ibm-watson-iot\/iot-python\/blob\/195f05adce3fba4ec997017e41e02ebd85c0c4cc\/src\/wiotp\/sdk\/application\/config.py#L257-L299"}
{"repo_name":"ibm-watson-iot\/iot-python","method_name":"GatewayClient._onCommand","method_code":"def _onCommand(self, client, userdata, pahoMessage):\n        \"\"\"\"\"\"\n        try:\n            command = Command(pahoMessage, self._messageCodecs)\n        except InvalidEventException as e:\n            self.logger.critical(str(e))\n        else:\n            self.logger.debug(\"Received device command '%s'\" % (command.command))\n            if self.commandCallback:\n                self.commandCallback(command)","method_summary":"Internal callback for device command messages, parses source device from topic string and passes the information on to the registered device command callback","original_method_code":"def _onCommand(self, client, userdata, pahoMessage):\n        \"\"\"\n        Internal callback for device command messages, parses source device from topic string and\n        passes the information on to the registered device command callback\n        \"\"\"\n        try:\n            command = Command(pahoMessage, self._messageCodecs)\n        except InvalidEventException as e:\n            self.logger.critical(str(e))\n        else:\n            self.logger.debug(\"Received device command '%s'\" % (command.command))\n            if self.commandCallback:\n                self.commandCallback(command)","method_path":"https:\/\/github.com\/ibm-watson-iot\/iot-python\/blob\/195f05adce3fba4ec997017e41e02ebd85c0c4cc\/src\/wiotp\/sdk\/gateway\/client.py#L111-L123"}
{"repo_name":"ibm-watson-iot\/iot-python","method_name":"GatewayClient._onDeviceCommand","method_code":"def _onDeviceCommand(self, client, userdata, pahoMessage):\n        \"\"\"\"\"\"\n        try:\n            command = Command(pahoMessage, self._messageCodecs)\n        except InvalidEventException as e:\n            self.logger.critical(str(e))\n        else:\n            self.logger.debug(\"Received gateway command '%s'\" % (command.command))\n            if self.deviceCommandCallback:\n                self.deviceCommandCallback(command)","method_summary":"Internal callback for gateway command messages, parses source device from topic string and passes the information on to the registered device command callback","original_method_code":"def _onDeviceCommand(self, client, userdata, pahoMessage):\n        \"\"\"\n        Internal callback for gateway command messages, parses source device from topic string and\n        passes the information on to the registered device command callback\n        \"\"\"\n        try:\n            command = Command(pahoMessage, self._messageCodecs)\n        except InvalidEventException as e:\n            self.logger.critical(str(e))\n        else:\n            self.logger.debug(\"Received gateway command '%s'\" % (command.command))\n            if self.deviceCommandCallback:\n                self.deviceCommandCallback(command)","method_path":"https:\/\/github.com\/ibm-watson-iot\/iot-python\/blob\/195f05adce3fba4ec997017e41e02ebd85c0c4cc\/src\/wiotp\/sdk\/gateway\/client.py#L125-L137"}
{"repo_name":"ibm-watson-iot\/iot-python","method_name":"GatewayClient._onMessageNotification","method_code":"def _onMessageNotification(self, client, userdata, pahoMessage):\n        \"\"\"\"\"\"\n        try:\n            note = Notification(pahoMessage, self._messageCodecs)\n        except InvalidEventException as e:\n            self.logger.critical(str(e))\n        else:\n            self.logger.debug(\"Received Notification\")\n            if self.notificationCallback:\n                self.notificationCallback(note)","method_summary":"Internal callback for gateway notification messages, parses source device from topic string and passes the information on to the registered device command callback","original_method_code":"def _onMessageNotification(self, client, userdata, pahoMessage):\n        \"\"\"\n        Internal callback for gateway notification messages, parses source device from topic string and\n        passes the information on to the registered device command callback\n        \"\"\"\n        try:\n            note = Notification(pahoMessage, self._messageCodecs)\n        except InvalidEventException as e:\n            self.logger.critical(str(e))\n        else:\n            self.logger.debug(\"Received Notification\")\n            if self.notificationCallback:\n                self.notificationCallback(note)","method_path":"https:\/\/github.com\/ibm-watson-iot\/iot-python\/blob\/195f05adce3fba4ec997017e41e02ebd85c0c4cc\/src\/wiotp\/sdk\/gateway\/client.py#L139-L151"}
{"repo_name":"ibm-watson-iot\/iot-python","method_name":"Connectors.find","method_code":"def find(self, nameFilter=None, typeFilter=None, enabledFilter=None, serviceId=None):\n        \"\"\"\"\"\"\n\n        queryParms = {}\n        if nameFilter:\n            queryParms[\"name\"] = nameFilter\n        if typeFilter:\n            queryParms[\"type\"] = typeFilter\n        if enabledFilter:\n            queryParms[\"enabled\"] = enabledFilter\n        if serviceId:\n            queryParms[\"serviceId\"] = serviceId\n\n        return IterableConnectorList(self._apiClient, filters=queryParms)","method_summary":"Gets the list of Historian connectors, they are used to configure the Watson IoT Platform to store IoT data in compatible services.","original_method_code":"def find(self, nameFilter=None, typeFilter=None, enabledFilter=None, serviceId=None):\n        \"\"\"\n        Gets the list of Historian connectors, they are used to configure the Watson IoT Platform to store IoT data in compatible services.\n        \n        Parameters:\n        \n            - nameFilter(string) -      Filter the results by the specified name\n            - typeFilter(string) -      Filter the results by the specified type, Available values : cloudant, eventstreams\n            - enabledFilter(boolean) -  Filter the results by the enabled flag \n            - serviceId(string) -       Filter the results by the service id\n            - limit(number) -           Max number of results returned, defaults 25\n            - bookmark(string) -        used for paging through results\n        \n        Throws APIException on failure.\n        \"\"\"\n\n        queryParms = {}\n        if nameFilter:\n            queryParms[\"name\"] = nameFilter\n        if typeFilter:\n            queryParms[\"type\"] = typeFilter\n        if enabledFilter:\n            queryParms[\"enabled\"] = enabledFilter\n        if serviceId:\n            queryParms[\"serviceId\"] = serviceId\n\n        return IterableConnectorList(self._apiClient, filters=queryParms)","method_path":"https:\/\/github.com\/ibm-watson-iot\/iot-python\/blob\/195f05adce3fba4ec997017e41e02ebd85c0c4cc\/src\/wiotp\/sdk\/api\/dsc\/connectors.py#L159-L185"}
{"repo_name":"ibm-watson-iot\/iot-python","method_name":"Connectors.create","method_code":"def create(self, name, serviceId, timezone, description, enabled):\n        \"\"\"\"\"\"\n\n        connector = {\n            \"name\": name,\n            \"description\": description,\n            \"serviceId\": serviceId,\n            \"timezone\": timezone,\n            \"enabled\": enabled,\n        }\n\n        url = \"api\/v0002\/historianconnectors\"\n\n        r = self._apiClient.post(url, data=connector)\n        if r.status_code == 201:\n            return Connector(apiClient=self._apiClient, **r.json())\n        else:\n            raise ApiException(r)","method_summary":"Create a connector for the organization in the Watson IoT Platform. The connector must reference the target service that the Watson IoT Platform will store the IoT data in.","original_method_code":"def create(self, name, serviceId, timezone, description, enabled):\n        \"\"\"\n        Create a connector for the organization in the Watson IoT Platform. \n        The connector must reference the target service that the Watson IoT Platform will store the IoT data in.\n        Parameters:\n            - name (string) - Name of the service\n            - serviceId (string) - must be either eventstreams or cloudant\n            - timezone (string) - \n            - description (string) - description of the service\n            - enabled (boolean) - enabled\n        Throws APIException on failure\n        \"\"\"\n\n        connector = {\n            \"name\": name,\n            \"description\": description,\n            \"serviceId\": serviceId,\n            \"timezone\": timezone,\n            \"enabled\": enabled,\n        }\n\n        url = \"api\/v0002\/historianconnectors\"\n\n        r = self._apiClient.post(url, data=connector)\n        if r.status_code == 201:\n            return Connector(apiClient=self._apiClient, **r.json())\n        else:\n            raise ApiException(r)","method_path":"https:\/\/github.com\/ibm-watson-iot\/iot-python\/blob\/195f05adce3fba4ec997017e41e02ebd85c0c4cc\/src\/wiotp\/sdk\/api\/dsc\/connectors.py#L187-L214"}
{"repo_name":"ibm-watson-iot\/iot-python","method_name":"Connectors.update","method_code":"def update(self, connectorId, name, description, timezone, enabled):\n        \"\"\"\"\"\"\n\n        url = \"api\/v0002\/historianconnectors\/%s\" % (connectorId)\n\n        connectorBody = {}\n        connectorBody[\"name\"] = name\n        connectorBody[\"description\"] = description\n        connectorBody[\"timezone\"] = timezone\n        connectorBody[\"enabled\"] = enabled\n\n        r = self._apiClient.put(url, data=connectorBody)\n        if r.status_code == 200:\n            return Connector(apiClient=self._apiClient, **r.json())\n        else:\n            raise ApiException(r)","method_summary":"Updates the connector with the specified uuid. if description is empty, the existing description will be removed.","original_method_code":"def update(self, connectorId, name, description, timezone, enabled):\n        \"\"\"\n        Updates the connector with the specified uuid.\n        if description is empty, the existing description will be removed.\n        Parameters:\n            - connector (String), Connnector Id which is a UUID\n            - name (string) - Name of the service\n            - timezone (json object) - Should have a valid structure for the service type.\n            - description (string) - description of the service\n            - enabled (boolean) - enabled\n        Throws APIException on failure.\n\n        \"\"\"\n\n        url = \"api\/v0002\/historianconnectors\/%s\" % (connectorId)\n\n        connectorBody = {}\n        connectorBody[\"name\"] = name\n        connectorBody[\"description\"] = description\n        connectorBody[\"timezone\"] = timezone\n        connectorBody[\"enabled\"] = enabled\n\n        r = self._apiClient.put(url, data=connectorBody)\n        if r.status_code == 200:\n            return Connector(apiClient=self._apiClient, **r.json())\n        else:\n            raise ApiException(r)","method_path":"https:\/\/github.com\/ibm-watson-iot\/iot-python\/blob\/195f05adce3fba4ec997017e41e02ebd85c0c4cc\/src\/wiotp\/sdk\/api\/dsc\/connectors.py#L216-L242"}
{"repo_name":"ibm-watson-iot\/iot-python","method_name":"ServiceBindings.find","method_code":"def find(self, nameFilter=None, typeFilter=None, bindingModeFilter=None, boundFilter=None):\n        \"\"\"\"\"\"\n\n        queryParms = {}\n        if nameFilter:\n            queryParms[\"name\"] = nameFilter\n        if typeFilter:\n            queryParms[\"type\"] = typeFilter\n        if bindingModeFilter:\n            queryParms[\"bindingMode\"] = bindingModeFilter\n        if boundFilter:\n            queryParms[\"bound\"] = boundFilter\n\n        return IterableServiceBindingsList(self._apiClient, filters=queryParms)","method_summary":"Gets the list of services that the Watson IoT Platform can connect to. The list can include a mixture of services that are either bound or unbound.","original_method_code":"def find(self, nameFilter=None, typeFilter=None, bindingModeFilter=None, boundFilter=None):\n        \"\"\"\n        Gets the list of services that the Watson IoT Platform can connect to. \n        The list can include a mixture of services that are either bound or unbound.\n        \n        Parameters:\n        \n            - nameFilter(string) - Filter the results by the specified name\n            - typeFilter(string) - Filter the results by the specified type, Available values : cloudant, eventstreams\n            - bindingModeFilter(string) - Filter the results by the specified binding mode, Available values : automatic, manual\n            - boundFilter(boolean) - Filter the results by the bound flag \n        \n        Throws APIException on failure.\n        \"\"\"\n\n        queryParms = {}\n        if nameFilter:\n            queryParms[\"name\"] = nameFilter\n        if typeFilter:\n            queryParms[\"type\"] = typeFilter\n        if bindingModeFilter:\n            queryParms[\"bindingMode\"] = bindingModeFilter\n        if boundFilter:\n            queryParms[\"bound\"] = boundFilter\n\n        return IterableServiceBindingsList(self._apiClient, filters=queryParms)","method_path":"https:\/\/github.com\/ibm-watson-iot\/iot-python\/blob\/195f05adce3fba4ec997017e41e02ebd85c0c4cc\/src\/wiotp\/sdk\/api\/services\/__init__.py#L207-L232"}
{"repo_name":"ibm-watson-iot\/iot-python","method_name":"ServiceBindings.create","method_code":"def create(self, serviceBinding):\n        \"\"\"\"\"\"\n        if not isinstance(serviceBinding, ServiceBindingCreateRequest):\n            if serviceBinding[\"type\"] == \"cloudant\":\n                serviceBinding = CloudantServiceBindingCreateRequest(**serviceBinding)\n            elif serviceBinding[\"type\"] == \"eventstreams\":\n                serviceBinding = EventStreamsServiceBindingCreateRequest(**serviceBinding)\n            else:\n                raise Exception(\"Unsupported service binding type\")\n\n        url = \"api\/v0002\/s2s\/services\"\n\n        r = self._apiClient.post(url, data=serviceBinding)\n        if r.status_code == 201:\n            return ServiceBinding(**r.json())\n        else:\n            raise ApiException(r)","method_summary":"Create a new external service. The service must include all of the details required to connect and authenticate to the external service in the credentials property.","original_method_code":"def create(self, serviceBinding):\n        \"\"\"\n        Create a new external service. \n        The service must include all of the details required to connect \n        and authenticate to the external service in the credentials property. \n        Parameters:\n            - serviceName (string) - Name of the service\n            - serviceType (string) - must be either eventstreams or cloudant\n            - credentials (json object) - Should have a valid structure for the service type.\n            - description (string) - description of the service\n        Throws APIException on failure\n        \"\"\"\n        if not isinstance(serviceBinding, ServiceBindingCreateRequest):\n            if serviceBinding[\"type\"] == \"cloudant\":\n                serviceBinding = CloudantServiceBindingCreateRequest(**serviceBinding)\n            elif serviceBinding[\"type\"] == \"eventstreams\":\n                serviceBinding = EventStreamsServiceBindingCreateRequest(**serviceBinding)\n            else:\n                raise Exception(\"Unsupported service binding type\")\n\n        url = \"api\/v0002\/s2s\/services\"\n\n        r = self._apiClient.post(url, data=serviceBinding)\n        if r.status_code == 201:\n            return ServiceBinding(**r.json())\n        else:\n            raise ApiException(r)","method_path":"https:\/\/github.com\/ibm-watson-iot\/iot-python\/blob\/195f05adce3fba4ec997017e41e02ebd85c0c4cc\/src\/wiotp\/sdk\/api\/services\/__init__.py#L234-L260"}
{"repo_name":"ibm-watson-iot\/iot-python","method_name":"ServiceBindings.update","method_code":"def update(self, serviceId, serviceName, credentials, description):\n        \"\"\"\"\"\"\n\n        url = \"api\/v0002\/s2s\/services\/%s\" % (serviceId)\n\n        serviceBody = {}\n        serviceBody[\"name\"] = serviceName\n        serviceBody[\"description\"] = description\n        serviceBody[\"credentials\"] = credentials\n\n        r = self._apiClient.put(url, data=serviceBody)\n        if r.status_code == 200:\n            return ServiceBinding(**r.json())\n        else:\n            raise ApiException(r)","method_summary":"Updates the service with the specified id. if description is empty, the existing description will be removed.","original_method_code":"def update(self, serviceId, serviceName, credentials, description):\n        \"\"\"\n        Updates the service with the specified id.\n        if description is empty, the existing description will be removed.\n        Parameters:\n            - serviceId (String), Service Id which is a UUID\n            - serviceName (string), name of service\n            - credentials (json), json object of credentials\n            - description - description of the service\n        Throws APIException on failure.\n\n        \"\"\"\n\n        url = \"api\/v0002\/s2s\/services\/%s\" % (serviceId)\n\n        serviceBody = {}\n        serviceBody[\"name\"] = serviceName\n        serviceBody[\"description\"] = description\n        serviceBody[\"credentials\"] = credentials\n\n        r = self._apiClient.put(url, data=serviceBody)\n        if r.status_code == 200:\n            return ServiceBinding(**r.json())\n        else:\n            raise ApiException(r)","method_path":"https:\/\/github.com\/ibm-watson-iot\/iot-python\/blob\/195f05adce3fba4ec997017e41e02ebd85c0c4cc\/src\/wiotp\/sdk\/api\/services\/__init__.py#L262-L286"}
{"repo_name":"rocky\/python3-trepan","method_name":"PrintProcessor.event_processor","method_code":"def event_processor(self, frame, event, arg):\n        'A simple event processor that prints out events.'\n        out = self.debugger.intf[-1].output\n        lineno = frame.f_lineno\n        filename = self.core.canonic_filename(frame)\n        filename = self.core.filename(filename)\n        if not out:\n            print(\"%s - %s:%d\" % (event, filename, lineno))\n        else:\n            out.write(\"%s - %s:%d\" % (event, filename, lineno))\n            if arg is not None:\n                out.writeline(', %s ' % repr(arg))\n            else:\n                out.writeline('')\n                pass\n            pass\n        return self.event_processor","method_summary":"A simple event processor that prints out events.","original_method_code":"def event_processor(self, frame, event, arg):\n        'A simple event processor that prints out events.'\n        out = self.debugger.intf[-1].output\n        lineno = frame.f_lineno\n        filename = self.core.canonic_filename(frame)\n        filename = self.core.filename(filename)\n        if not out:\n            print(\"%s - %s:%d\" % (event, filename, lineno))\n        else:\n            out.write(\"%s - %s:%d\" % (event, filename, lineno))\n            if arg is not None:\n                out.writeline(', %s ' % repr(arg))\n            else:\n                out.writeline('')\n                pass\n            pass\n        return self.event_processor","method_path":"https:\/\/github.com\/rocky\/python3-trepan\/blob\/14e91bc0acce090d67be145b1ac040cab92ac5f3\/trepan\/processor\/trace.py#L36-L52"}
{"repo_name":"rocky\/python3-trepan","method_name":"InfoPC.run","method_code":"def run(self, args):\n        \"\"\"\"\"\"\n        mainfile = self.core.filename(None)\n        if self.core.is_running():\n            curframe = self.proc.curframe\n            if curframe:\n                line_no = inspect.getlineno(curframe)\n                offset  = curframe.f_lasti\n                self.msg(\"PC offset is %d.\" % offset)\n                offset = max(offset, 0)\n                code = curframe.f_code\n                co_code = code.co_code\n                disassemble_bytes(self.msg, self.msg_nocr,\n                                  co_code, offset, line_no, line_no-1, line_no+1,\n                                  constants=code.co_consts, cells=code.co_cellvars,\n                                  varnames=code.co_varnames, freevars=code.co_freevars,\n                                  linestarts=dict(findlinestarts(code)),\n                                  end_offset=offset+10)\n                pass\n            pass\n        else:\n            if mainfile:\n                part1 = \"Python program '%s'\" % mainfile\n                msg   = \"is not currently running. \"\n                self.msg(Mmisc.wrapped_lines(part1, msg,\n                                             self.settings['width']))\n            else:\n                self.msg('No Python program is currently running.')\n                pass\n            self.msg(self.core.execution_status)\n            pass\n        return False","method_summary":"Program counter.","original_method_code":"def run(self, args):\n        \"\"\"Program counter.\"\"\"\n        mainfile = self.core.filename(None)\n        if self.core.is_running():\n            curframe = self.proc.curframe\n            if curframe:\n                line_no = inspect.getlineno(curframe)\n                offset  = curframe.f_lasti\n                self.msg(\"PC offset is %d.\" % offset)\n                offset = max(offset, 0)\n                code = curframe.f_code\n                co_code = code.co_code\n                disassemble_bytes(self.msg, self.msg_nocr,\n                                  co_code, offset, line_no, line_no-1, line_no+1,\n                                  constants=code.co_consts, cells=code.co_cellvars,\n                                  varnames=code.co_varnames, freevars=code.co_freevars,\n                                  linestarts=dict(findlinestarts(code)),\n                                  end_offset=offset+10)\n                pass\n            pass\n        else:\n            if mainfile:\n                part1 = \"Python program '%s'\" % mainfile\n                msg   = \"is not currently running. \"\n                self.msg(Mmisc.wrapped_lines(part1, msg,\n                                             self.settings['width']))\n            else:\n                self.msg('No Python program is currently running.')\n                pass\n            self.msg(self.core.execution_status)\n            pass\n        return False","method_path":"https:\/\/github.com\/rocky\/python3-trepan\/blob\/14e91bc0acce090d67be145b1ac040cab92ac5f3\/trepan\/processor\/command\/info_subcmd\/pc.py#L42-L73"}
{"repo_name":"rocky\/python3-trepan","method_name":"runcode","method_code":"def runcode(obj, code_obj):\n    \"\"\"\"\"\"\n    try:\n        exec(code_obj, obj.locals, obj.globals)\n    except SystemExit:\n        raise\n    except:\n        obj.showtraceback()\n    else:\n        if code.softspace(sys.stdout, 0):\n            print()\n            pass\n        pass\n    return","method_summary":"Execute a code object. When an exception occurs, self.showtraceback() is called to display a traceback. All exceptions are caught except SystemExit, which is reraised. A note about","original_method_code":"def runcode(obj, code_obj):\n    \"\"\"Execute a code object.\n\n    When an exception occurs, self.showtraceback() is called to\n    display a traceback.  All exceptions are caught except\n    SystemExit, which is reraised.\n\n    A note about KeyboardInterrupt: this exception may occur\n    elsewhere in this code, and may not always be caught.  The\n    caller should be prepared to deal with it.\n\n    \"\"\"\n    try:\n        exec(code_obj, obj.locals, obj.globals)\n    except SystemExit:\n        raise\n    except:\n        obj.showtraceback()\n    else:\n        if code.softspace(sys.stdout, 0):\n            print()\n            pass\n        pass\n    return","method_path":"https:\/\/github.com\/rocky\/python3-trepan\/blob\/14e91bc0acce090d67be145b1ac040cab92ac5f3\/trepan\/processor\/command\/ipython.py#L167-L190"}
{"repo_name":"rocky\/python3-trepan","method_name":"adjust_frame","method_code":"def adjust_frame(proc_obj, name, pos, absolute_pos):\n    \"\"\"\"\"\"\n    if not proc_obj.curframe:\n        proc_obj.errmsg(\"No stack.\")\n        return\n\n    \n    \n    if absolute_pos:\n        if pos >= 0:\n            pos = frame_num(proc_obj, pos)\n        else:\n            pos = -pos - 1\n            pass\n    else:\n        pos += proc_obj.curindex\n        pass\n\n    if pos < 0:\n        proc_obj.errmsg(\"Adjusting would put us beyond the oldest frame.\")\n        return\n    elif pos >= len(proc_obj.stack):\n        proc_obj.errmsg(\"Adjusting would put us beyond the newest frame.\")\n        return\n\n    proc_obj.curindex = pos\n    proc_obj.curframe = proc_obj.stack[proc_obj.curindex][0]\n    proc_obj.location()\n    proc_obj.list_lineno   = None\n    proc_obj.list_offset   = proc_obj.curframe.f_lasti\n    proc_obj.list_object   = proc_obj.curframe\n    proc_obj.list_filename = proc_obj.curframe.f_code.co_filename\n\n    return","method_summary":"Adjust stack frame by pos positions. If absolute_pos then pos is an absolute number. Otherwise it is a relative number. A negative number indexes from the other end.","original_method_code":"def adjust_frame(proc_obj, name, pos, absolute_pos):\n    \"\"\"Adjust stack frame by pos positions. If absolute_pos then\n    pos is an absolute number. Otherwise it is a relative number.\n\n    A negative number indexes from the other end.\"\"\"\n    if not proc_obj.curframe:\n        proc_obj.errmsg(\"No stack.\")\n        return\n\n    # Below we remove any negativity. At the end, pos will be\n    # the new value of proc_obj.curindex.\n    if absolute_pos:\n        if pos >= 0:\n            pos = frame_num(proc_obj, pos)\n        else:\n            pos = -pos - 1\n            pass\n    else:\n        pos += proc_obj.curindex\n        pass\n\n    if pos < 0:\n        proc_obj.errmsg(\"Adjusting would put us beyond the oldest frame.\")\n        return\n    elif pos >= len(proc_obj.stack):\n        proc_obj.errmsg(\"Adjusting would put us beyond the newest frame.\")\n        return\n\n    proc_obj.curindex = pos\n    proc_obj.curframe = proc_obj.stack[proc_obj.curindex][0]\n    proc_obj.location()\n    proc_obj.list_lineno   = None\n    proc_obj.list_offset   = proc_obj.curframe.f_lasti\n    proc_obj.list_object   = proc_obj.curframe\n    proc_obj.list_filename = proc_obj.curframe.f_code.co_filename\n\n    return","method_path":"https:\/\/github.com\/rocky\/python3-trepan\/blob\/14e91bc0acce090d67be145b1ac040cab92ac5f3\/trepan\/processor\/frame.py#L45-L81"}
{"repo_name":"rocky\/python3-trepan","method_name":"parse_list_cmd","method_code":"def parse_list_cmd(proc, args, listsize=10):\n    \"\"\"\"\"\"\n\n    text = proc.current_command[len(args[0])+1:].strip()\n\n    if text in frozenset(('', '.', '+', '-')):\n        if text == '.':\n            location = resolve_location(proc, '.')\n            return location.path, location.line_number, listsize\n        else:\n            if proc.list_lineno is None:\n                proc.errmsg(\"Don't have previous list location\")\n                return INVALID_PARSE_LIST\n            filename = proc.list_filename\n            if text == '+':\n                first = max(1, proc.list_lineno + listsize)\n            elif text == '-':\n                if proc.list_lineno == 1 + listsize:\n                    proc.errmsg(\"Already at start of %s.\" % proc.list_filename)\n                    return INVALID_PARSE_LIST\n                first = max(1, proc.list_lineno - (2*listsize) - 1)\n            elif text == '':\n                \n                first = proc.list_lineno + 1\n        last = first + listsize - 1\n        return filename, first, last\n    else:\n        try:\n            list_range = build_range(text)\n        except LocationError as e:\n            proc.errmsg(\"Error in parsing list range at or around:\")\n            proc.errmsg(e.text)\n            proc.errmsg(e.text_cursor)\n            return INVALID_PARSE_LIST\n        except ScannerError as e:\n            proc.errmsg(\"Lexical error in parsing list range at or around:\")\n            proc.errmsg(e.text)\n            proc.errmsg(e.text_cursor)\n            return INVALID_PARSE_LIST\n\n        if list_range.first is None:\n            \n            assert isinstance(list_range.last, Location)\n            location = resolve_location(proc, list_range.last)\n            if not location:\n                return INVALID_PARSE_LIST\n            last     = location.line_number\n            first    = max(1, last - listsize)\n            return location.path, first, last\n        elif isinstance(list_range.first, int):\n            first    = list_range.first\n            location = resolve_location(proc, list_range.last)\n            if not location:\n                return INVALID_PARSE_LIST\n            filename = location.path\n            last     = location.line_number\n            if last < first:\n                \n                last = first + last\n            return location.path, first, last\n        else:\n            \n            assert isinstance(list_range.first, Location)\n            location = resolve_location(proc, list_range.first)\n            if not location:\n                return INVALID_PARSE_LIST\n            first    = location.line_number\n            last     = list_range.last\n            if location.method:\n                first -= listsize \/\/ 2\n            if isinstance(last, str):\n                \n                assert last[0] == '+'\n                last = first + int(last[1:])\n            elif not last:\n                last = first + listsize\n            elif last < first:\n                \n                last = first + last\n\n            return location.path, first, last\n        pass\n    return","method_summary":"Parses arguments for the \"list\" command and returns the","original_method_code":"def parse_list_cmd(proc, args, listsize=10):\n    \"\"\"Parses arguments for the \"list\" command and returns the tuple:\n    (filename, first line number, last line number)\n    or sets these to None if there was some problem.\"\"\"\n\n    text = proc.current_command[len(args[0])+1:].strip()\n\n    if text in frozenset(('', '.', '+', '-')):\n        if text == '.':\n            location = resolve_location(proc, '.')\n            return location.path, location.line_number, listsize\n        else:\n            if proc.list_lineno is None:\n                proc.errmsg(\"Don't have previous list location\")\n                return INVALID_PARSE_LIST\n            filename = proc.list_filename\n            if text == '+':\n                first = max(1, proc.list_lineno + listsize)\n            elif text == '-':\n                if proc.list_lineno == 1 + listsize:\n                    proc.errmsg(\"Already at start of %s.\" % proc.list_filename)\n                    return INVALID_PARSE_LIST\n                first = max(1, proc.list_lineno - (2*listsize) - 1)\n            elif text == '':\n                # Continue from where we last left off\n                first = proc.list_lineno + 1\n        last = first + listsize - 1\n        return filename, first, last\n    else:\n        try:\n            list_range = build_range(text)\n        except LocationError as e:\n            proc.errmsg(\"Error in parsing list range at or around:\")\n            proc.errmsg(e.text)\n            proc.errmsg(e.text_cursor)\n            return INVALID_PARSE_LIST\n        except ScannerError as e:\n            proc.errmsg(\"Lexical error in parsing list range at or around:\")\n            proc.errmsg(e.text)\n            proc.errmsg(e.text_cursor)\n            return INVALID_PARSE_LIST\n\n        if list_range.first is None:\n            # Last must have been given\n            assert isinstance(list_range.last, Location)\n            location = resolve_location(proc, list_range.last)\n            if not location:\n                return INVALID_PARSE_LIST\n            last     = location.line_number\n            first    = max(1, last - listsize)\n            return location.path, first, last\n        elif isinstance(list_range.first, int):\n            first    = list_range.first\n            location = resolve_location(proc, list_range.last)\n            if not location:\n                return INVALID_PARSE_LIST\n            filename = location.path\n            last     = location.line_number\n            if last < first:\n                # Treat as a count rather than an absolute location\n                last = first + last\n            return location.path, first, last\n        else:\n            # First is location. Last may be empty or a number\n            assert isinstance(list_range.first, Location)\n            location = resolve_location(proc, list_range.first)\n            if not location:\n                return INVALID_PARSE_LIST\n            first    = location.line_number\n            last     = list_range.last\n            if location.method:\n                first -= listsize \/\/ 2\n            if isinstance(last, str):\n                # Is an offset +number\n                assert last[0] == '+'\n                last = first + int(last[1:])\n            elif not last:\n                last = first + listsize\n            elif last < first:\n                # Treat as a count rather than an absolute location\n                last = first + last\n\n            return location.path, first, last\n        pass\n    return","method_path":"https:\/\/github.com\/rocky\/python3-trepan\/blob\/14e91bc0acce090d67be145b1ac040cab92ac5f3\/trepan\/processor\/cmdlist.py#L23-L107"}
{"repo_name":"rocky\/python3-trepan","method_name":"DebuggerUserInput.readline","method_code":"def readline(self, use_raw=None, prompt=''):\n        \"\"\"\"\"\"\n        \n        if use_raw is None:\n            use_raw = self.use_raw\n            pass\n        if use_raw:\n            try:\n                inp = input(prompt)\n                \n                return inp\n            except ValueError:\n                raise EOFError\n            pass\n\n        else:\n            line = self.input.readline()\n            if not line: raise EOFError\n            return line.rstrip(\"\\n\")\n        pass","method_summary":"Read a line of input. EOFError will be raised on EOF.","original_method_code":"def readline(self, use_raw=None, prompt=''):\n        \"\"\"Read a line of input. EOFError will be raised on EOF.\n\n        Note: some user interfaces may decide to arrange to call\n        DebuggerOutput.write() first with the prompt rather than pass\n        it here.. If `use_raw' is set raw_input() will be used in that\n        is supported by the specific input input. If this option is\n        left None as is normally expected the value from the class\n        initialization is used.\n        \"\"\"\n        # FIXME we don't do command completion.\n        if use_raw is None:\n            use_raw = self.use_raw\n            pass\n        if use_raw:\n            try:\n                inp = input(prompt)\n                # import pdb; pdb.set_trace()\n                return inp\n            except ValueError:\n                raise EOFError\n            pass\n\n        else:\n            line = self.input.readline()\n            if not line: raise EOFError\n            return line.rstrip(\"\\n\")\n        pass","method_path":"https:\/\/github.com\/rocky\/python3-trepan\/blob\/14e91bc0acce090d67be145b1ac040cab92ac5f3\/trepan\/inout\/input.py#L92-L119"}
{"repo_name":"rocky\/python3-trepan","method_name":"Trepan.run_call","method_code":"def run_call(self, func, start_opts=None, *args, **kwds):\n        \"\"\"\"\"\"\n        res = None\n        self.core.start(opts=start_opts)\n        try:\n            res = func(*args, **kwds)\n        except DebuggerQuit:\n            pass\n        finally:\n            self.core.stop()\n        return res","method_summary":"Run debugger on function","original_method_code":"def run_call(self, func, start_opts=None, *args, **kwds):\n        \"\"\" Run debugger on function call: `func(*args, **kwds)'\n\n        See also `run_eval' if what you want to run is an eval'able\n        expression have that result returned and `run' if you want to\n        debug a statment via exec.\n        \"\"\"\n        res = None\n        self.core.start(opts=start_opts)\n        try:\n            res = func(*args, **kwds)\n        except DebuggerQuit:\n            pass\n        finally:\n            self.core.stop()\n        return res","method_path":"https:\/\/github.com\/rocky\/python3-trepan\/blob\/14e91bc0acce090d67be145b1ac040cab92ac5f3\/trepan\/debugger.py#L134-L149"}
{"repo_name":"rocky\/python3-trepan","method_name":"Trepan.run_eval","method_code":"def run_eval(self, expr, start_opts=None, globals_=None, locals_=None):\n        \"\"\"\"\"\"\n        if globals_ is None:\n            globals_ = globals()\n        if locals_ is None:\n            locals_ = globals_\n        if not isinstance(expr, types.CodeType):\n            self.eval_string = expr\n            expr = expr+'\\n'\n            pass\n        retval = None\n        self.core.start(start_opts)\n        try:\n            retval = eval(expr, globals_, locals_)\n        except DebuggerQuit:\n            pass\n        finally:\n            pyficache.remove_remap_file('<string>')\n            self.core.stop()\n        return retval","method_summary":"Run debugger on string `expr' which will executed via the built-in Python","original_method_code":"def run_eval(self, expr, start_opts=None, globals_=None, locals_=None):\n        \"\"\" Run debugger on string `expr' which will executed via the\n        built-in Python function: eval; `globals_' and `locals_' are\n        the dictionaries to use for local and global variables. If\n        `globals' is not given, __main__.__dict__ (the current global\n        variables) is used. If `locals_' is not given, it becomes a\n        copy of `globals_'.\n\n        See also `run_call' if what you to debug a function call and\n        `run' if you want to debug general Python statements.\n        \"\"\"\n        if globals_ is None:\n            globals_ = globals()\n        if locals_ is None:\n            locals_ = globals_\n        if not isinstance(expr, types.CodeType):\n            self.eval_string = expr\n            expr = expr+'\\n'\n            pass\n        retval = None\n        self.core.start(start_opts)\n        try:\n            retval = eval(expr, globals_, locals_)\n        except DebuggerQuit:\n            pass\n        finally:\n            pyficache.remove_remap_file('<string>')\n            self.core.stop()\n        return retval","method_path":"https:\/\/github.com\/rocky\/python3-trepan\/blob\/14e91bc0acce090d67be145b1ac040cab92ac5f3\/trepan\/debugger.py#L151-L179"}
{"repo_name":"rocky\/python3-trepan","method_name":"run_hooks","method_code":"def run_hooks(obj, hooks, *args):\n    \"\"\"\"\"\"\n    for hook in hooks:\n        if hook(obj, *args): return True\n        pass\n    return False","method_summary":"Run each function in `hooks' with args","original_method_code":"def run_hooks(obj, hooks, *args):\n    \"\"\"Run each function in `hooks' with args\"\"\"\n    for hook in hooks:\n        if hook(obj, *args): return True\n        pass\n    return False","method_path":"https:\/\/github.com\/rocky\/python3-trepan\/blob\/14e91bc0acce090d67be145b1ac040cab92ac5f3\/trepan\/processor\/cmdproc.py#L107-L112"}
{"repo_name":"rocky\/python3-trepan","method_name":"print_source_location_info","method_code":"def print_source_location_info(print_fn, filename, lineno, fn_name=None,\n                               f_lasti=None, remapped_file=None):\n    \"\"\"\"\"\"\n    if remapped_file:\n        mess = '(%s:%s remapped %s' % (remapped_file, lineno, filename)\n    else:\n        mess = '(%s:%s' % (filename, lineno)\n    if f_lasti and f_lasti != -1:\n        mess += ' @%d' % f_lasti\n        pass\n    mess += '):'\n    if fn_name and fn_name != '?':\n        mess += \" %s\" % fn_name\n        pass\n    print_fn(mess)\n    return","method_summary":"Print out a source location , e.g. the first line in line","original_method_code":"def print_source_location_info(print_fn, filename, lineno, fn_name=None,\n                               f_lasti=None, remapped_file=None):\n    \"\"\"Print out a source location , e.g. the first line in\n    line in:\n        (\/tmp.py:2 @21):  <module>\n        L -- 2 import sys,os\n        (trepan3k)\n    \"\"\"\n    if remapped_file:\n        mess = '(%s:%s remapped %s' % (remapped_file, lineno, filename)\n    else:\n        mess = '(%s:%s' % (filename, lineno)\n    if f_lasti and f_lasti != -1:\n        mess += ' @%d' % f_lasti\n        pass\n    mess += '):'\n    if fn_name and fn_name != '?':\n        mess += \" %s\" % fn_name\n        pass\n    print_fn(mess)\n    return","method_path":"https:\/\/github.com\/rocky\/python3-trepan\/blob\/14e91bc0acce090d67be145b1ac040cab92ac5f3\/trepan\/processor\/cmdproc.py#L143-L163"}
{"repo_name":"ToucanToco\/toucan-data-sdk","method_name":"add_aggregation_columns","method_code":"def add_aggregation_columns(\n        df, *,\n        group_cols: Union[str, List[str]],\n        aggregations: Dict[str, Agg]\n):\n    \"\"\"\"\"\"\n    group = df.groupby(group_cols)\n    for new_col, aggs in aggregations.items():\n        assert len(aggs) == 1\n        (col, agg), *_ = aggs.items()\n        df[new_col] = group[col].transform(agg)\n    return df","method_summary":"Add new columns containing aggregations values on existing columns --- ###","original_method_code":"def add_aggregation_columns(\n        df, *,\n        group_cols: Union[str, List[str]],\n        aggregations: Dict[str, Agg]\n):\n    \"\"\"\n    Add new columns containing aggregations values on existing columns\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `group_cols` (*str* or *list*): columns used to aggregate the data\n    - `aggregations` (*dict*): keys are name of new columns and values are aggregation functions\n       Examples of aggregation functions : 'sum', 'max'\n       Available aggregation functions are listed [here](\n       https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/groupby.html#aggregation)\n\n    ---\n\n    ### Example\n\n    **Input**\n\n    | ENTITY | YEAR | VALUE_1 | VALUE_2 |\n    |:------:|:----:|:-------:|:-------:|\n    |    A   | 2017 |    10   |    3    |\n    |    A   | 2017 |    20   |    1    |\n    |    A   | 2018 |    10   |    5    |\n    |    A   | 2018 |    30   |    4    |\n    |    B   | 2017 |    60   |    4    |\n    |    B   | 2017 |    40   |    3    |\n    |    B   | 2018 |    50   |    7    |\n    |    B   | 2018 |    60   |    6    |\n\n    ```cson\n    add_aggregation_columns:\n      group_cols: ['ENTITY', 'YEAR']\n      aggregations:\n        sum_value1:\n          VALUE_1: 'sum'  # sum of `VALUE_1` put in `sum_value1` column\n        max_value1:\n          VALUE_1: 'max'  # max of `VALUE_1` put in `max_value1` column\n        mean_value2:\n          VALUE_2: 'mean'  # mean of `VALUE_2` put in `mean_value2` column\n    ]\n    ```\n\n    **Output**\n\n    | ENTITY | YEAR | VALUE_1 | VALUE_2 | sum_value1 | max_value1 | mean_value2 |\n    |:------:|:----:|:-------:|:-------:|:----------:|:----------:|:-----------:|\n    |    A   | 2017 |    10   |    3    |     30     |     20     |     2.0     |\n    |    A   | 2017 |    20   |    1    |     30     |     20     |     2.0     |\n    |    A   | 2018 |    10   |    5    |     40     |     30     |     4.5     |\n    |    A   | 2018 |    30   |    4    |     40     |     30     |     4.5     |\n    |    B   | 2017 |    60   |    4    |    100     |     60     |     3.5     |\n    |    B   | 2017 |    40   |    3    |    100     |     60     |     3.5     |\n    |    B   | 2018 |    50   |    7    |    110     |     60     |     6.5     |\n    |    B   | 2018 |    60   |    6    |    110     |     60     |     6.5     |\n\n    \"\"\"\n    group = df.groupby(group_cols)\n    for new_col, aggs in aggregations.items():\n        assert len(aggs) == 1\n        (col, agg), *_ = aggs.items()\n        df[new_col] = group[col].transform(agg)\n    return df","method_path":"https:\/\/github.com\/ToucanToco\/toucan-data-sdk\/blob\/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a\/toucan_data_sdk\/utils\/postprocess\/add_aggregation_columns.py#L6-L74"}
{"repo_name":"ToucanToco\/toucan-data-sdk","method_name":"top","method_code":"def top(\n        df,\n        value: str,\n        limit: int,\n        order: str = 'asc',\n        group: Union[str, List[str]] = None\n):\n    \"\"\"\"\"\"\n    ascending = order != 'desc'\n    limit = int(limit)\n    filter_func = 'nlargest' if (limit > 0) ^ ascending else 'nsmallest'\n\n    def _top(df):\n        return getattr(df, filter_func)(abs(limit), value).sort_values(by=value,\n                                                                       ascending=ascending)\n\n    if group is None:\n        df = _top(df)\n    else:\n        df = df.groupby(group).apply(_top)\n\n    return df","method_summary":"Get the top or flop N results based on a column value for each specified group columns --- ###","original_method_code":"def top(\n        df,\n        value: str,\n        limit: int,\n        order: str = 'asc',\n        group: Union[str, List[str]] = None\n):\n    \"\"\"\n    Get the top or flop N results based on a column value for each specified group columns\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `value` (*str*): column name on which you will rank the results\n    - `limit` (*int*): Number to specify the N results you want to retrieve.\n        Use a positive number x to retrieve the first x results.\n        Use a negative number -x to retrieve the last x results.\n\n    *optional :*\n    - `order` (*str*): `\"asc\"` or `\"desc\"` to sort by ascending ou descending order. By default : `\"asc\"`.\n    - `group` (*str*, *list of str*): name(s) of columns on which you want to perform the group operation.\n\n    ---\n\n    ### Example\n\n    **Input**\n\n    | variable | Category | value |\n    |:--------:|:--------:|:-----:|\n    |   lili   |    1     |  50  |\n    |   lili   |    1     |  20  |\n    |   toto   |    1     |  100  |\n    |   toto   |    1     |  200  |\n    |   toto   |    1     |  300  |\n    |   lala   |    1     |  100  |\n    |   lala   |    1     |  150  |\n    |   lala   |    1     |  250  |\n    |   lala   |    2     |  350  |\n    |   lala   |    2     |  450  |\n\n\n    ```cson\n    top:\n      value: 'value'\n      limit: 4\n      order: 'asc'\n    ```\n\n    **Output**\n\n    | variable | Category | value |\n    |:--------:|:--------:|:-----:|\n    |   lala   |    1     |  250  |\n    |   toto   |    1     |  300  |\n    |   lala   |    2     |  350  |\n    |   lala   |    2     |  450  |\n    \"\"\"\n    ascending = order != 'desc'\n    limit = int(limit)\n    filter_func = 'nlargest' if (limit > 0) ^ ascending else 'nsmallest'\n\n    def _top(df):\n        return getattr(df, filter_func)(abs(limit), value).sort_values(by=value,\n                                                                       ascending=ascending)\n\n    if group is None:\n        df = _top(df)\n    else:\n        df = df.groupby(group).apply(_top)\n\n    return df","method_path":"https:\/\/github.com\/ToucanToco\/toucan-data-sdk\/blob\/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a\/toucan_data_sdk\/utils\/postprocess\/top.py#L4-L77"}
{"repo_name":"ToucanToco\/toucan-data-sdk","method_name":"convert_str_to_datetime","method_code":"def convert_str_to_datetime(df, *, column: str, format: str):\n    \"\"\"\"\"\"\n    df[column] = pd.to_datetime(df[column], format=format)\n    return df","method_summary":"Convert string column into datetime column --- ###","original_method_code":"def convert_str_to_datetime(df, *, column: str, format: str):\n    \"\"\"\n    Convert string column into datetime column\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `column` (*str*): name of the column to format\n    - `format` (*str*): current format of the values (see [available formats](\n    https:\/\/docs.python.org\/3\/library\/datetime.html#strftime-and-strptime-behavior))\n    \"\"\"\n    df[column] = pd.to_datetime(df[column], format=format)\n    return df","method_path":"https:\/\/github.com\/ToucanToco\/toucan-data-sdk\/blob\/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a\/toucan_data_sdk\/utils\/postprocess\/converter.py#L4-L18"}
{"repo_name":"ToucanToco\/toucan-data-sdk","method_name":"convert_datetime_to_str","method_code":"def convert_datetime_to_str(df, *, column: str, format: str, new_column: str = None):\n    \"\"\"\"\"\"\n    new_column = new_column or column\n    df[new_column] = df[column].dt.strftime(format)\n    return df","method_summary":"Convert datetime column into string column --- ###","original_method_code":"def convert_datetime_to_str(df, *, column: str, format: str, new_column: str = None):\n    \"\"\"\n    Convert datetime column into string column\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - column (*str*): name of the column to format\n    - format (*str*): format of the result values (see [available formats](\n    https:\/\/docs.python.org\/3\/library\/datetime.html#strftime-and-strptime-behavior))\n\n    *optional :*\n    - new_column (*str*): name of the output column. By default `column` is overwritten.\n    \"\"\"\n    new_column = new_column or column\n    df[new_column] = df[column].dt.strftime(format)\n    return df","method_path":"https:\/\/github.com\/ToucanToco\/toucan-data-sdk\/blob\/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a\/toucan_data_sdk\/utils\/postprocess\/converter.py#L21-L39"}
{"repo_name":"ToucanToco\/toucan-data-sdk","method_name":"change_date_format","method_code":"def change_date_format(\n        df, *,\n        column: str,\n        output_format: str,\n        input_format: str = None,\n        new_column: str = None,\n        new_time_zone=None\n):\n    \"\"\"\"\"\"\n    new_column = new_column or column\n    df[new_column] = (pd.to_datetime(df[column], format=input_format, utc=True)\n                      .dt.tz_convert(new_time_zone)\n                      .dt.strftime(output_format))\n    return df","method_summary":"Convert the format of a date --- ###","original_method_code":"def change_date_format(\n        df, *,\n        column: str,\n        output_format: str,\n        input_format: str = None,\n        new_column: str = None,\n        new_time_zone=None\n):\n    \"\"\"\n    Convert the format of a date\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `column` (*str*): name of the column to change the format\n    - `output_format` (*str*): format of the output values (see [available formats](\n    https:\/\/docs.python.org\/3\/library\/datetime.html#strftime-and-strptime-behavior))\n\n    *optional :*\n    - `input_format` (*str*): format of the input values (by default let the parser detect it)\n    - `new_column` (*str*): name of the output column  (by default overwrite `column`)\n    - `new_time_zone` (*str*): name of new time zone (by default no time zone conversion is done)\n\n    ---\n\n    ### Example\n\n    **Input**\n\n    label   | date\n    :------:|:----:\n    France  | 2017-03-22\n    Europe  | 2016-03-22\n\n    ```cson\n    change_date_format:\n      column: 'date'\n      input_format: '%Y-%m-%d'\n      output_format: '%Y-%m'\n    ```\n\n    Output :\n\n    label   | date\n    :------:|:----:\n    France  | 2017-03\n    Europe  | 2016-03\n    \"\"\"\n    new_column = new_column or column\n    df[new_column] = (pd.to_datetime(df[column], format=input_format, utc=True)\n                      .dt.tz_convert(new_time_zone)\n                      .dt.strftime(output_format))\n    return df","method_path":"https:\/\/github.com\/ToucanToco\/toucan-data-sdk\/blob\/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a\/toucan_data_sdk\/utils\/postprocess\/converter.py#L42-L96"}
{"repo_name":"ToucanToco\/toucan-data-sdk","method_name":"cast","method_code":"def cast(df, column: str, type: str, new_column=None):\n    \"\"\"\"\"\"\n    new_column = new_column or column\n    df[new_column] = df[column].astype(type)\n    return df","method_summary":"Convert column's type into type --- ###","original_method_code":"def cast(df, column: str, type: str, new_column=None):\n    \"\"\"\n    Convert column's type into type\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `column` (*str*): name of the column to convert\n    - `type` (*str*): output type. It can be :\n        - `\"int\"` : integer type\n        - `\"float\"` : general number type\n        - `\"str\"` : text type\n\n    *optional :*\n    - `new_column` (*str*): name of the output column.\n       By default the `column` arguments is modified.\n\n    ---\n\n    ### Example\n\n    **Input**\n\n    | Column 1 |  Column 2   |  Column 3  |\n    |:-------:|:--------:|:--------:|\n    |  'one'  |  '2014'  |   30.0   |\n    |  'two'  |  2015.0  |    '1'   |\n    |   3.1   |   2016   |    450   |\n\n    ```cson\n    postprocess: [\n      cast:\n        column: 'Column 1'\n        type: 'str'\n      cast:\n        column: 'Column 2'\n        type: 'int'\n      cast:\n        column: 'Column 3'\n        type: 'float'\n    ]\n    ```\n\n    **Output**\n\n    | Column 1 |  Column 2  |  Column 3  |\n    |:-------:|:------:|:--------:|\n    |  'one'  |  2014  |   30.0   |\n    |  'two'  |  2015  |    1.0   |\n    |  '3.1'  |  2016  |  450.0   |\n    \"\"\"\n    new_column = new_column or column\n    df[new_column] = df[column].astype(type)\n    return df","method_path":"https:\/\/github.com\/ToucanToco\/toucan-data-sdk\/blob\/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a\/toucan_data_sdk\/utils\/postprocess\/converter.py#L99-L154"}
{"repo_name":"ToucanToco\/toucan-data-sdk","method_name":"compute_evolution_by_frequency","method_code":"def compute_evolution_by_frequency(\n    df,\n    id_cols: List[str],\n    date_col: Union[str, Dict[str, str]],\n    value_col: str,\n    freq=1,\n    method: str = 'abs',\n    format: str = 'column',\n    offseted_suffix: str = '_offseted',\n    evolution_col_name: str = 'evolution_computed',\n    missing_date_as_zero: bool = False,\n    raise_duplicate_error: bool = True\n):\n    \"\"\"\"\"\"\n    if missing_date_as_zero:\n        how = 'outer'\n        fillna = 0\n    else:\n        how = 'left'\n        fillna = None\n\n    return __compute_evolution(\n        df=df,\n        id_cols=id_cols,\n        value_col=value_col,\n        date_col=date_col,\n        freq=freq,\n        method=method,\n        format=format,\n        offseted_suffix=offseted_suffix,\n        evolution_col_name=evolution_col_name,\n        how=how,\n        fillna=fillna,\n        raise_duplicate_error=raise_duplicate_error\n    )","method_summary":"This function answers the","original_method_code":"def compute_evolution_by_frequency(\n    df,\n    id_cols: List[str],\n    date_col: Union[str, Dict[str, str]],\n    value_col: str,\n    freq=1,\n    method: str = 'abs',\n    format: str = 'column',\n    offseted_suffix: str = '_offseted',\n    evolution_col_name: str = 'evolution_computed',\n    missing_date_as_zero: bool = False,\n    raise_duplicate_error: bool = True\n):\n    \"\"\"\n    This function answers the question: how has a value changed on a weekly, monthly, yearly basis ?\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `id_cols` (*list*): name of the columns used to create each group.\n    - `date_col` (*str or dict*): either directly the name of the column containing the date or a dictionary with:\n      - `selector` (*str*): the name of the column\n      - `format` (*str*): the format of the date (see [pandas doc](\n        https:\/\/docs.python.org\/3\/library\/datetime.html#strftime-and-strptime-behavior))\n    - `value_col` (*str*): name of the column containing the value to compare.\n\n    *optional :*\n    - `freq` (*int\/pd.DateOffset\/pd.Serie\/dict*): the frequency at which we calculate evolutions\n    - `method` (*str*): either `\"abs\"` for absolute values or `\"pct\"` for the evolution in percentage of previous value.\n    - `offseted_suffix` (*str*): suffix of the offseted column. By default, `\"_offseted\"`.\n    - `evolution_col_name` (*str*): name given to the evolution column. By default, `\"evolution_computed\"`.\n    - `missing_date_as_zero` (*boolean*): add missing date with zero value.\n    - `raise_duplicate_error` (*boolean*): raise an error when the dataset has duplicated values with the given `id_cols`.\n    - `format` (*str*): `'df'` # Do not change it !!!\n\n    ---\n\n    ### Example\n\n    **Input**\n\n    |   id_cols |    value_col |    date_col|\n    |:---------:|:------------:|:----------:|\n    |         A |           20 |        2010|\n    |           |            7 |        2011|\n    |         B |          200 |        2010|\n    |           |          220 |        2011|\n    |         C |          100 |        2011|\n\n    ```cson\n    compute_evolution_by_frequency:\n      id_cols: \"id_cols\"\n      date_col: \"date_col\"\n      value_col: \"value_col\"\n    ```\n\n    **Output**\n\n    |   id_cols |    value_col |    date_col|  evolution|\n    |:---------:|:------------:|:----------:|:---------:|\n    |         A |           20 |        2010|       null|\n    |           |            7 |        2011|        -13|\n    |         B |          200 |        2010|       null|\n    |           |          220 |        2011|         20|\n    |         C |          100 |        2011|       null|\n    \"\"\"\n    if missing_date_as_zero:\n        how = 'outer'\n        fillna = 0\n    else:\n        how = 'left'\n        fillna = None\n\n    return __compute_evolution(\n        df=df,\n        id_cols=id_cols,\n        value_col=value_col,\n        date_col=date_col,\n        freq=freq,\n        method=method,\n        format=format,\n        offseted_suffix=offseted_suffix,\n        evolution_col_name=evolution_col_name,\n        how=how,\n        fillna=fillna,\n        raise_duplicate_error=raise_duplicate_error\n    )","method_path":"https:\/\/github.com\/ToucanToco\/toucan-data-sdk\/blob\/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a\/toucan_data_sdk\/utils\/generic\/compute_evolution.py#L10-L98"}
{"repo_name":"ToucanToco\/toucan-data-sdk","method_name":"compute_evolution_by_criteria","method_code":"def compute_evolution_by_criteria(\n    df,\n    id_cols: List[str],\n    value_col: str,\n    compare_to: str,\n    method: str = 'abs',\n    format: str = 'column',\n    offseted_suffix: str = '_offseted',\n    evolution_col_name: str = 'evolution_computed',\n    raise_duplicate_error: bool = True\n):\n    \"\"\"\"\"\"\n    return __compute_evolution(**locals())","method_summary":"This function answers the","original_method_code":"def compute_evolution_by_criteria(\n    df,\n    id_cols: List[str],\n    value_col: str,\n    compare_to: str,\n    method: str = 'abs',\n    format: str = 'column',\n    offseted_suffix: str = '_offseted',\n    evolution_col_name: str = 'evolution_computed',\n    raise_duplicate_error: bool = True\n):\n    \"\"\"\n    This function answers the question: how has a value changed compare to a specific value ?\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `id_cols` (*list*): columns used to create each group\n    - `value_col` (*str*): name of the column containing the value to compare\n    - `compare_to` (*str*): the query identifying a specific set of values for comparison.\n\n    *optional :*\n    - `method` (*str*): either `\"abs\"` for absolute values or `\"pct\"` for the evolution in percentage of previous value.\n    - `offseted_suffix` (*str*): suffix of the offseted column. By default, `\"_offseted\"`.\n    - `evolution_col_name` (*str*): name given to the evolution column. By default, `\"evolution_computed\"`.\n    - `raise_duplicate_error` (*boolean*): raise an error when the dataset has duplicated values with the given `id_cols`.\n    - `format` (*str*): `'df'` # Do not change it !!!\n\n    ---\n\n    ### Example\n\n    **Input**\n\n    |   id_cols |    value_col |    month|\n    |:---------:|:------------:|:-------:|\n    |         A |          100 |        1|\n    |           |          250 |       12|\n    |         B |          300 |        1|\n    |           |          200 |       12|\n\n    ```cson\n    compute_evolution_by_criteria:\n      id_cols: \"id_cols\"\n      value_col: \"value_col\"\n      compare_to: \"month==12\"\n    ```\n\n    **Output**\n\n    |   id_cols |    value_col |    month|\tvalue_offseted\t| evolution_computed|\n    |:---------:|:------------:|:-------:|:----------------:|:-----------------:|\n    |         A |          100 |        1|               250|               -150|\n    |           |          250 |       12|               250|                  0|\n    |         B |          300 |        1|               200|                100|\n    |           |          200 |       12|               200|                  0|\n    \"\"\"\n    return __compute_evolution(**locals())","method_path":"https:\/\/github.com\/ToucanToco\/toucan-data-sdk\/blob\/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a\/toucan_data_sdk\/utils\/generic\/compute_evolution.py#L101-L160"}
{"repo_name":"ToucanToco\/toucan-data-sdk","method_name":"rank","method_code":"def rank(\n        df,\n        value_cols: Union[str, List[str]],\n        group_cols: List[str] = None,\n        rank_cols_names: List[str] = None,\n        method='min',\n        ascending: bool = True\n):\n    \"\"\"\"\"\"\n\n    value_cols = [value_cols] if not isinstance(value_cols, list) else value_cols\n    for col in value_cols:\n        if not np.issubdtype(df[col].dtype, np.number):\n            raise TypeError(col + \" specified in value_cols must be of numeric type\")\n\n    if rank_cols_names is None:\n        rank_cols_names = [x + '_rank' for x in value_cols]\n\n    if group_cols is None:\n        df[rank_cols_names] = df[value_cols].rank(method=method, ascending=ascending)\n    else:\n        df[rank_cols_names] = (df.groupby(group_cols)[value_cols]\n                                 .rank(method=method, ascending=ascending))\n\n    if method != 'average':\n        df[rank_cols_names] = df[rank_cols_names].astype('int')\n\n    return df","method_summary":"This function creates rank columns based on numeric values to be ranked. --- ###","original_method_code":"def rank(\n        df,\n        value_cols: Union[str, List[str]],\n        group_cols: List[str] = None,\n        rank_cols_names: List[str] = None,\n        method='min',\n        ascending: bool = True\n):\n    \"\"\"\n    This function creates rank columns based on numeric values to be ranked.\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `value_cols` (*list*): name(s) of the columns used\n\n    *optional :*\n    - `group_cols` (*list*): name(s) of the column(s) used to\n      create each group inside which independent ranking needs to be applied\n    - `rank_cols_names` (*list*): the names of the added ranking columns.\n      If not filled, the ranking will be named after the value_cols with a '_rank' suffix\n    - `method` (*str*): method to use when encountering equal values:\n        - `'min'` (default): lowest rank in group\n        - `'max'`: highest rank in group\n        - `'average'`: average rank of group\n        - `'first'`: ranks assigned in order the values appear in the series\n        - `'dense'`: like 'min', but rank always increases by 1 between groups\n    - `ascending` (*boolean*): whether the rank should be determined based on\n       ascending (default) or descending order\n\n    ---\n\n    ### Example\n\n    **Input**\n\n    | ENTITY | YEAR | VALUE_1 | VALUE_2 |\n    | :---: | :---: | :---: | :---: |\n    | A | 2017 | 10 | 3 |\n    | A | 2017 | 20 | 1 |\n    | A | 2018 | 10 | 5 |\n    | A | 2018 | 30 | 4 |\n    | B | 2017 | 60 | 4 |\n    | B | 2017 | 40 | 3 |\n    | B | 2018 | 50 | 7 |\n    | B | 2018 | 50 | 6 |\n\n    ```cson\n    rank :\n      value_cols: 'VALUE_1'\n    ```\n\n    **Output**\n\n    | ENTITY | YEAR | VALUE_1 | VALUE_2 | VALUE_1_rank\n    | :---: | :---: | :---: | :---: | :---: |\n    | A | 2017 | 10 | 3 | 1 |\n    | A | 2017 | 20 | 1 | 3 |\n    | A | 2018 | 10 | 5 | 1 |\n    | A | 2018 | 30 | 4 | 4 |\n    | B | 2017 | 60 | 4 | 8 |\n    | B | 2017 | 40 | 3 | 5 |\n    | B | 2018 | 50 | 7 | 6 |\n    | B | 2018 | 50 | 6 | 6 |\n    \"\"\"\n\n    value_cols = [value_cols] if not isinstance(value_cols, list) else value_cols\n    for col in value_cols:\n        if not np.issubdtype(df[col].dtype, np.number):\n            raise TypeError(col + \" specified in value_cols must be of numeric type\")\n\n    if rank_cols_names is None:\n        rank_cols_names = [x + '_rank' for x in value_cols]\n\n    if group_cols is None:\n        df[rank_cols_names] = df[value_cols].rank(method=method, ascending=ascending)\n    else:\n        df[rank_cols_names] = (df.groupby(group_cols)[value_cols]\n                                 .rank(method=method, ascending=ascending))\n\n    if method != 'average':\n        df[rank_cols_names] = df[rank_cols_names].astype('int')\n\n    return df","method_path":"https:\/\/github.com\/ToucanToco\/toucan-data-sdk\/blob\/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a\/toucan_data_sdk\/utils\/postprocess\/rank.py#L6-L91"}
{"repo_name":"ToucanToco\/toucan-data-sdk","method_name":"waterfall","method_code":"def waterfall(\n        df,\n        date: str,\n        value: str,\n        start: Dict[str, str],\n        end: Dict[str, str],\n        upperGroup: Dict[str, str],\n        insideGroup: Dict[str, str] = None,\n        filters: List[str] = None\n):\n    \"\"\"\"\"\"\n\n    if len(df) == 0:\n        return df\n\n    if filters is not None:\n        if isinstance(filters, str):\n            filters = [filters]\n\n        def sub_waterfall(df):\n            wa_df = waterfall(df, date, value, start, end, upperGroup, insideGroup)\n            for filters_col in filters:\n                wa_df[filters_col] = df[filters_col].values[0]\n            return wa_df\n\n        \n        list_of_sub_df = [df[(df[filters].values == i).all(axis=1)]\n                          for i in df[filters].drop_duplicates().values]\n\n        return pd.concat([sub_waterfall(df) for df in list_of_sub_df], sort=False)\n\n    groups = {\n        'upperGroup': {\n            'type': 'parent',\n            'id':  'upperGroup',\n            'order': {\n                'by': ['upperGroup_order', 'groups'],\n                'ascending': [True, True]\n            },\n            'obj': upperGroup\n        }\n    }\n    if insideGroup is not None:\n        groups['insideGroup'] = {\n            'type': 'child',\n            'id': 'insideGroup',\n            'order': {\n                'by': ['type', 'insideGroup_order', 'label'],\n                'ascending': [False, True, True]\n            },\n            'obj': insideGroup\n        }\n    \n    df = _compute_rename(df, date, value, groups)\n\n    agg_conf = {'value': sum}\n    agg_conf.update({f'{col}_label': 'first' for col in groups.keys()})\n    agg_conf.update({f'{col}_order': 'first' for col in groups.keys()})\n    df = df.groupby(list(groups.keys()) + ['date']).agg(agg_conf).reset_index()\n\n    df_start, df_end = _compute_start_end(df, start, end)\n\n    df = _compute_value_diff(df, start, end, groups)\n\n    middle = _compute_upper_group(df)\n    if insideGroup is not None:\n        middle = pd.concat([middle, _compute_inside_group(df)])\n\n    ret = _compute_order(df_start, df_end, middle, groups)\n\n    return ret","method_summary":"Return a line for each bars of a waterfall chart, totals, groups, subgroups. Compute the variation and variation rate for each line. --- ###","original_method_code":"def waterfall(\n        df,\n        date: str,\n        value: str,\n        start: Dict[str, str],\n        end: Dict[str, str],\n        upperGroup: Dict[str, str],\n        insideGroup: Dict[str, str] = None,\n        filters: List[str] = None\n):\n    \"\"\"\n    Return a line for each bars of a waterfall chart, totals, groups, subgroups.\n    Compute the variation and variation rate for each line.\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `date` (*str*): name of the column that id the period of each lines\n    - `value` (*str*): name of the column that contains the vaue for each lines\n    - `start` (*dict*):\n        - `label`: text displayed under the first master column\n        - `id`: value in the date col that id lines for the first period\n    - `end` (*dict*):\n        - `label`: text displayed under the last master column\n        - `id`: value in the date col that id lines for the second period\n\n    *optional :*\n    - `upperGroup` (*dict*):\n        - `id`: name of the column that contains upperGroups unique IDs\n        - `label`: not required, text displayed under each upperGroups bars,\n          using ID when it's absent\n        - `groupsOrder`: not required, order of upperGroups\n    - `insideGroup` (*dict*):\n        - `id`: name of the column that contains insideGroups unique IDs\n        - `label`: not required, text displayed under each insideGroups bars,\n          using ID when it's absent\n        - `groupsOrder`: not required, order of insideGroups\n    - `filters` (*list*): columns to filters on\n\n    ---\n\n    ### Example\n\n    **Input**\n\n    | product_id   |   played | date   |   ord | category_id   | category_name   |\n    |:------------:|:--------:|:------:|:-----:|:-------------:|:---------------:|\n    | super clap   |       12 | t1     |     1 | clap          | Clap            |\n    | clap clap    |        1 | t1     |    10 | clap          | Clap            |\n    | tac          |        1 | t1     |     1 | snare         | Snare           |\n    | super clap   |       10 | t2     |     1 | clap          | Clap            |\n    | tac          |      100 | t2     |     1 | snare         | Snare           |\n    | bom          |        1 | t2     |     1 | tom           | Tom             |\n\n\n    ```cson\n    waterfall:\n      upperGroup:\n        id: 'category_id'\n        label: 'category_name'\n      insideGroup:\n        id: 'product_id'\n        groupsOrder: 'ord'\n      date: 'date'\n      value: 'played'\n      start:\n        label: 'Trimestre 1'\n        id: 't1'\n      end:\n        label: 'Trimester 2'\n        id: 't2'\n    ```\n\n    **Output**\n\n    |   value | label       |   variation | groups   | type   |   order |\n    |:-------:|:-----------:|:-----------:|:--------:|:------:|:-------:|\n    |      14 | Trimestre 1 |  NaN        | NaN      | NaN    |     NaN |\n    |      -3 | Clap        |   -0.230769 | clap     | parent |     NaN |\n    |      -2 | super clap  |   -0.166667 | clap     | child  |       1 |\n    |      -1 | clap clap   |   -1        | clap     | child  |      10 |\n    |      99 | Snare       |   99        | snare    | parent |     NaN |\n    |      99 | tac         |   99        | snare    | child  |       1 |\n    |       1 | Tom         |  inf        | tom      | parent |     NaN |\n    |       1 | bom         |  inf        | tom      | child  |       1 |\n    |     111 | Trimester 2 |  NaN        | NaN      | NaN    |     NaN |\n    \"\"\"\n\n    if len(df) == 0:\n        return df\n\n    if filters is not None:\n        if isinstance(filters, str):\n            filters = [filters]\n\n        def sub_waterfall(df):\n            wa_df = waterfall(df, date, value, start, end, upperGroup, insideGroup)\n            for filters_col in filters:\n                wa_df[filters_col] = df[filters_col].values[0]\n            return wa_df\n\n        # filters df into a list of sub_df\n        list_of_sub_df = [df[(df[filters].values == i).all(axis=1)]\n                          for i in df[filters].drop_duplicates().values]\n\n        return pd.concat([sub_waterfall(df) for df in list_of_sub_df], sort=False)\n\n    groups = {\n        'upperGroup': {\n            'type': 'parent',\n            'id':  'upperGroup',\n            'order': {\n                'by': ['upperGroup_order', 'groups'],\n                'ascending': [True, True]\n            },\n            'obj': upperGroup\n        }\n    }\n    if insideGroup is not None:\n        groups['insideGroup'] = {\n            'type': 'child',\n            'id': 'insideGroup',\n            'order': {\n                'by': ['type', 'insideGroup_order', 'label'],\n                'ascending': [False, True, True]\n            },\n            'obj': insideGroup\n        }\n    # prepare the dataframe with standard column names\n    df = _compute_rename(df, date, value, groups)\n\n    agg_conf = {'value': sum}\n    agg_conf.update({f'{col}_label': 'first' for col in groups.keys()})\n    agg_conf.update({f'{col}_order': 'first' for col in groups.keys()})\n    df = df.groupby(list(groups.keys()) + ['date']).agg(agg_conf).reset_index()\n\n    df_start, df_end = _compute_start_end(df, start, end)\n\n    df = _compute_value_diff(df, start, end, groups)\n\n    middle = _compute_upper_group(df)\n    if insideGroup is not None:\n        middle = pd.concat([middle, _compute_inside_group(df)])\n\n    ret = _compute_order(df_start, df_end, middle, groups)\n\n    return ret","method_path":"https:\/\/github.com\/ToucanToco\/toucan-data-sdk\/blob\/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a\/toucan_data_sdk\/utils\/postprocess\/waterfall.py#L5-L153"}
{"repo_name":"zomux\/deepy","method_name":"ada_family_core","method_code":"def ada_family_core(params, gparams, learning_rate = 0.01, eps= 1e-6, rho=0.95, method=\"ADADELTA\",\n                        beta=0.0, gsum_regularization = 0.0001):\n    \"\"\"\"\"\"\n\n    _, _, _, args = inspect.getargvalues(inspect.currentframe())\n    logging.info(\"ada_family_core: %s\" % str(args.items()))\n    free_parameters = []\n\n    if method == \"FINETUNING_ADAGRAD\":\n        method = \"ADAGRAD\"\n        gsum_regularization = 0\n\n    oneMinusBeta = 1 - beta\n\n    gsums   = [theano.shared(np.zeros_like(param.get_value(borrow=True), dtype=FLOATX), name=\"gsum_%s\" % param.name) if (method == 'ADADELTA' or method == 'ADAGRAD') else None for param in params]\n    xsums   = [theano.shared(np.zeros_like(param.get_value(borrow=True), dtype=FLOATX), name=\"xsum_%s\" % param.name) if method == 'ADADELTA' else None for param in params]\n\n    \n    if method == 'ADAGRAD':\n        for gsum in gsums:\n            gsum.set_value(gsum.get_value() ** 0)\n\n    updates = OrderedDict()\n    \n    for gparam, param, gsum, xsum in zip(gparams, params, gsums, xsums):\n\n        if method == 'ADADELTA':\n            updates[gsum] = rho * gsum + (1. - rho) * (gparam **2)\n            dparam = -T.sqrt((xsum + eps) \/ (updates[gsum] + eps)) * gparam\n            updates[xsum] =rho * xsum + (1. - rho) * (dparam **2)\n            updates[param] = param * oneMinusBeta + dparam\n        elif method == 'ADAGRAD':\n            updates[gsum] = gsum + (gparam **2) - gsum_regularization * gsum\n            updates[param] =  param * oneMinusBeta - learning_rate * (gparam \/ (T.sqrt(updates[gsum] + eps)))\n\n        else:\n            updates[param] = param * oneMinusBeta - gparam * learning_rate\n    \n    if method == 'ADADELTA':\n        free_parameters.extend(gsums + xsums)\n    elif method == 'ADAGRAD':\n        free_parameters.extend(gsums)\n    \n    for k in updates:\n        if updates[k].dtype != FLOATX:\n            updates[k] = updates[k].astype(FLOATX)\n    return updates.items(), free_parameters","method_summary":"Optimize by SGD, AdaGrad, or AdaDelta.","original_method_code":"def ada_family_core(params, gparams, learning_rate = 0.01, eps= 1e-6, rho=0.95, method=\"ADADELTA\",\n                        beta=0.0, gsum_regularization = 0.0001):\n    \"\"\"\n    Optimize by SGD, AdaGrad, or AdaDelta.\n    \"\"\"\n\n    _, _, _, args = inspect.getargvalues(inspect.currentframe())\n    logging.info(\"ada_family_core: %s\" % str(args.items()))\n    free_parameters = []\n\n    if method == \"FINETUNING_ADAGRAD\":\n        method = \"ADAGRAD\"\n        gsum_regularization = 0\n\n    oneMinusBeta = 1 - beta\n\n    gsums   = [theano.shared(np.zeros_like(param.get_value(borrow=True), dtype=FLOATX), name=\"gsum_%s\" % param.name) if (method == 'ADADELTA' or method == 'ADAGRAD') else None for param in params]\n    xsums   = [theano.shared(np.zeros_like(param.get_value(borrow=True), dtype=FLOATX), name=\"xsum_%s\" % param.name) if method == 'ADADELTA' else None for param in params]\n\n    # Fix for AdaGrad, init gsum to 1\n    if method == 'ADAGRAD':\n        for gsum in gsums:\n            gsum.set_value(gsum.get_value() ** 0)\n\n    updates = OrderedDict()\n    # Updates\n    for gparam, param, gsum, xsum in zip(gparams, params, gsums, xsums):\n\n        if method == 'ADADELTA':\n            updates[gsum] = rho * gsum + (1. - rho) * (gparam **2)\n            dparam = -T.sqrt((xsum + eps) \/ (updates[gsum] + eps)) * gparam\n            updates[xsum] =rho * xsum + (1. - rho) * (dparam **2)\n            updates[param] = param * oneMinusBeta + dparam\n        elif method == 'ADAGRAD':\n            updates[gsum] = gsum + (gparam **2) - gsum_regularization * gsum\n            updates[param] =  param * oneMinusBeta - learning_rate * (gparam \/ (T.sqrt(updates[gsum] + eps)))\n\n        else:\n            updates[param] = param * oneMinusBeta - gparam * learning_rate\n    # Add free parameters\n    if method == 'ADADELTA':\n        free_parameters.extend(gsums + xsums)\n    elif method == 'ADAGRAD':\n        free_parameters.extend(gsums)\n    # Check dtype\n    for k in updates:\n        if updates[k].dtype != FLOATX:\n            updates[k] = updates[k].astype(FLOATX)\n    return updates.items(), free_parameters","method_path":"https:\/\/github.com\/zomux\/deepy\/blob\/090fbad22a08a809b12951cd0d4984f5bd432698\/deepy\/trainers\/cores\/ada_family.py#L13-L61"}
{"repo_name":"zomux\/deepy","method_name":"optimize_updates","method_code":"def optimize_updates(params, gradients, config=None, shapes=None):\n    \"\"\"\"\"\"\n    if config and isinstance(config, dict):\n        config = TrainerConfig(config)\n\n    \n    if config:\n        clip_value = config.get(\"gradient_clipping\", None)\n\n        if clip_value:\n            clip_constant = T.constant(clip_value, dtype=FLOATX)\n\n            if config.avoid_compute_embed_norm:\n                grad_norm = multiple_l2_norm([t[1] for t in zip(params, gradients) if not t[0].name.startswith(\"W_embed\")])\n            else:\n                grad_norm = multiple_l2_norm(gradients)\n            isnan = T.or_(T.isnan(grad_norm), T.isinf(grad_norm))\n            multiplier = ifelse(grad_norm < clip_constant,\n                                T.constant(1., dtype=FLOATX), clip_constant \/ (grad_norm + EPSILON))\n\n            \n            clipped_gradients = []\n            for param, g in zip(params, gradients):\n                g = multiplier * g\n                if config.avoid_nan:\n                    g = T.switch(isnan, np.float32(0.1) * param, g)\n                if config.gradient_tolerance:\n                    g = ifelse(grad_norm > config.gradient_tolerance, T.zeros_like(g) + EPSILON, g)\n                clipped_gradients.append(g)\n\n            gradients = clipped_gradients\n    \n    if config and config.weight_l2:\n        regularized_gradients = []\n        for param, grad in zip(params, gradients):\n            grad = grad + (2 * config.weight_l2 * param)\n            regularized_gradients.append(grad)\n        gradients = regularized_gradients\n\n    \n    \n    if config and config.avoid_nan and not config.gradient_clipping:\n        logging.info(\"avoid NaN gradients\")\n        new_gradients = []\n        for grad in gradients:\n            new_grad = ifelse(T.isnan(grad).any(), T.zeros_like(grad) + EPSILON, grad)\n            new_gradients.append(new_grad)\n        gradients = new_gradients\n\n\n    \n    method = \"SGD\"\n    if config:\n        method = config.get(\"method\", method).upper()\n    \n    func = None\n    if method in [\"SGD\", \"ADAGRAD\", \"ADADELTA\", \"FINETUNING_ADAGRAD\"]:\n        from cores.ada_family import ada_family_core\n        func = ada_family_core\n    elif method == \"ADAM\":\n        from cores.adam import adam_core\n        func = adam_core\n    elif method == \"RMSPROP\":\n        from cores.rmsprop import rmsprop_core\n        func = rmsprop_core\n    elif method == \"MOMENTUM\":\n        from cores.momentum import momentum_core\n        func = momentum_core\n\n    if not func:\n        raise NotImplementedError(\"method '%s' is not supported\" % method)\n\n    logging.info(\"optimize method=%s parameters=%s\" % (method, str(params)))\n\n    free_parameters = []\n    return_vals = wrap_core(func, config, params, gradients)\n    if type(return_vals) == list and type(return_vals[0]) == list:\n        updates, free_parameters = return_vals\n    else:\n        updates = return_vals\n\n    \n    if config and not config.record_free_params:\n        free_parameters = []\n\n    \n    if config.weight_bound:\n        logging.info(\"apply weight bound of %.2f\" % config.weight_bound)\n        new_updates = []\n        for param, update_value in updates:\n            bounded_value = (update_value * (T.abs_(update_value) <= config.weight_bound) +\n                             config.weight_bound * (update_value > config.weight_bound) +\n                             -config.weight_bound * (update_value < -config.weight_bound))\n            new_updates.append((param, bounded_value))\n        updates = new_updates\n    return updates, free_parameters","method_summary":"General optimization function for Theano.","original_method_code":"def optimize_updates(params, gradients, config=None, shapes=None):\n    \"\"\"\n    General optimization function for Theano.\n    Parameters:\n        params - parameters\n        gradients - gradients\n        config - training config\n    Returns:\n        Theano updates\n    :type config: deepy.TrainerConfig or dict\n    \"\"\"\n    if config and isinstance(config, dict):\n        config = TrainerConfig(config)\n\n    # Clipping\n    if config:\n        clip_value = config.get(\"gradient_clipping\", None)\n\n        if clip_value:\n            clip_constant = T.constant(clip_value, dtype=FLOATX)\n\n            if config.avoid_compute_embed_norm:\n                grad_norm = multiple_l2_norm([t[1] for t in zip(params, gradients) if not t[0].name.startswith(\"W_embed\")])\n            else:\n                grad_norm = multiple_l2_norm(gradients)\n            isnan = T.or_(T.isnan(grad_norm), T.isinf(grad_norm))\n            multiplier = ifelse(grad_norm < clip_constant,\n                                T.constant(1., dtype=FLOATX), clip_constant \/ (grad_norm + EPSILON))\n\n            # Clip\n            clipped_gradients = []\n            for param, g in zip(params, gradients):\n                g = multiplier * g\n                if config.avoid_nan:\n                    g = T.switch(isnan, np.float32(0.1) * param, g)\n                if config.gradient_tolerance:\n                    g = ifelse(grad_norm > config.gradient_tolerance, T.zeros_like(g) + EPSILON, g)\n                clipped_gradients.append(g)\n\n            gradients = clipped_gradients\n    # Regularization\n    if config and config.weight_l2:\n        regularized_gradients = []\n        for param, grad in zip(params, gradients):\n            grad = grad + (2 * config.weight_l2 * param)\n            regularized_gradients.append(grad)\n        gradients = regularized_gradients\n\n    # Avoid nan but not computing the norm\n    # This is not recommended\n    if config and config.avoid_nan and not config.gradient_clipping:\n        logging.info(\"avoid NaN gradients\")\n        new_gradients = []\n        for grad in gradients:\n            new_grad = ifelse(T.isnan(grad).any(), T.zeros_like(grad) + EPSILON, grad)\n            new_gradients.append(new_grad)\n        gradients = new_gradients\n\n\n    # Find method\n    method = \"SGD\"\n    if config:\n        method = config.get(\"method\", method).upper()\n    # Get Function\n    func = None\n    if method in [\"SGD\", \"ADAGRAD\", \"ADADELTA\", \"FINETUNING_ADAGRAD\"]:\n        from cores.ada_family import ada_family_core\n        func = ada_family_core\n    elif method == \"ADAM\":\n        from cores.adam import adam_core\n        func = adam_core\n    elif method == \"RMSPROP\":\n        from cores.rmsprop import rmsprop_core\n        func = rmsprop_core\n    elif method == \"MOMENTUM\":\n        from cores.momentum import momentum_core\n        func = momentum_core\n\n    if not func:\n        raise NotImplementedError(\"method '%s' is not supported\" % method)\n\n    logging.info(\"optimize method=%s parameters=%s\" % (method, str(params)))\n\n    free_parameters = []\n    return_vals = wrap_core(func, config, params, gradients)\n    if type(return_vals) == list and type(return_vals[0]) == list:\n        updates, free_parameters = return_vals\n    else:\n        updates = return_vals\n\n    # No free param recording\n    if config and not config.record_free_params:\n        free_parameters = []\n\n    # Weight bound\n    if config.weight_bound:\n        logging.info(\"apply weight bound of %.2f\" % config.weight_bound)\n        new_updates = []\n        for param, update_value in updates:\n            bounded_value = (update_value * (T.abs_(update_value) <= config.weight_bound) +\n                             config.weight_bound * (update_value > config.weight_bound) +\n                             -config.weight_bound * (update_value < -config.weight_bound))\n            new_updates.append((param, bounded_value))\n        updates = new_updates\n    return updates, free_parameters","method_path":"https:\/\/github.com\/zomux\/deepy\/blob\/090fbad22a08a809b12951cd0d4984f5bd432698\/deepy\/trainers\/optimize.py#L19-L123"}
{"repo_name":"zomux\/deepy","method_name":"optimize_function","method_code":"def optimize_function(params, config=None):\n    \"\"\"\"\"\"\n    gs = [dim_to_var(p.ndim) for p in params]\n    updates, _ = optimize_updates(params, gs, config)\n    return theano.function(gs, [], updates=updates)","method_summary":"Create a optimizing function receives gradients.","original_method_code":"def optimize_function(params, config=None):\n    \"\"\"\n    Create a optimizing function receives gradients.\n    Parameters:\n        params - parameters\n        config - training configuration\n    Returns:\n        updating function receives gradients\n    \"\"\"\n    gs = [dim_to_var(p.ndim) for p in params]\n    updates, _ = optimize_updates(params, gs, config)\n    return theano.function(gs, [], updates=updates)","method_path":"https:\/\/github.com\/zomux\/deepy\/blob\/090fbad22a08a809b12951cd0d4984f5bd432698\/deepy\/trainers\/optimize.py#L125-L136"}
{"repo_name":"zomux\/deepy","method_name":"GeneralNeuralTrainer._learning_updates","method_code":"def _learning_updates(self):\n        \"\"\"\"\"\"\n        params = self.training_params()\n        gradients = self.get_gradients(params)\n        return self.optimization_updates(params, gradients)","method_summary":"Return updates in the training.","original_method_code":"def _learning_updates(self):\n        \"\"\"\n        Return updates in the training.\n        \"\"\"\n        params = self.training_params()\n        gradients = self.get_gradients(params)\n        return self.optimization_updates(params, gradients)","method_path":"https:\/\/github.com\/zomux\/deepy\/blob\/090fbad22a08a809b12951cd0d4984f5bd432698\/deepy\/trainers\/trainers.py#L40-L46"}
{"repo_name":"zomux\/deepy","method_name":"GeneralNeuralTrainer.training_params","method_code":"def training_params(self):\n        \"\"\"\"\"\"\n        params = self.network.parameters\n        \n        if self.config.fixed_parameters:\n            logging.info(\"fixed parameters: %s\" % \", \".join(map(str, self.config.fixed_parameters)))\n            params = [p for p in params if p not in self.config.fixed_parameters]\n        return params","method_summary":"Get parameters to be optimized.","original_method_code":"def training_params(self):\n        \"\"\"\n        Get parameters to be optimized.\n        \"\"\"\n        params = self.network.parameters\n        # Freeze parameters\n        if self.config.fixed_parameters:\n            logging.info(\"fixed parameters: %s\" % \", \".join(map(str, self.config.fixed_parameters)))\n            params = [p for p in params if p not in self.config.fixed_parameters]\n        return params","method_path":"https:\/\/github.com\/zomux\/deepy\/blob\/090fbad22a08a809b12951cd0d4984f5bd432698\/deepy\/trainers\/trainers.py#L48-L57"}
{"repo_name":"zomux\/deepy","method_name":"GeneralNeuralTrainer.optimization_updates","method_code":"def optimization_updates(self, params, gradients):\n        \"\"\"\"\"\"\n        updates, free_parameters = optimize_updates(params, gradients, self.config)\n        self.network.free_parameters.extend(free_parameters)\n        logging.info(\"Added %d free parameters for optimization\" % len(free_parameters))\n        return updates","method_summary":"Return updates from optimization.","original_method_code":"def optimization_updates(self, params, gradients):\n        \"\"\"\n        Return updates from optimization.\n        \"\"\"\n        updates, free_parameters = optimize_updates(params, gradients, self.config)\n        self.network.free_parameters.extend(free_parameters)\n        logging.info(\"Added %d free parameters for optimization\" % len(free_parameters))\n        return updates","method_path":"https:\/\/github.com\/zomux\/deepy\/blob\/090fbad22a08a809b12951cd0d4984f5bd432698\/deepy\/trainers\/trainers.py#L65-L72"}
{"repo_name":"zomux\/deepy","method_name":"GeneralNeuralTrainer.learning_function","method_code":"def learning_function(self):\n        \"\"\"\"\"\"\n        network_updates = list(self.network.updates) + list(self.network.training_updates)\n        learning_updates = list(self._learning_updates())\n        update_list = network_updates + learning_updates\n\n        logging.info(\"network updates: %s\" % \" \".join(map(str, [x[0] for x in network_updates])))\n        logging.info(\"learning updates: %s\" % \" \".join(map(str, [x[0] for x in learning_updates])))\n\n        variables = self.network.input_variables + self.network.target_variables\n        givens = None\n        return theano.function(\n            variables,\n            map(lambda v: theano.Out(v, borrow=True), self.training_variables),\n            updates=update_list, allow_input_downcast=True,\n            mode=self.config.get(\"theano_mode\", None),\n            givens=givens)","method_summary":"Get the learning function.","original_method_code":"def learning_function(self):\n        \"\"\"\n        Get the learning function.\n        :param func:\n        :return:\n        \"\"\"\n        network_updates = list(self.network.updates) + list(self.network.training_updates)\n        learning_updates = list(self._learning_updates())\n        update_list = network_updates + learning_updates\n\n        logging.info(\"network updates: %s\" % \" \".join(map(str, [x[0] for x in network_updates])))\n        logging.info(\"learning updates: %s\" % \" \".join(map(str, [x[0] for x in learning_updates])))\n\n        variables = self.network.input_variables + self.network.target_variables\n        givens = None\n        return theano.function(\n            variables,\n            map(lambda v: theano.Out(v, borrow=True), self.training_variables),\n            updates=update_list, allow_input_downcast=True,\n            mode=self.config.get(\"theano_mode\", None),\n            givens=givens)","method_path":"https:\/\/github.com\/zomux\/deepy\/blob\/090fbad22a08a809b12951cd0d4984f5bd432698\/deepy\/trainers\/trainers.py#L74-L94"}
{"repo_name":"zomux\/deepy","method_name":"get_network","method_code":"def get_network(model=None, std=0.005, disable_reinforce=False, random_glimpse=False):\n    \"\"\"\"\"\"\n    network = NeuralClassifier(input_dim=28 * 28)\n    network.stack_layer(FirstGlimpseLayer(std=std, disable_reinforce=disable_reinforce, random_glimpse=random_glimpse))\n    if model and os.path.exists(model):\n        network.load_params(model)\n    return network","method_summary":"Get baseline model.","original_method_code":"def get_network(model=None, std=0.005, disable_reinforce=False, random_glimpse=False):\n    \"\"\"\n    Get baseline model.\n    Parameters:\n        model - model path\n    Returns:\n        network\n    \"\"\"\n    network = NeuralClassifier(input_dim=28 * 28)\n    network.stack_layer(FirstGlimpseLayer(std=std, disable_reinforce=disable_reinforce, random_glimpse=random_glimpse))\n    if model and os.path.exists(model):\n        network.load_params(model)\n    return network","method_path":"https:\/\/github.com\/zomux\/deepy\/blob\/090fbad22a08a809b12951cd0d4984f5bd432698\/examples\/attention_models\/first_glimpse_model.py#L192-L204"}
{"repo_name":"zomux\/deepy","method_name":"FirstGlimpseLayer._first_glimpse_sensor","method_code":"def _first_glimpse_sensor(self, x_t):\n        \"\"\"\"\"\"\n        downsampled_img = theano.tensor.signal.downsample.max_pool_2d(x_t, (4,4))\n        downsampled_img = downsampled_img.flatten()\n        first_l = T.dot(downsampled_img, self.W_f)\n        if self.disable_reinforce:\n            wf_grad = self.W_f\n            if self.random_glimpse:\n                first_l = self.srng.uniform((2,), low=-1.7, high=1.7)\n        else:\n            sampled_l_t = self._sample_gaussian(first_l, self.cov)\n            sampled_pdf = self._multi_gaussian_pdf(disconnected_grad(sampled_l_t), first_l)\n            wf_grad = T.grad(T.log(sampled_pdf), self.W_f)\n            first_l = sampled_l_t\n        return first_l, wf_grad","method_summary":"Compute first glimpse position using down-sampled image.","original_method_code":"def _first_glimpse_sensor(self, x_t):\n        \"\"\"\n        Compute first glimpse position using down-sampled image.\n        \"\"\"\n        downsampled_img = theano.tensor.signal.downsample.max_pool_2d(x_t, (4,4))\n        downsampled_img = downsampled_img.flatten()\n        first_l = T.dot(downsampled_img, self.W_f)\n        if self.disable_reinforce:\n            wf_grad = self.W_f\n            if self.random_glimpse:\n                first_l = self.srng.uniform((2,), low=-1.7, high=1.7)\n        else:\n            sampled_l_t = self._sample_gaussian(first_l, self.cov)\n            sampled_pdf = self._multi_gaussian_pdf(disconnected_grad(sampled_l_t), first_l)\n            wf_grad = T.grad(T.log(sampled_pdf), self.W_f)\n            first_l = sampled_l_t\n        return first_l, wf_grad","method_path":"https:\/\/github.com\/zomux\/deepy\/blob\/090fbad22a08a809b12951cd0d4984f5bd432698\/examples\/attention_models\/first_glimpse_model.py#L38-L54"}
{"repo_name":"zomux\/deepy","method_name":"MyJointTrainingModel.prepare","method_code":"def prepare(self):\n        \"\"\"\"\"\"\n        self.output_dim = 10\n        self.encoder = Chain(self.input_dim).stack(Dense(self.internal_layer_size, 'tanh'))\n        self.decoder = Chain(self.internal_layer_size).stack(Dense(self.input_dim))\n        self.classifier = Chain(self.internal_layer_size).stack(Dense(50, 'tanh'),\n                                                      Dense(self.output_dim),\n                                                      Softmax())\n\n        self.register_inner_layers(self.encoder, self.decoder, self.classifier)\n\n        self.target_input = T.ivector('target')\n        self.register_external_inputs(self.target_input)","method_summary":"All codes that create parameters should be put into 'setup' function.","original_method_code":"def prepare(self):\n        \"\"\"\n        All codes that create parameters should be put into 'setup' function.\n        \"\"\"\n        self.output_dim = 10\n        self.encoder = Chain(self.input_dim).stack(Dense(self.internal_layer_size, 'tanh'))\n        self.decoder = Chain(self.internal_layer_size).stack(Dense(self.input_dim))\n        self.classifier = Chain(self.internal_layer_size).stack(Dense(50, 'tanh'),\n                                                      Dense(self.output_dim),\n                                                      Softmax())\n\n        self.register_inner_layers(self.encoder, self.decoder, self.classifier)\n\n        self.target_input = T.ivector('target')\n        self.register_external_inputs(self.target_input)","method_path":"https:\/\/github.com\/zomux\/deepy\/blob\/090fbad22a08a809b12951cd0d4984f5bd432698\/examples\/tutorials\/tutorial2.py#L27-L41"}
{"repo_name":"authomatic\/authomatic","method_name":"OAuth2.create_request_elements","method_code":"def create_request_elements(\n            cls, request_type, credentials, url, method='GET', params=None,\n            headers=None, body='', secret=None, redirect_uri='', scope='',\n            csrf='', user_state=''\n    ):\n        \"\"\"\"\"\"\n\n        headers = headers or {}\n        params = params or {}\n\n        consumer_key = credentials.consumer_key or ''\n        consumer_secret = credentials.consumer_secret or ''\n        token = credentials.token or ''\n        refresh_token = credentials.refresh_token or credentials.token or ''\n\n        \n        url, base_params = cls._split_url(url)\n\n        \n        params.update(dict(base_params))\n\n        if request_type == cls.USER_AUTHORIZATION_REQUEST_TYPE:\n            \n            \n            if consumer_key and redirect_uri and (\n                    csrf or not cls.supports_csrf_protection):\n                params['client_id'] = consumer_key\n                params['redirect_uri'] = redirect_uri\n                params['scope'] = scope\n                if cls.supports_user_state:\n                    params['state'] = base64.urlsafe_b64encode(\n                        json.dumps(\n                            {\"csrf\": csrf, \"user_state\": user_state}\n                        ).encode('utf-8')\n                    )\n                else:\n                    params['state'] = csrf\n                params['response_type'] = 'code'\n\n                \n                headers.update(cls._authorization_header(credentials))\n            else:\n                raise OAuth2Error(\n                    'Credentials with valid consumer_key and arguments '\n                    'redirect_uri, scope and state are required to create '\n                    'OAuth 2.0 user authorization request elements!')\n\n        elif request_type == cls.ACCESS_TOKEN_REQUEST_TYPE:\n            \n            if consumer_key and consumer_secret:\n                params['code'] = token\n                params['client_id'] = consumer_key\n                params['client_secret'] = consumer_secret\n                params['redirect_uri'] = redirect_uri\n                params['grant_type'] = 'authorization_code'\n\n                \n                headers.update(cls._authorization_header(credentials))\n            else:\n                raise OAuth2Error(\n                    'Credentials with valid token, consumer_key, '\n                    'consumer_secret and argument redirect_uri are required '\n                    'to create OAuth 2.0 access token request elements!')\n\n        elif request_type == cls.REFRESH_TOKEN_REQUEST_TYPE:\n            \n            if refresh_token and consumer_key and consumer_secret:\n                params['refresh_token'] = refresh_token\n                params['client_id'] = consumer_key\n                params['client_secret'] = consumer_secret\n                params['grant_type'] = 'refresh_token'\n            else:\n                raise OAuth2Error(\n                    'Credentials with valid refresh_token, consumer_key, '\n                    'consumer_secret are required to create OAuth 2.0 '\n                    'refresh token request elements!')\n\n        elif request_type == cls.PROTECTED_RESOURCE_REQUEST_TYPE:\n            \n\n            \n            \n            if credentials.token_type == cls.BEARER:\n                \n                headers.update(\n                    {'Authorization': 'Bearer {0}'.format(credentials.token)})\n\n            elif token:\n                params['access_token'] = token\n            else:\n                raise OAuth2Error(\n                    'Credentials with valid token are required to create '\n                    'OAuth 2.0 protected resources request elements!')\n\n        request_elements = core.RequestElements(\n            url, method, params, headers, body)\n\n        return cls._x_request_elements_filter(\n            request_type, request_elements, credentials)","method_summary":"Creates |oauth2| request elements.","original_method_code":"def create_request_elements(\n            cls, request_type, credentials, url, method='GET', params=None,\n            headers=None, body='', secret=None, redirect_uri='', scope='',\n            csrf='', user_state=''\n    ):\n        \"\"\"\n        Creates |oauth2| request elements.\n        \"\"\"\n\n        headers = headers or {}\n        params = params or {}\n\n        consumer_key = credentials.consumer_key or ''\n        consumer_secret = credentials.consumer_secret or ''\n        token = credentials.token or ''\n        refresh_token = credentials.refresh_token or credentials.token or ''\n\n        # Separate url base and query parameters.\n        url, base_params = cls._split_url(url)\n\n        # Add params extracted from URL.\n        params.update(dict(base_params))\n\n        if request_type == cls.USER_AUTHORIZATION_REQUEST_TYPE:\n            # User authorization request.\n            # TODO: Raise error for specific message for each missing argument.\n            if consumer_key and redirect_uri and (\n                    csrf or not cls.supports_csrf_protection):\n                params['client_id'] = consumer_key\n                params['redirect_uri'] = redirect_uri\n                params['scope'] = scope\n                if cls.supports_user_state:\n                    params['state'] = base64.urlsafe_b64encode(\n                        json.dumps(\n                            {\"csrf\": csrf, \"user_state\": user_state}\n                        ).encode('utf-8')\n                    )\n                else:\n                    params['state'] = csrf\n                params['response_type'] = 'code'\n\n                # Add authorization header\n                headers.update(cls._authorization_header(credentials))\n            else:\n                raise OAuth2Error(\n                    'Credentials with valid consumer_key and arguments '\n                    'redirect_uri, scope and state are required to create '\n                    'OAuth 2.0 user authorization request elements!')\n\n        elif request_type == cls.ACCESS_TOKEN_REQUEST_TYPE:\n            # Access token request.\n            if consumer_key and consumer_secret:\n                params['code'] = token\n                params['client_id'] = consumer_key\n                params['client_secret'] = consumer_secret\n                params['redirect_uri'] = redirect_uri\n                params['grant_type'] = 'authorization_code'\n\n                # TODO: Check whether all providers accept it\n                headers.update(cls._authorization_header(credentials))\n            else:\n                raise OAuth2Error(\n                    'Credentials with valid token, consumer_key, '\n                    'consumer_secret and argument redirect_uri are required '\n                    'to create OAuth 2.0 access token request elements!')\n\n        elif request_type == cls.REFRESH_TOKEN_REQUEST_TYPE:\n            # Refresh access token request.\n            if refresh_token and consumer_key and consumer_secret:\n                params['refresh_token'] = refresh_token\n                params['client_id'] = consumer_key\n                params['client_secret'] = consumer_secret\n                params['grant_type'] = 'refresh_token'\n            else:\n                raise OAuth2Error(\n                    'Credentials with valid refresh_token, consumer_key, '\n                    'consumer_secret are required to create OAuth 2.0 '\n                    'refresh token request elements!')\n\n        elif request_type == cls.PROTECTED_RESOURCE_REQUEST_TYPE:\n            # Protected resource request.\n\n            # Add Authorization header. See:\n            # http:\/\/tools.ietf.org\/html\/rfc6749#section-7.1\n            if credentials.token_type == cls.BEARER:\n                # http:\/\/tools.ietf.org\/html\/rfc6750#section-2.1\n                headers.update(\n                    {'Authorization': 'Bearer {0}'.format(credentials.token)})\n\n            elif token:\n                params['access_token'] = token\n            else:\n                raise OAuth2Error(\n                    'Credentials with valid token are required to create '\n                    'OAuth 2.0 protected resources request elements!')\n\n        request_elements = core.RequestElements(\n            url, method, params, headers, body)\n\n        return cls._x_request_elements_filter(\n            request_type, request_elements, credentials)","method_path":"https:\/\/github.com\/authomatic\/authomatic\/blob\/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e\/authomatic\/providers\/oauth2.py#L115-L215"}
{"repo_name":"authomatic\/authomatic","method_name":"OAuth2.decode_state","method_code":"def decode_state(cls, state, param='user_state'):\n        \"\"\"\"\"\"\n        if state and cls.supports_user_state:\n            \n            \n            \n            return json.loads(base64.urlsafe_b64decode(\n                unquote(str(state))).decode('utf-8'))[param]\n        else:\n            return state if param == 'csrf' else ''","method_summary":"Decode state and return param.","original_method_code":"def decode_state(cls, state, param='user_state'):\n        \"\"\"\n        Decode state and return param.\n\n        :param str state:\n            state parameter passed through by provider\n\n        :param str param:\n            key to query from decoded state variable. Options include 'csrf'\n            and 'user_state'.\n\n        :returns:\n            string value from decoded state\n\n        \"\"\"\n        if state and cls.supports_user_state:\n            # urlsafe_b64 may include = which the browser quotes so must\n            # unquote Cast to str to void b64decode translation error. Base64\n            # should be str compatible.\n            return json.loads(base64.urlsafe_b64decode(\n                unquote(str(state))).decode('utf-8'))[param]\n        else:\n            return state if param == 'csrf' else ''","method_path":"https:\/\/github.com\/authomatic\/authomatic\/blob\/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e\/authomatic\/providers\/oauth2.py#L262-L284"}
{"repo_name":"authomatic\/authomatic","method_name":"OAuth2.refresh_credentials","method_code":"def refresh_credentials(self, credentials):\n        \"\"\"\"\"\"\n\n        if not self._x_refresh_credentials_if(credentials):\n            return\n\n        \n        cfg = credentials.config.get(credentials.provider_name)\n        credentials.consumer_key = cfg.get('consumer_key')\n        credentials.consumer_secret = cfg.get('consumer_secret')\n\n        request_elements = self.create_request_elements(\n            request_type=self.REFRESH_TOKEN_REQUEST_TYPE,\n            credentials=credentials,\n            url=self.access_token_url,\n            method='POST'\n        )\n\n        self._log(logging.INFO, u'Refreshing credentials.')\n        response = self._fetch(*request_elements)\n\n        \n        credentials.consumer_key = None\n        credentials.consumer_secret = None\n\n        \n        access_token = response.data.get('access_token')\n        refresh_token = response.data.get('refresh_token')\n\n        \n        if access_token:\n            credentials.token = access_token\n            credentials.expire_in = response.data.get('expires_in')\n\n            \n            if refresh_token:\n                credentials.refresh_token = refresh_token\n\n            \n            credentials = self._x_credentials_parser(\n                credentials, response.data)\n\n        return response","method_summary":"Refreshes :class:`.Credentials` if it gives sense.","original_method_code":"def refresh_credentials(self, credentials):\n        \"\"\"\n        Refreshes :class:`.Credentials` if it gives sense.\n\n        :param credentials:\n            :class:`.Credentials` to be refreshed.\n\n        :returns:\n            :class:`.Response`.\n\n        \"\"\"\n\n        if not self._x_refresh_credentials_if(credentials):\n            return\n\n        # We need consumer key and secret to make this kind of request.\n        cfg = credentials.config.get(credentials.provider_name)\n        credentials.consumer_key = cfg.get('consumer_key')\n        credentials.consumer_secret = cfg.get('consumer_secret')\n\n        request_elements = self.create_request_elements(\n            request_type=self.REFRESH_TOKEN_REQUEST_TYPE,\n            credentials=credentials,\n            url=self.access_token_url,\n            method='POST'\n        )\n\n        self._log(logging.INFO, u'Refreshing credentials.')\n        response = self._fetch(*request_elements)\n\n        # We no longer need consumer info.\n        credentials.consumer_key = None\n        credentials.consumer_secret = None\n\n        # Extract the refreshed data.\n        access_token = response.data.get('access_token')\n        refresh_token = response.data.get('refresh_token')\n\n        # Update credentials only if there is access token.\n        if access_token:\n            credentials.token = access_token\n            credentials.expire_in = response.data.get('expires_in')\n\n            # Update refresh token only if there is a new one.\n            if refresh_token:\n                credentials.refresh_token = refresh_token\n\n            # Handle different naming conventions across providers.\n            credentials = self._x_credentials_parser(\n                credentials, response.data)\n\n        return response","method_path":"https:\/\/github.com\/authomatic\/authomatic\/blob\/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e\/authomatic\/providers\/oauth2.py#L286-L337"}
{"repo_name":"authomatic\/authomatic","method_name":"Facebook._x_credentials_parser","method_code":"def _x_credentials_parser(credentials, data):\n        \"\"\"\"\"\"\n\n        \n        credentials.expire_in = data.get('expires')\n\n        if data.get('token_type') == 'bearer':\n            \n            credentials.token_type = 'Bearer'\n\n        return credentials","method_summary":"We need to override this method to fix Facebooks naming deviation.","original_method_code":"def _x_credentials_parser(credentials, data):\n        \"\"\"\n        We need to override this method to fix Facebooks naming deviation.\n        \"\"\"\n\n        # Facebook returns \"expires\" instead of \"expires_in\".\n        credentials.expire_in = data.get('expires')\n\n        if data.get('token_type') == 'bearer':\n            # TODO: cls is not available here, hardcode for now.\n            credentials.token_type = 'Bearer'\n\n        return credentials","method_path":"https:\/\/github.com\/authomatic\/authomatic\/blob\/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e\/authomatic\/providers\/oauth2.py#L976-L988"}
{"repo_name":"authomatic\/authomatic","method_name":"Google._x_request_elements_filter","method_code":"def _x_request_elements_filter(cls, request_type, request_elements,\n                                   credentials):\n        \"\"\"\"\"\"\n        if request_type is cls.ACCESS_TOKEN_REQUEST_TYPE:\n            params = request_elements[2]\n            del params['client_id']\n            del params['client_secret']\n        return request_elements","method_summary":"Google doesn't accept client ID and secret to be at the same time in request parameters and in the basic authorization header in the access token request.","original_method_code":"def _x_request_elements_filter(cls, request_type, request_elements,\n                                   credentials):\n        \"\"\"\n        Google doesn't accept client ID and secret to be at the same time in\n        request parameters and in the basic authorization header in the access\n        token request.\n        \"\"\"\n        if request_type is cls.ACCESS_TOKEN_REQUEST_TYPE:\n            params = request_elements[2]\n            del params['client_id']\n            del params['client_secret']\n        return request_elements","method_path":"https:\/\/github.com\/authomatic\/authomatic\/blob\/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e\/authomatic\/providers\/oauth2.py#L1283-L1294"}
{"repo_name":"authomatic\/authomatic","method_name":"login","method_code":"def login(provider_name):\n    \"\"\"\"\"\"\n\n    \n    response = make_response()\n\n    \n    result = authomatic.login(\n        WerkzeugAdapter(\n            request,\n            response),\n        provider_name)\n\n    \n    if result:\n        if result.user:\n            \n            result.user.update()\n\n        \n        return render_template('login.html', result=result)\n\n    \n    return response","method_summary":"Login handler, must accept both GET and POST to be able to use OpenID.","original_method_code":"def login(provider_name):\n    \"\"\"\n    Login handler, must accept both GET and POST to be able to use OpenID.\n    \"\"\"\n\n    # We need response object for the WerkzeugAdapter.\n    response = make_response()\n\n    # Log the user in, pass it the adapter and the provider name.\n    result = authomatic.login(\n        WerkzeugAdapter(\n            request,\n            response),\n        provider_name)\n\n    # If there is no LoginResult object, the login procedure is still pending.\n    if result:\n        if result.user:\n            # We need to update the user to get more info.\n            result.user.update()\n\n        # The rest happens inside the template.\n        return render_template('login.html', result=result)\n\n    # Don't forget to return the response.\n    return response","method_path":"https:\/\/github.com\/authomatic\/authomatic\/blob\/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e\/examples\/flask\/werkzeug_adapter\/main.py#L29-L54"}
{"repo_name":"authomatic\/authomatic","method_name":"normalize_dict","method_code":"def normalize_dict(dict_):\n    \"\"\"\"\"\"\n\n    return dict([(k, v[0] if not isinstance(v, str) and len(v) == 1 else v)\n                 for k, v in list(dict_.items())])","method_summary":"Replaces all values that are single-item iterables with the value of its index 0.","original_method_code":"def normalize_dict(dict_):\n    \"\"\"\n    Replaces all values that are single-item iterables with the value of its\n    index 0.\n\n    :param dict dict_:\n        Dictionary to normalize.\n\n    :returns:\n        Normalized dictionary.\n\n    \"\"\"\n\n    return dict([(k, v[0] if not isinstance(v, str) and len(v) == 1 else v)\n                 for k, v in list(dict_.items())])","method_path":"https:\/\/github.com\/authomatic\/authomatic\/blob\/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e\/authomatic\/core.py#L40-L54"}
{"repo_name":"authomatic\/authomatic","method_name":"items_to_dict","method_code":"def items_to_dict(items):\n    \"\"\"\"\"\"\n\n    res = collections.defaultdict(list)\n\n    for k, v in items:\n        res[k].append(v)\n\n    return normalize_dict(dict(res))","method_summary":"Converts list of tuples to dictionary with duplicate keys converted to lists.","original_method_code":"def items_to_dict(items):\n    \"\"\"\n    Converts list of tuples to dictionary with duplicate keys converted to\n    lists.\n\n    :param list items:\n        List of tuples.\n\n    :returns:\n        :class:`dict`\n\n    \"\"\"\n\n    res = collections.defaultdict(list)\n\n    for k, v in items:\n        res[k].append(v)\n\n    return normalize_dict(dict(res))","method_path":"https:\/\/github.com\/authomatic\/authomatic\/blob\/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e\/authomatic\/core.py#L57-L75"}
{"repo_name":"authomatic\/authomatic","method_name":"json_qs_parser","method_code":"def json_qs_parser(body):\n    \"\"\"\"\"\"\n    try:\n        \n        return json.loads(body)\n    except (OverflowError, TypeError, ValueError):\n        pass\n\n    try:\n        \n        return ElementTree.fromstring(body)\n    except (ElementTree.ParseError, TypeError, ValueError):\n        pass\n\n    \n    return dict(parse.parse_qsl(body))","method_summary":"Parses response body from JSON, XML or query string.","original_method_code":"def json_qs_parser(body):\n    \"\"\"\n    Parses response body from JSON, XML or query string.\n\n    :param body:\n        string\n\n    :returns:\n        :class:`dict`, :class:`list` if input is JSON or query string,\n        :class:`xml.etree.ElementTree.Element` if XML.\n\n    \"\"\"\n    try:\n        # Try JSON first.\n        return json.loads(body)\n    except (OverflowError, TypeError, ValueError):\n        pass\n\n    try:\n        # Then XML.\n        return ElementTree.fromstring(body)\n    except (ElementTree.ParseError, TypeError, ValueError):\n        pass\n\n    # Finally query string.\n    return dict(parse.parse_qsl(body))","method_path":"https:\/\/github.com\/authomatic\/authomatic\/blob\/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e\/authomatic\/core.py#L143-L168"}
{"repo_name":"authomatic\/authomatic","method_name":"Session.create_cookie","method_code":"def create_cookie(self, delete=None):\n        \"\"\"\"\"\"\n        value = 'deleted' if delete else self._serialize(self.data)\n        split_url = parse.urlsplit(self.adapter.url)\n        domain = split_url.netloc.split(':')[0]\n\n        \n        \n        if '.' not in domain:\n            template = '{name}={value}; Path={path}; HttpOnly{secure}{expires}'\n        else:\n            template = ('{name}={value}; Domain={domain}; Path={path}; '\n                        'HttpOnly{secure}{expires}')\n\n        return template.format(\n            name=self.name,\n            value=value,\n            domain=domain,\n            path=split_url.path,\n            secure='; Secure' if self.secure else '',\n            expires='; Expires=Thu, 01-Jan-1970 00:00:01 GMT' if delete else ''\n        )","method_summary":"Creates the value for ``Set-Cookie`` HTTP header.","original_method_code":"def create_cookie(self, delete=None):\n        \"\"\"\n        Creates the value for ``Set-Cookie`` HTTP header.\n\n        :param bool delete:\n            If ``True`` the cookie value will be ``deleted`` and the\n            Expires value will be ``Thu, 01-Jan-1970 00:00:01 GMT``.\n\n        \"\"\"\n        value = 'deleted' if delete else self._serialize(self.data)\n        split_url = parse.urlsplit(self.adapter.url)\n        domain = split_url.netloc.split(':')[0]\n\n        # Work-around for issue #11, failure of WebKit-based browsers to accept\n        # cookies set as part of a redirect response in some circumstances.\n        if '.' not in domain:\n            template = '{name}={value}; Path={path}; HttpOnly{secure}{expires}'\n        else:\n            template = ('{name}={value}; Domain={domain}; Path={path}; '\n                        'HttpOnly{secure}{expires}')\n\n        return template.format(\n            name=self.name,\n            value=value,\n            domain=domain,\n            path=split_url.path,\n            secure='; Secure' if self.secure else '',\n            expires='; Expires=Thu, 01-Jan-1970 00:00:01 GMT' if delete else ''\n        )","method_path":"https:\/\/github.com\/authomatic\/authomatic\/blob\/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e\/authomatic\/core.py#L364-L392"}
{"repo_name":"Nic30\/hwt","method_name":"getBits_from_array","method_code":"def getBits_from_array(array, wordWidth, start, end,\n                       reinterpretElmToType=None):\n    \"\"\"\"\"\"\n    inPartOffset = 0\n    value = Bits(end - start, None).fromPy(None)\n\n    while start != end:\n        assert start < end, (start, end)\n\n        dataWordIndex = start \/\/ wordWidth\n\n        v = array[dataWordIndex]\n        if reinterpretElmToType is not None:\n            v = v._reinterpret_cast(reinterpretElmToType)\n\n        endOfWord = (dataWordIndex + 1) * wordWidth\n        width = min(end, endOfWord) - start\n        offset = start % wordWidth\n\n        val = selectBitRange(v.val, offset, width)\n        vldMask = selectBitRange(v.vldMask, offset, width)\n        updateTime = v.updateTime\n\n        m = mask(width)\n        value.val |= (val & m) << inPartOffset\n        value.vldMask |= (vldMask & m) << inPartOffset\n        value.updateMask = max(value.updateTime, updateTime)\n\n        inPartOffset += width\n        start += width\n\n    return value","method_summary":"Gets value of bits between selected range from memory","original_method_code":"def getBits_from_array(array, wordWidth, start, end,\n                       reinterpretElmToType=None):\n    \"\"\"\n    Gets value of bits between selected range from memory\n\n    :param start: bit address of start of bit of bits\n    :param end: bit address of first bit behind bits\n    :return: instance of BitsVal (derived from SimBits type) which contains\n        copy of selected bits\n    \"\"\"\n    inPartOffset = 0\n    value = Bits(end - start, None).fromPy(None)\n\n    while start != end:\n        assert start < end, (start, end)\n\n        dataWordIndex = start \/\/ wordWidth\n\n        v = array[dataWordIndex]\n        if reinterpretElmToType is not None:\n            v = v._reinterpret_cast(reinterpretElmToType)\n\n        endOfWord = (dataWordIndex + 1) * wordWidth\n        width = min(end, endOfWord) - start\n        offset = start % wordWidth\n\n        val = selectBitRange(v.val, offset, width)\n        vldMask = selectBitRange(v.vldMask, offset, width)\n        updateTime = v.updateTime\n\n        m = mask(width)\n        value.val |= (val & m) << inPartOffset\n        value.vldMask |= (vldMask & m) << inPartOffset\n        value.updateMask = max(value.updateTime, updateTime)\n\n        inPartOffset += width\n        start += width\n\n    return value","method_path":"https:\/\/github.com\/Nic30\/hwt\/blob\/8cbb399e326da3b22c233b98188a9d08dec057e6\/hwt\/hdl\/types\/arrayCast.py#L11-L49"}
{"repo_name":"Nic30\/hwt","method_name":"reinterptet_harray_to_bits","method_code":"def reinterptet_harray_to_bits(typeFrom, sigOrVal, bitsT):\n    \"\"\"\"\"\"\n    size = int(typeFrom.size)\n    widthOfElm = typeFrom.elmType.bit_length()\n    w = bitsT.bit_length()\n    if size * widthOfElm != w:\n        raise TypeConversionErr(\n            \"Size of types is different\", size * widthOfElm, w)\n\n    partT = Bits(widthOfElm)\n    parts = [p._reinterpret_cast(partT) for p in sigOrVal]\n\n    return Concat(*reversed(parts))._reinterpret_cast(bitsT)","method_summary":"Cast HArray signal or value to signal or value of type Bits","original_method_code":"def reinterptet_harray_to_bits(typeFrom, sigOrVal, bitsT):\n    \"\"\"\n    Cast HArray signal or value to signal or value of type Bits\n    \"\"\"\n    size = int(typeFrom.size)\n    widthOfElm = typeFrom.elmType.bit_length()\n    w = bitsT.bit_length()\n    if size * widthOfElm != w:\n        raise TypeConversionErr(\n            \"Size of types is different\", size * widthOfElm, w)\n\n    partT = Bits(widthOfElm)\n    parts = [p._reinterpret_cast(partT) for p in sigOrVal]\n\n    return Concat(*reversed(parts))._reinterpret_cast(bitsT)","method_path":"https:\/\/github.com\/Nic30\/hwt\/blob\/8cbb399e326da3b22c233b98188a9d08dec057e6\/hwt\/hdl\/types\/arrayCast.py#L52-L66"}
{"repo_name":"Nic30\/hwt","method_name":"slice_to_SLICE","method_code":"def slice_to_SLICE(sliceVals, width):\n    \"\"\"\"\"\"\n    if sliceVals.step is not None:\n        raise NotImplementedError()\n\n    start = sliceVals.start\n    stop = sliceVals.stop\n\n    if sliceVals.start is None:\n        start = INT.fromPy(width)\n    else:\n        start = toHVal(sliceVals.start)\n\n    if sliceVals.stop is None:\n        stop = INT.fromPy(0)\n    else:\n        stop = toHVal(sliceVals.stop)\n\n    startIsVal = isinstance(start, Value)\n    stopIsVal = isinstance(stop, Value)\n\n    indexesAreValues = startIsVal and stopIsVal\n    if indexesAreValues:\n        updateTime = max(start.updateTime, stop.updateTime)\n    else:\n        updateTime = -1\n\n    return Slice.getValueCls()((start, stop), SLICE, 1, updateTime)","method_summary":"convert python slice to value of SLICE hdl type","original_method_code":"def slice_to_SLICE(sliceVals, width):\n    \"\"\"convert python slice to value of SLICE hdl type\"\"\"\n    if sliceVals.step is not None:\n        raise NotImplementedError()\n\n    start = sliceVals.start\n    stop = sliceVals.stop\n\n    if sliceVals.start is None:\n        start = INT.fromPy(width)\n    else:\n        start = toHVal(sliceVals.start)\n\n    if sliceVals.stop is None:\n        stop = INT.fromPy(0)\n    else:\n        stop = toHVal(sliceVals.stop)\n\n    startIsVal = isinstance(start, Value)\n    stopIsVal = isinstance(stop, Value)\n\n    indexesAreValues = startIsVal and stopIsVal\n    if indexesAreValues:\n        updateTime = max(start.updateTime, stop.updateTime)\n    else:\n        updateTime = -1\n\n    return Slice.getValueCls()((start, stop), SLICE, 1, updateTime)","method_path":"https:\/\/github.com\/Nic30\/hwt\/blob\/8cbb399e326da3b22c233b98188a9d08dec057e6\/hwt\/hdl\/types\/sliceUtils.py#L9-L36"}
{"repo_name":"Nic30\/hwt","method_name":"fill_stm_list_with_enclosure","method_code":"def fill_stm_list_with_enclosure(parentStm: Optional[HdlStatement],\n                                 current_enclosure: Set[RtlSignalBase],\n                                 statements: List[\"HdlStatement\"],\n                                 do_enclose_for: List[RtlSignalBase],\n                                 enclosure: Dict[RtlSignalBase, Union[Value, RtlSignalBase]])\\\n        -> None:\n    \"\"\"\"\"\"\n    if statements is None:\n        statements = []\n\n    for e_sig in do_enclose_for:\n        if e_sig in current_enclosure:\n            continue\n        enclosed = False\n        for stm in statements:\n            if e_sig in stm._outputs:\n                if e_sig not in stm._enclosed_for:\n                    stm._fill_enclosure(enclosure)\n                enclosed = True\n                break\n        \n        if not enclosed:\n            e = enclosure[e_sig]\n            a = Assignment(e, e_sig)\n            statements.append(a)\n\n            if parentStm is not None:\n                a._set_parent_stm(parentStm)\n\n    return statements","method_summary":"Apply enclosure on list of statements (fill all unused code branches with assignments from value specified by enclosure)","original_method_code":"def fill_stm_list_with_enclosure(parentStm: Optional[HdlStatement],\n                                 current_enclosure: Set[RtlSignalBase],\n                                 statements: List[\"HdlStatement\"],\n                                 do_enclose_for: List[RtlSignalBase],\n                                 enclosure: Dict[RtlSignalBase, Union[Value, RtlSignalBase]])\\\n        -> None:\n    \"\"\"\n    Apply enclosure on list of statements\n    (fill all unused code branches with assignments from value specified by enclosure)\n\n    :param parentStm: optional parent statement where this list is some branch\n    :param current_enclosure: list of signals for which this statement list is enclosed\n    :param statements: list of statements\n    :param do_enclose_for: selected signals for which enclosure should be used\n    :param enclosure: enclosure values for signals\n\n    :attention: original statements parameter can be modified\n    :return: new statements\n    \"\"\"\n    if statements is None:\n        statements = []\n\n    for e_sig in do_enclose_for:\n        if e_sig in current_enclosure:\n            continue\n        enclosed = False\n        for stm in statements:\n            if e_sig in stm._outputs:\n                if e_sig not in stm._enclosed_for:\n                    stm._fill_enclosure(enclosure)\n                enclosed = True\n                break\n        # any statement was not related with this signal,\n        if not enclosed:\n            e = enclosure[e_sig]\n            a = Assignment(e, e_sig)\n            statements.append(a)\n\n            if parentStm is not None:\n                a._set_parent_stm(parentStm)\n\n    return statements","method_path":"https:\/\/github.com\/Nic30\/hwt\/blob\/8cbb399e326da3b22c233b98188a9d08dec057e6\/hwt\/hdl\/statementUtils.py#L10-L51"}
{"repo_name":"Nic30\/hwt","method_name":"find_files","method_code":"def find_files(directory, pattern, recursive=True):\n    \"\"\"\"\"\"\n    if not os.path.isdir(directory):\n        if os.path.exists(directory):\n            raise IOError(directory + ' is not directory')\n        else:\n            raise IOError(directory + \" does not exists\")\n    if recursive:\n        for root, _, files in os.walk(directory):\n            for basename in files:\n                if fnmatch.fnmatch(basename, pattern):\n                    filename = os.path.join(root, basename)\n                    yield filename\n    else:\n        root = directory\n        for basename in os.listdir(root):\n            if fnmatch.fnmatch(basename, pattern):\n                filename = os.path.join(root, basename)\n                if os.path.isfile(filename):\n                    yield filename","method_summary":"Find files by pattern in directory","original_method_code":"def find_files(directory, pattern, recursive=True):\n    \"\"\"\n    Find files by pattern in directory\n    \"\"\"\n    if not os.path.isdir(directory):\n        if os.path.exists(directory):\n            raise IOError(directory + ' is not directory')\n        else:\n            raise IOError(directory + \" does not exists\")\n    if recursive:\n        for root, _, files in os.walk(directory):\n            for basename in files:\n                if fnmatch.fnmatch(basename, pattern):\n                    filename = os.path.join(root, basename)\n                    yield filename\n    else:\n        root = directory\n        for basename in os.listdir(root):\n            if fnmatch.fnmatch(basename, pattern):\n                filename = os.path.join(root, basename)\n                if os.path.isfile(filename):\n                    yield filename","method_path":"https:\/\/github.com\/Nic30\/hwt\/blob\/8cbb399e326da3b22c233b98188a9d08dec057e6\/hwt\/pyUtils\/fileHelpers.py#L5-L26"}
{"repo_name":"Nic30\/hwt","method_name":"SwitchLogic","method_code":"def SwitchLogic(cases, default=None):\n    \"\"\"\"\"\"\n    if default is not None:\n        assigTop = default\n    else:\n        assigTop = []\n\n    for cond, statements in reversed(cases):\n        assigTop = If(cond,\n                      statements\n                      ).Else(\n            assigTop\n        )\n\n    return assigTop","method_summary":"Generate if tree for cases like (syntax shugar for large elifs)","original_method_code":"def SwitchLogic(cases, default=None):\n    \"\"\"\n    Generate if tree for cases like (syntax shugar for large elifs)\n\n    ..code-block:: python\n        if cond0:\n            statements0\n        elif cond1:\n            statements1\n        else:\n            default\n\n    :param case: iterable of tuples (condition, statements)\n    :param default: default statements\n    \"\"\"\n    if default is not None:\n        assigTop = default\n    else:\n        assigTop = []\n\n    for cond, statements in reversed(cases):\n        assigTop = If(cond,\n                      statements\n                      ).Else(\n            assigTop\n        )\n\n    return assigTop","method_path":"https:\/\/github.com\/Nic30\/hwt\/blob\/8cbb399e326da3b22c233b98188a9d08dec057e6\/hwt\/code.py#L149-L176"}
{"repo_name":"Nic30\/hwt","method_name":"In","method_code":"def In(sigOrVal, iterable):\n    \"\"\"\"\"\"\n    res = None\n    for i in iterable:\n        i = toHVal(i)\n        if res is None:\n            res = sigOrVal._eq(i)\n        else:\n            res = res | sigOrVal._eq(i)\n\n    assert res is not None, \"Parameter iterable is empty\"\n    return res","method_summary":"Hdl convertible in operator, check if any of items in \"iterable\" equals \"sigOrVal\"","original_method_code":"def In(sigOrVal, iterable):\n    \"\"\"\n    Hdl convertible in operator, check if any of items\n    in \"iterable\" equals \"sigOrVal\"\n    \"\"\"\n    res = None\n    for i in iterable:\n        i = toHVal(i)\n        if res is None:\n            res = sigOrVal._eq(i)\n        else:\n            res = res | sigOrVal._eq(i)\n\n    assert res is not None, \"Parameter iterable is empty\"\n    return res","method_path":"https:\/\/github.com\/Nic30\/hwt\/blob\/8cbb399e326da3b22c233b98188a9d08dec057e6\/hwt\/code.py#L179-L193"}
{"repo_name":"Nic30\/hwt","method_name":"StaticForEach","method_code":"def StaticForEach(parentUnit, items, bodyFn, name=\"\"):\n    \"\"\"\"\"\"\n\n    items = list(items)\n    itemsCnt = len(items)\n    if itemsCnt == 0:\n        \n        return []\n    elif itemsCnt == 1:\n        \n        return bodyFn(items[0], 0)\n    else:\n        \n        index = parentUnit._reg(name + \"for_index\",\n                                Bits(log2ceil(itemsCnt + 1), signed=False),\n                                defVal=0)\n        ackSig = parentUnit._sig(name + \"for_ack\")\n\n        statementLists = []\n        for i, (statementList, ack) in [(i, bodyFn(item, i))\n                                        for i, item in enumerate(items)]:\n            statementLists.append(statementList + [(ackSig(ack)), ])\n\n        If(ackSig,\n           If(index._eq(itemsCnt - 1),\n              index(0)\n              ).Else(\n               index(index + 1)\n           )\n           )\n\n        return Switch(index)\\\n            .addCases(\n            enumerate(statementLists)\n        ).Default(\n            bodyFn(items[0], 0)[0],\n            ackSig(True)\n        )","method_summary":"Generate for loop for static items","original_method_code":"def StaticForEach(parentUnit, items, bodyFn, name=\"\"):\n    \"\"\"\n    Generate for loop for static items\n\n    :param parentUnit: unit where this code should be instantiated\n    :param items: items which this \"for\" itering on\n    :param bodyFn: function which fn(item, index) or fn(item)\n        returns (statementList, ack).\n        It's content is performed in every iteration.\n        When ack is high loop will fall to next iteration\n    \"\"\"\n\n    items = list(items)\n    itemsCnt = len(items)\n    if itemsCnt == 0:\n        # if there are no items there is nothing to generate\n        return []\n    elif itemsCnt == 1:\n        # if there is only one item do not generate counter logic generate\n        return bodyFn(items[0], 0)\n    else:\n        # if there is multiple items we have to generate counter logic\n        index = parentUnit._reg(name + \"for_index\",\n                                Bits(log2ceil(itemsCnt + 1), signed=False),\n                                defVal=0)\n        ackSig = parentUnit._sig(name + \"for_ack\")\n\n        statementLists = []\n        for i, (statementList, ack) in [(i, bodyFn(item, i))\n                                        for i, item in enumerate(items)]:\n            statementLists.append(statementList + [(ackSig(ack)), ])\n\n        If(ackSig,\n           If(index._eq(itemsCnt - 1),\n              index(0)\n              ).Else(\n               index(index + 1)\n           )\n           )\n\n        return Switch(index)\\\n            .addCases(\n            enumerate(statementLists)\n        ).Default(\n            bodyFn(items[0], 0)[0],\n            ackSig(True)\n        )","method_path":"https:\/\/github.com\/Nic30\/hwt\/blob\/8cbb399e326da3b22c233b98188a9d08dec057e6\/hwt\/code.py#L196-L242"}
{"repo_name":"Nic30\/hwt","method_name":"connect","method_code":"def connect(src, *destinations, exclude: set=None, fit=False):\n    \"\"\"\"\"\"\n    assignemnts = []\n\n    if isinstance(src, HObjList):\n        for dst in destinations:\n            assert len(src) == len(dst), (src, dst)\n        _destinations = [iter(d) for d in destinations]\n        for _src in src:\n            dsts = [next(d) for d in _destinations]\n            assignemnts.append(connect(_src, *dsts, exclude=exclude, fit=fit))\n    else:\n        for dst in destinations:\n            assignemnts.append(_connect(src, dst, exclude, fit))\n\n    return assignemnts","method_summary":"Connect src (signals\/interfaces\/values) to all destinations","original_method_code":"def connect(src, *destinations, exclude: set=None, fit=False):\n    \"\"\"\n    Connect src (signals\/interfaces\/values) to all destinations\n\n    :param exclude: interfaces on any level on src or destinations\n        which should be excluded from connection process\n    :param fit: auto fit source width to destination width\n    \"\"\"\n    assignemnts = []\n\n    if isinstance(src, HObjList):\n        for dst in destinations:\n            assert len(src) == len(dst), (src, dst)\n        _destinations = [iter(d) for d in destinations]\n        for _src in src:\n            dsts = [next(d) for d in _destinations]\n            assignemnts.append(connect(_src, *dsts, exclude=exclude, fit=fit))\n    else:\n        for dst in destinations:\n            assignemnts.append(_connect(src, dst, exclude, fit))\n\n    return assignemnts","method_path":"https:\/\/github.com\/Nic30\/hwt\/blob\/8cbb399e326da3b22c233b98188a9d08dec057e6\/hwt\/code.py#L308-L329"}
{"repo_name":"Nic30\/hwt","method_name":"rol","method_code":"def rol(sig, howMany) -> RtlSignalBase:\n    \"Rotate left\"\n    width = sig._dtype.bit_length()\n    return sig[(width - howMany):]._concat(sig[:(width - howMany)])","method_summary":"Rotate left","original_method_code":"def rol(sig, howMany) -> RtlSignalBase:\n    \"Rotate left\"\n    width = sig._dtype.bit_length()\n    return sig[(width - howMany):]._concat(sig[:(width - howMany)])","method_path":"https:\/\/github.com\/Nic30\/hwt\/blob\/8cbb399e326da3b22c233b98188a9d08dec057e6\/hwt\/code.py#L349-L352"}
{"repo_name":"singularityhub\/sregistry-cli","method_name":"search","method_code":"def search(self, query=None, args=None):\n    ''''''\n\n    if query is not None:\n\n        return self._container_search(query)\n\n    \n    return self._search_all()","method_summary":"query a s3 endpoint for an image based on a string EXAMPLE","original_method_code":"def search(self, query=None, args=None):\n    '''query a s3 endpoint for an image based on a string\n\n    EXAMPLE QUERIES:\n\n    [empty]             list all container collections\n    vsoch\/dinosaur      look for containers with name vsoch\/dinosaur\n    \n    '''\n\n    if query is not None:\n\n        return self._container_search(query)\n\n    # Search collections across all fields\n    return self._search_all()","method_path":"https:\/\/github.com\/singularityhub\/sregistry-cli\/blob\/abc96140a1d15b5e96d83432e1e0e1f4f8f36331\/sregistry\/main\/s3\/query.py#L18-L33"}
{"repo_name":"singularityhub\/sregistry-cli","method_name":"search_all","method_code":"def search_all(self, quiet=False):\n    ''''''\n\n    results = []\n\n    for obj in self.bucket.objects.all():\n       subsrc = obj.Object()\n\n       \n       \n       metadata = dict((k.lower(), v) for k, v in subsrc.metadata.items())\n       size = ''\n\n       \n       datestr = \"%s-%s-%s\" %(obj.last_modified.month,\n                              obj.last_modified.day,\n                              obj.last_modified.year)\n\n       if 'sizemb' in metadata:\n           size = '%sMB' % metadata['sizemb']\n\n       results.append([obj.key, datestr, size ])\n   \n    if len(results) == 0:\n        bot.info(\"No container collections found.\")\n        sys.exit(1)\n\n    if not quiet:\n        bot.info(\"Containers\")\n        bot.table(results)\n    return results","method_summary":"a \"show all\" search that doesn't require a query","original_method_code":"def search_all(self, quiet=False):\n    '''a \"show all\" search that doesn't require a query\n       \n       Parameters\n       ==========\n       quiet: if quiet is True, we only are using the function to return\n              rows of results.\n    '''\n\n    results = []\n\n    for obj in self.bucket.objects.all():\n       subsrc = obj.Object()\n\n       # Metadata bug will capitalize all fields, workaround is to lowercase\n       # https:\/\/github.com\/boto\/boto3\/issues\/1709\n       metadata = dict((k.lower(), v) for k, v in subsrc.metadata.items())\n       size = ''\n\n       # MM-DD-YYYY\n       datestr = \"%s-%s-%s\" %(obj.last_modified.month,\n                              obj.last_modified.day,\n                              obj.last_modified.year)\n\n       if 'sizemb' in metadata:\n           size = '%sMB' % metadata['sizemb']\n\n       results.append([obj.key, datestr, size ])\n   \n    if len(results) == 0:\n        bot.info(\"No container collections found.\")\n        sys.exit(1)\n\n    if not quiet:\n        bot.info(\"Containers\")\n        bot.table(results)\n    return results","method_path":"https:\/\/github.com\/singularityhub\/sregistry-cli\/blob\/abc96140a1d15b5e96d83432e1e0e1f4f8f36331\/sregistry\/main\/s3\/query.py#L41-L77"}
{"repo_name":"singularityhub\/sregistry-cli","method_name":"search","method_code":"def search(self, query=None, args=None):\n    ''''''\n\n    if query is not None:\n\n        \n        if query.endswith('\/'):  \n            return self._collection_search(query)\n\n        \n        elif query.startswith('\/'):  \n            return self._container_search(query, across_collections=True)\n\n        \n        elif \"\/\" in query or \":\" in query:\n            return self._container_search(query)\n\n        \n        return self._collection_search(query=query)\n\n\n    \n    return self._search_all()","method_summary":"query a Singularity registry for a list of images. If query is None, collections are listed. EXAMPLE","original_method_code":"def search(self, query=None, args=None):\n    '''query a Singularity registry for a list of images. \n     If query is None, collections are listed. \n\n    EXAMPLE QUERIES:\n\n    [empty]             list all collections in registry\n    vsoch               do a general search for the expression \"vsoch\"\n    vsoch\/              list all containers in collection vsoch\n    \/dinosaur           list containers across collections called \"dinosaur\"\n    vsoch\/dinosaur      list details of container vsoch\/dinosaur\n                          tag \"latest\" is used by default, and then the most recent\n    vsoch\/dinosaur:tag  list details for specific container\n    \n    '''\n\n    if query is not None:\n\n        # List all containers in collection query\/\n        if query.endswith('\/'):  # collection search\n            return self._collection_search(query)\n\n        # List containers across collections called \/query\n        elif query.startswith('\/'):  \n            return self._container_search(query, across_collections=True)\n\n        # List details of a specific collection container\n        elif \"\/\" in query or \":\" in query:\n            return self._container_search(query)\n\n        # Search collections across all fields\n        return self._collection_search(query=query)\n\n\n    # Search collections across all fields\n    return self._search_all()","method_path":"https:\/\/github.com\/singularityhub\/sregistry-cli\/blob\/abc96140a1d15b5e96d83432e1e0e1f4f8f36331\/sregistry\/main\/registry\/query.py#L20-L55"}
{"repo_name":"singularityhub\/sregistry-cli","method_name":"collection_search","method_code":"def collection_search(self, query):\n    ''''''\n\n    query = query.lower().strip('\/')\n    url = '%s\/collection\/%s' %(self.base, query)\n\n    result = self._get(url)\n    if len(result) == 0:\n        bot.info(\"No collections found.\")\n        sys.exit(1)\n\n    bot.custom(prefix=\"COLLECTION\", message=query)\n\n    rows = []\n    for container in result['containers']:\n        rows.append([ container['uri'],\n                      container['detail'] ])\n\n    bot.table(rows)\n    return rows","method_summary":"collection search will list all containers for a specific collection. We assume query is the name of a collection","original_method_code":"def collection_search(self, query):\n    '''collection search will list all containers for a specific\n    collection. We assume query is the name of a collection'''\n\n    query = query.lower().strip('\/')\n    url = '%s\/collection\/%s' %(self.base, query)\n\n    result = self._get(url)\n    if len(result) == 0:\n        bot.info(\"No collections found.\")\n        sys.exit(1)\n\n    bot.custom(prefix=\"COLLECTION\", message=query)\n\n    rows = []\n    for container in result['containers']:\n        rows.append([ container['uri'],\n                      container['detail'] ])\n\n    bot.table(rows)\n    return rows","method_path":"https:\/\/github.com\/singularityhub\/sregistry-cli\/blob\/abc96140a1d15b5e96d83432e1e0e1f4f8f36331\/sregistry\/main\/registry\/query.py#L87-L107"}
{"repo_name":"singularityhub\/sregistry-cli","method_name":"label_search","method_code":"def label_search(self, key=None, value=None):\n    ''''''\n\n    if key is not None:\n        key = key.lower()\n\n    if value is not None:\n        value = value.lower()\n\n    show_details = True\n    if key is None and value is None:\n        url = '%s\/labels\/search' % (self.base)\n        show_details = False\n\n    elif key is not None and value is not None:\n        url = '%s\/labels\/search\/%s\/key\/%s\/value' % (self.base, key, value)\n\n    elif key is None:\n        url = '%s\/labels\/search\/%s\/value' % (self.base, value)\n\n    else:\n        url = '%s\/labels\/search\/%s\/key' % (self.base, key)\n\n    result = self._get(url)\n    if len(result) == 0:\n        bot.info(\"No labels found.\")\n        sys.exit(0)\n\n    bot.info(\"Labels\\n\")\n\n    rows = []\n    for l in result:        \n        if show_details is True:\n            entry = [\"%s:%s\" %(l['key'],l['value']),\n                     \"\\n%s\\n\\n\" %\"\\n\".join(l['containers'])]\n        else:\n            entry = [\"N=%s\" %len(l['containers']),\n                    \"%s:%s\" %(l['key'],l['value']) ]\n        rows.append(entry)\n    bot.table(rows)\n    return rows","method_summary":"search across labels","original_method_code":"def label_search(self, key=None, value=None):\n    '''search across labels'''\n\n    if key is not None:\n        key = key.lower()\n\n    if value is not None:\n        value = value.lower()\n\n    show_details = True\n    if key is None and value is None:\n        url = '%s\/labels\/search' % (self.base)\n        show_details = False\n\n    elif key is not None and value is not None:\n        url = '%s\/labels\/search\/%s\/key\/%s\/value' % (self.base, key, value)\n\n    elif key is None:\n        url = '%s\/labels\/search\/%s\/value' % (self.base, value)\n\n    else:\n        url = '%s\/labels\/search\/%s\/key' % (self.base, key)\n\n    result = self._get(url)\n    if len(result) == 0:\n        bot.info(\"No labels found.\")\n        sys.exit(0)\n\n    bot.info(\"Labels\\n\")\n\n    rows = []\n    for l in result:        \n        if show_details is True:\n            entry = [\"%s:%s\" %(l['key'],l['value']),\n                     \"\\n%s\\n\\n\" %\"\\n\".join(l['containers'])]\n        else:\n            entry = [\"N=%s\" %len(l['containers']),\n                    \"%s:%s\" %(l['key'],l['value']) ]\n        rows.append(entry)\n    bot.table(rows)\n    return rows","method_path":"https:\/\/github.com\/singularityhub\/sregistry-cli\/blob\/abc96140a1d15b5e96d83432e1e0e1f4f8f36331\/sregistry\/main\/registry\/query.py#L109-L149"}
{"repo_name":"singularityhub\/sregistry-cli","method_name":"search","method_code":"def search(self, query=None, args=None):\n    ''''''\n    if query is None:\n        bot.exit('You must include a collection query, <collection>\/<repo>')\n\n    \n    return self._search_all(query)","method_summary":"query a GitLab artifacts folder for a list of images. If query is None, collections are listed.","original_method_code":"def search(self, query=None, args=None):\n    '''query a GitLab artifacts folder for a list of images. \n     If query is None, collections are listed. \n    '''\n    if query is None:\n        bot.exit('You must include a collection query, <collection>\/<repo>')\n\n    # or default to listing (searching) all things.\n    return self._search_all(query)","method_path":"https:\/\/github.com\/singularityhub\/sregistry-cli\/blob\/abc96140a1d15b5e96d83432e1e0e1f4f8f36331\/sregistry\/main\/gitlab\/query.py#L26-L34"}
{"repo_name":"singularityhub\/sregistry-cli","method_name":"search_all","method_code":"def search_all(self, collection, job_id=None):\n    ''''''\n\n    results = [['job_id', 'browser']]\n \n    url = \"%s\/projects\/%s\/jobs\" %(self.api_base, \n                                  quote_plus(collection.strip('\/')))\n\n    response = requests.get(url, headers=self.headers) \n    if response.status_code == 200:\n        jobs = response.json()\n\n        \n        \n        \n        for job in jobs:\n\n            \n            if job['status'] == 'success':\n                name = job['name']\n\n                for artifact in job['artifacts']:\n                    if artifact['filename'].endswith('zip'):\n                        \n                        \n                        artifact_url = (\"%s\/%s\/-\/jobs\/%s\/artifacts\/browse\/%s\" \n                                        %(self.base , \n                                          collection, \n                                          job['id'],\n                                          name))\n                        results.append([str(job['id']), artifact_url])   \n\n    if len(results) == 1:\n        bot.info(\"No potential archives found in artifacts.\")\n        sys.exit(0)\n\n    bot.info(\"Artifact Browsers (you will need path and job id for pull)\")\n    bot.table(results)\n    return results","method_summary":"a \"show all\" search that doesn't require a query the user is shown URLs to","original_method_code":"def search_all(self, collection, job_id=None):\n    '''a \"show all\" search that doesn't require a query\n       the user is shown URLs to \n    '''\n\n    results = [['job_id', 'browser']]\n \n    url = \"%s\/projects\/%s\/jobs\" %(self.api_base, \n                                  quote_plus(collection.strip('\/')))\n\n    response = requests.get(url, headers=self.headers) \n    if response.status_code == 200:\n        jobs = response.json()\n\n        # We can't get a listing of artifacts\n        # https:\/\/gitlab.com\/gitlab-org\/gitlab-ce\/issues\/51515\n        # Parse through jobs (each can have different tags for a collection):\n        for job in jobs:\n\n            # Only show jobs that are successful\n            if job['status'] == 'success':\n                name = job['name']\n\n                for artifact in job['artifacts']:\n                    if artifact['filename'].endswith('zip'):\n                        \n                        # The user must browse to see the names\n                        artifact_url = (\"%s\/%s\/-\/jobs\/%s\/artifacts\/browse\/%s\" \n                                        %(self.base , \n                                          collection, \n                                          job['id'],\n                                          name))\n                        results.append([str(job['id']), artifact_url])   \n\n    if len(results) == 1:\n        bot.info(\"No potential archives found in artifacts.\")\n        sys.exit(0)\n\n    bot.info(\"Artifact Browsers (you will need path and job id for pull)\")\n    bot.table(results)\n    return results","method_path":"https:\/\/github.com\/singularityhub\/sregistry-cli\/blob\/abc96140a1d15b5e96d83432e1e0e1f4f8f36331\/sregistry\/main\/gitlab\/query.py#L37-L77"}
{"repo_name":"singularityhub\/sregistry-cli","method_name":"ApiConnection._client_tagged","method_code":"def _client_tagged(self, tags):\n        ''''''\n\n        \n        name = self.client_name.lower()\n        tags = [t.lower() for t in tags]\n\n        if name not in tags:\n            bot.error('%s not found in %s, must match!' %(name, tags))\n            sys.exit(1)","method_summary":"ensure that the client name is included in a list of tags. This is important for matching builders to the correct client. We exit on fail.","original_method_code":"def _client_tagged(self, tags):\n        '''ensure that the client name is included in a list of tags. This is\n           important for matching builders to the correct client. We exit\n           on fail.\n            \n           Parameters\n           ==========\n           tags: a list of tags to look for client name in\n\n        '''\n\n        # We must match the client to a tag\n        name = self.client_name.lower()\n        tags = [t.lower() for t in tags]\n\n        if name not in tags:\n            bot.error('%s not found in %s, must match!' %(name, tags))\n            sys.exit(1)","method_path":"https:\/\/github.com\/singularityhub\/sregistry-cli\/blob\/abc96140a1d15b5e96d83432e1e0e1f4f8f36331\/sregistry\/main\/base\/__init__.py#L65-L82"}
{"repo_name":"singularityhub\/sregistry-cli","method_name":"ApiConnection.announce","method_code":"def announce(self, command=None):\n        ''''''\n        if command is not None:\n            if command not in ['get'] and self.quiet is False:\n                self.speak()","method_summary":"the client will announce itself given that a command is not in a particular predefined list.","original_method_code":"def announce(self, command=None):\n        '''the client will announce itself given that a command is not in a\n           particular predefined list.\n        '''\n        if command is not None:\n            if command not in ['get'] and self.quiet is False:\n                self.speak()","method_path":"https:\/\/github.com\/singularityhub\/sregistry-cli\/blob\/abc96140a1d15b5e96d83432e1e0e1f4f8f36331\/sregistry\/main\/base\/__init__.py#L109-L115"}
{"repo_name":"singularityhub\/sregistry-cli","method_name":"Client._update_secrets","method_code":"def _update_secrets(self):\n        ''''''\n        env = 'SREGISTRY_GOOGLE_DRIVE_CREDENTIALS'\n        self._secrets = self._get_and_update_setting(env)\n        self._base = self._get_and_update_setting('SREGISTRY_GOOGLE_DRIVE_ROOT')\n\n        if self._base is None:\n            self._base = 'sregistry'\n\n        if self._secrets is None:\n            bot.error('You must export %s to use Google Drive client' %env)\n            bot.info(\"https:\/\/singularityhub.github.io\/sregistry-cli\/client-google-drive\")\n            sys.exit(1)","method_summary":"The user is required to have an application secrets file in his or her environment. The client exists with error if the variable isn't found.","original_method_code":"def _update_secrets(self):\n        '''The user is required to have an application secrets file in his\n           or her environment. The client exists with error \n           if the variable isn't found.\n        '''\n        env = 'SREGISTRY_GOOGLE_DRIVE_CREDENTIALS'\n        self._secrets = self._get_and_update_setting(env)\n        self._base = self._get_and_update_setting('SREGISTRY_GOOGLE_DRIVE_ROOT')\n\n        if self._base is None:\n            self._base = 'sregistry'\n\n        if self._secrets is None:\n            bot.error('You must export %s to use Google Drive client' %env)\n            bot.info(\"https:\/\/singularityhub.github.io\/sregistry-cli\/client-google-drive\")\n            sys.exit(1)","method_path":"https:\/\/github.com\/singularityhub\/sregistry-cli\/blob\/abc96140a1d15b5e96d83432e1e0e1f4f8f36331\/sregistry\/main\/google_drive\/__init__.py#L46-L61"}
{"repo_name":"LLNL\/scraper","method_name":"GitHub_LLNL_Stats.get_stats","method_code":"def get_stats(self, username='', password='', organization='llnl',\n        force=True, repo_type='public'):\n        \"\"\"\"\"\"\n        date = str(datetime.date.today())\n        file_path =  ('..\/github_stats_output\/' + date[:4] + '\/' + date[:7] + '\/'\n            + date + '.csv')\n        if force or not os.path.isfile(file_path):\n            my_github.login(username, password)\n            calls_beginning = self.logged_in_gh.ratelimit_remaining + 1\n            print 'Rate Limit: ' + str(calls_beginning)\n            my_github.get_org(organization)\n            count_members = my_github.get_mems_of_org()\n            count_teams = my_github.get_teams_of_org()\n            my_github.repos(repo_type=repo_type, organization=organization)\n            \n            my_github.write_org_json(dict_to_write=self.members_json,\n                path_ending_type='members', is_list=True)\n            my_github.write_org_json(dict_to_write=\n                {'singleton': self.org_retrieved.to_json()},\n                path_ending_type='organization')\n            my_github.write_org_json(dict_to_write=self.teams_json,\n                path_ending_type='teams', is_list=True)\n\n            my_github.write_repo_json(dict_to_write=self.repos_json,\n                path_ending_type='repo')\n            my_github.write_repo_json(dict_to_write=self.contributors_json,\n                path_ending_type='contributors', is_list=True)\n            my_github.write_repo_json(dict_to_write=self.pull_requests_json,\n                path_ending_type='pull-requests', is_list=True)\n            my_github.write_repo_json(dict_to_write=self.issues_json,\n                path_ending_type='issues', is_list=True)\n            my_github.write_repo_json(dict_to_write=self.languages_json,\n                path_ending_type='languages', is_dict=True)\n            my_github.write_repo_json(dict_to_write=self.commits_json,\n                path_ending_type='commits', is_list=True)\n            \n            my_github.write_to_file(file_path,\n                                    date,\n                                    organization,\n                                    count_members,\n                                    count_teams)\n            calls_remaining = self.logged_in_gh.ratelimit_remaining\n            calls_used = calls_beginning - calls_remaining\n            print ('Rate Limit Remaining: ' + str(calls_remaining) + '\\nUsed '\n                + str(calls_used) + ' API calls.')","method_summary":"Retrieves the statistics from the given organization with the given credentials. Will not retreive data if file exists and force hasn't been set to True. This is to save GH API requests.","original_method_code":"def get_stats(self, username='', password='', organization='llnl',\n        force=True, repo_type='public'):\n        \"\"\"\n        Retrieves the statistics from the given organization with the given\n        credentials. Will not retreive data if file exists and force hasn't been\n        set to True. This is to save GH API requests.\n        \"\"\"\n        date = str(datetime.date.today())\n        file_path =  ('..\/github_stats_output\/' + date[:4] + '\/' + date[:7] + '\/'\n            + date + '.csv')\n        if force or not os.path.isfile(file_path):\n            my_github.login(username, password)\n            calls_beginning = self.logged_in_gh.ratelimit_remaining + 1\n            print 'Rate Limit: ' + str(calls_beginning)\n            my_github.get_org(organization)\n            count_members = my_github.get_mems_of_org()\n            count_teams = my_github.get_teams_of_org()\n            my_github.repos(repo_type=repo_type, organization=organization)\n            #Write JSON\n            my_github.write_org_json(dict_to_write=self.members_json,\n                path_ending_type='members', is_list=True)\n            my_github.write_org_json(dict_to_write=\n                {'singleton': self.org_retrieved.to_json()},\n                path_ending_type='organization')\n            my_github.write_org_json(dict_to_write=self.teams_json,\n                path_ending_type='teams', is_list=True)\n\n            my_github.write_repo_json(dict_to_write=self.repos_json,\n                path_ending_type='repo')\n            my_github.write_repo_json(dict_to_write=self.contributors_json,\n                path_ending_type='contributors', is_list=True)\n            my_github.write_repo_json(dict_to_write=self.pull_requests_json,\n                path_ending_type='pull-requests', is_list=True)\n            my_github.write_repo_json(dict_to_write=self.issues_json,\n                path_ending_type='issues', is_list=True)\n            my_github.write_repo_json(dict_to_write=self.languages_json,\n                path_ending_type='languages', is_dict=True)\n            my_github.write_repo_json(dict_to_write=self.commits_json,\n                path_ending_type='commits', is_list=True)\n            #Write CSV\n            my_github.write_to_file(file_path,\n                                    date,\n                                    organization,\n                                    count_members,\n                                    count_teams)\n            calls_remaining = self.logged_in_gh.ratelimit_remaining\n            calls_used = calls_beginning - calls_remaining\n            print ('Rate Limit Remaining: ' + str(calls_remaining) + '\\nUsed '\n                + str(calls_used) + ' API calls.')","method_path":"https:\/\/github.com\/LLNL\/scraper\/blob\/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea\/scripts\/github_stats.py#L38-L86"}
{"repo_name":"LLNL\/scraper","method_name":"GitHub_LLNL_Stats.get_mems_of_org","method_code":"def get_mems_of_org(self):\n        \"\"\"\"\"\"\n        print 'Getting members.'\n        counter = 0\n        for member in self.org_retrieved.iter_members():\n            self.members_json[member.id] = member.to_json()\n            counter += 1\n        return counter","method_summary":"Retrieves the number of members of the organization.","original_method_code":"def get_mems_of_org(self):\n        \"\"\"\n        Retrieves the number of members of the organization.\n        \"\"\"\n        print 'Getting members.'\n        counter = 0\n        for member in self.org_retrieved.iter_members():\n            self.members_json[member.id] = member.to_json()\n            counter += 1\n        return counter","method_path":"https:\/\/github.com\/LLNL\/scraper\/blob\/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea\/scripts\/github_stats.py#L147-L156"}
{"repo_name":"LLNL\/scraper","method_name":"GitHub_LLNL_Stats.get_teams_of_org","method_code":"def get_teams_of_org(self):\n        \"\"\"\"\"\"\n        print 'Getting teams.'\n        counter = 0\n        for team in self.org_retrieved.iter_teams():\n            self.teams_json[team.id] = team.to_json()\n            counter += 1\n        return counter","method_summary":"Retrieves the number of teams of the organization.","original_method_code":"def get_teams_of_org(self):\n        \"\"\"\n        Retrieves the number of teams of the organization.\n        \"\"\"\n        print 'Getting teams.'\n        counter = 0\n        for team in self.org_retrieved.iter_teams():\n            self.teams_json[team.id] = team.to_json()\n            counter += 1\n        return counter","method_path":"https:\/\/github.com\/LLNL\/scraper\/blob\/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea\/scripts\/github_stats.py#L158-L167"}
{"repo_name":"LLNL\/scraper","method_name":"GitHub_LLNL_Stats.repos","method_code":"def repos(self, repo_type='public', organization='llnl'):\n        \"\"\"\"\"\"\n        print 'Getting repos.'\n        for repo in self.org_retrieved.iter_repos(type=repo_type):\n            \n            json = repo.to_json()\n            self.repos_json[repo.name] = json\n            \n            temp_repo = my_repo.My_Repo()\n            temp_repo.name = repo.full_name\n            self.total_repos += 1\n            temp_repo.contributors = my_github.get_total_contributors(repo)\n            self.total_contributors += temp_repo.contributors\n            temp_repo.forks = repo.forks_count\n            self.total_forks += temp_repo.forks\n            temp_repo.stargazers = repo.stargazers\n            self.total_stars += temp_repo.stargazers\n            temp_repo.pull_requests_open, temp_repo.pull_requests_closed = \\\n                my_github.get_pull_reqs(repo)\n            temp_repo.pull_requests = (temp_repo.pull_requests_open\n                + temp_repo.pull_requests_closed)\n            self.total_pull_reqs += temp_repo.pull_requests_open\n            self.total_pull_reqs += temp_repo.pull_requests_closed\n            self.total_pull_reqs_open += temp_repo.pull_requests_open\n            self.total_pull_reqs_closed += temp_repo.pull_requests_closed\n            temp_repo.open_issues = repo.open_issues_count\n            self.total_open_issues += temp_repo.open_issues\n            temp_repo.closed_issues = my_github.get_issues(repo, organization=organization)\n            temp_repo.issues = temp_repo.closed_issues + temp_repo.open_issues\n            self.total_closed_issues += temp_repo.closed_issues\n            self.total_issues += temp_repo.issues\n            my_github.get_languages(repo, temp_repo)\n            temp_repo.readme = my_github.get_readme(repo)\n            \n            temp_repo.commits = self.get_commits(repo=repo, organization=organization)\n            self.total_commits += temp_repo.commits\n            self.all_repos.append(temp_repo)","method_summary":"Retrieves info about the repos of the current organization.","original_method_code":"def repos(self, repo_type='public', organization='llnl'):\n        \"\"\"\n        Retrieves info about the repos of the current organization.\n        \"\"\"\n        print 'Getting repos.'\n        for repo in self.org_retrieved.iter_repos(type=repo_type):\n            #JSON\n            json = repo.to_json()\n            self.repos_json[repo.name] = json\n            #CSV\n            temp_repo = my_repo.My_Repo()\n            temp_repo.name = repo.full_name\n            self.total_repos += 1\n            temp_repo.contributors = my_github.get_total_contributors(repo)\n            self.total_contributors += temp_repo.contributors\n            temp_repo.forks = repo.forks_count\n            self.total_forks += temp_repo.forks\n            temp_repo.stargazers = repo.stargazers\n            self.total_stars += temp_repo.stargazers\n            temp_repo.pull_requests_open, temp_repo.pull_requests_closed = \\\n                my_github.get_pull_reqs(repo)\n            temp_repo.pull_requests = (temp_repo.pull_requests_open\n                + temp_repo.pull_requests_closed)\n            self.total_pull_reqs += temp_repo.pull_requests_open\n            self.total_pull_reqs += temp_repo.pull_requests_closed\n            self.total_pull_reqs_open += temp_repo.pull_requests_open\n            self.total_pull_reqs_closed += temp_repo.pull_requests_closed\n            temp_repo.open_issues = repo.open_issues_count\n            self.total_open_issues += temp_repo.open_issues\n            temp_repo.closed_issues = my_github.get_issues(repo, organization=organization)\n            temp_repo.issues = temp_repo.closed_issues + temp_repo.open_issues\n            self.total_closed_issues += temp_repo.closed_issues\n            self.total_issues += temp_repo.issues\n            my_github.get_languages(repo, temp_repo)\n            temp_repo.readme = my_github.get_readme(repo)\n            #temp_repo.license = my_github.get_license(repo)\n            temp_repo.commits = self.get_commits(repo=repo, organization=organization)\n            self.total_commits += temp_repo.commits\n            self.all_repos.append(temp_repo)","method_path":"https:\/\/github.com\/LLNL\/scraper\/blob\/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea\/scripts\/github_stats.py#L169-L207"}
{"repo_name":"LLNL\/scraper","method_name":"GitHub_LLNL_Stats.get_total_contributors","method_code":"def get_total_contributors(self, repo):\n        \"\"\"\"\"\"\n        repo_contributors = 0\n        for contributor in repo.iter_contributors():\n            repo_contributors += 1\n            self.unique_contributors[contributor.id].append(repo.name)\n            self.contributors_json[repo.name].append(contributor.to_json())\n        return repo_contributors","method_summary":"Retrieves the number of contributors to a repo in the organization. Also adds to unique contributor list.","original_method_code":"def get_total_contributors(self, repo):\n        \"\"\"\n        Retrieves the number of contributors to a repo in the organization.\n        Also adds to unique contributor list.\n        \"\"\"\n        repo_contributors = 0\n        for contributor in repo.iter_contributors():\n            repo_contributors += 1\n            self.unique_contributors[contributor.id].append(repo.name)\n            self.contributors_json[repo.name].append(contributor.to_json())\n        return repo_contributors","method_path":"https:\/\/github.com\/LLNL\/scraper\/blob\/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea\/scripts\/github_stats.py#L209-L219"}
{"repo_name":"LLNL\/scraper","method_name":"GitHub_LLNL_Stats.get_pull_reqs","method_code":"def get_pull_reqs(self, repo):\n        \"\"\"\"\"\"\n        pull_reqs_open = 0\n        pull_reqs_closed = 0\n        for pull_request in repo.iter_pulls(state='all'):\n            self.pull_requests_json[repo.name].append(pull_request.to_json())\n            if pull_request.closed_at is not None:\n                pull_reqs_closed += 1\n            else:\n                pull_reqs_open += 1\n        return pull_reqs_open, pull_reqs_closed","method_summary":"Retrieves the number of pull requests on a repo in the organization.","original_method_code":"def get_pull_reqs(self, repo):\n        \"\"\"\n        Retrieves the number of pull requests on a repo in the organization.\n        \"\"\"\n        pull_reqs_open = 0\n        pull_reqs_closed = 0\n        for pull_request in repo.iter_pulls(state='all'):\n            self.pull_requests_json[repo.name].append(pull_request.to_json())\n            if pull_request.closed_at is not None:\n                pull_reqs_closed += 1\n            else:\n                pull_reqs_open += 1\n        return pull_reqs_open, pull_reqs_closed","method_path":"https:\/\/github.com\/LLNL\/scraper\/blob\/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea\/scripts\/github_stats.py#L221-L233"}
{"repo_name":"LLNL\/scraper","method_name":"GitHub_LLNL_Stats.get_issues","method_code":"def get_issues(self, repo, organization='llnl'):\n        \"\"\"\"\"\"\n        \n        path = ('..\/github-data\/' + organization + '\/' + repo.name + '\/issues')\n        is_only_today = False\n        if not os.path.exists(path): \n            all_issues = repo.iter_issues(state='all')\n            is_only_today = True\n        else:\n            files = os.listdir(path)\n            date = str(files[-1][:-5])\n            if date == str(datetime.date.today()):\n                \n                if len(files) > 2:\n                    date = str(files[-2][:-5])\n                else:\n                    \n                    all_issues = repo.iter_issues(state='all')\n                    is_only_today = True\n            if not is_only_today:\n                all_issues = repo.iter_issues(since=date, state='all')\n        for issue in all_issues:\n            self.issues_json[repo.name].append(issue.to_json())\n        \n        closed_issues = 0\n        for issue in repo.iter_issues(state='closed'):\n            if issue is not None:\n                closed_issues += 1\n        return closed_issues","method_summary":"Retrieves the number of closed issues.","original_method_code":"def get_issues(self, repo, organization='llnl'):\n        \"\"\"\n        Retrieves the number of closed issues.\n        \"\"\"\n        #JSON\n        path = ('..\/github-data\/' + organization + '\/' + repo.name + '\/issues')\n        is_only_today = False\n        if not os.path.exists(path): #no previous path, get all issues\n            all_issues = repo.iter_issues(state='all')\n            is_only_today = True\n        else:\n            files = os.listdir(path)\n            date = str(files[-1][:-5])\n            if date == str(datetime.date.today()):\n                #most recent date is actually today, get previous most recent date\n                if len(files) > 2:\n                    date = str(files[-2][:-5])\n                else:\n                    #This means there is only one file, today. Retrieve every issue\n                    all_issues = repo.iter_issues(state='all')\n                    is_only_today = True\n            if not is_only_today:#there's a previous saved JSON that's not today\n                all_issues = repo.iter_issues(since=date, state='all')\n        for issue in all_issues:\n            self.issues_json[repo.name].append(issue.to_json())\n        #CSV\n        closed_issues = 0\n        for issue in repo.iter_issues(state='closed'):\n            if issue is not None:\n                closed_issues += 1\n        return closed_issues","method_path":"https:\/\/github.com\/LLNL\/scraper\/blob\/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea\/scripts\/github_stats.py#L235-L265"}
{"repo_name":"LLNL\/scraper","method_name":"GitHub_LLNL_Stats.get_languages","method_code":"def get_languages(self, repo, temp_repo):\n        \"\"\"\"\"\"\n        try:\n            self.languages[repo.language] += 1\n        except KeyError:\n            count = self.languages[repo.language] = 1\n        for repo_languages in repo.iter_languages():\n            self.languages_json[repo.name][repo_languages[0]] = repo_languages[1]\n            for language in repo_languages:\n                if isinstance(language, basestring):\n                    temp_repo.languages.append(language)\n                    self.previous_language = language\n                else:\n                    try:\n                        self.languages_size[self.previous_language] += \\\n                            language\n                    except KeyError:\n                        size = self.languages_size[self.previous_language] \\\n                            = language","method_summary":"Retrieves the languages used in the repo and increments the respective counts of those languages. Only increments languages that have names. Anything else is not incremented (i.e. numbers).","original_method_code":"def get_languages(self, repo, temp_repo):\n        \"\"\"\n        Retrieves the languages used in the repo and increments the respective\n        counts of those languages. Only increments languages that have names.\n        Anything else is not incremented (i.e. numbers).\n        \"\"\"\n        try:\n            self.languages[repo.language] += 1\n        except KeyError:\n            count = self.languages[repo.language] = 1\n        for repo_languages in repo.iter_languages():\n            self.languages_json[repo.name][repo_languages[0]] = repo_languages[1]\n            for language in repo_languages:\n                if isinstance(language, basestring):#is language\n                    temp_repo.languages.append(language)\n                    self.previous_language = language\n                else:#record size bytes of language\n                    try:\n                        self.languages_size[self.previous_language] += \\\n                            language\n                    except KeyError:\n                        size = self.languages_size[self.previous_language] \\\n                            = language","method_path":"https:\/\/github.com\/LLNL\/scraper\/blob\/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea\/scripts\/github_stats.py#L267-L289"}
{"repo_name":"LLNL\/scraper","method_name":"GitHub_LLNL_Stats.get_readme","method_code":"def get_readme(self, repo):\n        \"\"\"\"\"\"\n        readme_contents = repo.readme()\n        if readme_contents is not None:\n            self.total_readmes += 1\n            return 'MD'\n        if self.search_limit >= 28:\n            print 'Hit search limit. Sleeping for 60 sec.'\n            time.sleep(60)\n            self.search_limit = 0\n        self.search_limit += 1\n        search_results = self.logged_in_gh.search_code('readme'\n            + 'in:path repo:' + repo.full_name)\n        try:\n            for result in search_results:\n                path = result.path[1:]\n                if '\/' not in path and 'readme' in path.lower():\n                    self.total_readmes += 1\n                    return path\n            return 'MISS'\n        except (github3.models.GitHubError, StopIteration) as e:\n            return 'MISS'","method_summary":"Checks to see if the given repo has a ReadMe. MD means it has a correct Readme recognized by GitHub.","original_method_code":"def get_readme(self, repo):\n        \"\"\"\n        Checks to see if the given repo has a ReadMe. MD means it has a correct\n        Readme recognized by GitHub.\n        \"\"\"\n        readme_contents = repo.readme()\n        if readme_contents is not None:\n            self.total_readmes += 1\n            return 'MD'\n        if self.search_limit >= 28:\n            print 'Hit search limit. Sleeping for 60 sec.'\n            time.sleep(60)\n            self.search_limit = 0\n        self.search_limit += 1\n        search_results = self.logged_in_gh.search_code('readme'\n            + 'in:path repo:' + repo.full_name)\n        try:\n            for result in search_results:\n                path = result.path[1:]\n                if '\/' not in path and 'readme' in path.lower():\n                    self.total_readmes += 1\n                    return path\n            return 'MISS'\n        except (github3.models.GitHubError, StopIteration) as e:\n            return 'MISS'","method_path":"https:\/\/github.com\/LLNL\/scraper\/blob\/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea\/scripts\/github_stats.py#L291-L315"}
{"repo_name":"LLNL\/scraper","method_name":"GitHub_LLNL_Stats.get_license","method_code":"def get_license(self, repo):\n        \"\"\"\"\"\"\n        if self.search_limit >= 28:\n            print 'Hit search limit. Sleeping for 60 sec.'\n            time.sleep(60)\n            self.search_limit = 0\n        self.search_limit += 1\n        search_results = self.logged_in_gh.search_code('license'\n            + 'in:path repo:' + repo.full_name)\n        try:\n            for result in search_results:\n                path = result.path[1:]\n                if '\/' not in path and 'license' in path.lower():\n                    self.total_licenses += 1\n                    return path\n            return 'MISS'\n        except (StopIteration) as e:\n            return 'MISS'","method_summary":"Checks to see if the given repo has a top level LICENSE file.","original_method_code":"def get_license(self, repo):\n        \"\"\"\n        Checks to see if the given repo has a top level LICENSE file.\n        \"\"\"\n        if self.search_limit >= 28:\n            print 'Hit search limit. Sleeping for 60 sec.'\n            time.sleep(60)\n            self.search_limit = 0\n        self.search_limit += 1\n        search_results = self.logged_in_gh.search_code('license'\n            + 'in:path repo:' + repo.full_name)\n        try:\n            for result in search_results:\n                path = result.path[1:]\n                if '\/' not in path and 'license' in path.lower():\n                    self.total_licenses += 1\n                    return path\n            return 'MISS'\n        except (StopIteration) as e:\n            return 'MISS'","method_path":"https:\/\/github.com\/LLNL\/scraper\/blob\/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea\/scripts\/github_stats.py#L317-L336"}
{"repo_name":"assemblerflow\/flowcraft","method_name":"clean_up","method_code":"def clean_up(fastq_pairs, clear):\n    \"\"\"\"\"\"\n\n    \n    unpaired_fastq = [f for f in os.listdir(\".\")\n                      if f.endswith(\"_U.fastq.gz\")]\n\n    \n    for fpath in unpaired_fastq:\n        os.remove(fpath)\n\n    \n    expected_out = [f for f in os.listdir(\".\") if f.endswith(\"_trim.fastq.gz\")]\n\n    if clear == \"true\" and len(expected_out) == 2:\n        for fq in fastq_pairs:\n            \n            rp = os.path.realpath(fq)\n            logger.debug(\"Removing temporary fastq file path: {}\".format(rp))\n            if re.match(\".*\/work\/.{2}\/.{30}\/.*\", rp):\n                os.remove(rp)","method_summary":"Cleans the working directory of unwanted temporary files","original_method_code":"def clean_up(fastq_pairs, clear):\n    \"\"\"Cleans the working directory of unwanted temporary files\"\"\"\n\n    # Find unpaired fastq files\n    unpaired_fastq = [f for f in os.listdir(\".\")\n                      if f.endswith(\"_U.fastq.gz\")]\n\n    # Remove unpaired fastq files, if any\n    for fpath in unpaired_fastq:\n        os.remove(fpath)\n\n    # Expected output to assess whether it is safe to remove temporary input\n    expected_out = [f for f in os.listdir(\".\") if f.endswith(\"_trim.fastq.gz\")]\n\n    if clear == \"true\" and len(expected_out) == 2:\n        for fq in fastq_pairs:\n            # Get real path of fastq files, following symlinks\n            rp = os.path.realpath(fq)\n            logger.debug(\"Removing temporary fastq file path: {}\".format(rp))\n            if re.match(\".*\/work\/.{2}\/.{30}\/.*\", rp):\n                os.remove(rp)","method_path":"https:\/\/github.com\/assemblerflow\/flowcraft\/blob\/fc3f4bddded1efc76006600016dc71a06dd908c0\/flowcraft\/templates\/trimmomatic.py#L242-L262"}
{"repo_name":"assemblerflow\/flowcraft","method_name":"merge_default_adapters","method_code":"def merge_default_adapters():\n    \"\"\"\"\"\"\n\n    default_adapters = [os.path.join(ADAPTERS_PATH, x) for x in\n                        os.listdir(ADAPTERS_PATH)]\n    filepath = os.path.join(os.getcwd(), \"default_adapters.fasta\")\n\n    with open(filepath, \"w\") as fh, \\\n            fileinput.input(default_adapters) as in_fh:\n        for line in in_fh:\n            fh.write(\"{}{}\".format(line, \"\\\\n\"))\n\n    return filepath","method_summary":"Merges the default adapters file in the trimmomatic adapters directory","original_method_code":"def merge_default_adapters():\n    \"\"\"Merges the default adapters file in the trimmomatic adapters directory\n\n    Returns\n    -------\n    str\n        Path with the merged adapters file.\n    \"\"\"\n\n    default_adapters = [os.path.join(ADAPTERS_PATH, x) for x in\n                        os.listdir(ADAPTERS_PATH)]\n    filepath = os.path.join(os.getcwd(), \"default_adapters.fasta\")\n\n    with open(filepath, \"w\") as fh, \\\n            fileinput.input(default_adapters) as in_fh:\n        for line in in_fh:\n            fh.write(\"{}{}\".format(line, \"\\\\n\"))\n\n    return filepath","method_path":"https:\/\/github.com\/assemblerflow\/flowcraft\/blob\/fc3f4bddded1efc76006600016dc71a06dd908c0\/flowcraft\/templates\/trimmomatic.py#L265-L283"}
{"repo_name":"assemblerflow\/flowcraft","method_name":"main","method_code":"def main(sample_id, fastq_pair, trim_range, trim_opts, phred, adapters_file,\n         clear):\n    \"\"\"\"\"\"\n\n    logger.info(\"Starting trimmomatic\")\n\n    \n    cli = [\n        \"java\",\n        \"-Xmx{}\".format(\"$task.memory\"[:-1].lower().replace(\" \", \"\")),\n        \"-jar\",\n        TRIM_PATH.strip(),\n        \"PE\",\n        \"-threads\",\n        \"$task.cpus\"\n    ]\n\n    \n    try:\n        \n        phred = int(phred)\n        phred_flag = \"-phred{}\".format(str(phred))\n        cli += [phred_flag]\n    \n    \n    except ValueError:\n        pass\n\n    \n    cli += fastq_pair\n\n    \n    output_names = []\n    for i in range(len(fastq_pair)):\n        output_names.append(\"{}_{}_trim.fastq.gz\".format(\n            SAMPLE_ID, str(i + 1)))\n        output_names.append(\"{}_{}_U.fastq.gz\".format(\n            SAMPLE_ID, str(i + 1)))\n    cli += output_names\n\n    if trim_range != [\"None\"]:\n        cli += [\n            \"CROP:{}\".format(trim_range[1]),\n            \"HEADCROP:{}\".format(trim_range[0]),\n        ]\n\n    if os.path.exists(adapters_file):\n        logger.debug(\"Using the provided adapters file '{}'\".format(\n            adapters_file))\n    else:\n        logger.debug(\"Adapters file '{}' not provided or does not exist. Using\"\n                     \" default adapters\".format(adapters_file))\n        adapters_file = merge_default_adapters()\n\n    cli += [\n        \"ILLUMINACLIP:{}:3:30:10:6:true\".format(adapters_file)\n    ]\n\n    \n    logfile = os.path.join(tempfile.mkdtemp(prefix='tmp'), \"{}_trimlog.txt\".format(sample_id))\n\n    \n    cli += [\n        \"SLIDINGWINDOW:{}\".format(trim_opts[0]),\n        \"LEADING:{}\".format(trim_opts[1]),\n        \"TRAILING:{}\".format(trim_opts[2]),\n        \"MINLEN:{}\".format(trim_opts[3]),\n        \"TOPHRED33\",\n        \"-trimlog\",\n        logfile\n    ]\n\n    logger.debug(\"Running trimmomatic subprocess with command: {}\".format(cli))\n\n    p = subprocess.Popen(cli, stdout=PIPE, stderr=PIPE)\n    stdout, stderr = p.communicate()\n\n    \n    \n    try:\n        stderr = stderr.decode(\"utf8\")\n    except (UnicodeDecodeError, AttributeError):\n        stderr = str(stderr)\n\n    logger.info(\"Finished trimmomatic subprocess with STDOUT:\\\\n\"\n                \"======================================\\\\n{}\".format(stdout))\n    logger.info(\"Finished trimmomatic subprocesswith STDERR:\\\\n\"\n                \"======================================\\\\n{}\".format(stderr))\n    logger.info(\"Finished trimmomatic with return code: {}\".format(\n        p.returncode))\n\n    trimmomatic_log(logfile, sample_id)\n\n    if p.returncode == 0 and os.path.exists(\"{}_1_trim.fastq.gz\".format(\n            SAMPLE_ID)):\n        clean_up(fastq_pair, clear)\n\n    \n    \n    with open(\".status\", \"w\") as status_fh:\n        if p.returncode != 0:\n            status_fh.write(\"fail\")\n            return\n        else:\n            status_fh.write(\"pass\")","method_summary":"Main executor of the trimmomatic template.","original_method_code":"def main(sample_id, fastq_pair, trim_range, trim_opts, phred, adapters_file,\n         clear):\n    \"\"\" Main executor of the trimmomatic template.\n\n    Parameters\n    ----------\n    sample_id : str\n        Sample Identification string.\n    fastq_pair : list\n        Two element list containing the paired FastQ files.\n    trim_range : list\n        Two element list containing the trimming range.\n    trim_opts : list\n        Four element list containing several trimmomatic options:\n        [*SLIDINGWINDOW*; *LEADING*; *TRAILING*; *MINLEN*]\n    phred : int\n        Guessed phred score for the sample. The phred score is a generated\n        output from :py:class:`templates.integrity_coverage`.\n    adapters_file : str\n        Path to adapters file. If not provided, or the path is not available,\n        it will use the default adapters from Trimmomatic will be used\n    clear : str\n        Can be either 'true' or 'false'. If 'true', the input fastq files will\n        be removed at the end of the run, IF they are in the working directory\n    \"\"\"\n\n    logger.info(\"Starting trimmomatic\")\n\n    # Create base CLI\n    cli = [\n        \"java\",\n        \"-Xmx{}\".format(\"$task.memory\"[:-1].lower().replace(\" \", \"\")),\n        \"-jar\",\n        TRIM_PATH.strip(),\n        \"PE\",\n        \"-threads\",\n        \"$task.cpus\"\n    ]\n\n    # If the phred encoding was detected, provide it\n    try:\n        # Check if the provided PHRED can be converted to int\n        phred = int(phred)\n        phred_flag = \"-phred{}\".format(str(phred))\n        cli += [phred_flag]\n    # Could not detect phred encoding. Do not add explicit encoding to\n    # trimmomatic and let it guess\n    except ValueError:\n        pass\n\n    # Add input samples to CLI\n    cli += fastq_pair\n\n    # Add output file names\n    output_names = []\n    for i in range(len(fastq_pair)):\n        output_names.append(\"{}_{}_trim.fastq.gz\".format(\n            SAMPLE_ID, str(i + 1)))\n        output_names.append(\"{}_{}_U.fastq.gz\".format(\n            SAMPLE_ID, str(i + 1)))\n    cli += output_names\n\n    if trim_range != [\"None\"]:\n        cli += [\n            \"CROP:{}\".format(trim_range[1]),\n            \"HEADCROP:{}\".format(trim_range[0]),\n        ]\n\n    if os.path.exists(adapters_file):\n        logger.debug(\"Using the provided adapters file '{}'\".format(\n            adapters_file))\n    else:\n        logger.debug(\"Adapters file '{}' not provided or does not exist. Using\"\n                     \" default adapters\".format(adapters_file))\n        adapters_file = merge_default_adapters()\n\n    cli += [\n        \"ILLUMINACLIP:{}:3:30:10:6:true\".format(adapters_file)\n    ]\n\n    #create log file im temporary dir to avoid issues when running on a docker container in macOS\n    logfile = os.path.join(tempfile.mkdtemp(prefix='tmp'), \"{}_trimlog.txt\".format(sample_id))\n\n    # Add trimmomatic options\n    cli += [\n        \"SLIDINGWINDOW:{}\".format(trim_opts[0]),\n        \"LEADING:{}\".format(trim_opts[1]),\n        \"TRAILING:{}\".format(trim_opts[2]),\n        \"MINLEN:{}\".format(trim_opts[3]),\n        \"TOPHRED33\",\n        \"-trimlog\",\n        logfile\n    ]\n\n    logger.debug(\"Running trimmomatic subprocess with command: {}\".format(cli))\n\n    p = subprocess.Popen(cli, stdout=PIPE, stderr=PIPE)\n    stdout, stderr = p.communicate()\n\n    # Attempt to decode STDERR output from bytes. If unsuccessful, coerce to\n    # string\n    try:\n        stderr = stderr.decode(\"utf8\")\n    except (UnicodeDecodeError, AttributeError):\n        stderr = str(stderr)\n\n    logger.info(\"Finished trimmomatic subprocess with STDOUT:\\\\n\"\n                \"======================================\\\\n{}\".format(stdout))\n    logger.info(\"Finished trimmomatic subprocesswith STDERR:\\\\n\"\n                \"======================================\\\\n{}\".format(stderr))\n    logger.info(\"Finished trimmomatic with return code: {}\".format(\n        p.returncode))\n\n    trimmomatic_log(logfile, sample_id)\n\n    if p.returncode == 0 and os.path.exists(\"{}_1_trim.fastq.gz\".format(\n            SAMPLE_ID)):\n        clean_up(fastq_pair, clear)\n\n    # Check if trimmomatic ran successfully. If not, write the error message\n    # to the status channel and exit.\n    with open(\".status\", \"w\") as status_fh:\n        if p.returncode != 0:\n            status_fh.write(\"fail\")\n            return\n        else:\n            status_fh.write(\"pass\")","method_path":"https:\/\/github.com\/assemblerflow\/flowcraft\/blob\/fc3f4bddded1efc76006600016dc71a06dd908c0\/flowcraft\/templates\/trimmomatic.py#L287-L413"}
{"repo_name":"assemblerflow\/flowcraft","method_name":"depth_file_reader","method_code":"def depth_file_reader(depth_file):\n    \"\"\"\"\"\"\n\n    \n    depth_dic_coverage = {}\n\n    for line in depth_file:\n        tab_split = line.split()  \n        reference = \"_\".join(tab_split[0].strip().split(\"_\")[0:3])  \n        \n        position = tab_split[1]\n        num_reads_align = float(tab_split[2].rstrip())\n\n        if reference not in depth_dic_coverage:\n            depth_dic_coverage[reference] = {}\n\n        depth_dic_coverage[reference][position] = num_reads_align\n\n    logger.info(\"Finished parsing depth file.\")\n    depth_file.close()\n\n    logger.debug(\"Size of dict_cov: {} kb\".format(\n        asizeof(depth_dic_coverage)\/1024))\n\n    return depth_dic_coverage","method_summary":"Function that parse samtools depth file and creates 3 dictionaries that will be useful to make the outputs of this script, both the tabular file and the json file that may be imported by pATLAS","original_method_code":"def depth_file_reader(depth_file):\n    \"\"\"\n    Function that parse samtools depth file and creates 3 dictionaries that\n    will be useful to make the outputs of this script, both the tabular file\n    and the json file that may be imported by pATLAS\n\n    Parameters\n    ----------\n    depth_file: textIO\n        the path to depth file for each sample\n\n    Returns\n    -------\n    depth_dic_coverage: dict\n            dictionary with the coverage per position for each plasmid\n    \"\"\"\n\n    # dict to store the mean coverage for each reference\n    depth_dic_coverage = {}\n\n    for line in depth_file:\n        tab_split = line.split()  # split by any white space\n        reference = \"_\".join(tab_split[0].strip().split(\"_\")[0:3])  # store\n        # only the gi for the reference\n        position = tab_split[1]\n        num_reads_align = float(tab_split[2].rstrip())\n\n        if reference not in depth_dic_coverage:\n            depth_dic_coverage[reference] = {}\n\n        depth_dic_coverage[reference][position] = num_reads_align\n\n    logger.info(\"Finished parsing depth file.\")\n    depth_file.close()\n\n    logger.debug(\"Size of dict_cov: {} kb\".format(\n        asizeof(depth_dic_coverage)\/1024))\n\n    return depth_dic_coverage","method_path":"https:\/\/github.com\/assemblerflow\/flowcraft\/blob\/fc3f4bddded1efc76006600016dc71a06dd908c0\/flowcraft\/templates\/mapping2json.py#L74-L112"}
{"repo_name":"assemblerflow\/flowcraft","method_name":"main","method_code":"def main(depth_file, json_dict, cutoff, sample_id):\n    \"\"\"\"\"\"\n\n    \n    logger.debug(\"Cutoff value: {}. Type: {}\".format(cutoff, type(cutoff)))\n    try:\n        cutoff_val = float(cutoff)\n        if cutoff_val < 0.4:\n            logger.warning(\"This cutoff value will generate a high volume of \"\n                           \"plot data. Therefore '.report.json' can be too big\")\n    except ValueError:\n        logger.error(\"Cutoff value should be a string such as: '0.6'. \"\n                     \"The outputted value: {}. Make sure to provide an \"\n                     \"appropriate value for --cov_cutoff\".format(cutoff))\n        sys.exit(1)\n\n    \n\n    plasmid_length = json.load(open(json_dict))\n    if plasmid_length:\n        logger.info(\"Loaded dictionary of plasmid lengths\")\n    else:\n        logger.error(\"Something went wrong and plasmid lengths dictionary\"\n                     \"could not be loaded. Check if process received this\"\n                     \"param successfully.\")\n        sys.exit(1)\n\n    \n    depth_file_in = open(depth_file)\n\n    \n    \n    logger.info(\"Reading depth file and creating dictionary to dump.\")\n    depth_dic_coverage = depth_file_reader(depth_file_in)\n    percentage_bases_covered, dict_cov = generate_jsons(depth_dic_coverage,\n                                                        plasmid_length,\n                                                        cutoff_val)\n\n    if percentage_bases_covered and dict_cov:\n        logger.info(\"percentage_bases_covered length: {}\".format(\n            str(len(percentage_bases_covered))))\n        logger.info(\"dict_cov length: {}\".format(str(len(dict_cov))))\n    else:\n        logger.error(\"Both dicts that dump to JSON file or .report.json are \"\n                     \"empty.\")\n\n    \n    logger.info(\"Dumping to {}\".format(\"{}_mapping.json\".format(depth_file)))\n    with open(\"{}_mapping.json\".format(depth_file), \"w\") as output_json:\n        output_json.write(json.dumps(percentage_bases_covered))\n\n    json_dic = {\n        \"tableRow\": [{\n            \"sample\": sample_id,\n            \"data\": [{\n                \"header\": \"Mapping\",\n                \"table\": \"plasmids\",\n                \"patlas_mapping\": percentage_bases_covered,\n                \"value\": len(percentage_bases_covered)\n            }]\n        }],\n        \"sample\": sample_id,\n        \"patlas_mapping\": percentage_bases_covered,\n        \"plotData\": [{\n            \"sample\": sample_id,\n            \"data\": {\n                \"patlasMappingSliding\": dict_cov\n            },\n        }]\n    }\n\n    logger.debug(\"Size of dict_cov: {} kb\".format(asizeof(json_dic)\/1024))\n    logger.info(\"Writing to .report.json\")\n    with open(\".report.json\", \"w\") as json_report:\n        json_report.write(json.dumps(json_dic, separators=(\",\", \":\")))","method_summary":"Function that handles the inputs required to parse depth files from bowtie and dumps a dict to a json file that can be imported into pATLAS.","original_method_code":"def main(depth_file, json_dict, cutoff, sample_id):\n    \"\"\"\n    Function that handles the inputs required to parse depth files from bowtie\n    and dumps a dict to a json file that can be imported into pATLAS.\n\n    Parameters\n    ----------\n    depth_file: str\n         the path to depth file for each sample\n    json_dict: str\n        the file that contains the dictionary with keys and values for\n        accessions\n        and their respective lengths\n    cutoff: str\n        the cutoff used to trim the unwanted matches for the minimum coverage\n        results from mapping. This value may range between 0 and 1.\n    sample_id: str\n        the id of the sample being parsed\n\n    \"\"\"\n\n    # check for the appropriate value for the cutoff value for coverage results\n    logger.debug(\"Cutoff value: {}. Type: {}\".format(cutoff, type(cutoff)))\n    try:\n        cutoff_val = float(cutoff)\n        if cutoff_val < 0.4:\n            logger.warning(\"This cutoff value will generate a high volume of \"\n                           \"plot data. Therefore '.report.json' can be too big\")\n    except ValueError:\n        logger.error(\"Cutoff value should be a string such as: '0.6'. \"\n                     \"The outputted value: {}. Make sure to provide an \"\n                     \"appropriate value for --cov_cutoff\".format(cutoff))\n        sys.exit(1)\n\n    # loads dict from file, this file is provided in docker image\n\n    plasmid_length = json.load(open(json_dict))\n    if plasmid_length:\n        logger.info(\"Loaded dictionary of plasmid lengths\")\n    else:\n        logger.error(\"Something went wrong and plasmid lengths dictionary\"\n                     \"could not be loaded. Check if process received this\"\n                     \"param successfully.\")\n        sys.exit(1)\n\n    # read depth file\n    depth_file_in = open(depth_file)\n\n    # first reads the depth file and generates dictionaries to handle the input\n    # to a simpler format\n    logger.info(\"Reading depth file and creating dictionary to dump.\")\n    depth_dic_coverage = depth_file_reader(depth_file_in)\n    percentage_bases_covered, dict_cov = generate_jsons(depth_dic_coverage,\n                                                        plasmid_length,\n                                                        cutoff_val)\n\n    if percentage_bases_covered and dict_cov:\n        logger.info(\"percentage_bases_covered length: {}\".format(\n            str(len(percentage_bases_covered))))\n        logger.info(\"dict_cov length: {}\".format(str(len(dict_cov))))\n    else:\n        logger.error(\"Both dicts that dump to JSON file or .report.json are \"\n                     \"empty.\")\n\n    # then dump do file\n    logger.info(\"Dumping to {}\".format(\"{}_mapping.json\".format(depth_file)))\n    with open(\"{}_mapping.json\".format(depth_file), \"w\") as output_json:\n        output_json.write(json.dumps(percentage_bases_covered))\n\n    json_dic = {\n        \"tableRow\": [{\n            \"sample\": sample_id,\n            \"data\": [{\n                \"header\": \"Mapping\",\n                \"table\": \"plasmids\",\n                \"patlas_mapping\": percentage_bases_covered,\n                \"value\": len(percentage_bases_covered)\n            }]\n        }],\n        \"sample\": sample_id,\n        \"patlas_mapping\": percentage_bases_covered,\n        \"plotData\": [{\n            \"sample\": sample_id,\n            \"data\": {\n                \"patlasMappingSliding\": dict_cov\n            },\n        }]\n    }\n\n    logger.debug(\"Size of dict_cov: {} kb\".format(asizeof(json_dic)\/1024))\n    logger.info(\"Writing to .report.json\")\n    with open(\".report.json\", \"w\") as json_report:\n        json_report.write(json.dumps(json_dic, separators=(\",\", \":\")))","method_path":"https:\/\/github.com\/assemblerflow\/flowcraft\/blob\/fc3f4bddded1efc76006600016dc71a06dd908c0\/flowcraft\/templates\/mapping2json.py#L195-L287"}
{"repo_name":"assemblerflow\/flowcraft","method_name":"Process.set_main_channel_names","method_code":"def set_main_channel_names(self, input_suffix, output_suffix, lane):\n        \"\"\"\"\"\"\n\n        self.input_channel = \"{}_in_{}\".format(self.template, input_suffix)\n        self.output_channel = \"{}_out_{}\".format(self.template, output_suffix)\n        self.lane = lane","method_summary":"Sets the main channel names based on the provide input and output channel suffixes. This is performed when connecting processes.","original_method_code":"def set_main_channel_names(self, input_suffix, output_suffix, lane):\n        \"\"\"Sets the main channel names based on the provide input and\n        output channel suffixes. This is performed when connecting processes.\n\n        Parameters\n        ----------\n        input_suffix : str\n            Suffix added to the input channel. Should be based on the lane\n            and an arbitrary unique id\n        output_suffix : str\n            Suffix added to the output channel. Should be based on the lane\n            and an arbitrary unique id\n        lane : int\n            Sets the lane of the process.\n        \"\"\"\n\n        self.input_channel = \"{}_in_{}\".format(self.template, input_suffix)\n        self.output_channel = \"{}_out_{}\".format(self.template, output_suffix)\n        self.lane = lane","method_path":"https:\/\/github.com\/assemblerflow\/flowcraft\/blob\/fc3f4bddded1efc76006600016dc71a06dd908c0\/flowcraft\/generator\/process.py#L330-L348"}
{"repo_name":"assemblerflow\/flowcraft","method_name":"Process.render","method_code":"def render(template, context):\n        \"\"\"\"\"\"\n\n        path, filename = os.path.split(template)\n\n        return jinja2.Environment(\n            loader=jinja2.FileSystemLoader(path or '.\/')\n        ).get_template(filename).render(context)","method_summary":"Wrapper to the jinja2 render method from a template file","original_method_code":"def render(template, context):\n        \"\"\"Wrapper to the jinja2 render method from a template file\n\n        Parameters\n        ----------\n        template : str\n            Path to template file.\n        context : dict\n            Dictionary with kwargs context to populate the template\n        \"\"\"\n\n        path, filename = os.path.split(template)\n\n        return jinja2.Environment(\n            loader=jinja2.FileSystemLoader(path or '.\/')\n        ).get_template(filename).render(context)","method_path":"https:\/\/github.com\/assemblerflow\/flowcraft\/blob\/fc3f4bddded1efc76006600016dc71a06dd908c0\/flowcraft\/generator\/process.py#L402-L417"}
{"repo_name":"assemblerflow\/flowcraft","method_name":"Process.template_str","method_code":"def template_str(self):\n        \"\"\"\"\"\"\n\n        if not self._context:\n            raise eh.ProcessError(\"Channels must be setup first using the \"\n                                  \"set_channels method\")\n\n        logger.debug(\"Setting context for template {}: {}\".format(\n            self.template, self._context\n        ))\n\n        x = self.render(self._template_path, self._context)\n        return x","method_summary":"Class property that returns a populated template string This property allows the template of a particular process to be dynamically generated and returned when doing ``Process.template_str``.","original_method_code":"def template_str(self):\n        \"\"\"Class property that returns a populated template string\n\n        This property allows the template of a particular process to be\n        dynamically generated and returned when doing ``Process.template_str``.\n\n        Returns\n        -------\n        x : str\n            String with the complete and populated process template\n\n        \"\"\"\n\n        if not self._context:\n            raise eh.ProcessError(\"Channels must be setup first using the \"\n                                  \"set_channels method\")\n\n        logger.debug(\"Setting context for template {}: {}\".format(\n            self.template, self._context\n        ))\n\n        x = self.render(self._template_path, self._context)\n        return x","method_path":"https:\/\/github.com\/assemblerflow\/flowcraft\/blob\/fc3f4bddded1efc76006600016dc71a06dd908c0\/flowcraft\/generator\/process.py#L420-L442"}
{"repo_name":"assemblerflow\/flowcraft","method_name":"Process.update_main_forks","method_code":"def update_main_forks(self, sink):\n        \"\"\"\"\"\"\n\n        if not self.main_forks:\n            self.main_forks = [self.output_channel]\n            self.output_channel = \"_{}\".format(self.output_channel)\n        self.main_forks.append(sink)\n\n        \n        operator = \"set\" if len(self.main_forks) == 1 else \"into\"\n        self.forks = [\"\\n{}.{}{{ {} }}\\n\".format(\n            self.output_channel, operator, \";\".join(self.main_forks))]\n\n        self._context = {**self._context,\n                         **{\"forks\": \"\".join(self.forks),\n                            \"output_channel\": self.output_channel}}","method_summary":"Updates the forks attribute with the sink channel destination","original_method_code":"def update_main_forks(self, sink):\n        \"\"\"Updates the forks attribute with the sink channel destination\n\n        Parameters\n        ----------\n        sink : str\n            Channel onto which the main input will be forked to\n\n        \"\"\"\n\n        if not self.main_forks:\n            self.main_forks = [self.output_channel]\n            self.output_channel = \"_{}\".format(self.output_channel)\n        self.main_forks.append(sink)\n\n        # fork_lst = self.forks + self.main_forks\n        operator = \"set\" if len(self.main_forks) == 1 else \"into\"\n        self.forks = [\"\\n{}.{}{{ {} }}\\n\".format(\n            self.output_channel, operator, \";\".join(self.main_forks))]\n\n        self._context = {**self._context,\n                         **{\"forks\": \"\".join(self.forks),\n                            \"output_channel\": self.output_channel}}","method_path":"https:\/\/github.com\/assemblerflow\/flowcraft\/blob\/fc3f4bddded1efc76006600016dc71a06dd908c0\/flowcraft\/generator\/process.py#L494-L516"}
{"repo_name":"assemblerflow\/flowcraft","method_name":"Process.update_attributes","method_code":"def update_attributes(self, attr_dict):\n        \"\"\"\"\"\"\n\n        \n        \n        valid_directives = [\"pid\", \"ignore_type\", \"ignore_pid\", \"extra_input\",\n                            \"group\", \"input_type\"]\n\n        for attribute, val in attr_dict.items():\n\n            \n            \n            if attribute in valid_directives and hasattr(self, attribute):\n                setattr(self, attribute, val)\n\n            \n            \n            elif attribute == \"params\":\n                for name, value in val.items():\n                    if name in self.params:\n                        self.params[name][\"default\"] = value\n                    else:\n                        raise eh.ProcessError(\n                            \"The parameter name '{}' does not exist for \"\n                            \"component '{}'\".format(name, self.template))\n\n            else:\n                for p in self.directives:\n                    self.directives[p][attribute] = val","method_summary":"Updates the directives attribute from a dictionary object. This will only update the directives for processes that have been defined in the subclass.","original_method_code":"def update_attributes(self, attr_dict):\n        \"\"\"Updates the directives attribute from a dictionary object.\n\n        This will only update the directives for processes that have been\n        defined in the subclass.\n\n        Parameters\n        ----------\n        attr_dict : dict\n            Dictionary containing the attributes that will be used to update\n            the process attributes and\/or directives.\n\n        \"\"\"\n\n        # Update directives\n        # Allowed attributes to write\n        valid_directives = [\"pid\", \"ignore_type\", \"ignore_pid\", \"extra_input\",\n                            \"group\", \"input_type\"]\n\n        for attribute, val in attr_dict.items():\n\n            # If the attribute has a valid directive key, update that\n            # directive\n            if attribute in valid_directives and hasattr(self, attribute):\n                setattr(self, attribute, val)\n\n            # The params attribute is special, in the sense that it provides\n            # information for the self.params attribute.\n            elif attribute == \"params\":\n                for name, value in val.items():\n                    if name in self.params:\n                        self.params[name][\"default\"] = value\n                    else:\n                        raise eh.ProcessError(\n                            \"The parameter name '{}' does not exist for \"\n                            \"component '{}'\".format(name, self.template))\n\n            else:\n                for p in self.directives:\n                    self.directives[p][attribute] = val","method_path":"https:\/\/github.com\/assemblerflow\/flowcraft\/blob\/fc3f4bddded1efc76006600016dc71a06dd908c0\/flowcraft\/generator\/process.py#L569-L608"}
{"repo_name":"urinieto\/msaf","method_name":"CMD.factorize","method_code":"def factorize(self):\n        \"\"\"\"\"\"\n\n        [prow, pcol] = self.sample_probability()\n\n        self._rid = self.sample(self._rrank, prow)\n        self._cid = self.sample(self._crank, pcol)\n\n        self._cmdinit()\n\n        self.computeUCR()","method_summary":"Factorize s.t. CUR = data Updated Values -------------- .C : updated values for C. .U : updated values for U. .R : updated values for R.","original_method_code":"def factorize(self):\n        \"\"\" Factorize s.t. CUR = data\n\n            Updated Values\n            --------------\n            .C : updated values for C.\n            .U : updated values for U.\n            .R : updated values for R.\n        \"\"\"\n\n        [prow, pcol] = self.sample_probability()\n\n        self._rid = self.sample(self._rrank, prow)\n        self._cid = self.sample(self._crank, pcol)\n\n        self._cmdinit()\n\n        self.computeUCR()","method_path":"https:\/\/github.com\/urinieto\/msaf\/blob\/9dbb57d77a1310465a65cc40f1641d083ca74385\/msaf\/pymf\/cmd.py#L72-L89"}
{"repo_name":"urinieto\/msaf","method_name":"CUR.factorize","method_code":"def factorize(self):\n        \"\"\"\"\"\"\n        [prow, pcol] = self.sample_probability()\n        self._rid = self.sample(self._rrank, prow)\n        self._cid = self.sample(self._crank, pcol)\n\n        self._rcnt = np.ones(len(self._rid))\n        self._ccnt = np.ones(len(self._cid))\n\n        self.computeUCR()","method_summary":"Factorize s.t. CUR = data Updated Values -------------- .C : updated values for C. .U : updated values for U. .R : updated values for R.","original_method_code":"def factorize(self):\n        \"\"\" Factorize s.t. CUR = data\n\n            Updated Values\n            --------------\n            .C : updated values for C.\n            .U : updated values for U.\n            .R : updated values for R.\n        \"\"\"\n        [prow, pcol] = self.sample_probability()\n        self._rid = self.sample(self._rrank, prow)\n        self._cid = self.sample(self._crank, pcol)\n\n        self._rcnt = np.ones(len(self._rid))\n        self._ccnt = np.ones(len(self._cid))\n\n        self.computeUCR()","method_path":"https:\/\/github.com\/urinieto\/msaf\/blob\/9dbb57d77a1310465a65cc40f1641d083ca74385\/msaf\/pymf\/cur.py#L122-L138"}
{"repo_name":"urinieto\/msaf","method_name":"quickhull","method_code":"def quickhull(sample):\n    \"\"\"\"\"\"\n\n    link = lambda a, b: np.concatenate((a, b[1:]))\n    edge = lambda a, b: np.concatenate(([a], [b]))\n\n    def dome(sample, base):\n        h, t = base\n        dists = np.dot(sample - h, np.dot(((0, -1), (1, 0)), (t - h)))\n        outer = np.repeat(sample, dists > 0, axis=0)\n\n        if len(outer):\n            pivot = sample[np.argmax(dists)]\n            return link(dome(outer, edge(h, pivot)),\n                dome(outer, edge(pivot, t)))\n        else:\n            return base\n\n    if len(sample) > 2:\n        axis = sample[:, 0]\n        base = np.take(sample, [np.argmin(axis), np.argmax(axis)], axis=0)\n        return link(dome(sample, base),\n            dome(sample, base[::-1]))\n    else:\n        return sample","method_summary":"Find data points on the convex hull of a supplied data set","original_method_code":"def quickhull(sample):\n    \"\"\" Find data points on the convex hull of a supplied data set\n\n    Args:\n        sample: data points as column vectors n x d\n                    n - number samples\n                    d - data dimension (should be two)\n\n    Returns:\n        a k x d matrix containint the convex hull data points\n    \"\"\"\n\n    link = lambda a, b: np.concatenate((a, b[1:]))\n    edge = lambda a, b: np.concatenate(([a], [b]))\n\n    def dome(sample, base):\n        h, t = base\n        dists = np.dot(sample - h, np.dot(((0, -1), (1, 0)), (t - h)))\n        outer = np.repeat(sample, dists > 0, axis=0)\n\n        if len(outer):\n            pivot = sample[np.argmax(dists)]\n            return link(dome(outer, edge(h, pivot)),\n                dome(outer, edge(pivot, t)))\n        else:\n            return base\n\n    if len(sample) > 2:\n        axis = sample[:, 0]\n        base = np.take(sample, [np.argmin(axis), np.argmax(axis)], axis=0)\n        return link(dome(sample, base),\n            dome(sample, base[::-1]))\n    else:\n        return sample","method_path":"https:\/\/github.com\/urinieto\/msaf\/blob\/9dbb57d77a1310465a65cc40f1641d083ca74385\/msaf\/pymf\/chnmf.py#L27-L60"}
{"repo_name":"urinieto\/msaf","method_name":"CHNMF._map_w_to_data","method_code":"def _map_w_to_data(self):\n        \"\"\"\"\"\"\n\n        \n        self._Wmapped_index = vq(self.data, self.W)\n        self.Wmapped = np.zeros(self.W.shape)\n\n        \n        \n        \n        \n        for i, s in enumerate(self._Wmapped_index):\n            self.Wmapped[:,i] = self.data[:,s]","method_summary":"Return data points that are most similar to basis vectors W","original_method_code":"def _map_w_to_data(self):\n        \"\"\" Return data points that are most similar to basis vectors W\n        \"\"\"\n\n        # assign W to the next best data sample\n        self._Wmapped_index = vq(self.data, self.W)\n        self.Wmapped = np.zeros(self.W.shape)\n\n        # do not directly assign, i.e. Wdist = self.data[:,sel]\n        # as self might be unsorted (in non ascending order)\n        # -> sorting sel would screw the matching to W if\n        # self.data is stored as a hdf5 table (see h5py)\n        for i, s in enumerate(self._Wmapped_index):\n            self.Wmapped[:,i] = self.data[:,s]","method_path":"https:\/\/github.com\/urinieto\/msaf\/blob\/9dbb57d77a1310465a65cc40f1641d083ca74385\/msaf\/pymf\/chnmf.py#L137-L150"}
{"repo_name":"urinieto\/msaf","method_name":"CHNMF.update_w","method_code":"def update_w(self):\n        \"\"\"\"\"\"\n        def select_hull_points(data, n=3):\n            \"\"\"\"\"\"\n\n            \n            idx = np.array([])\n\n            \n            for i in combinations(range(n), 2):\n                \n                convex_hull_d = quickhull(data[i, :].T)\n\n                \n                idx = np.append(idx, vq(data[i, :], convex_hull_d.T))\n                idx = np.unique(idx)\n\n            return np.int32(idx)\n\n        \n        \n        method = 'randomprojection'\n        if method == 'pca':\n            pcamodel = PCA(self.data)\n            pcamodel.factorize(show_progress=False)\n            proj = pcamodel.H\n        else:\n            R = np.random.randn(self._base_sel, self._data_dimension)\n            proj = np.dot(R, self.data)\n\n        self._hull_idx = select_hull_points(proj, n=self._base_sel)\n        aa_mdl = AA(self.data[:, self._hull_idx], num_bases=self._num_bases)\n\n        \n        aa_mdl.factorize(niter=50, compute_h=True, compute_w=True,\n                         compute_err=True, show_progress=False)\n\n        self.W = aa_mdl.W\n        self._map_w_to_data()","method_summary":"compute new W","original_method_code":"def update_w(self):\n        \"\"\" compute new W \"\"\"\n        def select_hull_points(data, n=3):\n            \"\"\" select data points for pairwise projections of the first n\n            dimensions \"\"\"\n\n            # iterate over all projections and select data points\n            idx = np.array([])\n\n            # iterate over some pairwise combinations of dimensions\n            for i in combinations(range(n), 2):\n                # sample convex hull points in 2D projection\n                convex_hull_d = quickhull(data[i, :].T)\n\n                # get indices for convex hull data points\n                idx = np.append(idx, vq(data[i, :], convex_hull_d.T))\n                idx = np.unique(idx)\n\n            return np.int32(idx)\n\n        # determine convex hull data points using either PCA or random\n        # projections\n        method = 'randomprojection'\n        if method == 'pca':\n            pcamodel = PCA(self.data)\n            pcamodel.factorize(show_progress=False)\n            proj = pcamodel.H\n        else:\n            R = np.random.randn(self._base_sel, self._data_dimension)\n            proj = np.dot(R, self.data)\n\n        self._hull_idx = select_hull_points(proj, n=self._base_sel)\n        aa_mdl = AA(self.data[:, self._hull_idx], num_bases=self._num_bases)\n\n        # determine W\n        aa_mdl.factorize(niter=50, compute_h=True, compute_w=True,\n                         compute_err=True, show_progress=False)\n\n        self.W = aa_mdl.W\n        self._map_w_to_data()","method_path":"https:\/\/github.com\/urinieto\/msaf\/blob\/9dbb57d77a1310465a65cc40f1641d083ca74385\/msaf\/pymf\/chnmf.py#L152-L191"}
{"repo_name":"urinieto\/msaf","method_name":"CHNMF.factorize","method_code":"def factorize(self, show_progress=False, compute_w=True, compute_h=True,\n                  compute_err=True, niter=1):\n        \"\"\"\"\"\"\n\n        AA.factorize(self, niter=1, show_progress=show_progress,\n                  compute_w=compute_w, compute_h=compute_h,\n                  compute_err=compute_err)","method_summary":"Factorize s.t. WH = data","original_method_code":"def factorize(self, show_progress=False, compute_w=True, compute_h=True,\n                  compute_err=True, niter=1):\n        \"\"\" Factorize s.t. WH = data\n\n            Parameters\n            ----------\n            show_progress : bool\n                    print some extra information to stdout.\n            compute_h : bool\n                    iteratively update values for H.\n            compute_w : bool\n                    iteratively update values for W.\n            compute_err : bool\n                    compute Frobenius norm |data-WH| after each update and store\n                    it to .ferr[k].\n\n            Updated Values\n            --------------\n            .W : updated values for W.\n            .H : updated values for H.\n            .ferr : Frobenius norm |data-WH|.\n        \"\"\"\n\n        AA.factorize(self, niter=1, show_progress=show_progress,\n                  compute_w=compute_w, compute_h=compute_h,\n                  compute_err=compute_err)","method_path":"https:\/\/github.com\/urinieto\/msaf\/blob\/9dbb57d77a1310465a65cc40f1641d083ca74385\/msaf\/pymf\/chnmf.py#L193-L218"}
{"repo_name":"urinieto\/msaf","method_name":"GMAP.update_w","method_code":"def update_w(self):\n        \"\"\"\"\"\"\n\n        def select_next(iterval):\n            \"\"\"\"\"\"\n\n            if self._robust_map:\n                k = np.argsort(iterval)[::-1]\n                d_sub = self.data[:,k[:self._robust_nselect]]\n                self.sub.extend(k[:self._robust_nselect])\n\n                \n                kmeans_mdl = Kmeans(d_sub, num_bases=self._robust_cluster)\n                kmeans_mdl.factorize(niter=10)\n\n                \n                h = np.histogram(kmeans_mdl.assigned, range(self._robust_cluster+1))[0]\n                largest_cluster = np.argmax(h)\n                sel = pdist(kmeans_mdl.W[:, largest_cluster:largest_cluster+1], d_sub)\n                sel = k[np.argmin(sel)]\n            else:\n                sel = np.argmax(iterval)\n\n            return sel\n\n        EPS = 10**-8\n\n        if scipy.sparse.issparse(self.data):\n            norm_data = np.sqrt(self.data.multiply(self.data).sum(axis=0))\n            norm_data = np.array(norm_data).reshape((-1))\n        else:\n            norm_data = np.sqrt(np.sum(self.data**2, axis=0))\n\n\n        self.select = []\n\n        if self._method == 'pca' or self._method == 'aa':\n            iterval = norm_data.copy()\n\n        if self._method == 'nmf':\n            iterval = np.sum(self.data, axis=0)\/(np.sqrt(self.data.shape[0])*norm_data)\n            iterval = 1.0 - iterval\n\n        self.select.append(select_next(iterval))\n\n\n        for l in range(1, self._num_bases):\n\n            if scipy.sparse.issparse(self.data):\n                c = self.data[:, self.select[-1]:self.select[-1]+1].T * self.data\n                c = np.array(c.todense())\n            else:\n                c = np.dot(self.data[:,self.select[-1]], self.data)\n\n            c = c\/(norm_data * norm_data[self.select[-1]])\n\n            if self._method == 'pca':\n                c = 1.0 - np.abs(c)\n                c = c * norm_data\n\n            elif self._method == 'aa':\n                c = (c*-1.0 + 1.0)\/2.0\n                c = c * norm_data\n\n            elif self._method == 'nmf':\n                c = 1.0 - np.abs(c)\n\n            \n            iterval = c * iterval\n\n            \n            self.select.append(select_next(iterval))\n\n            self._logger.info('cur_nodes: ' + str(self.select))\n\n        \n        self.W = self.data[:, np.sort(self.select)]\n\n        \n        self.W = self.W[:, np.argsort(np.argsort(self.select))]","method_summary":"compute new W","original_method_code":"def update_w(self):\n        \"\"\" compute new W \"\"\"\n\n        def select_next(iterval):\n            \"\"\" select the next best data sample using robust map\n            or simply the max iterval ... \"\"\"\n\n            if self._robust_map:\n                k = np.argsort(iterval)[::-1]\n                d_sub = self.data[:,k[:self._robust_nselect]]\n                self.sub.extend(k[:self._robust_nselect])\n\n                # cluster d_sub\n                kmeans_mdl = Kmeans(d_sub, num_bases=self._robust_cluster)\n                kmeans_mdl.factorize(niter=10)\n\n                # get largest cluster\n                h = np.histogram(kmeans_mdl.assigned, range(self._robust_cluster+1))[0]\n                largest_cluster = np.argmax(h)\n                sel = pdist(kmeans_mdl.W[:, largest_cluster:largest_cluster+1], d_sub)\n                sel = k[np.argmin(sel)]\n            else:\n                sel = np.argmax(iterval)\n\n            return sel\n\n        EPS = 10**-8\n\n        if scipy.sparse.issparse(self.data):\n            norm_data = np.sqrt(self.data.multiply(self.data).sum(axis=0))\n            norm_data = np.array(norm_data).reshape((-1))\n        else:\n            norm_data = np.sqrt(np.sum(self.data**2, axis=0))\n\n\n        self.select = []\n\n        if self._method == 'pca' or self._method == 'aa':\n            iterval = norm_data.copy()\n\n        if self._method == 'nmf':\n            iterval = np.sum(self.data, axis=0)\/(np.sqrt(self.data.shape[0])*norm_data)\n            iterval = 1.0 - iterval\n\n        self.select.append(select_next(iterval))\n\n\n        for l in range(1, self._num_bases):\n\n            if scipy.sparse.issparse(self.data):\n                c = self.data[:, self.select[-1]:self.select[-1]+1].T * self.data\n                c = np.array(c.todense())\n            else:\n                c = np.dot(self.data[:,self.select[-1]], self.data)\n\n            c = c\/(norm_data * norm_data[self.select[-1]])\n\n            if self._method == 'pca':\n                c = 1.0 - np.abs(c)\n                c = c * norm_data\n\n            elif self._method == 'aa':\n                c = (c*-1.0 + 1.0)\/2.0\n                c = c * norm_data\n\n            elif self._method == 'nmf':\n                c = 1.0 - np.abs(c)\n\n            ### update the estimated volume\n            iterval = c * iterval\n\n            # detect the next best data point\n            self.select.append(select_next(iterval))\n\n            self._logger.info('cur_nodes: ' + str(self.select))\n\n        # sort indices, otherwise h5py won't work\n        self.W = self.data[:, np.sort(self.select)]\n\n        # \"unsort\" it again to keep the correct order\n        self.W = self.W[:, np.argsort(np.argsort(self.select))]","method_path":"https:\/\/github.com\/urinieto\/msaf\/blob\/9dbb57d77a1310465a65cc40f1641d083ca74385\/msaf\/pymf\/gmap.py#L93-L173"}
{"repo_name":"urinieto\/msaf","method_name":"GMAP.factorize","method_code":"def factorize(self, show_progress=False, compute_w=True, compute_h=True,\n                  compute_err=True, robust_cluster=3, niter=1, robust_nselect=-1):\n        \"\"\"\"\"\"\n        self._robust_cluster = robust_cluster\n        self._robust_nselect = robust_nselect\n\n        if self._robust_nselect == -1:\n            self._robust_nselect = np.round(np.log(self.data.shape[1])*2)\n\n        AA.factorize(self, niter=1, show_progress=show_progress,\n                  compute_w=compute_w, compute_h=compute_h,\n                  compute_err=compute_err)","method_summary":"Factorize s.t. WH = data","original_method_code":"def factorize(self, show_progress=False, compute_w=True, compute_h=True,\n                  compute_err=True, robust_cluster=3, niter=1, robust_nselect=-1):\n        \"\"\" Factorize s.t. WH = data\n\n            Parameters\n            ----------\n            show_progress : bool\n                    print some extra information to stdout.\n                    False, default\n            compute_h : bool\n                    iteratively update values for H.\n                    True, default\n            compute_w : bool\n                    iteratively update values for W.\n                    default, True\n            compute_err : bool\n                    compute Frobenius norm |data-WH| after each update and store\n                    it to .ferr[k].\n            robust_cluster : int, optional\n                    set the number of clusters for robust map selection.\n                    3, default\n            robust_nselect : int, optional\n                    set the number of samples to consider for robust map\n                    selection.\n                    -1, default (automatically determine suitable number)\n\n            Updated Values\n            --------------\n            .W : updated values for W.\n            .H : updated values for H.\n            .ferr : Frobenius norm |data-WH|.\n        \"\"\"\n        self._robust_cluster = robust_cluster\n        self._robust_nselect = robust_nselect\n\n        if self._robust_nselect == -1:\n            self._robust_nselect = np.round(np.log(self.data.shape[1])*2)\n\n        AA.factorize(self, niter=1, show_progress=show_progress,\n                  compute_w=compute_w, compute_h=compute_h,\n                  compute_err=compute_err)","method_path":"https:\/\/github.com\/urinieto\/msaf\/blob\/9dbb57d77a1310465a65cc40f1641d083ca74385\/msaf\/pymf\/gmap.py#L175-L215"}
{"repo_name":"urinieto\/msaf","method_name":"Segmenter.process","method_code":"def process(self):\n        \"\"\"\"\"\"\n        \n        \n        pcp_obj = Features.select_features(\n            \"pcp\", self.file_struct, self.annot_beats, self.framesync)\n        mfcc_obj = Features.select_features(\n            \"mfcc\", self.file_struct, self.annot_beats, self.framesync)\n\n        \n        frame_times = pcp_obj.frame_times\n        assert np.array_equal(frame_times, mfcc_obj.frame_times)\n\n        \n        \n        \n        est_idxs, est_labels, F = main2.do_segmentation(\n            pcp_obj.features.T, mfcc_obj.features.T, self.config,\n            self.in_bound_idxs)\n\n        return est_idxs, est_labels, F","method_summary":"Main process.","original_method_code":"def process(self):\n        \"\"\"Main process.\n        Returns\n        -------\n        est_idxs : np.array(N) or list\n            Estimated times for the segment boundaries in frame indeces.\n            List if hierarchical segmentation.\n        est_labels : np.array(N-1) or list\n            Estimated labels for the segments.\n            List if hierarchical segmentation.\n        \"\"\"\n        # This algorithm only accepts one specific kind of features:\n        # Combination of PCP + MFCC. Let's get them:\n        pcp_obj = Features.select_features(\n            \"pcp\", self.file_struct, self.annot_beats, self.framesync)\n        mfcc_obj = Features.select_features(\n            \"mfcc\", self.file_struct, self.annot_beats, self.framesync)\n\n        # Get frame times and make sure they're the same in both features\n        frame_times = pcp_obj.frame_times\n        assert np.array_equal(frame_times, mfcc_obj.frame_times)\n\n        # Brian wants PCP and MFCC\n        # (tranpsosed, because he's that kind of person)\n        # TODO: self.in_bound_idxs\n        est_idxs, est_labels, F = main2.do_segmentation(\n            pcp_obj.features.T, mfcc_obj.features.T, self.config,\n            self.in_bound_idxs)\n\n        return est_idxs, est_labels, F","method_path":"https:\/\/github.com\/urinieto\/msaf\/blob\/9dbb57d77a1310465a65cc40f1641d083ca74385\/msaf\/algorithms\/scluster\/segmenter.py#L22-L51"}
{"repo_name":"urinieto\/msaf","method_name":"Segmenter.processFlat","method_code":"def processFlat(self):\n        \"\"\"\"\"\"\n        self.config[\"hier\"] = False\n        est_idxs, est_labels, F = self.process()\n        assert est_idxs[0] == 0 and est_idxs[-1] == F.shape[1] - 1\n        return self._postprocess(est_idxs, est_labels)","method_summary":"Main process.for flat segmentation.","original_method_code":"def processFlat(self):\n        \"\"\"Main process.for flat segmentation.\n        Returns\n        -------\n        est_idxs : np.array(N)\n            Estimated times for the segment boundaries in frame indeces.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"\n        self.config[\"hier\"] = False\n        est_idxs, est_labels, F = self.process()\n        assert est_idxs[0] == 0 and est_idxs[-1] == F.shape[1] - 1\n        return self._postprocess(est_idxs, est_labels)","method_path":"https:\/\/github.com\/urinieto\/msaf\/blob\/9dbb57d77a1310465a65cc40f1641d083ca74385\/msaf\/algorithms\/scluster\/segmenter.py#L53-L65"}
{"repo_name":"neherlab\/treetime","method_name":"base_regression","method_code":"def base_regression(Q, slope=None):\n    \"\"\"\"\"\"\n    if slope is None:\n        slope = (Q[dtavgii] - Q[tavgii]*Q[davgii]\/Q[sii]) \\\n                \/(Q[tsqii] - Q[tavgii]**2\/Q[sii])\n        only_intercept=False\n    else:\n        only_intercept=True\n\n    intercept = (Q[davgii] - Q[tavgii]*slope)\/Q[sii]\n\n    if only_intercept:\n        return {'slope':slope, 'intercept':intercept,\n                'chisq': 0.5*(Q[dsqii]\/Q[sii] - Q[davgii]**2\/Q[sii]**2)}\n\n    chisq = 0.5*(Q[dsqii] - Q[davgii]**2\/Q[sii]\n                - (Q[dtavgii] - Q[davgii]*Q[tavgii]\/Q[sii])**2\/(Q[tsqii]\n                - Q[tavgii]**2\/Q[sii]))\n\n    estimator_hessian = np.array([[Q[tsqii], Q[tavgii]], [Q[tavgii], Q[sii]]])\n\n    return {'slope':slope, 'intercept':intercept,\n            'chisq':chisq, 'hessian':estimator_hessian,\n            'cov':np.linalg.inv(estimator_hessian)}","method_summary":"this function calculates the regression coefficients for a given vector containing the averages of tip and branch quantities.","original_method_code":"def base_regression(Q, slope=None):\n    \"\"\"\n    this function calculates the regression coefficients for a\n    given vector containing the averages of tip and branch\n    quantities.\n\n    Parameters\n    ----------\n    Q : numpy.array\n        vector with\n    slope : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\n    if slope is None:\n        slope = (Q[dtavgii] - Q[tavgii]*Q[davgii]\/Q[sii]) \\\n                \/(Q[tsqii] - Q[tavgii]**2\/Q[sii])\n        only_intercept=False\n    else:\n        only_intercept=True\n\n    intercept = (Q[davgii] - Q[tavgii]*slope)\/Q[sii]\n\n    if only_intercept:\n        return {'slope':slope, 'intercept':intercept,\n                'chisq': 0.5*(Q[dsqii]\/Q[sii] - Q[davgii]**2\/Q[sii]**2)}\n\n    chisq = 0.5*(Q[dsqii] - Q[davgii]**2\/Q[sii]\n                - (Q[dtavgii] - Q[davgii]*Q[tavgii]\/Q[sii])**2\/(Q[tsqii]\n                - Q[tavgii]**2\/Q[sii]))\n\n    estimator_hessian = np.array([[Q[tsqii], Q[tavgii]], [Q[tavgii], Q[sii]]])\n\n    return {'slope':slope, 'intercept':intercept,\n            'chisq':chisq, 'hessian':estimator_hessian,\n            'cov':np.linalg.inv(estimator_hessian)}","method_path":"https:\/\/github.com\/neherlab\/treetime\/blob\/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0\/treetime\/treeregression.py#L6-L45"}
{"repo_name":"neherlab\/treetime","method_name":"TreeRegression.Cov","method_code":"def Cov(self):\n        \"\"\"\"\"\"\n        \n        M = np.zeros((self.N, self.N))\n        for n in self.tree.find_clades():\n            if n == self.tree.root:\n                continue\n            M[np.meshgrid(n._ii, n._ii)] += self.branch_variance(n)\n        return M","method_summary":"calculate the covariance matrix of the tips assuming variance has accumulated along branches of the tree accoriding to the the provided","original_method_code":"def Cov(self):\n        \"\"\"\n        calculate the covariance matrix of the tips assuming variance\n        has accumulated along branches of the tree accoriding to the\n        the provided\n        Returns\n        -------\n\n         M : (np.array)\n            covariance matrix with tips arranged standard transersal order.\n        \"\"\"\n        # accumulate the covariance matrix by adding 'squares'\n        M = np.zeros((self.N, self.N))\n        for n in self.tree.find_clades():\n            if n == self.tree.root:\n                continue\n            M[np.meshgrid(n._ii, n._ii)] += self.branch_variance(n)\n        return M","method_path":"https:\/\/github.com\/neherlab\/treetime\/blob\/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0\/treetime\/treeregression.py#L113-L130"}
{"repo_name":"neherlab\/treetime","method_name":"TreeRegression.CovInv","method_code":"def CovInv(self):\n        \"\"\"\"\"\"\n        self.recurse(full_matrix=True)\n        return self.tree.root.cinv","method_summary":"Inverse of the covariance matrix","original_method_code":"def CovInv(self):\n        \"\"\"\n        Inverse of the covariance matrix\n\n        Returns\n        -------\n\n         H : (np.array)\n            inverse of the covariance matrix.\n        \"\"\"\n        self.recurse(full_matrix=True)\n        return self.tree.root.cinv","method_path":"https:\/\/github.com\/neherlab\/treetime\/blob\/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0\/treetime\/treeregression.py#L133-L144"}
{"repo_name":"neherlab\/treetime","method_name":"TreeRegression.recurse","method_code":"def recurse(self, full_matrix=False):\n        \"\"\"\"\"\"\n        for n in self.tree.get_nonterminals(order='postorder'):\n            n_leaves = len(n._ii)\n            if full_matrix: M = np.zeros((n_leaves, n_leaves), dtype=float)\n            r = np.zeros(n_leaves, dtype=float)\n            c_count = 0\n            for c in n:\n                ssq = self.branch_variance(c)\n                nc = len(c._ii)\n                if c.is_terminal():\n                    if full_matrix:\n                        M[c_count, c_count] = 1.0\/ssq\n                    r[c_count] = 1.0\/ssq\n                else:\n                    if full_matrix:\n                        M[c_count:c_count+nc, c_count:c_count+nc] = c.cinv - ssq*np.outer(c.r,c.r)\/(1+ssq*c.s)\n                    r[c_count:c_count+nc] = c.r\/(1+ssq*c.s)\n                c_count += nc\n\n            if full_matrix: n.cinv = M\n            n.r = r \n            n.s = n.r.sum()","method_summary":"recursion to calculate inverse covariance matrix","original_method_code":"def recurse(self, full_matrix=False):\n        \"\"\"\n        recursion to calculate inverse covariance matrix\n\n        Parameters\n        ----------\n        full_matrix : bool, optional\n            if True, the entire inverse matrix is calculated. otherwise, only the weighing vector.\n        \"\"\"\n        for n in self.tree.get_nonterminals(order='postorder'):\n            n_leaves = len(n._ii)\n            if full_matrix: M = np.zeros((n_leaves, n_leaves), dtype=float)\n            r = np.zeros(n_leaves, dtype=float)\n            c_count = 0\n            for c in n:\n                ssq = self.branch_variance(c)\n                nc = len(c._ii)\n                if c.is_terminal():\n                    if full_matrix:\n                        M[c_count, c_count] = 1.0\/ssq\n                    r[c_count] = 1.0\/ssq\n                else:\n                    if full_matrix:\n                        M[c_count:c_count+nc, c_count:c_count+nc] = c.cinv - ssq*np.outer(c.r,c.r)\/(1+ssq*c.s)\n                    r[c_count:c_count+nc] = c.r\/(1+ssq*c.s)\n                c_count += nc\n\n            if full_matrix: n.cinv = M\n            n.r = r #M.sum(axis=1)\n            n.s = n.r.sum()","method_path":"https:\/\/github.com\/neherlab\/treetime\/blob\/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0\/treetime\/treeregression.py#L147-L176"}
{"repo_name":"neherlab\/treetime","method_name":"TreeRegression._calculate_averages","method_code":"def _calculate_averages(self):\n        \"\"\"\"\"\"\n        for n in self.tree.get_nonterminals(order='postorder'):\n            Q = np.zeros(6, dtype=float)\n            for c in n:\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                Q += self.propagate_averages(c, tv, bv, var)\n            n.Q=Q\n\n        for n in self.tree.find_clades(order='preorder'):\n            O = np.zeros(6, dtype=float)\n            if n==self.tree.root:\n                n.Qtot = n.Q\n                continue\n\n            for c in n.up:\n                if c==n:\n                    continue\n\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                O += self.propagate_averages(c, tv, bv, var)\n\n            if n.up!=self.tree.root:\n                c = n.up\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                O += self.propagate_averages(c, tv, bv, var, outgroup=True)\n            n.O = O\n\n            if not n.is_terminal():\n                tv = self.tip_value(n)\n                bv = self.branch_value(n)\n                var = self.branch_variance(n)\n                n.Qtot = n.Q + self.propagate_averages(n, tv, bv, var, outgroup=True)","method_summary":"calculate the weighted sums of the tip and branch values and their second moments.","original_method_code":"def _calculate_averages(self):\n        \"\"\"\n        calculate the weighted sums of the tip and branch values and\n        their second moments.\n        \"\"\"\n        for n in self.tree.get_nonterminals(order='postorder'):\n            Q = np.zeros(6, dtype=float)\n            for c in n:\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                Q += self.propagate_averages(c, tv, bv, var)\n            n.Q=Q\n\n        for n in self.tree.find_clades(order='preorder'):\n            O = np.zeros(6, dtype=float)\n            if n==self.tree.root:\n                n.Qtot = n.Q\n                continue\n\n            for c in n.up:\n                if c==n:\n                    continue\n\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                O += self.propagate_averages(c, tv, bv, var)\n\n            if n.up!=self.tree.root:\n                c = n.up\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                O += self.propagate_averages(c, tv, bv, var, outgroup=True)\n            n.O = O\n\n            if not n.is_terminal():\n                tv = self.tip_value(n)\n                bv = self.branch_value(n)\n                var = self.branch_variance(n)\n                n.Qtot = n.Q + self.propagate_averages(n, tv, bv, var, outgroup=True)","method_path":"https:\/\/github.com\/neherlab\/treetime\/blob\/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0\/treetime\/treeregression.py#L179-L220"}
{"repo_name":"neherlab\/treetime","method_name":"TreeRegression.propagate_averages","method_code":"def propagate_averages(self, n, tv, bv, var, outgroup=False):\n        \"\"\"\"\"\"\n        if n.is_terminal() and outgroup==False:\n            if tv is None or np.isinf(tv) or np.isnan(tv):\n                res = np.array([0, 0, 0, 0, 0, 0])\n            elif var==0:\n                res = np.array([np.inf, np.inf, np.inf, np.inf, np.inf, np.inf])\n            else:\n                res = np.array([\n                    tv\/var,\n                    bv\/var,\n                    tv**2\/var,\n                    bv*tv\/var,\n                    bv**2\/var,\n                    1.0\/var], dtype=float)\n        else:\n            tmpQ = n.O if outgroup else n.Q\n            denom = 1.0\/(1+var*tmpQ[sii])\n            res = np.array([\n                tmpQ[tavgii]*denom,\n                (tmpQ[davgii] + bv*tmpQ[sii])*denom,\n                tmpQ[tsqii] - var*tmpQ[tavgii]**2*denom,\n                tmpQ[dtavgii] + tmpQ[tavgii]*bv - var*tmpQ[tavgii]*(tmpQ[davgii] + bv*tmpQ[sii])*denom,\n                tmpQ[dsqii] + 2*bv*tmpQ[davgii] + bv**2*tmpQ[sii] - var*(tmpQ[davgii]**2 + 2*bv*tmpQ[davgii]*tmpQ[sii] + bv**2*tmpQ[sii]**2)*denom,\n                tmpQ[sii]*denom]\n            )\n\n        return res","method_summary":"This function implements the propagation of the means, variance, and covariances along a branch. It operates both towards the root and tips.","original_method_code":"def propagate_averages(self, n, tv, bv, var, outgroup=False):\n        \"\"\"\n        This function implements the propagation of the means,\n        variance, and covariances along a branch. It operates\n        both towards the root and tips.\n\n        Parameters\n        ----------\n         n : (node)\n            the branch connecting this node to its parent is used\n            for propagation\n         tv : (float)\n            tip value. Only required if not is terminal\n         bl : (float)\n            branch value. The increment of the tree associated quantity'\n\n         var : (float)\n            the variance increment along the branch\n\n        Returns\n        -------\n         Q : (np.array)\n            a vector of length 6 containing the updated quantities\n        \"\"\"\n        if n.is_terminal() and outgroup==False:\n            if tv is None or np.isinf(tv) or np.isnan(tv):\n                res = np.array([0, 0, 0, 0, 0, 0])\n            elif var==0:\n                res = np.array([np.inf, np.inf, np.inf, np.inf, np.inf, np.inf])\n            else:\n                res = np.array([\n                    tv\/var,\n                    bv\/var,\n                    tv**2\/var,\n                    bv*tv\/var,\n                    bv**2\/var,\n                    1.0\/var], dtype=float)\n        else:\n            tmpQ = n.O if outgroup else n.Q\n            denom = 1.0\/(1+var*tmpQ[sii])\n            res = np.array([\n                tmpQ[tavgii]*denom,\n                (tmpQ[davgii] + bv*tmpQ[sii])*denom,\n                tmpQ[tsqii] - var*tmpQ[tavgii]**2*denom,\n                tmpQ[dtavgii] + tmpQ[tavgii]*bv - var*tmpQ[tavgii]*(tmpQ[davgii] + bv*tmpQ[sii])*denom,\n                tmpQ[dsqii] + 2*bv*tmpQ[davgii] + bv**2*tmpQ[sii] - var*(tmpQ[davgii]**2 + 2*bv*tmpQ[davgii]*tmpQ[sii] + bv**2*tmpQ[sii]**2)*denom,\n                tmpQ[sii]*denom]\n            )\n\n        return res","method_path":"https:\/\/github.com\/neherlab\/treetime\/blob\/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0\/treetime\/treeregression.py#L223-L272"}
{"repo_name":"neherlab\/treetime","method_name":"TreeRegression.explained_variance","method_code":"def explained_variance(self):\n        \"\"\"\"\"\"\n        self.tree.root._v=0\n        for n in self.tree.get_nonterminals(order='preorder'):\n            for c in n:\n                c._v = n._v + self.branch_value(c)\n        raw = np.array([(self.tip_value(n), n._v) for n in self.tree.get_terminals()\n                         if self.tip_value(n) is not None])\n        return np.corrcoef(raw.T)[0,1]","method_summary":"calculate standard explained variance","original_method_code":"def explained_variance(self):\n        \"\"\"calculate standard explained variance\n\n        Returns\n        -------\n        float\n            r-value of the root-to-tip distance and time.\n            independent of regression model, but dependent on root choice\n        \"\"\"\n        self.tree.root._v=0\n        for n in self.tree.get_nonterminals(order='preorder'):\n            for c in n:\n                c._v = n._v + self.branch_value(c)\n        raw = np.array([(self.tip_value(n), n._v) for n in self.tree.get_terminals()\n                         if self.tip_value(n) is not None])\n        return np.corrcoef(raw.T)[0,1]","method_path":"https:\/\/github.com\/neherlab\/treetime\/blob\/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0\/treetime\/treeregression.py#L274-L289"}
{"repo_name":"neherlab\/treetime","method_name":"TreeRegression.regression","method_code":"def regression(self, slope=None):\n        \"\"\"\"\"\"\n        self._calculate_averages()\n\n        clock_model = base_regression(self.tree.root.Q, slope)\n        clock_model['r_val'] = self.explained_variance()\n\n        return clock_model","method_summary":"regress tip values against branch values","original_method_code":"def regression(self, slope=None):\n        \"\"\"regress tip values against branch values\n\n        Parameters\n        ----------\n        slope : None, optional\n            if given, the slope isn't optimized\n\n        Returns\n        -------\n        dict\n            regression parameters\n        \"\"\"\n        self._calculate_averages()\n\n        clock_model = base_regression(self.tree.root.Q, slope)\n        clock_model['r_val'] = self.explained_variance()\n\n        return clock_model","method_path":"https:\/\/github.com\/neherlab\/treetime\/blob\/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0\/treetime\/treeregression.py#L292-L310"}
{"repo_name":"neherlab\/treetime","method_name":"TreeRegression.find_best_root","method_code":"def find_best_root(self, force_positive=True, slope=None):\n        \"\"\"\"\"\"\n        self._calculate_averages()\n        best_root = {\"chisq\": np.inf}\n        for n in self.tree.find_clades():\n            if n==self.tree.root:\n                continue\n\n            tv = self.tip_value(n)\n            bv = self.branch_value(n)\n            var = self.branch_variance(n)\n            x, chisq = self._optimal_root_along_branch(n, tv, bv, var, slope=slope)\n\n            if (chisq<best_root[\"chisq\"]):\n                tmpQ = self.propagate_averages(n, tv, bv*x, var*x) \\\n                     + self.propagate_averages(n, tv, bv*(1-x), var*(1-x), outgroup=True)\n                reg = base_regression(tmpQ, slope=slope)\n                if reg[\"slope\"]>=0 or (force_positive==False):\n                    best_root = {\"node\":n, \"split\":x}\n                    best_root.update(reg)\n\n        if 'node' not in best_root:\n            print(\"TreeRegression.find_best_root: No valid root found!\", force_positive)\n            return None\n\n        if 'hessian' in best_root:\n            \n            deriv = []\n            n = best_root[\"node\"]\n            tv = self.tip_value(n)\n            bv = self.branch_value(n)\n            var = self.branch_variance(n)\n            for dx in [-0.001, 0.001]:\n                y = min(1.0, max(0.0, best_root[\"split\"]+dx))\n                tmpQ = self.propagate_averages(n, tv, bv*y, var*y) \\\n                     + self.propagate_averages(n, tv, bv*(1-y), var*(1-y), outgroup=True)\n                reg = base_regression(tmpQ, slope=slope)\n                deriv.append([y,reg['chisq'], tmpQ[tavgii], tmpQ[davgii]])\n\n            estimator_hessian = np.zeros((3,3))\n            estimator_hessian[:2,:2] = best_root['hessian']\n            estimator_hessian[2,2] = (deriv[0][1] + deriv[1][1] - 2.0*best_root['chisq'])\/(deriv[0][0] - deriv[1][0])**2\n            \n            \n            estimator_hessian[0,2] = estimator_hessian[2,0]\n            estimator_hessian[1,2] = estimator_hessian[2,1]\n            best_root['hessian'] = estimator_hessian\n            best_root['cov'] = np.linalg.inv(estimator_hessian)\n\n        return best_root","method_summary":"determine the position on the tree that minimizes the bilinear product of the inverse covariance and the data vectors.","original_method_code":"def find_best_root(self, force_positive=True, slope=None):\n        \"\"\"\n        determine the position on the tree that minimizes the bilinear\n        product of the inverse covariance and the data vectors.\n\n        Returns\n        -------\n         best_root : (dict)\n            dictionary with the node, the fraction `x` at which the branch\n            is to be split, and the regression parameters\n        \"\"\"\n        self._calculate_averages()\n        best_root = {\"chisq\": np.inf}\n        for n in self.tree.find_clades():\n            if n==self.tree.root:\n                continue\n\n            tv = self.tip_value(n)\n            bv = self.branch_value(n)\n            var = self.branch_variance(n)\n            x, chisq = self._optimal_root_along_branch(n, tv, bv, var, slope=slope)\n\n            if (chisq<best_root[\"chisq\"]):\n                tmpQ = self.propagate_averages(n, tv, bv*x, var*x) \\\n                     + self.propagate_averages(n, tv, bv*(1-x), var*(1-x), outgroup=True)\n                reg = base_regression(tmpQ, slope=slope)\n                if reg[\"slope\"]>=0 or (force_positive==False):\n                    best_root = {\"node\":n, \"split\":x}\n                    best_root.update(reg)\n\n        if 'node' not in best_root:\n            print(\"TreeRegression.find_best_root: No valid root found!\", force_positive)\n            return None\n\n        if 'hessian' in best_root:\n            # calculate differentials with respect to x\n            deriv = []\n            n = best_root[\"node\"]\n            tv = self.tip_value(n)\n            bv = self.branch_value(n)\n            var = self.branch_variance(n)\n            for dx in [-0.001, 0.001]:\n                y = min(1.0, max(0.0, best_root[\"split\"]+dx))\n                tmpQ = self.propagate_averages(n, tv, bv*y, var*y) \\\n                     + self.propagate_averages(n, tv, bv*(1-y), var*(1-y), outgroup=True)\n                reg = base_regression(tmpQ, slope=slope)\n                deriv.append([y,reg['chisq'], tmpQ[tavgii], tmpQ[davgii]])\n\n            estimator_hessian = np.zeros((3,3))\n            estimator_hessian[:2,:2] = best_root['hessian']\n            estimator_hessian[2,2] = (deriv[0][1] + deriv[1][1] - 2.0*best_root['chisq'])\/(deriv[0][0] - deriv[1][0])**2\n            # estimator_hessian[2,0] = (deriv[0][2] - deriv[1][2])\/(deriv[0][0] - deriv[1][0])\n            # estimator_hessian[2,1] = (deriv[0][3] - deriv[1][3])\/(deriv[0][0] - deriv[1][0])\n            estimator_hessian[0,2] = estimator_hessian[2,0]\n            estimator_hessian[1,2] = estimator_hessian[2,1]\n            best_root['hessian'] = estimator_hessian\n            best_root['cov'] = np.linalg.inv(estimator_hessian)\n\n        return best_root","method_path":"https:\/\/github.com\/neherlab\/treetime\/blob\/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0\/treetime\/treeregression.py#L314-L372"}
{"repo_name":"neherlab\/treetime","method_name":"TreeRegression.clock_plot","method_code":"def clock_plot(self, add_internal=False, ax=None, regression=None,\n                   confidence=True, n_sigma = 2, fs=14):\n        \"\"\"\"\"\"\n        import matplotlib.pyplot as plt\n        if ax is None:\n            plt.figure()\n            ax=plt.subplot(111)\n\n        self.tree.root._v=0\n        for n in self.tree.get_nonterminals(order='preorder'):\n            for c in n:\n                c._v = n._v + self.branch_value(c)\n\n        tips = self.tree.get_terminals()\n        internal = self.tree.get_nonterminals()\n\n        \n        xi = np.array([self.tip_value(n) for n in tips])\n        yi = np.array([n._v for n in tips])\n        ind = np.array([n.bad_branch  if hasattr(n, 'bad_branch') else False for n in tips])\n        if add_internal:\n            xi_int = np.array([n.numdate for n in internal])\n            yi_int = np.array([n._v for n in internal])\n            ind_int = np.array([n.bad_branch  if hasattr(n, 'bad_branch') else False  for n in internal])\n\n        if regression:\n            \n            t_mrca = -regression['intercept']\/regression['slope']\n            if add_internal:\n                time_span = np.max(xi_int[~ind_int]) - np.min(xi_int[~ind_int])\n                x_vals = np.array([max(np.min(xi_int[~ind_int]), t_mrca) - 0.1*time_span, np.max(xi[~ind])+0.05*time_span])\n            else:\n                time_span = np.max(xi[~ind]) - np.min(xi[~ind])\n                x_vals = np.array([max(np.min(xi[~ind]), t_mrca) - 0.1*time_span, np.max(xi[~ind]+0.05*time_span)])\n\n            \n            if confidence and 'cov' in regression:\n                x_vals = np.linspace(x_vals[0], x_vals[1], 100)\n                y_vals = regression['slope']*x_vals + regression['intercept']\n                dev = n_sigma*np.array([np.sqrt(regression['cov'][:2,:2].dot(np.array([x, 1])).dot(np.array([x,1]))) for x in x_vals])\n                dev_slope = n_sigma*np.sqrt(regression['cov'][0,0])\n                ax.fill_between(x_vals, y_vals-dev, y_vals+dev, alpha=0.2)\n                dp = np.array([regression['intercept']\/regression['slope']**2,-1.\/regression['slope']])\n                dev_rtt = n_sigma*np.sqrt(regression['cov'][:2,:2].dot(dp).dot(dp))\n\n            else:\n                dev_rtt = None\n                dev_slope = None\n\n            ax.plot(x_vals, regression['slope']*x_vals + regression['intercept'],\n                    label = r\"$y=\\alpha + \\beta t$\"+\"\\n\"+\n                            r\"$\\beta=$%1.2e\"%(regression[\"slope\"])\n                            + (\"+\/- %1.e\"%dev_slope if dev_slope else \"\") +\n                            \"\\nroot date: %1.1f\"%(-regression['intercept']\/regression['slope']) +\n                            (\"+\/- %1.2f\"%dev_rtt if dev_rtt else \"\"))\n\n\n        ax.scatter(xi[~ind], yi[~ind], label=(\"tips\" if add_internal else None))\n        if ind.sum():\n            try:\n                \n                tmp_x = np.array([np.mean(n.raw_date_constraint) if n.raw_date_constraint else None\n                                  for n in self.tree.get_terminals()])\n                ax.scatter(tmp_x[ind], yi[ind], label=\"ignored tips\", c='r')\n            except:\n                pass\n        if add_internal:\n            ax.scatter(xi_int[~ind_int], yi_int[~ind_int], label=\"internal nodes\")\n\n        ax.set_ylabel('root-to-tip distance', fontsize=fs)\n        ax.set_xlabel('date', fontsize=fs)\n        ax.ticklabel_format(useOffset=False)\n        ax.tick_params(labelsize=fs*0.8)\n        ax.set_ylim([0, 1.1*np.max(yi)])\n        plt.tight_layout()\n        plt.legend(fontsize=fs*0.8)","method_summary":"Plot root-to-tip distance vs time as a basic time-tree diagnostic","original_method_code":"def clock_plot(self, add_internal=False, ax=None, regression=None,\n                   confidence=True, n_sigma = 2, fs=14):\n        \"\"\"Plot root-to-tip distance vs time as a basic time-tree diagnostic\n\n        Parameters\n        ----------\n        add_internal : bool, optional\n            add internal nodes. this will only work if the tree has been dated already\n        ax : None, optional\n            an matplotlib axis to plot into. if non provided, a new figure is opened\n        regression : None, optional\n            a dict containing parameters of a root-to-tip vs time regression as\n            returned by the function base_regression\n        confidence : bool, optional\n            add confidence area to the regression line\n        n_sigma : int, optional\n            number of standard deviations for the confidence area.\n        fs : int, optional\n            fontsize\n\n        \"\"\"\n        import matplotlib.pyplot as plt\n        if ax is None:\n            plt.figure()\n            ax=plt.subplot(111)\n\n        self.tree.root._v=0\n        for n in self.tree.get_nonterminals(order='preorder'):\n            for c in n:\n                c._v = n._v + self.branch_value(c)\n\n        tips = self.tree.get_terminals()\n        internal = self.tree.get_nonterminals()\n\n        # get values of terminals\n        xi = np.array([self.tip_value(n) for n in tips])\n        yi = np.array([n._v for n in tips])\n        ind = np.array([n.bad_branch  if hasattr(n, 'bad_branch') else False for n in tips])\n        if add_internal:\n            xi_int = np.array([n.numdate for n in internal])\n            yi_int = np.array([n._v for n in internal])\n            ind_int = np.array([n.bad_branch  if hasattr(n, 'bad_branch') else False  for n in internal])\n\n        if regression:\n            # plot regression line\n            t_mrca = -regression['intercept']\/regression['slope']\n            if add_internal:\n                time_span = np.max(xi_int[~ind_int]) - np.min(xi_int[~ind_int])\n                x_vals = np.array([max(np.min(xi_int[~ind_int]), t_mrca) - 0.1*time_span, np.max(xi[~ind])+0.05*time_span])\n            else:\n                time_span = np.max(xi[~ind]) - np.min(xi[~ind])\n                x_vals = np.array([max(np.min(xi[~ind]), t_mrca) - 0.1*time_span, np.max(xi[~ind]+0.05*time_span)])\n\n            # plot confidence interval\n            if confidence and 'cov' in regression:\n                x_vals = np.linspace(x_vals[0], x_vals[1], 100)\n                y_vals = regression['slope']*x_vals + regression['intercept']\n                dev = n_sigma*np.array([np.sqrt(regression['cov'][:2,:2].dot(np.array([x, 1])).dot(np.array([x,1]))) for x in x_vals])\n                dev_slope = n_sigma*np.sqrt(regression['cov'][0,0])\n                ax.fill_between(x_vals, y_vals-dev, y_vals+dev, alpha=0.2)\n                dp = np.array([regression['intercept']\/regression['slope']**2,-1.\/regression['slope']])\n                dev_rtt = n_sigma*np.sqrt(regression['cov'][:2,:2].dot(dp).dot(dp))\n\n            else:\n                dev_rtt = None\n                dev_slope = None\n\n            ax.plot(x_vals, regression['slope']*x_vals + regression['intercept'],\n                    label = r\"$y=\\alpha + \\beta t$\"+\"\\n\"+\n                            r\"$\\beta=$%1.2e\"%(regression[\"slope\"])\n                            + (\"+\/- %1.e\"%dev_slope if dev_slope else \"\") +\n                            \"\\nroot date: %1.1f\"%(-regression['intercept']\/regression['slope']) +\n                            (\"+\/- %1.2f\"%dev_rtt if dev_rtt else \"\"))\n\n\n        ax.scatter(xi[~ind], yi[~ind], label=(\"tips\" if add_internal else None))\n        if ind.sum():\n            try:\n                # note: this is treetime specific\n                tmp_x = np.array([np.mean(n.raw_date_constraint) if n.raw_date_constraint else None\n                                  for n in self.tree.get_terminals()])\n                ax.scatter(tmp_x[ind], yi[ind], label=\"ignored tips\", c='r')\n            except:\n                pass\n        if add_internal:\n            ax.scatter(xi_int[~ind_int], yi_int[~ind_int], label=\"internal nodes\")\n\n        ax.set_ylabel('root-to-tip distance', fontsize=fs)\n        ax.set_xlabel('date', fontsize=fs)\n        ax.ticklabel_format(useOffset=False)\n        ax.tick_params(labelsize=fs*0.8)\n        ax.set_ylim([0, 1.1*np.max(yi)])\n        plt.tight_layout()\n        plt.legend(fontsize=fs*0.8)","method_path":"https:\/\/github.com\/neherlab\/treetime\/blob\/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0\/treetime\/treeregression.py#L457-L550"}
{"repo_name":"Clinical-Genomics\/scout","method_name":"panel","method_code":"def panel(context, panel, build, bed, version):\n    \"\"\"\"\"\"\n    LOG.info(\"Running scout export panel\")\n    adapter = context.obj['adapter']\n    \n    chromosomes_found = set()\n    \n    if not panel:\n        LOG.warning(\"Please provide at least one gene panel\")\n        context.abort()\n\n    LOG.info(\"Exporting panels: {}\".format(', '.join(panel)))\n    if bed:\n        if version:\n            version = [version]\n        lines = export_panels(\n            adapter=adapter, \n            panels=panel, \n            versions=version, \n            build=build,\n        )\n    else:\n        lines = export_gene_panels(\n            adapter=adapter, \n            panels=panel, \n            version=version,\n            )\n    for line in lines:\n        click.echo(line)","method_summary":"Export gene panels to .bed like format. Specify any number of panels on the command line","original_method_code":"def panel(context, panel, build, bed, version):\n    \"\"\"Export gene panels to .bed like format.\n    \n        Specify any number of panels on the command line\n    \"\"\"\n    LOG.info(\"Running scout export panel\")\n    adapter = context.obj['adapter']\n    # Save all chromosomes found in the collection if panels\n    chromosomes_found = set()\n    \n    if not panel:\n        LOG.warning(\"Please provide at least one gene panel\")\n        context.abort()\n\n    LOG.info(\"Exporting panels: {}\".format(', '.join(panel)))\n    if bed:\n        if version:\n            version = [version]\n        lines = export_panels(\n            adapter=adapter, \n            panels=panel, \n            versions=version, \n            build=build,\n        )\n    else:\n        lines = export_gene_panels(\n            adapter=adapter, \n            panels=panel, \n            version=version,\n            )\n    for line in lines:\n        click.echo(line)","method_path":"https:\/\/github.com\/Clinical-Genomics\/scout\/blob\/90a551e2e1653a319e654c2405c2866f93d0ebb9\/scout\/commands\/export\/panel.py#L26-L57"}
{"repo_name":"Clinical-Genomics\/scout","method_name":"load_panel","method_code":"def load_panel(panel_path, adapter, date=None, display_name=None, version=None, panel_type=None, \n               panel_id=None, institute=None):\n    \"\"\"\"\"\"\n    panel_lines = get_file_handle(panel_path)\n\n    try:\n        \n        panel_info = get_panel_info(\n            panel_lines=panel_lines,\n            panel_id=panel_id,\n            institute=institute,\n            version=version,\n            date=date,\n            display_name=display_name\n            )\n    except Exception as err:\n        raise err\n\n    version = None\n    if panel_info.get('version'):\n        version = float(panel_info['version'])\n\n    panel_id = panel_info['panel_id']\n    display_name = panel_info['display_name'] or panel_id\n    institute = panel_info['institute']\n    date = panel_info['date']\n\n    if not institute:\n        raise SyntaxError(\"A Panel has to belong to a institute\")\n\n    \n    if not adapter.institute(institute):\n        raise SyntaxError(\"Institute {0} does not exist in database\".format(institute))\n\n    if not panel_id:\n        raise SyntaxError(\"A Panel has to have a panel id\")\n    \n    if version:\n        existing_panel = adapter.gene_panel(panel_id, version)\n    else:\n        \n        existing_panel = adapter.gene_panel(panel_id)\n        version = 1.0\n        LOG.info(\"Set version to %s\", version)\n\n    if existing_panel:\n        LOG.info(\"found existing panel\")\n        if version == existing_panel['version']:\n            LOG.warning(\"Panel with same version exists in database\")\n            LOG.info(\"Reload with updated version\")\n            raise SyntaxError()\n        display_name = display_name or existing_panel['display_name']\n        institute = institute or existing_panel['institute']\n    \n    parsed_panel = parse_gene_panel(\n        path=panel_path,\n        institute=institute,\n        panel_type=panel_type,\n        date=date,\n        version=version,\n        panel_id=panel_id,\n        display_name=display_name,\n    )\n    \n    try:\n        adapter.load_panel(parsed_panel=parsed_panel)\n    except Exception as err:\n        raise err","method_summary":"Load a manually curated gene panel into scout","original_method_code":"def load_panel(panel_path, adapter, date=None, display_name=None, version=None, panel_type=None, \n               panel_id=None, institute=None):\n    \"\"\"Load a manually curated gene panel into scout\n    \n    Args:\n        panel_path(str): path to gene panel file\n        adapter(scout.adapter.MongoAdapter)\n        date(str): date of gene panel on format 2017-12-24\n        display_name(str)\n        version(float)\n        panel_type(str)\n        panel_id(str)\n        institute(str)\n    \n    \"\"\"\n    panel_lines = get_file_handle(panel_path)\n\n    try:\n        # This will parse panel metadata if includeed in panel file\n        panel_info = get_panel_info(\n            panel_lines=panel_lines,\n            panel_id=panel_id,\n            institute=institute,\n            version=version,\n            date=date,\n            display_name=display_name\n            )\n    except Exception as err:\n        raise err\n\n    version = None\n    if panel_info.get('version'):\n        version = float(panel_info['version'])\n\n    panel_id = panel_info['panel_id']\n    display_name = panel_info['display_name'] or panel_id\n    institute = panel_info['institute']\n    date = panel_info['date']\n\n    if not institute:\n        raise SyntaxError(\"A Panel has to belong to a institute\")\n\n    #Check if institute exists in database\n    if not adapter.institute(institute):\n        raise SyntaxError(\"Institute {0} does not exist in database\".format(institute))\n\n    if not panel_id:\n        raise SyntaxError(\"A Panel has to have a panel id\")\n    \n    if version:\n        existing_panel = adapter.gene_panel(panel_id, version)\n    else:\n        ## Assuming version 1.0\n        existing_panel = adapter.gene_panel(panel_id)\n        version = 1.0\n        LOG.info(\"Set version to %s\", version)\n\n    if existing_panel:\n        LOG.info(\"found existing panel\")\n        if version == existing_panel['version']:\n            LOG.warning(\"Panel with same version exists in database\")\n            LOG.info(\"Reload with updated version\")\n            raise SyntaxError()\n        display_name = display_name or existing_panel['display_name']\n        institute = institute or existing_panel['institute']\n    \n    parsed_panel = parse_gene_panel(\n        path=panel_path,\n        institute=institute,\n        panel_type=panel_type,\n        date=date,\n        version=version,\n        panel_id=panel_id,\n        display_name=display_name,\n    )\n    \n    try:\n        adapter.load_panel(parsed_panel=parsed_panel)\n    except Exception as err:\n        raise err","method_path":"https:\/\/github.com\/Clinical-Genomics\/scout\/blob\/90a551e2e1653a319e654c2405c2866f93d0ebb9\/scout\/load\/panel.py#L19-L98"}
{"repo_name":"Clinical-Genomics\/scout","method_name":"load_panel_app","method_code":"def load_panel_app(adapter, panel_id=None, institute='cust000'):\n    \"\"\"\"\"\"\n    base_url = 'https:\/\/panelapp.genomicsengland.co.uk\/WebServices\/{0}\/'\n    \n    hgnc_map = adapter.genes_by_alias()\n    \n    if panel_id:\n        panel_ids = [panel_id]\n\n    if not panel_id:\n        \n        LOG.info(\"Fetching all panel app panels\")\n        data = get_request(base_url.format('list_panels'))\n    \n        json_lines = json.loads(data)\n        \n        panel_ids = [panel_info['Panel_Id'] for panel_info in json_lines['result']]\n    \n    for panel_id in panel_ids:\n        panel_data = get_request(base_url.format('get_panel') + panel_id)\n        \n        parsed_panel = parse_panel_app_panel(\n            panel_info = json.loads(panel_data)['result'], \n            hgnc_map=hgnc_map,\n            institute=institute\n        )\n        parsed_panel['panel_id'] = panel_id\n        \n        if len(parsed_panel['genes']) == 0:\n            LOG.warning(\"Panel {} is missing genes. Skipping.\".format(parsed_panel['display_name']))\n            continue\n        \n        try:\n            adapter.load_panel(parsed_panel=parsed_panel)\n        except Exception as err:\n            raise err","method_summary":"Load PanelApp panels into scout database If no panel_id load all PanelApp panels","original_method_code":"def load_panel_app(adapter, panel_id=None, institute='cust000'):\n    \"\"\"Load PanelApp panels into scout database\n    \n    If no panel_id load all PanelApp panels \n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        panel_id(str): The panel app panel id\n    \"\"\"\n    base_url = 'https:\/\/panelapp.genomicsengland.co.uk\/WebServices\/{0}\/'\n    \n    hgnc_map = adapter.genes_by_alias()\n    \n    if panel_id:\n        panel_ids = [panel_id]\n\n    if not panel_id:\n        \n        LOG.info(\"Fetching all panel app panels\")\n        data = get_request(base_url.format('list_panels'))\n    \n        json_lines = json.loads(data)\n        \n        panel_ids = [panel_info['Panel_Id'] for panel_info in json_lines['result']]\n    \n    for panel_id in panel_ids:\n        panel_data = get_request(base_url.format('get_panel') + panel_id)\n        \n        parsed_panel = parse_panel_app_panel(\n            panel_info = json.loads(panel_data)['result'], \n            hgnc_map=hgnc_map,\n            institute=institute\n        )\n        parsed_panel['panel_id'] = panel_id\n        \n        if len(parsed_panel['genes']) == 0:\n            LOG.warning(\"Panel {} is missing genes. Skipping.\".format(parsed_panel['display_name']))\n            continue\n        \n        try:\n            adapter.load_panel(parsed_panel=parsed_panel)\n        except Exception as err:\n            raise err","method_path":"https:\/\/github.com\/Clinical-Genomics\/scout\/blob\/90a551e2e1653a319e654c2405c2866f93d0ebb9\/scout\/load\/panel.py#L100-L142"}
{"repo_name":"Clinical-Genomics\/scout","method_name":"export_variants","method_code":"def export_variants(adapter, collaborator, document_id=None, case_id=None):\n    \"\"\"\"\"\"\n\n    \n    variants = []\n    if document_id:\n        yield adapter.variant(document_id)\n        return\n\n    variant_ids = adapter.get_causatives(\n        institute_id=collaborator,\n        case_id=case_id\n        )\n    \n    for document_id in variant_ids:\n\n        variant_obj = adapter.variant(document_id)\n        chrom = variant_obj['chromosome']\n        \n        chrom_int = CHROMOSOME_INTEGERS.get(chrom)\n        if not chrom_int:\n            LOG.info(\"Unknown chromosome %s\", chrom)\n            continue\n\n        \n        variants.append((chrom_int, variant_obj['position'], variant_obj))\n\n    \n    variants.sort(key=lambda x: (x[0], x[1]))\n\n    for variant in variants:\n        variant_obj = variant[2]\n        yield variant_obj","method_summary":"Export causative variants for a collaborator","original_method_code":"def export_variants(adapter, collaborator, document_id=None, case_id=None):\n    \"\"\"Export causative variants for a collaborator\n\n    Args:\n        adapter(MongoAdapter)\n        collaborator(str)\n        document_id(str): Search for a specific variant\n        case_id(str): Search causative variants for a case\n\n    Yields:\n        variant_obj(scout.Models.Variant): Variants marked as causative ordered by position.\n    \"\"\"\n\n    # Store the variants in a list for sorting\n    variants = []\n    if document_id:\n        yield adapter.variant(document_id)\n        return\n\n    variant_ids = adapter.get_causatives(\n        institute_id=collaborator,\n        case_id=case_id\n        )\n    ##TODO add check so that same variant is not included more than once\n    for document_id in variant_ids:\n\n        variant_obj = adapter.variant(document_id)\n        chrom = variant_obj['chromosome']\n        # Convert chromosome to integer for sorting\n        chrom_int = CHROMOSOME_INTEGERS.get(chrom)\n        if not chrom_int:\n            LOG.info(\"Unknown chromosome %s\", chrom)\n            continue\n\n        # Add chromosome and position to prepare for sorting\n        variants.append((chrom_int, variant_obj['position'], variant_obj))\n\n    # Sort varants based on position\n    variants.sort(key=lambda x: (x[0], x[1]))\n\n    for variant in variants:\n        variant_obj = variant[2]\n        yield variant_obj","method_path":"https:\/\/github.com\/Clinical-Genomics\/scout\/blob\/90a551e2e1653a319e654c2405c2866f93d0ebb9\/scout\/export\/variant.py#L9-L51"}
{"repo_name":"Clinical-Genomics\/scout","method_name":"export_verified_variants","method_code":"def export_verified_variants(aggregate_variants, unique_callers):\n    \"\"\"\"\"\"\n    document_lines = []\n    for variant in aggregate_variants:\n        \n        samples = []\n        for sample in variant['samples']:\n            line = [] \n            line.append(variant['institute'])\n            line.append(variant['_id']) \n            line.append(variant['category'])\n            line.append(variant['variant_type'])\n            line.append(variant['display_name'][:30]) \n            \n            case_name = variant['case_obj']['display_name']  \n            local_link = '\/'.join([ '', variant['institute'], case_name, variant['_id'] ])\n            line.append(local_link)\n            line.append(variant.get('validation'))\n            line.append(case_name)\n            case_individual = next(ind for ind in variant['case_obj']['individuals'] if ind['individual_id'] == sample['sample_id'])\n            if case_individual['phenotype'] == 2:\n                line.append(' '.join([sample.get('display_name'),'(A)'])) \n            else:\n                line.append(sample.get('display_name'))\n            line.append(''.join(['chr',variant['chromosome'],':',str(variant['position'])])) \n            line.append('>'.join([variant.get('reference')[:10],variant.get('alternative')[:10]])) \n            genes = []\n            prot_effect = []\n            funct_anno = []\n            for gene in variant.get('genes'): \n                genes.append(gene.get('hgnc_symbol',''))\n                funct_anno.append(gene.get('functional_annotation'))\n                for transcript in gene.get('transcripts'):\n                    if transcript.get('is_canonical') and transcript.get('protein_sequence_name'):\n                        prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name')))\n            line.append(','.join(prot_effect))\n            line.append(','.join(funct_anno))\n            line.append(','.join(genes))\n            line.append(variant.get('rank_score'))\n            line.append(variant.get('cadd_score'))\n            line.append(sample.get('genotype_call'))\n            line.append(sample['allele_depths'][0])\n            line.append(sample['allele_depths'][1])\n            line.append(sample['genotype_quality'])\n\n            \n            for caller in unique_callers:\n                if variant.get(caller):\n                    line.append(variant.get(caller))\n                else:\n                    line.append('-')\n            document_lines.append(line)\n    return document_lines","method_summary":"Create the lines for an excel file with verified variants for an institute","original_method_code":"def export_verified_variants(aggregate_variants, unique_callers):\n    \"\"\"Create the lines for an excel file with verified variants for\n        an institute\n\n        Args:\n            aggregate_variants(list): a list of variants with aggregates case data\n            unique_callers(set): a unique list of available callers\n\n        Returns:\n            document_lines(list): list of lines to include in the document\n    \"\"\"\n    document_lines = []\n    for variant in aggregate_variants:\n        # get genotype and allele depth for each sample\n        samples = []\n        for sample in variant['samples']:\n            line = [] # line elements corespond to contants.variants_export.VERIFIED_VARIANTS_HEADER\n            line.append(variant['institute'])\n            line.append(variant['_id']) # variant database ID\n            line.append(variant['category'])\n            line.append(variant['variant_type'])\n            line.append(variant['display_name'][:30]) # variant display name\n            # Build local link to variant:\n            case_name = variant['case_obj']['display_name']  # case display name\n            local_link = '\/'.join([ '', variant['institute'], case_name, variant['_id'] ])\n            line.append(local_link)\n            line.append(variant.get('validation'))\n            line.append(case_name)\n            case_individual = next(ind for ind in variant['case_obj']['individuals'] if ind['individual_id'] == sample['sample_id'])\n            if case_individual['phenotype'] == 2:\n                line.append(' '.join([sample.get('display_name'),'(A)'])) # label sample as affected\n            else:\n                line.append(sample.get('display_name'))\n            line.append(''.join(['chr',variant['chromosome'],':',str(variant['position'])])) # position\n            line.append('>'.join([variant.get('reference')[:10],variant.get('alternative')[:10]])) # change\n            genes = []\n            prot_effect = []\n            funct_anno = []\n            for gene in variant.get('genes'): # this will be a unique long field in the document\n                genes.append(gene.get('hgnc_symbol',''))\n                funct_anno.append(gene.get('functional_annotation'))\n                for transcript in gene.get('transcripts'):\n                    if transcript.get('is_canonical') and transcript.get('protein_sequence_name'):\n                        prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name')))\n            line.append(','.join(prot_effect))\n            line.append(','.join(funct_anno))\n            line.append(','.join(genes))\n            line.append(variant.get('rank_score'))\n            line.append(variant.get('cadd_score'))\n            line.append(sample.get('genotype_call'))\n            line.append(sample['allele_depths'][0])\n            line.append(sample['allele_depths'][1])\n            line.append(sample['genotype_quality'])\n\n            # Set callers values. One cell per caller, leave blank if not applicable\n            for caller in unique_callers:\n                if variant.get(caller):\n                    line.append(variant.get(caller))\n                else:\n                    line.append('-')\n            document_lines.append(line)\n    return document_lines","method_path":"https:\/\/github.com\/Clinical-Genomics\/scout\/blob\/90a551e2e1653a319e654c2405c2866f93d0ebb9\/scout\/export\/variant.py#L54-L115"}
{"repo_name":"Clinical-Genomics\/scout","method_name":"export_mt_variants","method_code":"def export_mt_variants(variants, sample_id):\n    \"\"\"\"\"\"\n    document_lines = []\n    for variant in variants:\n        line = []\n        position = variant.get('position')\n        change = '>'.join([variant.get('reference'),variant.get('alternative')])\n        line.append(position)\n        line.append(change)\n        line.append(str(position)+change)\n        genes = []\n        prot_effect = []\n        for gene in variant.get('genes'):\n            genes.append(gene.get('hgnc_symbol',''))\n            for transcript in gene.get('transcripts'):\n                if transcript.get('is_canonical') and transcript.get('protein_sequence_name'):\n                    prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name')))\n        line.append(','.join(prot_effect))\n        line.append(','.join(genes))\n        ref_ad = ''\n        alt_ad = ''\n        for sample in variant['samples']:\n            if sample.get('sample_id') == sample_id:\n                ref_ad = sample['allele_depths'][0]\n                alt_ad = sample['allele_depths'][1]\n        line.append(ref_ad)\n        line.append(alt_ad)\n        document_lines.append(line)\n    return document_lines","method_summary":"Export mitochondrial variants for a case to create a MT excel report","original_method_code":"def export_mt_variants(variants, sample_id):\n    \"\"\"Export mitochondrial variants for a case to create a MT excel report\n\n    Args:\n        variants(list): all MT variants for a case, sorted by position\n        sample_id(str) : the id of a sample within the case\n\n    Returns:\n        document_lines(list): list of lines to include in the document\n    \"\"\"\n    document_lines = []\n    for variant in variants:\n        line = []\n        position = variant.get('position')\n        change = '>'.join([variant.get('reference'),variant.get('alternative')])\n        line.append(position)\n        line.append(change)\n        line.append(str(position)+change)\n        genes = []\n        prot_effect = []\n        for gene in variant.get('genes'):\n            genes.append(gene.get('hgnc_symbol',''))\n            for transcript in gene.get('transcripts'):\n                if transcript.get('is_canonical') and transcript.get('protein_sequence_name'):\n                    prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name')))\n        line.append(','.join(prot_effect))\n        line.append(','.join(genes))\n        ref_ad = ''\n        alt_ad = ''\n        for sample in variant['samples']:\n            if sample.get('sample_id') == sample_id:\n                ref_ad = sample['allele_depths'][0]\n                alt_ad = sample['allele_depths'][1]\n        line.append(ref_ad)\n        line.append(alt_ad)\n        document_lines.append(line)\n    return document_lines","method_path":"https:\/\/github.com\/Clinical-Genomics\/scout\/blob\/90a551e2e1653a319e654c2405c2866f93d0ebb9\/scout\/export\/variant.py#L118-L154"}
{"repo_name":"Clinical-Genomics\/scout","method_name":"user","method_code":"def user(context, user_id, update_role, add_institute, remove_admin, remove_institute):\n    \"\"\"\"\"\"\n    adapter = context.obj['adapter']\n\n    user_obj = adapter.user(user_id)\n\n    if not user_obj:\n        LOG.warning(\"User %s could not be found\", user_id)\n        context.abort()\n\n    existing_roles = set(user_obj.get('roles',[]))\n    if update_role:\n        if not update_role in user_obj['roles']:\n            existing_roles = set(user_obj['roles'])\n            existing_roles.add(update_role)\n            LOG.info(\"Adding role %s to user\", update_role)\n        else:\n            LOG.warning(\"User already have role %s\", update_role)\n\n    if remove_admin:\n        try:\n            existing_roles.remove('admin')\n            LOG.info(\"Removing admin rights from user %s\", user_id)\n        except KeyError as err:\n            LOG.info(\"User %s does not have admin rights\", user_id)\n\n    user_obj['roles'] = list(existing_roles)\n\n    existing_institutes = set(user_obj.get('institutes',[]))\n    for institute_id in add_institute:\n        institute_obj = adapter.institute(institute_id)\n        if not institute_obj:\n            LOG.warning(\"Institute %s could not be found\", institute_id)\n        else:\n            existing_institutes.add(institute_id)\n            LOG.info(\"Adding institute %s to user\", institute_id)\n\n    for institute_id in remove_institute:\n        try:\n            existing_institutes.remove(institute_id)\n            LOG.info(\"Removing institute %s from user\", institute_id)\n        except KeyError as err:\n            LOG.info(\"User does not have access to institute %s\", institute_id)\n\n    user_obj['institutes'] = list(existing_institutes)\n\n    updated_user = adapter.update_user(user_obj)","method_summary":"Update a user in the database","original_method_code":"def user(context, user_id, update_role, add_institute, remove_admin, remove_institute):\n    \"\"\"\n    Update a user in the database\n    \"\"\"\n    adapter = context.obj['adapter']\n\n    user_obj = adapter.user(user_id)\n\n    if not user_obj:\n        LOG.warning(\"User %s could not be found\", user_id)\n        context.abort()\n\n    existing_roles = set(user_obj.get('roles',[]))\n    if update_role:\n        if not update_role in user_obj['roles']:\n            existing_roles = set(user_obj['roles'])\n            existing_roles.add(update_role)\n            LOG.info(\"Adding role %s to user\", update_role)\n        else:\n            LOG.warning(\"User already have role %s\", update_role)\n\n    if remove_admin:\n        try:\n            existing_roles.remove('admin')\n            LOG.info(\"Removing admin rights from user %s\", user_id)\n        except KeyError as err:\n            LOG.info(\"User %s does not have admin rights\", user_id)\n\n    user_obj['roles'] = list(existing_roles)\n\n    existing_institutes = set(user_obj.get('institutes',[]))\n    for institute_id in add_institute:\n        institute_obj = adapter.institute(institute_id)\n        if not institute_obj:\n            LOG.warning(\"Institute %s could not be found\", institute_id)\n        else:\n            existing_institutes.add(institute_id)\n            LOG.info(\"Adding institute %s to user\", institute_id)\n\n    for institute_id in remove_institute:\n        try:\n            existing_institutes.remove(institute_id)\n            LOG.info(\"Removing institute %s from user\", institute_id)\n        except KeyError as err:\n            LOG.info(\"User does not have access to institute %s\", institute_id)\n\n    user_obj['institutes'] = list(existing_institutes)\n\n    updated_user = adapter.update_user(user_obj)","method_path":"https:\/\/github.com\/Clinical-Genomics\/scout\/blob\/90a551e2e1653a319e654c2405c2866f93d0ebb9\/scout\/commands\/update\/user.py#L31-L79"}
{"repo_name":"Clinical-Genomics\/scout","method_name":"variants","method_code":"def variants(institute_id, case_name):\n    \"\"\"\"\"\"\n    page = int(request.form.get('page', 1))\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_type = request.args.get('variant_type', 'clinical')\n\n    \n\n    default_panels = []\n    for panel in case_obj['panels']:\n        if panel.get('is_default'):\n            default_panels.append(panel['panel_name'])\n\n    request.form.get('gene_panels')\n    if bool(request.form.get('clinical_filter')):\n        clinical_filter = MultiDict({\n            'variant_type': 'clinical',\n            'region_annotations': ['exonic','splicing'],\n            'functional_annotations': SEVERE_SO_TERMS,\n            'clinsig': [4,5],\n            'clinsig_confident_always_returned': True,\n            'gnomad_frequency': str(institute_obj['frequency_cutoff']),\n            'variant_type': 'clinical',\n            'gene_panels': default_panels\n             })\n\n    if(request.method == \"POST\"):\n        if bool(request.form.get('clinical_filter')):\n            form = FiltersForm(clinical_filter)\n            form.csrf_token = request.args.get('csrf_token')\n        else:\n            form = FiltersForm(request.form)\n    else:\n        form = FiltersForm(request.args)\n\n    \n    available_panels = case_obj.get('panels', []) + [\n        {'panel_name': 'hpo', 'display_name': 'HPO'}]\n\n    panel_choices = [(panel['panel_name'], panel['display_name'])\n                     for panel in available_panels]\n\n    form.gene_panels.choices = panel_choices\n\n    \n    if (request.files):\n        file = request.files[form.symbol_file.name]\n\n    if request.files and file and file.filename != '':\n        log.debug(\"Upload file request files: {0}\".format(request.files.to_dict()))\n        try:\n            stream = io.StringIO(file.stream.read().decode('utf-8'), newline=None)\n        except UnicodeDecodeError as error:\n            flash(\"Only text files are supported!\", 'warning')\n            return redirect(request.referrer)\n\n        hgnc_symbols_set = set(form.hgnc_symbols.data)\n        log.debug(\"Symbols prior to upload: {0}\".format(hgnc_symbols_set))\n        new_hgnc_symbols = controllers.upload_panel(store, institute_id, case_name, stream)\n        hgnc_symbols_set.update(new_hgnc_symbols)\n        form.hgnc_symbols.data = hgnc_symbols_set\n        \n        form.gene_panels.data = ''\n\n    \n    if case_obj['status'] == 'inactive' and not current_user.is_admin:\n        flash('You just activated this case!', 'info')\n        user_obj = store.user(current_user.email)\n        case_link = url_for('cases.case', institute_id=institute_obj['_id'],\n                            case_name=case_obj['display_name'])\n        store.update_status(institute_obj, case_obj, user_obj, 'active', case_link)\n\n    \n    hgnc_symbols = []\n    non_clinical_symbols = []\n    not_found_symbols = []\n    not_found_ids = []\n    if (form.hgnc_symbols.data) and len(form.hgnc_symbols.data) > 0:\n        is_clinical = form.data.get('variant_type', 'clinical') == 'clinical'\n        clinical_symbols = store.clinical_symbols(case_obj) if is_clinical else None\n        for hgnc_symbol in form.hgnc_symbols.data:\n            if hgnc_symbol.isdigit():\n                hgnc_gene = store.hgnc_gene(int(hgnc_symbol))\n                if hgnc_gene is None:\n                    not_found_ids.append(hgnc_symbol)\n                else:\n                    hgnc_symbols.append(hgnc_gene['hgnc_symbol'])\n            elif store.hgnc_genes(hgnc_symbol).count() == 0:\n                  not_found_symbols.append(hgnc_symbol)\n            elif is_clinical and (hgnc_symbol not in clinical_symbols):\n                 non_clinical_symbols.append(hgnc_symbol)\n            else:\n                hgnc_symbols.append(hgnc_symbol)\n\n    if (not_found_ids):\n        flash(\"HGNC id not found: {}\".format(\", \".join(not_found_ids)), 'warning')\n    if (not_found_symbols):\n        flash(\"HGNC symbol not found: {}\".format(\", \".join(not_found_symbols)), 'warning')\n    if (non_clinical_symbols):\n        flash(\"Gene not included in clinical list: {}\".format(\", \".join(non_clinical_symbols)), 'warning')\n    form.hgnc_symbols.data = hgnc_symbols\n\n    \n    if form.data['gene_panels'] == ['hpo']:\n        hpo_symbols = list(set(term_obj['hgnc_symbol'] for term_obj in\n                               case_obj['dynamic_gene_list']))\n        form.hgnc_symbols.data = hpo_symbols\n\n    variants_query = store.variants(case_obj['_id'], query=form.data)\n    data = {}\n\n    if request.form.get('export'):\n        document_header = controllers.variants_export_header(case_obj)\n        export_lines = []\n        if form.data['chrom'] == 'MT':\n            \n            export_lines = controllers.variant_export_lines(store, case_obj, variants_query)\n        else:\n            \n            export_lines = controllers.variant_export_lines(store, case_obj, variants_query.limit(500))\n\n        def generate(header, lines):\n            yield header + '\\n'\n            for line in lines:\n                yield line + '\\n'\n\n        headers = Headers()\n        headers.add('Content-Disposition','attachment', filename=str(case_obj['display_name'])+'-filtered_variants.csv')\n\n        \n        return Response(generate(\",\".join(document_header), export_lines), mimetype='text\/csv',\n                        headers=headers)\n\n    data = controllers.variants(store, institute_obj, case_obj, variants_query, page)\n\n    return dict(institute=institute_obj, case=case_obj, form=form,\n                    severe_so_terms=SEVERE_SO_TERMS, page=page, **data)","method_summary":"Display a list of SNV variants.","original_method_code":"def variants(institute_id, case_name):\n    \"\"\"Display a list of SNV variants.\"\"\"\n    page = int(request.form.get('page', 1))\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_type = request.args.get('variant_type', 'clinical')\n\n    # Update filter settings if Clinical Filter was requested\n\n    default_panels = []\n    for panel in case_obj['panels']:\n        if panel.get('is_default'):\n            default_panels.append(panel['panel_name'])\n\n    request.form.get('gene_panels')\n    if bool(request.form.get('clinical_filter')):\n        clinical_filter = MultiDict({\n            'variant_type': 'clinical',\n            'region_annotations': ['exonic','splicing'],\n            'functional_annotations': SEVERE_SO_TERMS,\n            'clinsig': [4,5],\n            'clinsig_confident_always_returned': True,\n            'gnomad_frequency': str(institute_obj['frequency_cutoff']),\n            'variant_type': 'clinical',\n            'gene_panels': default_panels\n             })\n\n    if(request.method == \"POST\"):\n        if bool(request.form.get('clinical_filter')):\n            form = FiltersForm(clinical_filter)\n            form.csrf_token = request.args.get('csrf_token')\n        else:\n            form = FiltersForm(request.form)\n    else:\n        form = FiltersForm(request.args)\n\n    # populate available panel choices\n    available_panels = case_obj.get('panels', []) + [\n        {'panel_name': 'hpo', 'display_name': 'HPO'}]\n\n    panel_choices = [(panel['panel_name'], panel['display_name'])\n                     for panel in available_panels]\n\n    form.gene_panels.choices = panel_choices\n\n    # upload gene panel if symbol file exists\n    if (request.files):\n        file = request.files[form.symbol_file.name]\n\n    if request.files and file and file.filename != '':\n        log.debug(\"Upload file request files: {0}\".format(request.files.to_dict()))\n        try:\n            stream = io.StringIO(file.stream.read().decode('utf-8'), newline=None)\n        except UnicodeDecodeError as error:\n            flash(\"Only text files are supported!\", 'warning')\n            return redirect(request.referrer)\n\n        hgnc_symbols_set = set(form.hgnc_symbols.data)\n        log.debug(\"Symbols prior to upload: {0}\".format(hgnc_symbols_set))\n        new_hgnc_symbols = controllers.upload_panel(store, institute_id, case_name, stream)\n        hgnc_symbols_set.update(new_hgnc_symbols)\n        form.hgnc_symbols.data = hgnc_symbols_set\n        # reset gene panels\n        form.gene_panels.data = ''\n\n    # update status of case if vistited for the first time\n    if case_obj['status'] == 'inactive' and not current_user.is_admin:\n        flash('You just activated this case!', 'info')\n        user_obj = store.user(current_user.email)\n        case_link = url_for('cases.case', institute_id=institute_obj['_id'],\n                            case_name=case_obj['display_name'])\n        store.update_status(institute_obj, case_obj, user_obj, 'active', case_link)\n\n    # check if supplied gene symbols exist\n    hgnc_symbols = []\n    non_clinical_symbols = []\n    not_found_symbols = []\n    not_found_ids = []\n    if (form.hgnc_symbols.data) and len(form.hgnc_symbols.data) > 0:\n        is_clinical = form.data.get('variant_type', 'clinical') == 'clinical'\n        clinical_symbols = store.clinical_symbols(case_obj) if is_clinical else None\n        for hgnc_symbol in form.hgnc_symbols.data:\n            if hgnc_symbol.isdigit():\n                hgnc_gene = store.hgnc_gene(int(hgnc_symbol))\n                if hgnc_gene is None:\n                    not_found_ids.append(hgnc_symbol)\n                else:\n                    hgnc_symbols.append(hgnc_gene['hgnc_symbol'])\n            elif store.hgnc_genes(hgnc_symbol).count() == 0:\n                  not_found_symbols.append(hgnc_symbol)\n            elif is_clinical and (hgnc_symbol not in clinical_symbols):\n                 non_clinical_symbols.append(hgnc_symbol)\n            else:\n                hgnc_symbols.append(hgnc_symbol)\n\n    if (not_found_ids):\n        flash(\"HGNC id not found: {}\".format(\", \".join(not_found_ids)), 'warning')\n    if (not_found_symbols):\n        flash(\"HGNC symbol not found: {}\".format(\", \".join(not_found_symbols)), 'warning')\n    if (non_clinical_symbols):\n        flash(\"Gene not included in clinical list: {}\".format(\", \".join(non_clinical_symbols)), 'warning')\n    form.hgnc_symbols.data = hgnc_symbols\n\n    # handle HPO gene list separately\n    if form.data['gene_panels'] == ['hpo']:\n        hpo_symbols = list(set(term_obj['hgnc_symbol'] for term_obj in\n                               case_obj['dynamic_gene_list']))\n        form.hgnc_symbols.data = hpo_symbols\n\n    variants_query = store.variants(case_obj['_id'], query=form.data)\n    data = {}\n\n    if request.form.get('export'):\n        document_header = controllers.variants_export_header(case_obj)\n        export_lines = []\n        if form.data['chrom'] == 'MT':\n            # Return all MT variants\n            export_lines = controllers.variant_export_lines(store, case_obj, variants_query)\n        else:\n            # Return max 500 variants\n            export_lines = controllers.variant_export_lines(store, case_obj, variants_query.limit(500))\n\n        def generate(header, lines):\n            yield header + '\\n'\n            for line in lines:\n                yield line + '\\n'\n\n        headers = Headers()\n        headers.add('Content-Disposition','attachment', filename=str(case_obj['display_name'])+'-filtered_variants.csv')\n\n        # return a csv with the exported variants\n        return Response(generate(\",\".join(document_header), export_lines), mimetype='text\/csv',\n                        headers=headers)\n\n    data = controllers.variants(store, institute_obj, case_obj, variants_query, page)\n\n    return dict(institute=institute_obj, case=case_obj, form=form,\n                    severe_so_terms=SEVERE_SO_TERMS, page=page, **data)","method_path":"https:\/\/github.com\/Clinical-Genomics\/scout\/blob\/90a551e2e1653a319e654c2405c2866f93d0ebb9\/scout\/server\/blueprints\/variants\/views.py#L30-L167"}
{"repo_name":"Clinical-Genomics\/scout","method_name":"variant","method_code":"def variant(institute_id, case_name, variant_id):\n    \"\"\"\"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    log.debug(\"Variants view requesting data for variant {}\".format(variant_id))\n    data = controllers.variant(store, institute_obj, case_obj, variant_id=variant_id)\n    if data is None:\n        log.warning(\"An error occurred: variants view requesting data for variant {}\".format(variant_id))\n        flash('An error occurred while retrieving variant object', 'danger')\n        return redirect(request.referrer)\n\n    if current_app.config.get('LOQUSDB_SETTINGS'):\n        data['observations'] = controllers.observations(store, loqusdb,\n            case_obj, data['variant'])\n    data['cancer'] = request.args.get('cancer') == 'yes'\n    return dict(institute=institute_obj, case=case_obj, **data)","method_summary":"Display a specific SNV variant.","original_method_code":"def variant(institute_id, case_name, variant_id):\n    \"\"\"Display a specific SNV variant.\"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    log.debug(\"Variants view requesting data for variant {}\".format(variant_id))\n    data = controllers.variant(store, institute_obj, case_obj, variant_id=variant_id)\n    if data is None:\n        log.warning(\"An error occurred: variants view requesting data for variant {}\".format(variant_id))\n        flash('An error occurred while retrieving variant object', 'danger')\n        return redirect(request.referrer)\n\n    if current_app.config.get('LOQUSDB_SETTINGS'):\n        data['observations'] = controllers.observations(store, loqusdb,\n            case_obj, data['variant'])\n    data['cancer'] = request.args.get('cancer') == 'yes'\n    return dict(institute=institute_obj, case=case_obj, **data)","method_path":"https:\/\/github.com\/Clinical-Genomics\/scout\/blob\/90a551e2e1653a319e654c2405c2866f93d0ebb9\/scout\/server\/blueprints\/variants\/views.py#L172-L186"}
{"repo_name":"Clinical-Genomics\/scout","method_name":"str_variants","method_code":"def str_variants(institute_id, case_name):\n    \"\"\"\"\"\"\n    page = int(request.args.get('page', 1))\n    variant_type = request.args.get('variant_type', 'clinical')\n\n    form = StrFiltersForm(request.args)\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n\n    query = form.data\n    query['variant_type'] = variant_type\n\n    variants_query = store.variants(case_obj['_id'], category='str',\n        query=query)\n    data = controllers.str_variants(store, institute_obj, case_obj,\n        variants_query, page)\n    return dict(institute=institute_obj, case=case_obj,\n        variant_type = variant_type, form=form, page=page, **data)","method_summary":"Display a list of STR variants.","original_method_code":"def str_variants(institute_id, case_name):\n    \"\"\"Display a list of STR variants.\"\"\"\n    page = int(request.args.get('page', 1))\n    variant_type = request.args.get('variant_type', 'clinical')\n\n    form = StrFiltersForm(request.args)\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n\n    query = form.data\n    query['variant_type'] = variant_type\n\n    variants_query = store.variants(case_obj['_id'], category='str',\n        query=query)\n    data = controllers.str_variants(store, institute_obj, case_obj,\n        variants_query, page)\n    return dict(institute=institute_obj, case=case_obj,\n        variant_type = variant_type, form=form, page=page, **data)","method_path":"https:\/\/github.com\/Clinical-Genomics\/scout\/blob\/90a551e2e1653a319e654c2405c2866f93d0ebb9\/scout\/server\/blueprints\/variants\/views.py#L190-L207"}
{"repo_name":"UCBerkeleySETI\/blimpy","method_name":"cmd_tool","method_code":"def cmd_tool(args=None):\n    \"\"\"\"\"\"\n\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser(description=\"Command line utility for reading and plotting filterbank files.\")\n\n    parser.add_argument('-p', action='store',  default='ank', dest='what_to_plot', type=str,\n                        help='Show: \"w\" waterfall (freq vs. time) plot; \"s\" integrated spectrum plot; \\\n                        \"t\" for time series; \"mm\" for spectrum including min max; \"k\" for kurtosis; \\\n                        \"a\" for all available plots and information; and \"ank\" for all but kurtosis.')\n    parser.add_argument('filename', type=str,\n                        help='Name of file to read')\n    parser.add_argument('-b', action='store', default=None, dest='f_start', type=float,\n                        help='Start frequency (begin), in MHz')\n    parser.add_argument('-e', action='store', default=None, dest='f_stop', type=float,\n                        help='Stop frequency (end), in MHz')\n    parser.add_argument('-B', action='store', default=None, dest='t_start', type=int,\n                        help='Start integration (begin) ID')\n    parser.add_argument('-E', action='store', default=None, dest='t_stop', type=int,\n                        help='Stop integration (end) ID')\n    parser.add_argument('-i', action='store_true', default=False, dest='info_only',\n                        help='Show info only')\n    parser.add_argument('-a', action='store_true', default=False, dest='average',\n                       help='average along time axis (plot spectrum only)')\n    parser.add_argument('-s', action='store', default='', dest='plt_filename', type=str,\n                       help='save plot graphic to file (give filename as argument)')\n    parser.add_argument('-S', action='store_true', default=False, dest='save_only',\n                       help='Turn off plotting of data and only save to file.')\n    parser.add_argument('-D', action='store_false', default=True, dest='blank_dc',\n                       help='Use to not blank DC bin.')\n    parser.add_argument('-c', action='store_true', default=False, dest='calibrate_band_pass',\n                        help='Calibrate band pass.')\n    args = parser.parse_args()\n\n    \n    filename = args.filename\n    load_data = not args.info_only\n\n    \n    wtp = args.what_to_plot\n    if not wtp or 's' in wtp:\n        if args.t_start == None:\n            t_start = 0\n        else:\n            t_start = args.t_start\n        t_stop  = t_start + 1\n\n        if args.average:\n            t_start = None\n            t_stop  = None\n    else:\n        t_start = args.t_start\n        t_stop  = args.t_stop\n\n    if args.info_only:\n        args.blank_dc = False\n        args.calibrate_band_pass = False\n\n    fil = Filterbank(filename, f_start=args.f_start, f_stop=args.f_stop,\n                     t_start=t_start, t_stop=t_stop,\n                     load_data=load_data,blank_dc=args.blank_dc,\n                     cal_band_pass=args.calibrate_band_pass)\n    fil.info()\n\n    \n    if not args.info_only:\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n\n        if args.what_to_plot == \"w\":\n            plt.figure(\"waterfall\", figsize=(8, 6))\n            fil.plot_waterfall(f_start=args.f_start, f_stop=args.f_stop)\n        elif args.what_to_plot == \"s\":\n            plt.figure(\"Spectrum\", figsize=(8, 6))\n            fil.plot_spectrum(logged=True, f_start=args.f_start, f_stop=args.f_stop, t='all')\n        elif args.what_to_plot == \"mm\":\n            plt.figure(\"min max\", figsize=(8, 6))\n            fil.plot_spectrum_min_max(logged=True, f_start=args.f_start, f_stop=args.f_stop, t='all')\n        elif args.what_to_plot == \"k\":\n            plt.figure(\"kurtosis\", figsize=(8, 6))\n            fil.plot_kurtosis(f_start=args.f_start, f_stop=args.f_stop)\n        elif args.what_to_plot == \"t\":\n            plt.figure(\"Time Series\", figsize=(8, 6))\n            fil.plot_time_series(f_start=args.f_start, f_stop=args.f_stop,orientation='h')\n        elif args.what_to_plot == \"a\":\n            plt.figure(\"Multiple diagnostic plots\", figsize=(12, 9),facecolor='white')\n            fil.plot_all(logged=True, f_start=args.f_start, f_stop=args.f_stop, t='all')\n        elif args.what_to_plot == \"ank\":\n            plt.figure(\"Multiple diagnostic plots\", figsize=(12, 9),facecolor='white')\n            fil.plot_all(logged=True, f_start=args.f_start, f_stop=args.f_stop, t='all',kurtosis=False)\n\n        if args.plt_filename != '':\n            plt.savefig(args.plt_filename)\n\n        if not args.save_only:\n            if 'DISPLAY' in os.environ.keys():\n                plt.show()\n            else:\n                print(\"No $DISPLAY available.\")","method_summary":"Command line tool for plotting and viewing info on filterbank files","original_method_code":"def cmd_tool(args=None):\n    \"\"\" Command line tool for plotting and viewing info on filterbank files \"\"\"\n\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser(description=\"Command line utility for reading and plotting filterbank files.\")\n\n    parser.add_argument('-p', action='store',  default='ank', dest='what_to_plot', type=str,\n                        help='Show: \"w\" waterfall (freq vs. time) plot; \"s\" integrated spectrum plot; \\\n                        \"t\" for time series; \"mm\" for spectrum including min max; \"k\" for kurtosis; \\\n                        \"a\" for all available plots and information; and \"ank\" for all but kurtosis.')\n    parser.add_argument('filename', type=str,\n                        help='Name of file to read')\n    parser.add_argument('-b', action='store', default=None, dest='f_start', type=float,\n                        help='Start frequency (begin), in MHz')\n    parser.add_argument('-e', action='store', default=None, dest='f_stop', type=float,\n                        help='Stop frequency (end), in MHz')\n    parser.add_argument('-B', action='store', default=None, dest='t_start', type=int,\n                        help='Start integration (begin) ID')\n    parser.add_argument('-E', action='store', default=None, dest='t_stop', type=int,\n                        help='Stop integration (end) ID')\n    parser.add_argument('-i', action='store_true', default=False, dest='info_only',\n                        help='Show info only')\n    parser.add_argument('-a', action='store_true', default=False, dest='average',\n                       help='average along time axis (plot spectrum only)')\n    parser.add_argument('-s', action='store', default='', dest='plt_filename', type=str,\n                       help='save plot graphic to file (give filename as argument)')\n    parser.add_argument('-S', action='store_true', default=False, dest='save_only',\n                       help='Turn off plotting of data and only save to file.')\n    parser.add_argument('-D', action='store_false', default=True, dest='blank_dc',\n                       help='Use to not blank DC bin.')\n    parser.add_argument('-c', action='store_true', default=False, dest='calibrate_band_pass',\n                        help='Calibrate band pass.')\n    args = parser.parse_args()\n\n    # Open blimpy data\n    filename = args.filename\n    load_data = not args.info_only\n\n    # only load one integration if looking at spectrum\n    wtp = args.what_to_plot\n    if not wtp or 's' in wtp:\n        if args.t_start == None:\n            t_start = 0\n        else:\n            t_start = args.t_start\n        t_stop  = t_start + 1\n\n        if args.average:\n            t_start = None\n            t_stop  = None\n    else:\n        t_start = args.t_start\n        t_stop  = args.t_stop\n\n    if args.info_only:\n        args.blank_dc = False\n        args.calibrate_band_pass = False\n\n    fil = Filterbank(filename, f_start=args.f_start, f_stop=args.f_stop,\n                     t_start=t_start, t_stop=t_stop,\n                     load_data=load_data,blank_dc=args.blank_dc,\n                     cal_band_pass=args.calibrate_band_pass)\n    fil.info()\n\n    # And if we want to plot data, then plot data.\n    if not args.info_only:\n        # check start & stop frequencies make sense\n        #try:\n        #    if args.f_start:\n        #        print \"Start freq: %2.2f\" % args.f_start\n        #        assert args.f_start >= fil.freqs[0] or np.isclose(args.f_start, fil.freqs[0])\n        #\n        #    if args.f_stop:\n        #        print \"Stop freq: %2.2f\" % args.f_stop\n        #        assert args.f_stop <= fil.freqs[-1] or np.isclose(args.f_stop, fil.freqs[-1])\n        #except AssertionError:\n        #    print \"Error: Start and stop frequencies must lie inside file's frequency range.\"\n        #    print \"i.e. between %2.2f-%2.2f MHz.\" % (fil.freqs[0], fil.freqs[-1])\n        #    exit()\n\n        if args.what_to_plot == \"w\":\n            plt.figure(\"waterfall\", figsize=(8, 6))\n            fil.plot_waterfall(f_start=args.f_start, f_stop=args.f_stop)\n        elif args.what_to_plot == \"s\":\n            plt.figure(\"Spectrum\", figsize=(8, 6))\n            fil.plot_spectrum(logged=True, f_start=args.f_start, f_stop=args.f_stop, t='all')\n        elif args.what_to_plot == \"mm\":\n            plt.figure(\"min max\", figsize=(8, 6))\n            fil.plot_spectrum_min_max(logged=True, f_start=args.f_start, f_stop=args.f_stop, t='all')\n        elif args.what_to_plot == \"k\":\n            plt.figure(\"kurtosis\", figsize=(8, 6))\n            fil.plot_kurtosis(f_start=args.f_start, f_stop=args.f_stop)\n        elif args.what_to_plot == \"t\":\n            plt.figure(\"Time Series\", figsize=(8, 6))\n            fil.plot_time_series(f_start=args.f_start, f_stop=args.f_stop,orientation='h')\n        elif args.what_to_plot == \"a\":\n            plt.figure(\"Multiple diagnostic plots\", figsize=(12, 9),facecolor='white')\n            fil.plot_all(logged=True, f_start=args.f_start, f_stop=args.f_stop, t='all')\n        elif args.what_to_plot == \"ank\":\n            plt.figure(\"Multiple diagnostic plots\", figsize=(12, 9),facecolor='white')\n            fil.plot_all(logged=True, f_start=args.f_start, f_stop=args.f_stop, t='all',kurtosis=False)\n\n        if args.plt_filename != '':\n            plt.savefig(args.plt_filename)\n\n        if not args.save_only:\n            if 'DISPLAY' in os.environ.keys():\n                plt.show()\n            else:\n                print(\"No $DISPLAY available.\")","method_path":"https:\/\/github.com\/UCBerkeleySETI\/blimpy\/blob\/b8822d3e3e911944370d84371a91fa0c29e9772e\/blimpy\/filterbank.py#L977-L1087"}
{"repo_name":"UCBerkeleySETI\/blimpy","method_name":"Filterbank.read_hdf5","method_code":"def read_hdf5(self, filename, f_start=None, f_stop=None,\n                        t_start=None, t_stop=None, load_data=True):\n        \"\"\"\"\"\"\n        print(\"Warning: this function will be deprecated in the future. Please use Waterfall to open HDF5 files.\")\n\n\n        self.header = {}\n        self.filename = filename\n        self.h5 = h5py.File(filename)\n        for key, val in self.h5[b'data'].attrs.items():\n            if six.PY3:\n                key = bytes(key, 'ascii')\n            if key == b'src_raj':\n                self.header[key] = Angle(val, unit='hr')\n            elif key == b'src_dej':\n                self.header[key] = Angle(val, unit='deg')\n            else:\n                self.header[key] = val\n\n        self.n_ints_in_file = self.h5[b\"data\"].shape[0]\n        i_start, i_stop, chan_start_idx, chan_stop_idx = self._setup_freqs(f_start=f_start, f_stop=f_stop)\n        ii_start, ii_stop, n_ints = self._setup_time_axis(t_start=t_start, t_stop=t_stop)\n\n        if load_data:\n            self.data = self.h5[b\"data\"][ii_start:ii_stop, :, chan_start_idx:chan_stop_idx]\n\n            self.file_size_bytes = os.path.getsize(self.filename)\n\n\n\n\n        else:\n            print(\"Skipping data load...\")\n            self.data = np.array([0])\n            self.n_ints_in_file  = 0\n            self.file_size_bytes = os.path.getsize(self.filename)","method_summary":"Populate Filterbank instance with data from HDF5 file","original_method_code":"def read_hdf5(self, filename, f_start=None, f_stop=None,\n                        t_start=None, t_stop=None, load_data=True):\n        \"\"\" Populate Filterbank instance with data from HDF5 file\n\n        Note:\n            This is to be deprecated in future, please use Waterfall() to open files.\n        \"\"\"\n        print(\"Warning: this function will be deprecated in the future. Please use Waterfall to open HDF5 files.\")\n#        raise DeprecationWarning('Please use Waterfall to open HDF5 files.')\n\n        self.header = {}\n        self.filename = filename\n        self.h5 = h5py.File(filename)\n        for key, val in self.h5[b'data'].attrs.items():\n            if six.PY3:\n                key = bytes(key, 'ascii')\n            if key == b'src_raj':\n                self.header[key] = Angle(val, unit='hr')\n            elif key == b'src_dej':\n                self.header[key] = Angle(val, unit='deg')\n            else:\n                self.header[key] = val\n\n        self.n_ints_in_file = self.h5[b\"data\"].shape[0]\n        i_start, i_stop, chan_start_idx, chan_stop_idx = self._setup_freqs(f_start=f_start, f_stop=f_stop)\n        ii_start, ii_stop, n_ints = self._setup_time_axis(t_start=t_start, t_stop=t_stop)\n\n        if load_data:\n            self.data = self.h5[b\"data\"][ii_start:ii_stop, :, chan_start_idx:chan_stop_idx]\n\n            self.file_size_bytes = os.path.getsize(self.filename)\n\n#         if self.header[b'foff'] < 0:\n#             self.data = self.data[..., ::-1] # Reverse data\n\n        else:\n            print(\"Skipping data load...\")\n            self.data = np.array([0])\n            self.n_ints_in_file  = 0\n            self.file_size_bytes = os.path.getsize(self.filename)","method_path":"https:\/\/github.com\/UCBerkeleySETI\/blimpy\/blob\/b8822d3e3e911944370d84371a91fa0c29e9772e\/blimpy\/filterbank.py#L144-L183"}
{"repo_name":"UCBerkeleySETI\/blimpy","method_name":"Filterbank._setup_freqs","method_code":"def _setup_freqs(self, f_start=None, f_stop=None):\n        \"\"\"\"\"\"\n        \n        f0 = self.header[b'fch1']\n        f_delt = self.header[b'foff']\n\n        i_start, i_stop = 0, self.header[b'nchans']\n        if f_start:\n            i_start = int((f_start - f0) \/ f_delt)\n        if f_stop:\n            i_stop  = int((f_stop - f0)  \/ f_delt)\n\n        \n        chan_start_idx = np.int(i_start)\n        chan_stop_idx  = np.int(i_stop)\n\n        \n        if i_start < i_stop:\n            i_vals = np.arange(chan_start_idx, chan_stop_idx)\n        else:\n            i_vals = np.arange(chan_stop_idx, chan_start_idx)\n\n        self.freqs = f_delt * i_vals + f0\n\n\n\n\n        if chan_stop_idx < chan_start_idx:\n            chan_stop_idx, chan_start_idx = chan_start_idx,chan_stop_idx\n\n        return i_start, i_stop, chan_start_idx, chan_stop_idx","method_summary":"Setup frequency axis","original_method_code":"def _setup_freqs(self, f_start=None, f_stop=None):\n        \"\"\" Setup frequency axis \"\"\"\n        ## Setup frequency axis\n        f0 = self.header[b'fch1']\n        f_delt = self.header[b'foff']\n\n        i_start, i_stop = 0, self.header[b'nchans']\n        if f_start:\n            i_start = int((f_start - f0) \/ f_delt)\n        if f_stop:\n            i_stop  = int((f_stop - f0)  \/ f_delt)\n\n        #calculate closest true index value\n        chan_start_idx = np.int(i_start)\n        chan_stop_idx  = np.int(i_stop)\n\n        #create freq array\n        if i_start < i_stop:\n            i_vals = np.arange(chan_start_idx, chan_stop_idx)\n        else:\n            i_vals = np.arange(chan_stop_idx, chan_start_idx)\n\n        self.freqs = f_delt * i_vals + f0\n\n#         if f_delt < 0:\n#             self.freqs = self.freqs[::-1]\n\n        if chan_stop_idx < chan_start_idx:\n            chan_stop_idx, chan_start_idx = chan_start_idx,chan_stop_idx\n\n        return i_start, i_stop, chan_start_idx, chan_stop_idx","method_path":"https:\/\/github.com\/UCBerkeleySETI\/blimpy\/blob\/b8822d3e3e911944370d84371a91fa0c29e9772e\/blimpy\/filterbank.py#L186-L216"}
{"repo_name":"UCBerkeleySETI\/blimpy","method_name":"Filterbank._setup_time_axis","method_code":"def _setup_time_axis(self, t_start=None, t_stop=None):\n        \"\"\"\"\"\"\n\n        \n        ii_start, ii_stop = 0, self.n_ints_in_file\n        if t_start:\n            ii_start = t_start\n        if t_stop:\n            ii_stop = t_stop\n        n_ints = ii_stop - ii_start\n\n        \n        t0 = self.header[b'tstart']\n        t_delt = self.header[b'tsamp']\n\n        self.timestamps = np.arange(0, n_ints) * t_delt \/ 24.\/60.\/60 + t0\n\n        return ii_start, ii_stop, n_ints","method_summary":"Setup time axis.","original_method_code":"def _setup_time_axis(self, t_start=None, t_stop=None):\n        \"\"\"  Setup time axis. \"\"\"\n\n        # now check to see how many integrations requested\n        ii_start, ii_stop = 0, self.n_ints_in_file\n        if t_start:\n            ii_start = t_start\n        if t_stop:\n            ii_stop = t_stop\n        n_ints = ii_stop - ii_start\n\n        ## Setup time axis\n        t0 = self.header[b'tstart']\n        t_delt = self.header[b'tsamp']\n\n        self.timestamps = np.arange(0, n_ints) * t_delt \/ 24.\/60.\/60 + t0\n\n        return ii_start, ii_stop, n_ints","method_path":"https:\/\/github.com\/UCBerkeleySETI\/blimpy\/blob\/b8822d3e3e911944370d84371a91fa0c29e9772e\/blimpy\/filterbank.py#L218-L235"}
{"repo_name":"UCBerkeleySETI\/blimpy","method_name":"Filterbank.read_filterbank","method_code":"def read_filterbank(self, filename=None, f_start=None, f_stop=None,\n                        t_start=None, t_stop=None, load_data=True):\n        \"\"\"\"\"\"\n        if filename is None:\n            filename = self.filename\n        else:\n            self.filename = filename\n\n        self.header = read_header(filename)\n\n        \n        i_start, i_stop, chan_start_idx, chan_stop_idx = self._setup_freqs(f_start=f_start, f_stop=f_stop)\n\n        n_bits  = self.header[b'nbits']\n        n_bytes  = int(self.header[b'nbits'] \/ 8)\n        n_chans = self.header[b'nchans']\n        n_chans_selected = self.freqs.shape[0]\n        n_ifs   = self.header[b'nifs']\n\n        \n        self.idx_data = len_header(filename)\n        f = open(filename, 'rb')\n        f.seek(self.idx_data)\n        filesize = os.path.getsize(self.filename)\n        n_bytes_data = filesize - self.idx_data\n\n        \n        self.n_ints_in_file  = calc_n_ints_in_file(self.filename)\n        self.file_size_bytes = filesize\n\n        \n        ii_start, ii_stop, n_ints = self._setup_time_axis(t_start=t_start, t_stop=t_stop)\n\n        \n        f.seek(int(ii_start * n_bits * n_ifs * n_chans \/ 8), 1)\n\n        \n        i0 = np.min((chan_start_idx, chan_stop_idx))\n        i1 = np.max((chan_start_idx, chan_stop_idx))\n\n        \n        if n_bits == 2:\n            dd_type = b'uint8'\n            n_chans_selected = int(n_chans_selected\/4)\n        elif n_bytes == 4:\n            dd_type = b'float32'\n        elif n_bytes == 2:\n            dd_type = b'uint16'\n        elif n_bytes == 1:\n            dd_type = b'uint8'\n\n        if load_data:\n\n            if n_ints * n_ifs * n_chans_selected > MAX_DATA_ARRAY_SIZE:\n                print(\"[Filterbank]  Error: data array is too large to load. Either select fewer points or manually increase MAX_DATA_ARRAY_SIZE. Large files are now handle with Waterfall .\")\n                sys.exit()\n\n            if n_bits == 2:\n                self.data = np.zeros((n_ints, n_ifs, n_chans_selected*4), dtype=dd_type)\n            else:\n                self.data = np.zeros((n_ints, n_ifs, n_chans_selected), dtype=dd_type)\n\n            for ii in range(n_ints):\n                \"\"\"\"\"\"\n\n                for jj in range(n_ifs):\n\n                    f.seek(n_bytes * i0, 1) \n                    \n                    \n\n                    dd = np.fromfile(f, count=n_chans_selected, dtype=dd_type)\n\n                    \n\n\n\n                    if n_bits == 2:\n                        dd = unpack_2to8(dd)\n                    self.data[ii, jj] = dd\n\n                    f.seek(n_bytes * (n_chans - i1), 1)  \n        else:\n            print(\"Skipping data load...\")\n            self.data = np.array([0], dtype=dd_type)","method_summary":"Populate Filterbank instance with data from Filterbank file","original_method_code":"def read_filterbank(self, filename=None, f_start=None, f_stop=None,\n                        t_start=None, t_stop=None, load_data=True):\n        \"\"\" Populate Filterbank instance with data from Filterbank file\n\n        Note:\n            This is to be deprecated in future, please use Waterfall() to open files.\n        \"\"\"\n        if filename is None:\n            filename = self.filename\n        else:\n            self.filename = filename\n\n        self.header = read_header(filename)\n\n        #convert input frequencies into what their corresponding index would be\n        i_start, i_stop, chan_start_idx, chan_stop_idx = self._setup_freqs(f_start=f_start, f_stop=f_stop)\n\n        n_bits  = self.header[b'nbits']\n        n_bytes  = int(self.header[b'nbits'] \/ 8)\n        n_chans = self.header[b'nchans']\n        n_chans_selected = self.freqs.shape[0]\n        n_ifs   = self.header[b'nifs']\n\n        # Load binary data\n        self.idx_data = len_header(filename)\n        f = open(filename, 'rb')\n        f.seek(self.idx_data)\n        filesize = os.path.getsize(self.filename)\n        n_bytes_data = filesize - self.idx_data\n\n        # Finally add some other info to the class as objects\n        self.n_ints_in_file  = calc_n_ints_in_file(self.filename)\n        self.file_size_bytes = filesize\n\n        ## Setup time axis\n        ii_start, ii_stop, n_ints = self._setup_time_axis(t_start=t_start, t_stop=t_stop)\n\n        # Seek to first integration\n        f.seek(int(ii_start * n_bits * n_ifs * n_chans \/ 8), 1)\n\n        # Set up indexes used in file read (taken out of loop for speed)\n        i0 = np.min((chan_start_idx, chan_stop_idx))\n        i1 = np.max((chan_start_idx, chan_stop_idx))\n\n        #Set up the data type (taken out of loop for speed)\n        if n_bits == 2:\n            dd_type = b'uint8'\n            n_chans_selected = int(n_chans_selected\/4)\n        elif n_bytes == 4:\n            dd_type = b'float32'\n        elif n_bytes == 2:\n            dd_type = b'uint16'\n        elif n_bytes == 1:\n            dd_type = b'uint8'\n\n        if load_data:\n\n            if n_ints * n_ifs * n_chans_selected > MAX_DATA_ARRAY_SIZE:\n                print(\"[Filterbank]  Error: data array is too large to load. Either select fewer points or manually increase MAX_DATA_ARRAY_SIZE. Large files are now handle with Waterfall .\")\n                sys.exit()\n\n            if n_bits == 2:\n                self.data = np.zeros((n_ints, n_ifs, n_chans_selected*4), dtype=dd_type)\n            else:\n                self.data = np.zeros((n_ints, n_ifs, n_chans_selected), dtype=dd_type)\n\n            for ii in range(n_ints):\n                \"\"\"d = f.read(n_bytes * n_chans * n_ifs)\n                \"\"\"\n\n                for jj in range(n_ifs):\n\n                    f.seek(n_bytes * i0, 1) # 1 = from current location\n                    #d = f.read(n_bytes * n_chans_selected)\n                    #bytes_to_read = n_bytes * n_chans_selected\n\n                    dd = np.fromfile(f, count=n_chans_selected, dtype=dd_type)\n\n                    # Reverse array if frequency axis is flipped\n#                     if f_delt < 0:\n#                         dd = dd[::-1]\n\n                    if n_bits == 2:\n                        dd = unpack_2to8(dd)\n                    self.data[ii, jj] = dd\n\n                    f.seek(n_bytes * (n_chans - i1), 1)  # Seek to start of next block\n        else:\n            print(\"Skipping data load...\")\n            self.data = np.array([0], dtype=dd_type)","method_path":"https:\/\/github.com\/UCBerkeleySETI\/blimpy\/blob\/b8822d3e3e911944370d84371a91fa0c29e9772e\/blimpy\/filterbank.py#L237-L326"}
{"repo_name":"UCBerkeleySETI\/blimpy","method_name":"Filterbank.compute_lst","method_code":"def compute_lst(self):\n        \"\"\"\"\"\"\n        if self.header[b'telescope_id'] == 6:\n            self.coords = gbt_coords\n        elif self.header[b'telescope_id'] == 4:\n            self.coords = parkes_coords\n        else:\n            raise RuntimeError(\"Currently only Parkes and GBT supported\")\n        if HAS_SLALIB:\n            \n            dut1 = 0.0\n            mjd = self.header[b'tstart']\n            tellong = np.deg2rad(self.coords[1])\n            last = s.sla_gmst(mjd) - tellong + s.sla_eqeqx(mjd) + dut1\n            \n            if last < 0.0 : last = last + 2.0*np.pi\n            return last\n        else:\n            raise RuntimeError(\"This method requires pySLALIB\")","method_summary":"Compute LST for observation","original_method_code":"def compute_lst(self):\n        \"\"\" Compute LST for observation \"\"\"\n        if self.header[b'telescope_id'] == 6:\n            self.coords = gbt_coords\n        elif self.header[b'telescope_id'] == 4:\n            self.coords = parkes_coords\n        else:\n            raise RuntimeError(\"Currently only Parkes and GBT supported\")\n        if HAS_SLALIB:\n            # dut1 = (0.2 \/3600.0) * np.pi\/12.0\n            dut1 = 0.0\n            mjd = self.header[b'tstart']\n            tellong = np.deg2rad(self.coords[1])\n            last = s.sla_gmst(mjd) - tellong + s.sla_eqeqx(mjd) + dut1\n            # lmst = s.sla_gmst(mjd) - tellong\n            if last < 0.0 : last = last + 2.0*np.pi\n            return last\n        else:\n            raise RuntimeError(\"This method requires pySLALIB\")","method_path":"https:\/\/github.com\/UCBerkeleySETI\/blimpy\/blob\/b8822d3e3e911944370d84371a91fa0c29e9772e\/blimpy\/filterbank.py#L328-L346"}
{"repo_name":"UCBerkeleySETI\/blimpy","method_name":"Filterbank.compute_lsrk","method_code":"def compute_lsrk(self):\n        \"\"\"\"\"\"\n        ra = Angle(self.header[b'src_raj'], unit='hourangle')\n        dec = Angle(self.header[b'src_dej'], unit='degree')\n        mjdd = self.header[b'tstart']\n        rarad = ra.to('radian').value\n        dcrad = dec.to('radian').value\n        last = self.compute_lst()\n        tellat  = np.deg2rad(self.coords[0])\n        tellong = np.deg2rad(self.coords[1])\n\n        \n        starvect = s.sla_dcs2c(rarad, dcrad)\n\n        \n        Rgeo = s.sla_rverot( tellat, rarad, dcrad, last)\n\n        \n        evp = s.sla_evp(mjdd, 2000.0)\n        dvb = evp[0]   \n        dpb = evp[1]   \n        dvh = evp[2]   \n        dph = evp[3]   \n\n        \n        \n        vcorhelio = -s.sla_dvdv( starvect, dvh) *149.597870e6\n        vcorbary  = -s.sla_dvdv( starvect, dvb) *149.597870e6\n\n        \n        \n        rvlsrd = s.sla_rvlsrd( rarad,dcrad)\n\n        \n        \n        rvlsrk = s.sla_rvlsrk( rarad,dcrad)\n\n        \n        \n        rvgalc = s.sla_rvgalc( rarad,dcrad)\n        totalhelio = Rgeo + vcorhelio\n        totalbary  = Rgeo + vcorbary\n        totallsrk = totalhelio + rvlsrk\n        totalgal  = totalbary  + rvlsrd + rvgalc\n\n        return totallsrk","method_summary":"Computes the LSR in km\/s uses the MJD, RA and DEC of observation to compute along with the telescope location. Requires pyslalib","original_method_code":"def compute_lsrk(self):\n        \"\"\" Computes the LSR in km\/s\n\n        uses the MJD, RA and DEC of observation to compute\n        along with the telescope location. Requires pyslalib\n        \"\"\"\n        ra = Angle(self.header[b'src_raj'], unit='hourangle')\n        dec = Angle(self.header[b'src_dej'], unit='degree')\n        mjdd = self.header[b'tstart']\n        rarad = ra.to('radian').value\n        dcrad = dec.to('radian').value\n        last = self.compute_lst()\n        tellat  = np.deg2rad(self.coords[0])\n        tellong = np.deg2rad(self.coords[1])\n\n        # convert star position to vector\n        starvect = s.sla_dcs2c(rarad, dcrad)\n\n        # velocity component in ra,dec due to Earth rotation\n        Rgeo = s.sla_rverot( tellat, rarad, dcrad, last)\n\n        # get Barycentric and heliocentric velocity and position of the Earth.\n        evp = s.sla_evp(mjdd, 2000.0)\n        dvb = evp[0]   # barycentric velocity vector, in AU\/sec\n        dpb = evp[1]   # barycentric position vector, in AU\n        dvh = evp[2]   # heliocentric velocity vector, in AU\/sec\n        dph = evp[3]   # heliocentric position vector, in AU\n\n        # dot product of vector to object and heliocentric velocity\n        # convert AU\/sec to km\/sec\n        vcorhelio = -s.sla_dvdv( starvect, dvh) *149.597870e6\n        vcorbary  = -s.sla_dvdv( starvect, dvb) *149.597870e6\n\n        # rvlsrd is velocity component in ra,dec direction due to the Sun's\n        # motion with respect to the \"dynamical\" local standard of rest\n        rvlsrd = s.sla_rvlsrd( rarad,dcrad)\n\n        # rvlsrk is velocity component in ra,dec direction due to i\n        # the Sun's motion w.r.t the \"kinematic\" local standard of rest\n        rvlsrk = s.sla_rvlsrk( rarad,dcrad)\n\n        # rvgalc is velocity component in ra,dec direction due to\n        #the rotation of the Galaxy.\n        rvgalc = s.sla_rvgalc( rarad,dcrad)\n        totalhelio = Rgeo + vcorhelio\n        totalbary  = Rgeo + vcorbary\n        totallsrk = totalhelio + rvlsrk\n        totalgal  = totalbary  + rvlsrd + rvgalc\n\n        return totallsrk","method_path":"https:\/\/github.com\/UCBerkeleySETI\/blimpy\/blob\/b8822d3e3e911944370d84371a91fa0c29e9772e\/blimpy\/filterbank.py#L348-L397"}
{"repo_name":"UCBerkeleySETI\/blimpy","method_name":"Filterbank.blank_dc","method_code":"def blank_dc(self, n_coarse_chan):\n        \"\"\"\"\"\"\n\n        if n_coarse_chan < 1:\n            logger.warning('Coarse channel number < 1, unable to blank DC bin.')\n            return None\n\n        if not n_coarse_chan % int(n_coarse_chan) == 0:\n            logger.warning('Selection does not contain an interger number of coarse channels, unable to blank DC bin.')\n            return None\n\n        n_coarse_chan = int(n_coarse_chan)\n\n        n_chan = self.data.shape[-1]\n        n_chan_per_coarse = int(n_chan \/ n_coarse_chan)\n\n        mid_chan = int(n_chan_per_coarse \/ 2)\n\n        for ii in range(n_coarse_chan):\n            ss = ii*n_chan_per_coarse\n            self.data[..., ss+mid_chan] = np.median(self.data[..., ss+mid_chan+5:ss+mid_chan+10])","method_summary":"Blank DC bins in coarse channels.","original_method_code":"def blank_dc(self, n_coarse_chan):\n        \"\"\" Blank DC bins in coarse channels.\n\n        Note: currently only works if entire file is read\n        \"\"\"\n\n        if n_coarse_chan < 1:\n            logger.warning('Coarse channel number < 1, unable to blank DC bin.')\n            return None\n\n        if not n_coarse_chan % int(n_coarse_chan) == 0:\n            logger.warning('Selection does not contain an interger number of coarse channels, unable to blank DC bin.')\n            return None\n\n        n_coarse_chan = int(n_coarse_chan)\n\n        n_chan = self.data.shape[-1]\n        n_chan_per_coarse = int(n_chan \/ n_coarse_chan)\n\n        mid_chan = int(n_chan_per_coarse \/ 2)\n\n        for ii in range(n_coarse_chan):\n            ss = ii*n_chan_per_coarse\n            self.data[..., ss+mid_chan] = np.median(self.data[..., ss+mid_chan+5:ss+mid_chan+10])","method_path":"https:\/\/github.com\/UCBerkeleySETI\/blimpy\/blob\/b8822d3e3e911944370d84371a91fa0c29e9772e\/blimpy\/filterbank.py#L399-L422"}
{"repo_name":"UCBerkeleySETI\/blimpy","method_name":"Filterbank.info","method_code":"def info(self):\n        \"\"\"\"\"\"\n\n        for key, val in self.header.items():\n            if key == b'src_raj':\n                val = val.to_string(unit=u.hour, sep=':')\n            if key == b'src_dej':\n                val = val.to_string(unit=u.deg, sep=':')\n            if key == b'tsamp':\n                val *= u.second\n            if key in ('foff', 'fch1'):\n                val *= u.MHz\n            if key == b'tstart':\n                print(\"%16s : %32s\" % (\"tstart (ISOT)\", Time(val, format='mjd').isot))\n                key = \"tstart (MJD)\"\n            print(\"%16s : %32s\" % (key, val))\n\n        print(\"\\n%16s : %32s\" % (\"Num ints in file\", self.n_ints_in_file))\n        print(\"%16s : %32s\" % (\"Data shape\", self.data.shape))\n        print(\"%16s : %32s\" % (\"Start freq (MHz)\", self.freqs[0]))\n        print(\"%16s : %32s\" % (\"Stop freq (MHz)\", self.freqs[-1]))","method_summary":"Print header information","original_method_code":"def info(self):\n        \"\"\" Print header information \"\"\"\n\n        for key, val in self.header.items():\n            if key == b'src_raj':\n                val = val.to_string(unit=u.hour, sep=':')\n            if key == b'src_dej':\n                val = val.to_string(unit=u.deg, sep=':')\n            if key == b'tsamp':\n                val *= u.second\n            if key in ('foff', 'fch1'):\n                val *= u.MHz\n            if key == b'tstart':\n                print(\"%16s : %32s\" % (\"tstart (ISOT)\", Time(val, format='mjd').isot))\n                key = \"tstart (MJD)\"\n            print(\"%16s : %32s\" % (key, val))\n\n        print(\"\\n%16s : %32s\" % (\"Num ints in file\", self.n_ints_in_file))\n        print(\"%16s : %32s\" % (\"Data shape\", self.data.shape))\n        print(\"%16s : %32s\" % (\"Start freq (MHz)\", self.freqs[0]))\n        print(\"%16s : %32s\" % (\"Stop freq (MHz)\", self.freqs[-1]))","method_path":"https:\/\/github.com\/UCBerkeleySETI\/blimpy\/blob\/b8822d3e3e911944370d84371a91fa0c29e9772e\/blimpy\/filterbank.py#L424-L444"}
{"repo_name":"UCBerkeleySETI\/blimpy","method_name":"Filterbank.generate_freqs","method_code":"def generate_freqs(self, f_start, f_stop):\n        \"\"\"\"\"\"\n\n        fch1 = self.header[b'fch1']\n        foff = self.header[b'foff']\n\n        \n        i_start = int((f_start - fch1) \/ foff)\n        i_stop  = int((f_stop - fch1)  \/ foff)\n\n        \n        chan_start_idx = np.int(i_start)\n        chan_stop_idx  = np.int(i_stop)\n\n        \n        i_vals = np.arange(chan_stop_idx, chan_start_idx, 1)\n\n        freqs = foff * i_vals + fch1\n\n        return freqs","method_summary":"returns frequency array [f_start","original_method_code":"def generate_freqs(self, f_start, f_stop):\n        \"\"\"\n        returns frequency array [f_start...f_stop]\n        \"\"\"\n\n        fch1 = self.header[b'fch1']\n        foff = self.header[b'foff']\n\n        #convert input frequencies into what their corresponding index would be\n        i_start = int((f_start - fch1) \/ foff)\n        i_stop  = int((f_stop - fch1)  \/ foff)\n\n        #calculate closest true index value\n        chan_start_idx = np.int(i_start)\n        chan_stop_idx  = np.int(i_stop)\n\n        #create freq array\n        i_vals = np.arange(chan_stop_idx, chan_start_idx, 1)\n\n        freqs = foff * i_vals + fch1\n\n        return freqs","method_path":"https:\/\/github.com\/UCBerkeleySETI\/blimpy\/blob\/b8822d3e3e911944370d84371a91fa0c29e9772e\/blimpy\/filterbank.py#L446-L467"}
{"repo_name":"RRZE-HPC\/kerncraft","method_name":"serach_path","method_code":"def serach_path():\n    \"\"\"\"\"\"\n    operating_system = get_os()\n    \n    \n    return [os.path.expanduser(\"~\/.kerncraft\/iaca\/{}\/\".format(operating_system)),\n            os.path.abspath(os.path.dirname(os.path.realpath(__file__))) + '\/iaca\/{}\/'.format(\n                operating_system)]","method_summary":"Return potential locations of IACA installation.","original_method_code":"def serach_path():\n    \"\"\"Return potential locations of IACA installation.\"\"\"\n    operating_system = get_os()\n    # 1st choice: in ~\/.kerncraft\/iaca-{}\n    # 2nd choice: in package directory \/ iaca-{}\n    return [os.path.expanduser(\"~\/.kerncraft\/iaca\/{}\/\".format(operating_system)),\n            os.path.abspath(os.path.dirname(os.path.realpath(__file__))) + '\/iaca\/{}\/'.format(\n                operating_system)]","method_path":"https:\/\/github.com\/RRZE-HPC\/kerncraft\/blob\/c60baf8043e4da8d8d66da7575021c2f4c6c78af\/kerncraft\/iaca_get.py#L32-L39"}
{"repo_name":"RRZE-HPC\/kerncraft","method_name":"find_iaca","method_code":"def find_iaca():\n    \"\"\"\"\"\"\n    requires = ['iaca2.2', 'iaca2.3', 'iaca3.0']\n    for path in serach_path():\n        path += 'bin\/'\n        valid = True\n        for r in requires:\n            if not os.path.exists(path + r):\n                valid = False\n                break\n        if valid:\n            return path\n    raise RuntimeError(\"No IACA installation found in {}. Run iaca_get command to fix this issue.\"\n                       \"\".format(serach_path()))","method_summary":"Return (hopefully) valid installation of IACA.","original_method_code":"def find_iaca():\n    \"\"\"Return (hopefully) valid installation of IACA.\"\"\"\n    requires = ['iaca2.2', 'iaca2.3', 'iaca3.0']\n    for path in serach_path():\n        path += 'bin\/'\n        valid = True\n        for r in requires:\n            if not os.path.exists(path + r):\n                valid = False\n                break\n        if valid:\n            return path\n    raise RuntimeError(\"No IACA installation found in {}. Run iaca_get command to fix this issue.\"\n                       \"\".format(serach_path()))","method_path":"https:\/\/github.com\/RRZE-HPC\/kerncraft\/blob\/c60baf8043e4da8d8d66da7575021c2f4c6c78af\/kerncraft\/iaca_get.py#L42-L55"}
{"repo_name":"RRZE-HPC\/kerncraft","method_name":"register_options","method_code":"def register_options(regdescr):\n    \"\"\"\"\"\"\n    if not regdescr:\n        yield None\n    tokenizer = ('\\[(?P<grp>[^]]+)\\]|'\n                 '(?P<chr>.)')\n    for u in regdescr.split('|'):\n        m = re.match(tokenizer, u)\n\n        if m.group('grp'):\n            current = group_iterator(m.group('grp'))\n        else:\n            current = [m.group('chr')]\n\n        for c in current:\n            if u[m.end():]:\n                for r in register_options(u[m.end():]):\n                    yield c + r\n            else:\n                yield c","method_summary":"Very reduced regular expressions for describing a group of registers. Only groups in square bracktes and unions with pipes (|) are supported.","original_method_code":"def register_options(regdescr):\n    \"\"\"\n    Very reduced regular expressions for describing a group of registers.\n\n    Only groups in square bracktes and unions with pipes (|) are supported.\n\n    Examples:\n    >>> list(register_options('PMC[0-3]'))\n    ['PMC0', 'PMC1', 'PMC2', 'PMC3']\n    >>> list(register_options('MBOX0C[01]'))\n    ['MBOX0C0', 'MBOX0C1']\n    >>> list(register_options('CBOX2C1'))\n    ['CBOX2C1']\n    >>> list(register_options('CBOX[0-3]C[01]'))\n    ['CBOX0C0', 'CBOX0C1', 'CBOX1C0', 'CBOX1C1', 'CBOX2C0', 'CBOX2C1', 'CBOX3C0', 'CBOX3C1']\n    >>> list(register_options('PMC[0-1]|PMC[23]'))\n    ['PMC0', 'PMC1', 'PMC2', 'PMC3']\n\n    \"\"\"\n    if not regdescr:\n        yield None\n    tokenizer = ('\\[(?P<grp>[^]]+)\\]|'\n                 '(?P<chr>.)')\n    for u in regdescr.split('|'):\n        m = re.match(tokenizer, u)\n\n        if m.group('grp'):\n            current = group_iterator(m.group('grp'))\n        else:\n            current = [m.group('chr')]\n\n        for c in current:\n            if u[m.end():]:\n                for r in register_options(u[m.end():]):\n                    yield c + r\n            else:\n                yield c","method_path":"https:\/\/github.com\/RRZE-HPC\/kerncraft\/blob\/c60baf8043e4da8d8d66da7575021c2f4c6c78af\/likwid-counter-packing.py#L36-L72"}
{"repo_name":"RRZE-HPC\/kerncraft","method_name":"eventstr","method_code":"def eventstr(event_tuple=None, event=None, register=None, parameters=None):\n    \"\"\"\"\"\"\n    if len(event_tuple) == 3:\n        event, register, parameters = event_tuple\n    elif len(event_tuple) == 2:\n        event, register = event_tuple\n    event_dscr = [event, register]\n\n    if parameters:\n        for k, v in sorted(event_tuple[2].items()):  \n            if type(v) is int:\n                k += \"={}\".format(hex(v))\n            event_dscr.append(k)\n    return \":\".join(event_dscr)","method_summary":"Return a LIKWID event string from an event tuple or keyword arguments. *event_tuple","original_method_code":"def eventstr(event_tuple=None, event=None, register=None, parameters=None):\n    \"\"\"\n    Return a LIKWID event string from an event tuple or keyword arguments.\n\n    *event_tuple* may have two or three arguments: (event, register) or\n    (event, register, parameters)\n\n    Keyword arguments will be overwritten by *event_tuple*.\n\n    >>> eventstr(('L1D_REPLACEMENT', 'PMC0', None))\n    'L1D_REPLACEMENT:PMC0'\n    >>> eventstr(('L1D_REPLACEMENT', 'PMC0'))\n    'L1D_REPLACEMENT:PMC0'\n    >>> eventstr(('MEM_UOPS_RETIRED_LOADS', 'PMC3', {'EDGEDETECT': None, 'THRESHOLD': 2342}))\n    'MEM_UOPS_RETIRED_LOADS:PMC3:EDGEDETECT:THRESHOLD=0x926'\n    >>> eventstr(event='DTLB_LOAD_MISSES_WALK_DURATION', register='PMC3')\n    'DTLB_LOAD_MISSES_WALK_DURATION:PMC3'\n    \"\"\"\n    if len(event_tuple) == 3:\n        event, register, parameters = event_tuple\n    elif len(event_tuple) == 2:\n        event, register = event_tuple\n    event_dscr = [event, register]\n\n    if parameters:\n        for k, v in sorted(event_tuple[2].items()):  # sorted for reproducability\n            if type(v) is int:\n                k += \"={}\".format(hex(v))\n            event_dscr.append(k)\n    return \":\".join(event_dscr)","method_path":"https:\/\/github.com\/RRZE-HPC\/kerncraft\/blob\/c60baf8043e4da8d8d66da7575021c2f4c6c78af\/likwid-counter-packing.py#L75-L104"}
{"repo_name":"RRZE-HPC\/kerncraft","method_name":"build_minimal_runs","method_code":"def build_minimal_runs(events):\n    \"\"\"\"\"\"\n    \n    events = [e for i, e in enumerate(events) if events.index(e) == i]\n\n    \n    scheduled_runs = {}\n    scheduled_events = []\n    cur_run = 0\n    while len(scheduled_events) != len(events):\n        for event_tpl in events:\n            event, registers, parameters = event_tpl\n            \n            if event_tpl in scheduled_events:\n                continue\n            \n            for possible_reg in register_options(registers):\n                \n                s = scheduled_runs.setdefault(cur_run, {})\n                if possible_reg not in s:\n                    s[possible_reg] = (event, possible_reg, parameters)\n                    \n                    scheduled_events.append(event_tpl)\n                    break\n        cur_run += 1\n\n    \n    runs = [list(v.values()) for v in scheduled_runs.values()]\n\n    return runs","method_summary":"Compile list of minimal runs for given events.","original_method_code":"def build_minimal_runs(events):\n    \"\"\"Compile list of minimal runs for given events.\"\"\"\n    # Eliminate multiples\n    events = [e for i, e in enumerate(events) if events.index(e) == i]\n\n    # Build list of runs per register group\n    scheduled_runs = {}\n    scheduled_events = []\n    cur_run = 0\n    while len(scheduled_events) != len(events):\n        for event_tpl in events:\n            event, registers, parameters = event_tpl\n            # Skip allready scheduled events\n            if event_tpl in scheduled_events:\n                continue\n            # Compile explicit list of possible register locations\n            for possible_reg in register_options(registers):\n                # Schedule in current run, if register is not yet in use\n                s = scheduled_runs.setdefault(cur_run, {})\n                if possible_reg not in s:\n                    s[possible_reg] = (event, possible_reg, parameters)\n                    # ban from further scheduling attempts\n                    scheduled_events.append(event_tpl)\n                    break\n        cur_run += 1\n\n    # Collaps all register dicts to single runs\n    runs = [list(v.values()) for v in scheduled_runs.values()]\n\n    return runs","method_path":"https:\/\/github.com\/RRZE-HPC\/kerncraft\/blob\/c60baf8043e4da8d8d66da7575021c2f4c6c78af\/likwid-counter-packing.py#L107-L136"}
{"repo_name":"RRZE-HPC\/kerncraft","method_name":"Roofline.calculate_cache_access","method_code":"def calculate_cache_access(self):\n        \"\"\"\"\"\"\n        self.results = {'misses': self.predictor.get_misses(),\n                        'hits': self.predictor.get_hits(),\n                        'evicts': self.predictor.get_evicts(),\n                        'verbose infos': self.predictor.get_infos(),  \n                        'bottleneck level': 0,\n                        'mem bottlenecks': []}\n\n        element_size = self.kernel.datatypes_size[self.kernel.datatype]\n        cacheline_size = float(self.machine['cacheline size'])\n        elements_per_cacheline = int(cacheline_size \/\/ element_size)\n\n        total_flops = sum(self.kernel._flops.values())*elements_per_cacheline\n\n        \n        threads_per_core = 1\n\n        \n\n        \n        \n        read_offsets, write_offsets = zip(*list(self.kernel.compile_global_offsets(\n            iteration=range(0, elements_per_cacheline))))\n        read_offsets = set([item for sublist in read_offsets if sublist is not None\n                            for item in sublist])\n        write_offsets = set([item for sublist in write_offsets if sublist is not None\n                             for item in sublist])\n\n        write_streams = len(write_offsets)\n        read_streams = len(read_offsets) + write_streams  \n        total_loads = read_streams * element_size\n        \n        bw, measurement_kernel = self.machine.get_bandwidth(\n            0,\n            read_streams - write_streams,  \n            write_streams,\n            threads_per_core,\n            cores=self.cores)\n\n        \n        \n        if total_loads == 0:\n            \n            arith_intens = None\n            performance = None\n        else:\n            arith_intens = float(total_flops)\/total_loads\n            performance = PrefixedUnit(arith_intens * float(bw), 'FLOP\/s')\n\n        self.results['mem bottlenecks'].append({\n            'performance': self.conv_perf(PrefixedUnit(performance, 'FLOP\/s')),\n            'level': self.machine['memory hierarchy'][0]['level'],\n            'arithmetic intensity': arith_intens,\n            'bw kernel': measurement_kernel,\n            'bandwidth': bw,\n            'bytes transfered': total_loads})\n        self.results['bottleneck level'] = len(self.results['mem bottlenecks'])-1\n        self.results['min performance'] = self.conv_perf(performance)\n\n        \n        for cache_level, cache_info in list(enumerate(self.machine['memory hierarchy']))[:-1]:\n            \n            total_misses = self.results['misses'][cache_level]*cacheline_size\n            total_evicts = self.results['evicts'][cache_level]*cacheline_size\n\n            \n            \n            \n            read_streams = self.results['misses'][cache_level]\n            write_streams = self.results['evicts'][cache_level]\n            \n            bw, measurement_kernel = self.machine.get_bandwidth(\n                cache_level+1, read_streams, write_streams, threads_per_core,\n                cores=self.cores)\n\n            \n            \n            bytes_transfered = total_misses + total_evicts\n\n            if bytes_transfered == 0:\n                \n                arith_intens = float('inf')\n                performance = PrefixedUnit(float('inf'), 'FLOP\/s')\n            else:\n                arith_intens = float(total_flops)\/bytes_transfered\n                performance = PrefixedUnit(arith_intens * float(bw), 'FLOP\/s')\n\n            self.results['mem bottlenecks'].append({\n                'performance': self.conv_perf(performance),\n                'level': (self.machine['memory hierarchy'][cache_level + 1]['level']),\n                'arithmetic intensity': arith_intens,\n                'bw kernel': measurement_kernel,\n                'bandwidth': bw,\n                'bytes transfered': bytes_transfered})\n            if performance < self.results.get('min performance', {'FLOP\/s': performance})['FLOP\/s']:\n                self.results['bottleneck level'] = len(self.results['mem bottlenecks'])-1\n                self.results['min performance'] = self.conv_perf(performance)\n\n        return self.results","method_summary":"Apply cache prediction to generate cache access behaviour.","original_method_code":"def calculate_cache_access(self):\n        \"\"\"Apply cache prediction to generate cache access behaviour.\"\"\"\n        self.results = {'misses': self.predictor.get_misses(),\n                        'hits': self.predictor.get_hits(),\n                        'evicts': self.predictor.get_evicts(),\n                        'verbose infos': self.predictor.get_infos(),  # only for verbose outputs\n                        'bottleneck level': 0,\n                        'mem bottlenecks': []}\n\n        element_size = self.kernel.datatypes_size[self.kernel.datatype]\n        cacheline_size = float(self.machine['cacheline size'])\n        elements_per_cacheline = int(cacheline_size \/\/ element_size)\n\n        total_flops = sum(self.kernel._flops.values())*elements_per_cacheline\n\n        # TODO let user choose threads_per_core:\n        threads_per_core = 1\n\n        # Compile relevant information\n\n        # CPU-L1 stats (in bytes!)\n        # We compile CPU-L1 stats on our own, because cacheprediction only works on cache lines\n        read_offsets, write_offsets = zip(*list(self.kernel.compile_global_offsets(\n            iteration=range(0, elements_per_cacheline))))\n        read_offsets = set([item for sublist in read_offsets if sublist is not None\n                            for item in sublist])\n        write_offsets = set([item for sublist in write_offsets if sublist is not None\n                             for item in sublist])\n\n        write_streams = len(write_offsets)\n        read_streams = len(read_offsets) + write_streams  # write-allocate\n        total_loads = read_streams * element_size\n        # total_evicts = write_streams * element_size\n        bw, measurement_kernel = self.machine.get_bandwidth(\n            0,\n            read_streams - write_streams,  # no write-allocate in L1\n            write_streams,\n            threads_per_core,\n            cores=self.cores)\n\n        # Calculate performance (arithmetic intensity * bandwidth with\n        # arithmetic intensity = flops \/ bytes loaded )\n        if total_loads == 0:\n            # This happens in case of full-caching\n            arith_intens = None\n            performance = None\n        else:\n            arith_intens = float(total_flops)\/total_loads\n            performance = PrefixedUnit(arith_intens * float(bw), 'FLOP\/s')\n\n        self.results['mem bottlenecks'].append({\n            'performance': self.conv_perf(PrefixedUnit(performance, 'FLOP\/s')),\n            'level': self.machine['memory hierarchy'][0]['level'],\n            'arithmetic intensity': arith_intens,\n            'bw kernel': measurement_kernel,\n            'bandwidth': bw,\n            'bytes transfered': total_loads})\n        self.results['bottleneck level'] = len(self.results['mem bottlenecks'])-1\n        self.results['min performance'] = self.conv_perf(performance)\n\n        # for other cache and memory levels:\n        for cache_level, cache_info in list(enumerate(self.machine['memory hierarchy']))[:-1]:\n            # Compiling stats (in bytes!)\n            total_misses = self.results['misses'][cache_level]*cacheline_size\n            total_evicts = self.results['evicts'][cache_level]*cacheline_size\n\n            # choose bw according to cache level and problem\n            # first, compile stream counts at current cache level\n            # write-allocate is allready resolved above\n            read_streams = self.results['misses'][cache_level]\n            write_streams = self.results['evicts'][cache_level]\n            # second, try to find best fitting kernel (closest to stream seen stream counts):\n            bw, measurement_kernel = self.machine.get_bandwidth(\n                cache_level+1, read_streams, write_streams, threads_per_core,\n                cores=self.cores)\n\n            # Calculate performance (arithmetic intensity * bandwidth with\n            # arithmetic intensity = flops \/ bytes transfered)\n            bytes_transfered = total_misses + total_evicts\n\n            if bytes_transfered == 0:\n                # This happens in case of full-caching\n                arith_intens = float('inf')\n                performance = PrefixedUnit(float('inf'), 'FLOP\/s')\n            else:\n                arith_intens = float(total_flops)\/bytes_transfered\n                performance = PrefixedUnit(arith_intens * float(bw), 'FLOP\/s')\n\n            self.results['mem bottlenecks'].append({\n                'performance': self.conv_perf(performance),\n                'level': (self.machine['memory hierarchy'][cache_level + 1]['level']),\n                'arithmetic intensity': arith_intens,\n                'bw kernel': measurement_kernel,\n                'bandwidth': bw,\n                'bytes transfered': bytes_transfered})\n            if performance < self.results.get('min performance', {'FLOP\/s': performance})['FLOP\/s']:\n                self.results['bottleneck level'] = len(self.results['mem bottlenecks'])-1\n                self.results['min performance'] = self.conv_perf(performance)\n\n        return self.results","method_path":"https:\/\/github.com\/RRZE-HPC\/kerncraft\/blob\/c60baf8043e4da8d8d66da7575021c2f4c6c78af\/kerncraft\/models\/roofline.py#L62-L161"}
{"repo_name":"RRZE-HPC\/kerncraft","method_name":"Roofline.analyze","method_code":"def analyze(self):\n        \"\"\"\"\"\"\n        precision = 'DP' if self.kernel.datatype == 'double' else 'SP'\n        self.calculate_cache_access()\n\n        self.results['max_perf'] = self.conv_perf(self.machine['clock'] * self.cores * \\\n            self.machine['FLOPs per cycle'][precision]['total'])","method_summary":"Run analysis.","original_method_code":"def analyze(self):\n        \"\"\"Run analysis.\"\"\"\n        precision = 'DP' if self.kernel.datatype == 'double' else 'SP'\n        self.calculate_cache_access()\n\n        self.results['max_perf'] = self.conv_perf(self.machine['clock'] * self.cores * \\\n            self.machine['FLOPs per cycle'][precision]['total'])","method_path":"https:\/\/github.com\/RRZE-HPC\/kerncraft\/blob\/c60baf8043e4da8d8d66da7575021c2f4c6c78af\/kerncraft\/models\/roofline.py#L163-L169"}
{"repo_name":"RRZE-HPC\/kerncraft","method_name":"Roofline.conv_perf","method_code":"def conv_perf(self, performance):\n        \"\"\"\"\"\"\n        clock = self.machine['clock']\n        flops_per_it = sum(self.kernel._flops.values())\n        it_s = performance\/flops_per_it\n        it_s.unit = 'It\/s'\n        element_size = self.kernel.datatypes_size[self.kernel.datatype]\n        elements_per_cacheline = int(float(self.machine['cacheline size'])) \/ element_size\n        cy_cl = clock\/it_s*elements_per_cacheline\n        cy_cl.unit = 'cy\/CL'\n        cy_it = clock\/it_s\n        cy_it.unit = 'cy\/It'\n\n        return {'It\/s': it_s,\n                'cy\/CL': cy_cl,\n                'cy\/It': cy_it,\n                'FLOP\/s': performance}","method_summary":"Convert performance (FLOP\/s) to other units, such as It\/s or cy\/CL.","original_method_code":"def conv_perf(self, performance):\n        \"\"\"Convert performance (FLOP\/s) to other units, such as It\/s or cy\/CL.\"\"\"\n        clock = self.machine['clock']\n        flops_per_it = sum(self.kernel._flops.values())\n        it_s = performance\/flops_per_it\n        it_s.unit = 'It\/s'\n        element_size = self.kernel.datatypes_size[self.kernel.datatype]\n        elements_per_cacheline = int(float(self.machine['cacheline size'])) \/ element_size\n        cy_cl = clock\/it_s*elements_per_cacheline\n        cy_cl.unit = 'cy\/CL'\n        cy_it = clock\/it_s\n        cy_it.unit = 'cy\/It'\n\n        return {'It\/s': it_s,\n                'cy\/CL': cy_cl,\n                'cy\/It': cy_it,\n                'FLOP\/s': performance}","method_path":"https:\/\/github.com\/RRZE-HPC\/kerncraft\/blob\/c60baf8043e4da8d8d66da7575021c2f4c6c78af\/kerncraft\/models\/roofline.py#L171-L187"}
{"repo_name":"RRZE-HPC\/kerncraft","method_name":"Roofline.report","method_code":"def report(self, output_file=sys.stdout):\n        \"\"\"\"\"\"\n        max_perf = self.results['max_perf']\n\n        if self._args and self._args.verbose >= 3:\n            print('{}'.format(pformat(self.results)), file=output_file)\n\n        if self._args and self._args.verbose >= 1:\n            print('{}'.format(pformat(self.results['verbose infos'])), file=output_file)\n            print('Bottlenecks:', file=output_file)\n            print('  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel',\n                  file=output_file)\n            print('--------+--------------+-----------------+-------------------+----------------------',\n                  file=output_file)\n            print('    CPU |              | {!s:>15} |                   |'.format(\n                max_perf[self._args.unit]),\n                  file=output_file)\n            for b in self.results['mem bottlenecks']:\n                print('{level:>7} | {arithmetic intensity:>5.2} FLOP\/B | {0!s:>15} |'\n                      ' {bandwidth!s:>17} | {bw kernel:<8}'.format(\n                          b['performance'][self._args.unit], **b),\n                      file=output_file)\n            print('', file=output_file)\n\n        if self.results['min performance']['FLOP\/s'] > max_perf['FLOP\/s']:\n            \n            print('CPU bound. {!s} due to CPU max. FLOP\/s'.format(max_perf), file=output_file)\n        else:\n            \n            print('Cache or mem bound.', file=output_file)\n\n            bottleneck = self.results['mem bottlenecks'][self.results['bottleneck level']]\n            print('{!s} due to {} transfer bottleneck (with bw from {} benchmark)'.format(\n                    bottleneck['performance'][self._args.unit],\n                    bottleneck['level'],\n                    bottleneck['bw kernel']),\n                  file=output_file)\n            print('Arithmetic Intensity: {:.2f} FLOP\/B'.format(bottleneck['arithmetic intensity']),\n                  file=output_file)","method_summary":"Report analysis outcome in human readable form.","original_method_code":"def report(self, output_file=sys.stdout):\n        \"\"\"Report analysis outcome in human readable form.\"\"\"\n        max_perf = self.results['max_perf']\n\n        if self._args and self._args.verbose >= 3:\n            print('{}'.format(pformat(self.results)), file=output_file)\n\n        if self._args and self._args.verbose >= 1:\n            print('{}'.format(pformat(self.results['verbose infos'])), file=output_file)\n            print('Bottlenecks:', file=output_file)\n            print('  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel',\n                  file=output_file)\n            print('--------+--------------+-----------------+-------------------+----------------------',\n                  file=output_file)\n            print('    CPU |              | {!s:>15} |                   |'.format(\n                max_perf[self._args.unit]),\n                  file=output_file)\n            for b in self.results['mem bottlenecks']:\n                print('{level:>7} | {arithmetic intensity:>5.2} FLOP\/B | {0!s:>15} |'\n                      ' {bandwidth!s:>17} | {bw kernel:<8}'.format(\n                          b['performance'][self._args.unit], **b),\n                      file=output_file)\n            print('', file=output_file)\n\n        if self.results['min performance']['FLOP\/s'] > max_perf['FLOP\/s']:\n            # CPU bound\n            print('CPU bound. {!s} due to CPU max. FLOP\/s'.format(max_perf), file=output_file)\n        else:\n            # Cache or mem bound\n            print('Cache or mem bound.', file=output_file)\n\n            bottleneck = self.results['mem bottlenecks'][self.results['bottleneck level']]\n            print('{!s} due to {} transfer bottleneck (with bw from {} benchmark)'.format(\n                    bottleneck['performance'][self._args.unit],\n                    bottleneck['level'],\n                    bottleneck['bw kernel']),\n                  file=output_file)\n            print('Arithmetic Intensity: {:.2f} FLOP\/B'.format(bottleneck['arithmetic intensity']),\n                  file=output_file)","method_path":"https:\/\/github.com\/RRZE-HPC\/kerncraft\/blob\/c60baf8043e4da8d8d66da7575021c2f4c6c78af\/kerncraft\/models\/roofline.py#L189-L227"}
{"repo_name":"RRZE-HPC\/kerncraft","method_name":"RooflineIACA.analyze","method_code":"def analyze(self):\n        \"\"\"\"\"\"\n        self.results = self.calculate_cache_access()\n        try:\n            iaca_analysis, asm_block = self.kernel.iaca_analysis(\n                micro_architecture=self.machine['micro-architecture'],\n                asm_block=self.asm_block,\n                pointer_increment=self.pointer_increment,\n                verbose=self.verbose > 2)\n        except RuntimeError as e:\n            print(\"IACA analysis failed: \" + str(e))\n            sys.exit(1)\n\n        block_throughput = iaca_analysis['throughput']\n        uops = iaca_analysis['uops']\n        iaca_output = iaca_analysis['output']\n        port_cycles = iaca_analysis['port cycles']\n\n        \n        elements_per_block = abs(asm_block['pointer_increment']\n                                 \/ self.kernel.datatypes_size[self.kernel.datatype])\n        block_size = elements_per_block*self.kernel.datatypes_size[self.kernel.datatype]\n        try:\n            block_to_cl_ratio = float(self.machine['cacheline size'])\/block_size\n        except ZeroDivisionError as e:\n            print(\"Too small block_size \/ pointer_increment:\", e, file=sys.stderr)\n            sys.exit(1)\n\n        port_cycles = dict([(i[0], i[1]*block_to_cl_ratio) for i in list(port_cycles.items())])\n        uops = uops*block_to_cl_ratio\n        cl_throughput = block_throughput*block_to_cl_ratio\n        flops_per_element = sum(self.kernel._flops.values())\n\n        \n        self.results['mem bottlenecks'][0] = None\n\n        \n        self.results['min performance'] = self.conv_perf(PrefixedUnit(float('inf'), 'FLOP\/s'))\n        self.results['bottleneck level'] = None\n        for level, bottleneck in enumerate(self.results['mem bottlenecks']):\n            if level == 0:\n                \n                continue\n            if bottleneck['performance']['FLOP\/s'] < self.results['min performance']['FLOP\/s']:\n                self.results['bottleneck level'] = level\n                self.results['min performance'] = bottleneck['performance']\n\n        \n        self.results.update({\n            'cpu bottleneck': {\n                'port cycles': port_cycles,\n                'cl throughput': cl_throughput,\n                'uops': uops,\n                'performance throughput': self.conv_perf(PrefixedUnit(\n                    self.machine['clock']\/block_throughput*elements_per_block*flops_per_element\n                    * self.cores, \"FLOP\/s\")),\n                'IACA output': iaca_output}})","method_summary":"Run complete analysis.","original_method_code":"def analyze(self):\n        \"\"\"Run complete analysis.\"\"\"\n        self.results = self.calculate_cache_access()\n        try:\n            iaca_analysis, asm_block = self.kernel.iaca_analysis(\n                micro_architecture=self.machine['micro-architecture'],\n                asm_block=self.asm_block,\n                pointer_increment=self.pointer_increment,\n                verbose=self.verbose > 2)\n        except RuntimeError as e:\n            print(\"IACA analysis failed: \" + str(e))\n            sys.exit(1)\n\n        block_throughput = iaca_analysis['throughput']\n        uops = iaca_analysis['uops']\n        iaca_output = iaca_analysis['output']\n        port_cycles = iaca_analysis['port cycles']\n\n        # Normalize to cycles per cacheline\n        elements_per_block = abs(asm_block['pointer_increment']\n                                 \/ self.kernel.datatypes_size[self.kernel.datatype])\n        block_size = elements_per_block*self.kernel.datatypes_size[self.kernel.datatype]\n        try:\n            block_to_cl_ratio = float(self.machine['cacheline size'])\/block_size\n        except ZeroDivisionError as e:\n            print(\"Too small block_size \/ pointer_increment:\", e, file=sys.stderr)\n            sys.exit(1)\n\n        port_cycles = dict([(i[0], i[1]*block_to_cl_ratio) for i in list(port_cycles.items())])\n        uops = uops*block_to_cl_ratio\n        cl_throughput = block_throughput*block_to_cl_ratio\n        flops_per_element = sum(self.kernel._flops.values())\n\n        # Overwrite CPU-L1 stats, because they are covered by IACA\n        self.results['mem bottlenecks'][0] = None\n\n        # Reevaluate mem bottleneck\n        self.results['min performance'] = self.conv_perf(PrefixedUnit(float('inf'), 'FLOP\/s'))\n        self.results['bottleneck level'] = None\n        for level, bottleneck in enumerate(self.results['mem bottlenecks']):\n            if level == 0:\n                # ignoring CPU-L1\n                continue\n            if bottleneck['performance']['FLOP\/s'] < self.results['min performance']['FLOP\/s']:\n                self.results['bottleneck level'] = level\n                self.results['min performance'] = bottleneck['performance']\n\n        # Create result dictionary\n        self.results.update({\n            'cpu bottleneck': {\n                'port cycles': port_cycles,\n                'cl throughput': cl_throughput,\n                'uops': uops,\n                'performance throughput': self.conv_perf(PrefixedUnit(\n                    self.machine['clock']\/block_throughput*elements_per_block*flops_per_element\n                    * self.cores, \"FLOP\/s\")),\n                'IACA output': iaca_output}})","method_path":"https:\/\/github.com\/RRZE-HPC\/kerncraft\/blob\/c60baf8043e4da8d8d66da7575021c2f4c6c78af\/kerncraft\/models\/roofline.py#L281-L337"}
{"repo_name":"gholt\/swiftly","method_name":"cli_decrypt","method_code":"def cli_decrypt(context, key):\n    \"\"\"\"\"\"\n    with context.io_manager.with_stdout() as stdout:\n        with context.io_manager.with_stdin() as stdin:\n            crypt_type = stdin.read(1)\n            if crypt_type == AES256CBC:\n                for chunk in aes_decrypt(key, stdin):\n                    stdout.write(chunk)\n                stdout.flush()\n            else:\n                raise ReturnCode(\n                    'contents encrypted with unsupported type %r' % crypt_type)","method_summary":"Decrypts context.io_manager's stdin and sends that to context.io_manager's stdout. See :py:mod:`swiftly.cli.decrypt` for context usage information. See :py:class:`CLIDecrypt` for more information.","original_method_code":"def cli_decrypt(context, key):\n    \"\"\"\n    Decrypts context.io_manager's stdin and sends that to\n    context.io_manager's stdout.\n\n    See :py:mod:`swiftly.cli.decrypt` for context usage information.\n\n    See :py:class:`CLIDecrypt` for more information.\n    \"\"\"\n    with context.io_manager.with_stdout() as stdout:\n        with context.io_manager.with_stdin() as stdin:\n            crypt_type = stdin.read(1)\n            if crypt_type == AES256CBC:\n                for chunk in aes_decrypt(key, stdin):\n                    stdout.write(chunk)\n                stdout.flush()\n            else:\n                raise ReturnCode(\n                    'contents encrypted with unsupported type %r' % crypt_type)","method_path":"https:\/\/github.com\/gholt\/swiftly\/blob\/5bcc1c65323b1caf1f85adbefd9fc4988c072149\/swiftly\/cli\/decrypt.py#L31-L49"}
{"repo_name":"gholt\/swiftly","method_name":"IOManager.client_path_to_os_path","method_code":"def client_path_to_os_path(self, client_path):\n        \"\"\"\"\"\"\n        if os.path.sep == '\/':\n            return client_path\n        return client_path.replace(os.path.sep, '-').replace('\/', os.path.sep)","method_summary":"Converts a client path into the operating system's path by replacing instances of '\/' with os.path.sep.","original_method_code":"def client_path_to_os_path(self, client_path):\n        \"\"\"\n        Converts a client path into the operating system's path by\n        replacing instances of '\/' with os.path.sep.\n\n        Note: If the client path contains any instances of\n        os.path.sep already, they will be replaced with '-'.\n        \"\"\"\n        if os.path.sep == '\/':\n            return client_path\n        return client_path.replace(os.path.sep, '-').replace('\/', os.path.sep)","method_path":"https:\/\/github.com\/gholt\/swiftly\/blob\/5bcc1c65323b1caf1f85adbefd9fc4988c072149\/swiftly\/cli\/iomanager.py#L86-L96"}
{"repo_name":"gholt\/swiftly","method_name":"IOManager.os_path_to_client_path","method_code":"def os_path_to_client_path(self, os_path):\n        \"\"\"\"\"\"\n        if os.path.sep == '\/':\n            return os_path\n        return os_path.replace('\/', '-').replace(os.path.sep, '\/')","method_summary":"Converts an operating system path into a client path by replacing instances of os.path.sep with '\/'.","original_method_code":"def os_path_to_client_path(self, os_path):\n        \"\"\"\n        Converts an operating system path into a client path by\n        replacing instances of os.path.sep with '\/'.\n\n        Note: If the client path contains any instances of '\/'\n        already, they will be replaced with '-'.\n        \"\"\"\n        if os.path.sep == '\/':\n            return os_path\n        return os_path.replace('\/', '-').replace(os.path.sep, '\/')","method_path":"https:\/\/github.com\/gholt\/swiftly\/blob\/5bcc1c65323b1caf1f85adbefd9fc4988c072149\/swiftly\/cli\/iomanager.py#L98-L108"}
{"repo_name":"gholt\/swiftly","method_name":"IOManager.with_stdin","method_code":"def with_stdin(self, os_path=None, skip_sub_command=False,\n                   disk_closed_callback=None):\n        \"\"\"\"\"\"\n        sub_command = None if skip_sub_command else self.stdin_sub_command\n        inn, path = self._get_in_and_path(\n            self.stdin, self.stdin_root, sub_command, os_path)\n        try:\n            if hasattr(inn, 'stdout'):\n                yield inn.stdout\n            else:\n                yield inn\n        finally:\n            if hasattr(inn, 'stdout'):\n                self._close(inn.stdout)\n            self._wait(inn, path)\n            self._close(inn)\n            if disk_closed_callback and path:\n                disk_closed_callback(path)","method_summary":"A context manager yielding a stdin-suitable file-like object based on the optional os_path and optionally skipping any configured sub-command.","original_method_code":"def with_stdin(self, os_path=None, skip_sub_command=False,\n                   disk_closed_callback=None):\n        \"\"\"\n        A context manager yielding a stdin-suitable file-like object\n        based on the optional os_path and optionally skipping any\n        configured sub-command.\n\n        :param os_path: Optional path to base the file-like object\n            on.\n        :param skip_sub_command: Set True to skip any configured\n            sub-command filter.\n        :param disk_closed_callback: If the backing of the file-like\n            object is an actual file that will be closed,\n            disk_closed_callback (if set) will be called with the\n            on-disk path just after closing it.\n        \"\"\"\n        sub_command = None if skip_sub_command else self.stdin_sub_command\n        inn, path = self._get_in_and_path(\n            self.stdin, self.stdin_root, sub_command, os_path)\n        try:\n            if hasattr(inn, 'stdout'):\n                yield inn.stdout\n            else:\n                yield inn\n        finally:\n            if hasattr(inn, 'stdout'):\n                self._close(inn.stdout)\n            self._wait(inn, path)\n            self._close(inn)\n            if disk_closed_callback and path:\n                disk_closed_callback(path)","method_path":"https:\/\/github.com\/gholt\/swiftly\/blob\/5bcc1c65323b1caf1f85adbefd9fc4988c072149\/swiftly\/cli\/iomanager.py#L221-L251"}
{"repo_name":"gholt\/swiftly","method_name":"IOManager.with_stdout","method_code":"def with_stdout(self, os_path=None, skip_sub_command=False,\n                    disk_closed_callback=None):\n        \"\"\"\"\"\"\n        sub_command = None if skip_sub_command else self.stdout_sub_command\n        out, path = self._get_out_and_path(\n            self.stdout, self.stdout_root, sub_command, os_path)\n        try:\n            if hasattr(out, 'stdin'):\n                yield out.stdin\n            else:\n                yield out\n        finally:\n            if hasattr(out, 'stdin'):\n                self._close(out.stdin)\n            self._wait(out, path)\n            self._close(out)\n            if disk_closed_callback and path:\n                disk_closed_callback(path)","method_summary":"A context manager yielding a stdout-suitable file-like object based on the optional os_path and optionally skipping any configured sub-command.","original_method_code":"def with_stdout(self, os_path=None, skip_sub_command=False,\n                    disk_closed_callback=None):\n        \"\"\"\n        A context manager yielding a stdout-suitable file-like object\n        based on the optional os_path and optionally skipping any\n        configured sub-command.\n\n        :param os_path: Optional path to base the file-like object\n            on.\n        :param skip_sub_command: Set True to skip any configured\n            sub-command filter.\n        :param disk_closed_callback: If the backing of the file-like\n            object is an actual file that will be closed,\n            disk_closed_callback (if set) will be called with the\n            on-disk path just after closing it.\n        \"\"\"\n        sub_command = None if skip_sub_command else self.stdout_sub_command\n        out, path = self._get_out_and_path(\n            self.stdout, self.stdout_root, sub_command, os_path)\n        try:\n            if hasattr(out, 'stdin'):\n                yield out.stdin\n            else:\n                yield out\n        finally:\n            if hasattr(out, 'stdin'):\n                self._close(out.stdin)\n            self._wait(out, path)\n            self._close(out)\n            if disk_closed_callback and path:\n                disk_closed_callback(path)","method_path":"https:\/\/github.com\/gholt\/swiftly\/blob\/5bcc1c65323b1caf1f85adbefd9fc4988c072149\/swiftly\/cli\/iomanager.py#L254-L284"}
{"repo_name":"gholt\/swiftly","method_name":"IOManager.with_stderr","method_code":"def with_stderr(self, os_path=None, skip_sub_command=False,\n                    disk_closed_callback=None):\n        \"\"\"\"\"\"\n        sub_command = None if skip_sub_command else self.stderr_sub_command\n        out, path = self._get_out_and_path(\n            self.stderr, self.stderr_root, sub_command, os_path)\n        try:\n            if hasattr(out, 'stdin'):\n                yield out.stdin\n            else:\n                yield out\n        finally:\n            if hasattr(out, 'stdin'):\n                self._close(out.stdin)\n            self._wait(out, path)\n            self._close(out)\n            if disk_closed_callback and path:\n                disk_closed_callback(path)","method_summary":"A context manager yielding a stderr-suitable file-like object based on the optional os_path and optionally skipping any configured sub-command.","original_method_code":"def with_stderr(self, os_path=None, skip_sub_command=False,\n                    disk_closed_callback=None):\n        \"\"\"\n        A context manager yielding a stderr-suitable file-like object\n        based on the optional os_path and optionally skipping any\n        configured sub-command.\n\n        :param os_path: Optional path to base the file-like object\n            on.\n        :param skip_sub_command: Set True to skip any configured\n            sub-command filter.\n        :param disk_closed_callback: If the backing of the file-like\n            object is an actual file that will be closed,\n            disk_closed_callback (if set) will be called with the\n            on-disk path just after closing it.\n        \"\"\"\n        sub_command = None if skip_sub_command else self.stderr_sub_command\n        out, path = self._get_out_and_path(\n            self.stderr, self.stderr_root, sub_command, os_path)\n        try:\n            if hasattr(out, 'stdin'):\n                yield out.stdin\n            else:\n                yield out\n        finally:\n            if hasattr(out, 'stdin'):\n                self._close(out.stdin)\n            self._wait(out, path)\n            self._close(out)\n            if disk_closed_callback and path:\n                disk_closed_callback(path)","method_path":"https:\/\/github.com\/gholt\/swiftly\/blob\/5bcc1c65323b1caf1f85adbefd9fc4988c072149\/swiftly\/cli\/iomanager.py#L287-L317"}
{"repo_name":"gholt\/swiftly","method_name":"IOManager.with_debug","method_code":"def with_debug(self, os_path=None, skip_sub_command=False,\n                   disk_closed_callback=None):\n        \"\"\"\"\"\"\n        sub_command = None if skip_sub_command else self.debug_sub_command\n        out, path = self._get_out_and_path(\n            self.debug, self.debug_root, sub_command, os_path)\n        try:\n            if hasattr(out, 'stdin'):\n                yield out.stdin\n            else:\n                yield out\n        finally:\n            if hasattr(out, 'stdin'):\n                self._close(out.stdin)\n            self._wait(out, path)\n            self._close(out)\n            if disk_closed_callback and path:\n                disk_closed_callback(path)","method_summary":"A context manager yielding a debug-output-suitable file-like object based on the optional os_path and optionally skipping any configured sub-command.","original_method_code":"def with_debug(self, os_path=None, skip_sub_command=False,\n                   disk_closed_callback=None):\n        \"\"\"\n        A context manager yielding a debug-output-suitable file-like\n        object based on the optional os_path and optionally skipping\n        any configured sub-command.\n\n        :param os_path: Optional path to base the file-like object\n            on.\n        :param skip_sub_command: Set True to skip any configured\n            sub-command filter.\n        :param disk_closed_callback: If the backing of the file-like\n            object is an actual file that will be closed,\n            disk_closed_callback (if set) will be called with the\n            on-disk path just after closing it.\n        \"\"\"\n        sub_command = None if skip_sub_command else self.debug_sub_command\n        out, path = self._get_out_and_path(\n            self.debug, self.debug_root, sub_command, os_path)\n        try:\n            if hasattr(out, 'stdin'):\n                yield out.stdin\n            else:\n                yield out\n        finally:\n            if hasattr(out, 'stdin'):\n                self._close(out.stdin)\n            self._wait(out, path)\n            self._close(out)\n            if disk_closed_callback and path:\n                disk_closed_callback(path)","method_path":"https:\/\/github.com\/gholt\/swiftly\/blob\/5bcc1c65323b1caf1f85adbefd9fc4988c072149\/swiftly\/cli\/iomanager.py#L320-L350"}
{"repo_name":"gholt\/swiftly","method_name":"cli_delete","method_code":"def cli_delete(context, path, body=None, recursive=False,\n               yes_empty_account=False, yes_delete_account=False,\n               until_empty=False):\n    \"\"\"\"\"\"\n    path = path.lstrip('\/') if path else ''\n    if not path:\n        if yes_empty_account:\n            cli_empty_account(\n                context, yes_empty_account=yes_empty_account,\n                until_empty=until_empty)\n        if yes_delete_account:\n            with context.client_manager.with_client() as client:\n                status, reason, headers, contents = client.delete_account(\n                    headers=context.headers, query=context.query,\n                    cdn=context.cdn, body=body,\n                    yes_i_mean_delete_the_account=yes_delete_account)\n                if status \/\/ 100 != 2:\n                    if status == 404 and context.ignore_404:\n                        return\n                    raise ReturnCode(\n                        'deleting account: %s %s' % (status, reason))\n    elif '\/' not in path.rstrip('\/'):\n        path = path.rstrip('\/')\n        if recursive:\n            cli_empty_container(context, path, until_empty=until_empty)\n        with context.client_manager.with_client() as client:\n            status, reason, headers, contents = client.delete_container(\n                path, headers=context.headers,\n                query=context.query, cdn=context.cdn, body=body)\n            if status \/\/ 100 != 2:\n                if status == 404 and context.ignore_404:\n                    return\n                raise ReturnCode(\n                    'deleting container %r: %s %s' % (path, status, reason))\n    else:\n        with context.client_manager.with_client() as client:\n            status, reason, headers, contents = client.delete_object(\n                *path.split('\/', 1), headers=context.headers,\n                query=context.query, cdn=context.cdn, body=body)\n            if status \/\/ 100 != 2:\n                if status == 404 and context.ignore_404:\n                    return\n                raise ReturnCode(\n                    'deleting object %r: %s %s' % (path, status, reason))","method_summary":"Deletes the item (account, container, or object) at the path. See :py:mod:`swiftly.cli.delete` for context usage information. See :py:class:`CLIDelete` for more information.","original_method_code":"def cli_delete(context, path, body=None, recursive=False,\n               yes_empty_account=False, yes_delete_account=False,\n               until_empty=False):\n    \"\"\"\n    Deletes the item (account, container, or object) at the path.\n\n    See :py:mod:`swiftly.cli.delete` for context usage information.\n\n    See :py:class:`CLIDelete` for more information.\n\n    :param context: The :py:class:`swiftly.cli.context.CLIContext` to\n        use.\n    :param path: The path of the item (acount, container, or object)\n        to delete.\n    :param body: The body to send with the DELETE request. Bodies are\n        not normally sent with DELETE requests, but this can be\n        useful with bulk deletes for instance.\n    :param recursive: If True and the item is an account or\n        container, deletes will be issued for any containing items as\n        well. This does one pass at the deletion; so if objects revert\n        to previous versions or if new objects otherwise arise during\n        the process, the container(s) may not be empty once done. Set\n        `until_empty` to True if you want multiple passes to keep trying\n        to fully empty the containers.\n    :param until_empty: If True and recursive is True, this will cause\n        Swiftly to keep looping through the deletes until the containers\n        are completely empty. Useful if you have object versioning\n        turned on or otherwise have objects that seemingly reappear\n        after being deleted. It could also run forever if you have\n        something that's uploading objects at a faster rate than they\n        are deleted.\n    :param yes_empty_account: This must be set to True for\n        verification when the item is an account and recursive is\n        True.\n    :param yes_delete_account: This must be set to True for\n        verification when the item is an account and you really wish\n        a delete to be issued for the account itself.\n    \"\"\"\n    path = path.lstrip('\/') if path else ''\n    if not path:\n        if yes_empty_account:\n            cli_empty_account(\n                context, yes_empty_account=yes_empty_account,\n                until_empty=until_empty)\n        if yes_delete_account:\n            with context.client_manager.with_client() as client:\n                status, reason, headers, contents = client.delete_account(\n                    headers=context.headers, query=context.query,\n                    cdn=context.cdn, body=body,\n                    yes_i_mean_delete_the_account=yes_delete_account)\n                if status \/\/ 100 != 2:\n                    if status == 404 and context.ignore_404:\n                        return\n                    raise ReturnCode(\n                        'deleting account: %s %s' % (status, reason))\n    elif '\/' not in path.rstrip('\/'):\n        path = path.rstrip('\/')\n        if recursive:\n            cli_empty_container(context, path, until_empty=until_empty)\n        with context.client_manager.with_client() as client:\n            status, reason, headers, contents = client.delete_container(\n                path, headers=context.headers,\n                query=context.query, cdn=context.cdn, body=body)\n            if status \/\/ 100 != 2:\n                if status == 404 and context.ignore_404:\n                    return\n                raise ReturnCode(\n                    'deleting container %r: %s %s' % (path, status, reason))\n    else:\n        with context.client_manager.with_client() as client:\n            status, reason, headers, contents = client.delete_object(\n                *path.split('\/', 1), headers=context.headers,\n                query=context.query, cdn=context.cdn, body=body)\n            if status \/\/ 100 != 2:\n                if status == 404 and context.ignore_404:\n                    return\n                raise ReturnCode(\n                    'deleting object %r: %s %s' % (path, status, reason))","method_path":"https:\/\/github.com\/gholt\/swiftly\/blob\/5bcc1c65323b1caf1f85adbefd9fc4988c072149\/swiftly\/cli\/delete.py#L140-L217"}
{"repo_name":"gholt\/swiftly","method_name":"_stdout_filed","method_code":"def _stdout_filed(func):\n    \"\"\"\"\"\"\n    def wrapper(self, file=None):\n        if file:\n            return func(self, file=file)\n        elif self.io_manager:\n            with self.io_manager.with_stdout() as stdout:\n                return func(self, file=stdout)\n        else:\n            return func(self, file=sys.stdout)\n    wrapper.__doc__ = func.__doc__\n    return wrapper","method_summary":"Instance method decorator to convert an optional file keyword argument into an actual value, whether it be a passed value, a value obtained from an io_manager, or sys.stdout.","original_method_code":"def _stdout_filed(func):\n    \"\"\"\n    Instance method decorator to convert an optional file keyword\n    argument into an actual value, whether it be a passed value, a\n    value obtained from an io_manager, or sys.stdout.\n    \"\"\"\n    def wrapper(self, file=None):\n        if file:\n            return func(self, file=file)\n        elif self.io_manager:\n            with self.io_manager.with_stdout() as stdout:\n                return func(self, file=stdout)\n        else:\n            return func(self, file=sys.stdout)\n    wrapper.__doc__ = func.__doc__\n    return wrapper","method_path":"https:\/\/github.com\/gholt\/swiftly\/blob\/5bcc1c65323b1caf1f85adbefd9fc4988c072149\/swiftly\/cli\/optionparser.py#L25-L40"}
{"repo_name":"gholt\/swiftly","method_name":"_stderr_filed","method_code":"def _stderr_filed(func):\n    \"\"\"\"\"\"\n    def wrapper(self, msg, file=None):\n        if file:\n            return func(self, msg, file=file)\n        elif self.io_manager:\n            with self.io_manager.with_stderr() as stderr:\n                return func(self, msg, file=stderr)\n        else:\n            return func(self, msg, file=sys.stderr)\n    wrapper.__doc__ = func.__doc__\n    return wrapper","method_summary":"Instance method decorator to convert an optional file keyword argument into an actual value, whether it be a passed value, a value obtained from an io_manager, or sys.stderr.","original_method_code":"def _stderr_filed(func):\n    \"\"\"\n    Instance method decorator to convert an optional file keyword\n    argument into an actual value, whether it be a passed value, a\n    value obtained from an io_manager, or sys.stderr.\n    \"\"\"\n    def wrapper(self, msg, file=None):\n        if file:\n            return func(self, msg, file=file)\n        elif self.io_manager:\n            with self.io_manager.with_stderr() as stderr:\n                return func(self, msg, file=stderr)\n        else:\n            return func(self, msg, file=sys.stderr)\n    wrapper.__doc__ = func.__doc__\n    return wrapper","method_path":"https:\/\/github.com\/gholt\/swiftly\/blob\/5bcc1c65323b1caf1f85adbefd9fc4988c072149\/swiftly\/cli\/optionparser.py#L43-L58"}
{"repo_name":"SmokinCaterpillar\/pypet","method_name":"cartesian_product","method_code":"def cartesian_product(parameter_dict, combined_parameters=()):\n    \"\"\"\"\"\"\n    if not combined_parameters:\n        combined_parameters = list(parameter_dict)\n    else:\n        combined_parameters = list(combined_parameters)\n\n    for idx, item in enumerate(combined_parameters):\n        if isinstance(item, str):\n            combined_parameters[idx] = (item,)\n\n    iterator_list = []\n    for item_tuple in combined_parameters:\n        inner_iterator_list = [parameter_dict[key] for key in item_tuple]\n        zipped_iterator = zip(*inner_iterator_list)\n        iterator_list.append(zipped_iterator)\n\n    result_dict = {}\n    for key in parameter_dict:\n        result_dict[key] = []\n\n    cartesian_iterator = itools.product(*iterator_list)\n\n    for cartesian_tuple in cartesian_iterator:\n        for idx, item_tuple in enumerate(combined_parameters):\n            for inneridx, key in enumerate(item_tuple):\n                result_dict[key].append(cartesian_tuple[idx][inneridx])\n\n    return result_dict","method_summary":"Generates a Cartesian product of the input parameter dictionary. For","original_method_code":"def cartesian_product(parameter_dict, combined_parameters=()):\n    \"\"\" Generates a Cartesian product of the input parameter dictionary.\n\n    For example:\n\n    >>> print cartesian_product({'param1':[1,2,3], 'param2':[42.0, 52.5]})\n    {'param1':[1,1,2,2,3,3],'param2': [42.0,52.5,42.0,52.5,42.0,52.5]}\n\n    :param parameter_dict:\n\n        Dictionary containing parameter names as keys and iterables of data to explore.\n\n    :param combined_parameters:\n\n        Tuple of tuples. Defines the order of the parameters and parameters that are\n        linked together.\n        If an inner tuple contains only a single item, you can spare the\n        inner tuple brackets.\n\n\n        For example:\n\n        >>> print cartesian_product( {'param1': [42.0, 52.5], 'param2':['a', 'b'], 'param3' : [1,2,3]}, ('param3',('param1', 'param2')))\n        {param3':[1,1,2,2,3,3],'param1' : [42.0,52.5,42.0,52.5,42.0,52.5], 'param2':['a','b','a','b','a','b']}\n\n    :returns: Dictionary with cartesian product lists.\n\n    \"\"\"\n    if not combined_parameters:\n        combined_parameters = list(parameter_dict)\n    else:\n        combined_parameters = list(combined_parameters)\n\n    for idx, item in enumerate(combined_parameters):\n        if isinstance(item, str):\n            combined_parameters[idx] = (item,)\n\n    iterator_list = []\n    for item_tuple in combined_parameters:\n        inner_iterator_list = [parameter_dict[key] for key in item_tuple]\n        zipped_iterator = zip(*inner_iterator_list)\n        iterator_list.append(zipped_iterator)\n\n    result_dict = {}\n    for key in parameter_dict:\n        result_dict[key] = []\n\n    cartesian_iterator = itools.product(*iterator_list)\n\n    for cartesian_tuple in cartesian_iterator:\n        for idx, item_tuple in enumerate(combined_parameters):\n            for inneridx, key in enumerate(item_tuple):\n                result_dict[key].append(cartesian_tuple[idx][inneridx])\n\n    return result_dict","method_path":"https:\/\/github.com\/SmokinCaterpillar\/pypet\/blob\/97ad3e80d46dbdea02deeb98ea41f05a19565826\/pypet\/utils\/explore.py#L9-L63"}
{"repo_name":"SmokinCaterpillar\/pypet","method_name":"find_unique_points","method_code":"def find_unique_points(explored_parameters):\n    \"\"\"\"\"\"\n    ranges = [param.f_get_range(copy=False) for param in explored_parameters]\n    zipped_tuples = list(zip(*ranges))\n    try:\n        unique_elements = OrderedDict()\n        for idx, val_tuple in enumerate(zipped_tuples):\n            if val_tuple not in unique_elements:\n                unique_elements[val_tuple] = []\n            unique_elements[val_tuple].append(idx)\n        return list(unique_elements.items())\n    except TypeError:\n        logger = logging.getLogger('pypet.find_unique')\n        logger.error('Your parameter entries could not be hashed, '\n                     'now I am sorting slowly in O(N**2).')\n        unique_elements = []\n        for idx, val_tuple in enumerate(zipped_tuples):\n            matches = False\n            for added_tuple, pos_list in unique_elements:\n                matches = True\n                for idx2, val in enumerate(added_tuple):\n                    if not explored_parameters[idx2]._equal_values(val_tuple[idx2], val):\n                        matches = False\n                        break\n                if matches:\n                    pos_list.append(idx)\n                    break\n            if not matches:\n                unique_elements.append((val_tuple, [idx]))\n        return unique_elements","method_summary":"Takes a list of explored parameters and finds unique parameter combinations. If parameter ranges are hashable operates in O(N), otherwise O(N**2).","original_method_code":"def find_unique_points(explored_parameters):\n    \"\"\"Takes a list of explored parameters and finds unique parameter combinations.\n\n    If parameter ranges are hashable operates in O(N), otherwise O(N**2).\n\n    :param explored_parameters:\n\n        List of **explored** parameters\n\n    :return:\n\n        List of tuples, first entry being the parameter values, second entry a list\n        containing the run position of the unique combination.\n\n    \"\"\"\n    ranges = [param.f_get_range(copy=False) for param in explored_parameters]\n    zipped_tuples = list(zip(*ranges))\n    try:\n        unique_elements = OrderedDict()\n        for idx, val_tuple in enumerate(zipped_tuples):\n            if val_tuple not in unique_elements:\n                unique_elements[val_tuple] = []\n            unique_elements[val_tuple].append(idx)\n        return list(unique_elements.items())\n    except TypeError:\n        logger = logging.getLogger('pypet.find_unique')\n        logger.error('Your parameter entries could not be hashed, '\n                     'now I am sorting slowly in O(N**2).')\n        unique_elements = []\n        for idx, val_tuple in enumerate(zipped_tuples):\n            matches = False\n            for added_tuple, pos_list in unique_elements:\n                matches = True\n                for idx2, val in enumerate(added_tuple):\n                    if not explored_parameters[idx2]._equal_values(val_tuple[idx2], val):\n                        matches = False\n                        break\n                if matches:\n                    pos_list.append(idx)\n                    break\n            if not matches:\n                unique_elements.append((val_tuple, [idx]))\n        return unique_elements","method_path":"https:\/\/github.com\/SmokinCaterpillar\/pypet\/blob\/97ad3e80d46dbdea02deeb98ea41f05a19565826\/pypet\/utils\/explore.py#L66-L108"}
{"repo_name":"SmokinCaterpillar\/pypet","method_name":"_change_logging_kwargs","method_code":"def _change_logging_kwargs(kwargs):\n    \"\"\"\"\"\"\n    log_levels = kwargs.pop('log_level', None)\n    log_folder = kwargs.pop('log_folder', 'logs')\n    logger_names = kwargs.pop('logger_names', '')\n    if log_levels is None:\n        log_levels = kwargs.pop('log_levels', logging.INFO)\n    log_multiproc = kwargs.pop('log_multiproc', True)\n\n    if not isinstance(logger_names, (tuple, list)):\n        logger_names = [logger_names]\n    if not isinstance(log_levels, (tuple, list)):\n        log_levels = [log_levels]\n    if len(log_levels) == 1:\n        log_levels = [log_levels[0] for _ in logger_names]\n\n    \n    dictionary = copy.deepcopy(LOGGING_DICT)\n    prefixes = ['']\n    if not log_multiproc:\n        for key in list(dictionary.keys()):\n            if key.startswith('multiproc_'):\n                del dictionary[key]\n    else:\n        prefixes.append('multiproc_')\n\n    \n    for prefix in prefixes:\n        for handler_dict in dictionary[prefix + 'handlers'].values():\n            if 'filename' in handler_dict:\n                filename = os.path.join(log_folder, handler_dict['filename'])\n                filename = os.path.normpath(filename)\n                handler_dict['filename'] = filename\n        dictionary[prefix + 'loggers'] = {}\n        logger_dict = dictionary[prefix + 'loggers']\n        for idx, logger_name in enumerate(logger_names):\n            logger_dict[logger_name] = {\n                'level': log_levels[idx],\n                'handlers': list(dictionary[prefix + 'handlers'].keys())\n            }\n\n    kwargs['log_config'] = dictionary","method_summary":"Helper function to turn the simple logging kwargs into a `log_config`.","original_method_code":"def _change_logging_kwargs(kwargs):\n    \"\"\" Helper function to turn the simple logging kwargs into a `log_config`.\"\"\"\n    log_levels = kwargs.pop('log_level', None)\n    log_folder = kwargs.pop('log_folder', 'logs')\n    logger_names = kwargs.pop('logger_names', '')\n    if log_levels is None:\n        log_levels = kwargs.pop('log_levels', logging.INFO)\n    log_multiproc = kwargs.pop('log_multiproc', True)\n\n    if not isinstance(logger_names, (tuple, list)):\n        logger_names = [logger_names]\n    if not isinstance(log_levels, (tuple, list)):\n        log_levels = [log_levels]\n    if len(log_levels) == 1:\n        log_levels = [log_levels[0] for _ in logger_names]\n\n    # We don't want to manipulate the original dictionary\n    dictionary = copy.deepcopy(LOGGING_DICT)\n    prefixes = ['']\n    if not log_multiproc:\n        for key in list(dictionary.keys()):\n            if key.startswith('multiproc_'):\n                del dictionary[key]\n    else:\n        prefixes.append('multiproc_')\n\n    # Add all handlers to all loggers\n    for prefix in prefixes:\n        for handler_dict in dictionary[prefix + 'handlers'].values():\n            if 'filename' in handler_dict:\n                filename = os.path.join(log_folder, handler_dict['filename'])\n                filename = os.path.normpath(filename)\n                handler_dict['filename'] = filename\n        dictionary[prefix + 'loggers'] = {}\n        logger_dict = dictionary[prefix + 'loggers']\n        for idx, logger_name in enumerate(logger_names):\n            logger_dict[logger_name] = {\n                'level': log_levels[idx],\n                'handlers': list(dictionary[prefix + 'handlers'].keys())\n            }\n\n    kwargs['log_config'] = dictionary","method_path":"https:\/\/github.com\/SmokinCaterpillar\/pypet\/blob\/97ad3e80d46dbdea02deeb98ea41f05a19565826\/pypet\/pypetlogging.py#L105-L146"}
{"repo_name":"SmokinCaterpillar\/pypet","method_name":"simple_logging_config","method_code":"def simple_logging_config(func):\n    \"\"\"\"\"\"\n\n    @functools.wraps(func)\n    def new_func(self, *args, **kwargs):\n        if use_simple_logging(kwargs):\n            if 'log_config' in kwargs:\n                raise ValueError('Please do not specify `log_config` '\n                                 'if you want to use the simple '\n                                 'way of providing logging configuration '\n                                 '(i.e using `log_folder`, `logger_names` and\/or `log_levels`).')\n            _change_logging_kwargs(kwargs)\n\n        return func(self, *args, **kwargs)\n\n    return new_func","method_summary":"Decorator to allow a simple logging configuration. This encompasses giving a `log_folder`, `logger_names` as well as `log_levels`.","original_method_code":"def simple_logging_config(func):\n    \"\"\"Decorator to allow a simple logging configuration.\n\n    This encompasses giving a `log_folder`, `logger_names` as well as `log_levels`.\n\n    \"\"\"\n\n    @functools.wraps(func)\n    def new_func(self, *args, **kwargs):\n        if use_simple_logging(kwargs):\n            if 'log_config' in kwargs:\n                raise ValueError('Please do not specify `log_config` '\n                                 'if you want to use the simple '\n                                 'way of providing logging configuration '\n                                 '(i.e using `log_folder`, `logger_names` and\/or `log_levels`).')\n            _change_logging_kwargs(kwargs)\n\n        return func(self, *args, **kwargs)\n\n    return new_func","method_path":"https:\/\/github.com\/SmokinCaterpillar\/pypet\/blob\/97ad3e80d46dbdea02deeb98ea41f05a19565826\/pypet\/pypetlogging.py#L155-L174"}
{"repo_name":"SmokinCaterpillar\/pypet","method_name":"try_make_dirs","method_code":"def try_make_dirs(filename):\n    \"\"\"\"\"\"\n    try:\n        dirname = os.path.dirname(os.path.normpath(filename))\n        racedirs(dirname)\n    except Exception as exc:\n        sys.stderr.write('ERROR during log config file handling, could not create dirs for '\n                         'filename `%s` because of: %s' % (filename, repr(exc)))","method_summary":"Tries to make directories for a given `filename`. Ignores any error but notifies via stderr.","original_method_code":"def try_make_dirs(filename):\n    \"\"\" Tries to make directories for a given `filename`.\n\n    Ignores any error but notifies via stderr.\n\n    \"\"\"\n    try:\n        dirname = os.path.dirname(os.path.normpath(filename))\n        racedirs(dirname)\n    except Exception as exc:\n        sys.stderr.write('ERROR during log config file handling, could not create dirs for '\n                         'filename `%s` because of: %s' % (filename, repr(exc)))","method_path":"https:\/\/github.com\/SmokinCaterpillar\/pypet\/blob\/97ad3e80d46dbdea02deeb98ea41f05a19565826\/pypet\/pypetlogging.py#L177-L188"}
{"repo_name":"SmokinCaterpillar\/pypet","method_name":"HasLogger._set_logger","method_code":"def _set_logger(self, name=None):\n        \"\"\"\"\"\"\n        if name is None:\n            cls = self.__class__\n            name = '%s.%s' % (cls.__module__, cls.__name__)\n        self._logger = logging.getLogger(name)","method_summary":"Adds a logger with a given `name`. If no name is given, name is constructed as `type(self).__name__`.","original_method_code":"def _set_logger(self, name=None):\n        \"\"\"Adds a logger with a given `name`.\n\n        If no name is given, name is constructed as\n        `type(self).__name__`.\n\n        \"\"\"\n        if name is None:\n            cls = self.__class__\n            name = '%s.%s' % (cls.__module__, cls.__name__)\n        self._logger = logging.getLogger(name)","method_path":"https:\/\/github.com\/SmokinCaterpillar\/pypet\/blob\/97ad3e80d46dbdea02deeb98ea41f05a19565826\/pypet\/pypetlogging.py#L311-L321"}
{"repo_name":"SmokinCaterpillar\/pypet","method_name":"LoggingManager.extract_replacements","method_code":"def extract_replacements(self, trajectory):\n        \"\"\"\"\"\"\n        self.env_name = trajectory.v_environment_name\n        self.traj_name = trajectory.v_name\n        self.set_name =  trajectory.f_wildcard('$set')\n        self.run_name = trajectory.f_wildcard('$')","method_summary":"Extracts the wildcards and file replacements from the `trajectory`","original_method_code":"def extract_replacements(self, trajectory):\n        \"\"\"Extracts the wildcards and file replacements from the `trajectory`\"\"\"\n        self.env_name = trajectory.v_environment_name\n        self.traj_name = trajectory.v_name\n        self.set_name =  trajectory.f_wildcard('$set')\n        self.run_name = trajectory.f_wildcard('$')","method_path":"https:\/\/github.com\/SmokinCaterpillar\/pypet\/blob\/97ad3e80d46dbdea02deeb98ea41f05a19565826\/pypet\/pypetlogging.py#L359-L364"}
{"repo_name":"SmokinCaterpillar\/pypet","method_name":"LoggingManager.show_progress","method_code":"def show_progress(self, n, total_runs):\n        \"\"\"\"\"\"\n        if self.report_progress:\n            percentage, logger_name, log_level = self.report_progress\n            if logger_name == 'print':\n                logger = 'print'\n            else:\n                logger = logging.getLogger(logger_name)\n\n            if n == -1:\n                \n                digits = int(math.log10(total_runs + 0.1)) + 1\n                self._format_string = 'PROGRESS: Finished %' + '%d' % digits + 'd\/%d runs '\n\n            fmt_string = self._format_string % (n + 1, total_runs) + '%s'\n            reprint = log_level == 0\n            progressbar(n, total_runs, percentage_step=percentage,\n                        logger=logger, log_level=log_level,\n                        fmt_string=fmt_string, reprint=reprint)","method_summary":"Displays a progressbar","original_method_code":"def show_progress(self, n, total_runs):\n        \"\"\"Displays a progressbar\"\"\"\n        if self.report_progress:\n            percentage, logger_name, log_level = self.report_progress\n            if logger_name == 'print':\n                logger = 'print'\n            else:\n                logger = logging.getLogger(logger_name)\n\n            if n == -1:\n                # Compute the number of digits and avoid log10(0)\n                digits = int(math.log10(total_runs + 0.1)) + 1\n                self._format_string = 'PROGRESS: Finished %' + '%d' % digits + 'd\/%d runs '\n\n            fmt_string = self._format_string % (n + 1, total_runs) + '%s'\n            reprint = log_level == 0\n            progressbar(n, total_runs, percentage_step=percentage,\n                        logger=logger, log_level=log_level,\n                        fmt_string=fmt_string, reprint=reprint)","method_path":"https:\/\/github.com\/SmokinCaterpillar\/pypet\/blob\/97ad3e80d46dbdea02deeb98ea41f05a19565826\/pypet\/pypetlogging.py#L375-L393"}
{"repo_name":"SmokinCaterpillar\/pypet","method_name":"LoggingManager._check_and_replace_parser_args","method_code":"def _check_and_replace_parser_args(parser, section, option, rename_func, make_dirs=True):\n        \"\"\"\"\"\"\n        args = parser.get(section, option, raw=True)\n        strings = get_strings(args)\n        replace = False\n        for string in strings:\n            isfilename = any(x in string for x in FILENAME_INDICATORS)\n            if isfilename:\n                newstring = rename_func(string)\n                if make_dirs:\n                    try_make_dirs(newstring)\n                \n                raw_string = string.replace('\\\\', '\\\\\\\\')\n                raw_newstring = newstring.replace('\\\\', '\\\\\\\\')\n                args = args.replace(raw_string, raw_newstring)\n                replace = True\n        if replace:\n            parser.set(section, option, args)","method_summary":"Searches for parser settings that define filenames. If such settings are found, they are renamed according to the wildcard rules. Moreover, it is also tried to create the corresponding folders.","original_method_code":"def _check_and_replace_parser_args(parser, section, option, rename_func, make_dirs=True):\n        \"\"\" Searches for parser settings that define filenames.\n\n        If such settings are found, they are renamed according to the wildcard\n        rules. Moreover, it is also tried to create the corresponding folders.\n\n        :param parser:  A config parser\n        :param section: A config section\n        :param option: The section option\n        :param rename_func: A function to rename found files\n        :param make_dirs: If the directories of the file should be created.\n\n        \"\"\"\n        args = parser.get(section, option, raw=True)\n        strings = get_strings(args)\n        replace = False\n        for string in strings:\n            isfilename = any(x in string for x in FILENAME_INDICATORS)\n            if isfilename:\n                newstring = rename_func(string)\n                if make_dirs:\n                    try_make_dirs(newstring)\n                # To work with windows path specifications we need this replacement:\n                raw_string = string.replace('\\\\', '\\\\\\\\')\n                raw_newstring = newstring.replace('\\\\', '\\\\\\\\')\n                args = args.replace(raw_string, raw_newstring)\n                replace = True\n        if replace:\n            parser.set(section, option, args)","method_path":"https:\/\/github.com\/SmokinCaterpillar\/pypet\/blob\/97ad3e80d46dbdea02deeb98ea41f05a19565826\/pypet\/pypetlogging.py#L417-L445"}
{"repo_name":"SmokinCaterpillar\/pypet","method_name":"LoggingManager._parser_to_string_io","method_code":"def _parser_to_string_io(parser):\n        \"\"\"\"\"\"\n        memory_file = StringIO()\n        parser.write(memory_file)\n        memory_file.flush()\n        memory_file.seek(0)\n        return memory_file","method_summary":"Turns a ConfigParser into a StringIO stream.","original_method_code":"def _parser_to_string_io(parser):\n        \"\"\"Turns a ConfigParser into a StringIO stream.\"\"\"\n        memory_file = StringIO()\n        parser.write(memory_file)\n        memory_file.flush()\n        memory_file.seek(0)\n        return memory_file","method_path":"https:\/\/github.com\/SmokinCaterpillar\/pypet\/blob\/97ad3e80d46dbdea02deeb98ea41f05a19565826\/pypet\/pypetlogging.py#L448-L454"}
{"repo_name":"uw-it-aca\/uw-restclients-canvas","method_name":"Enrollments.get_enrollments_for_course","method_code":"def get_enrollments_for_course(self, course_id, params={}):\n        \"\"\"\"\"\"\n        url = COURSES_API.format(course_id) + \"\/enrollments\"\n\n        enrollments = []\n        for datum in self._get_paged_resource(url, params=params):\n            enrollments.append(CanvasEnrollment(data=datum))\n\n        return enrollments","method_summary":"Return a list of all enrollments for the passed course_id.","original_method_code":"def get_enrollments_for_course(self, course_id, params={}):\n        \"\"\"\n        Return a list of all enrollments for the passed course_id.\n\n        https:\/\/canvas.instructure.com\/doc\/api\/enrollments.html#method.enrollments_api.index\n        \"\"\"\n        url = COURSES_API.format(course_id) + \"\/enrollments\"\n\n        enrollments = []\n        for datum in self._get_paged_resource(url, params=params):\n            enrollments.append(CanvasEnrollment(data=datum))\n\n        return enrollments","method_path":"https:\/\/github.com\/uw-it-aca\/uw-restclients-canvas\/blob\/9845faf33d49a8f06908efc22640c001116d6ea2\/uw_canvas\/enrollments.py#L10-L22"}
{"repo_name":"uw-it-aca\/uw-restclients-canvas","method_name":"Enrollments.get_enrollments_for_course_by_sis_id","method_code":"def get_enrollments_for_course_by_sis_id(self, sis_course_id, params={}):\n        \"\"\"\"\"\"\n        return self.get_enrollments_for_course(\n            self._sis_id(sis_course_id, sis_field=\"course\"), params)","method_summary":"Return a list of all enrollments for the passed course sis id.","original_method_code":"def get_enrollments_for_course_by_sis_id(self, sis_course_id, params={}):\n        \"\"\"\n        Return a list of all enrollments for the passed course sis id.\n        \"\"\"\n        return self.get_enrollments_for_course(\n            self._sis_id(sis_course_id, sis_field=\"course\"), params)","method_path":"https:\/\/github.com\/uw-it-aca\/uw-restclients-canvas\/blob\/9845faf33d49a8f06908efc22640c001116d6ea2\/uw_canvas\/enrollments.py#L24-L29"}
{"repo_name":"uw-it-aca\/uw-restclients-canvas","method_name":"Enrollments.get_enrollments_for_section","method_code":"def get_enrollments_for_section(self, section_id, params={}):\n        \"\"\"\"\"\"\n        url = SECTIONS_API.format(section_id) + \"\/enrollments\"\n\n        enrollments = []\n        for datum in self._get_paged_resource(url, params=params):\n            enrollments.append(CanvasEnrollment(data=datum))\n\n        return enrollments","method_summary":"Return a list of all enrollments for the passed section_id.","original_method_code":"def get_enrollments_for_section(self, section_id, params={}):\n        \"\"\"\n        Return a list of all enrollments for the passed section_id.\n\n        https:\/\/canvas.instructure.com\/doc\/api\/enrollments.html#method.enrollments_api.index\n        \"\"\"\n        url = SECTIONS_API.format(section_id) + \"\/enrollments\"\n\n        enrollments = []\n        for datum in self._get_paged_resource(url, params=params):\n            enrollments.append(CanvasEnrollment(data=datum))\n\n        return enrollments","method_path":"https:\/\/github.com\/uw-it-aca\/uw-restclients-canvas\/blob\/9845faf33d49a8f06908efc22640c001116d6ea2\/uw_canvas\/enrollments.py#L31-L43"}
{"repo_name":"uw-it-aca\/uw-restclients-canvas","method_name":"Enrollments.get_enrollments_for_section_by_sis_id","method_code":"def get_enrollments_for_section_by_sis_id(self, sis_section_id, params={}):\n        \"\"\"\"\"\"\n        return self.get_enrollments_for_section(\n            self._sis_id(sis_section_id, sis_field=\"section\"), params)","method_summary":"Return a list of all enrollments for the passed section sis id.","original_method_code":"def get_enrollments_for_section_by_sis_id(self, sis_section_id, params={}):\n        \"\"\"\n        Return a list of all enrollments for the passed section sis id.\n        \"\"\"\n        return self.get_enrollments_for_section(\n            self._sis_id(sis_section_id, sis_field=\"section\"), params)","method_path":"https:\/\/github.com\/uw-it-aca\/uw-restclients-canvas\/blob\/9845faf33d49a8f06908efc22640c001116d6ea2\/uw_canvas\/enrollments.py#L45-L50"}
{"repo_name":"uw-it-aca\/uw-restclients-canvas","method_name":"Enrollments.get_enrollments_for_regid","method_code":"def get_enrollments_for_regid(self, regid, params={},\n                                  include_courses=True):\n        \"\"\"\"\"\"\n        sis_user_id = self._sis_id(regid, sis_field=\"user\")\n        url = USERS_API.format(sis_user_id) + \"\/enrollments\"\n\n        courses = Courses() if include_courses else None\n\n        enrollments = []\n        for datum in self._get_paged_resource(url, params=params):\n            enrollment = CanvasEnrollment(data=datum)\n            if include_courses:\n                course_id = datum[\"course_id\"]\n                course = courses.get_course(course_id)\n\n                if course.sis_course_id is not None:\n                    enrollment.course = course\n                    \n                    \n                    enrollment.course_url = course.course_url\n                    enrollment.course_name = course.name\n                    enrollment.sis_course_id = course.sis_course_id\n            else:\n                enrollment.course_url = re.sub(\n                    r'\/users\/\\d+$', '', enrollment.html_url)\n\n            enrollments.append(enrollment)\n        return enrollments","method_summary":"Return a list of enrollments for the passed user regid.","original_method_code":"def get_enrollments_for_regid(self, regid, params={},\n                                  include_courses=True):\n        \"\"\"\n        Return a list of enrollments for the passed user regid.\n\n        https:\/\/canvas.instructure.com\/doc\/api\/enrollments.html#method.enrollments_api.index\n        \"\"\"\n        sis_user_id = self._sis_id(regid, sis_field=\"user\")\n        url = USERS_API.format(sis_user_id) + \"\/enrollments\"\n\n        courses = Courses() if include_courses else None\n\n        enrollments = []\n        for datum in self._get_paged_resource(url, params=params):\n            enrollment = CanvasEnrollment(data=datum)\n            if include_courses:\n                course_id = datum[\"course_id\"]\n                course = courses.get_course(course_id)\n\n                if course.sis_course_id is not None:\n                    enrollment.course = course\n                    # the following 3 lines are not removed\n                    # to be backward compatible.\n                    enrollment.course_url = course.course_url\n                    enrollment.course_name = course.name\n                    enrollment.sis_course_id = course.sis_course_id\n            else:\n                enrollment.course_url = re.sub(\n                    r'\/users\/\\d+$', '', enrollment.html_url)\n\n            enrollments.append(enrollment)\n        return enrollments","method_path":"https:\/\/github.com\/uw-it-aca\/uw-restclients-canvas\/blob\/9845faf33d49a8f06908efc22640c001116d6ea2\/uw_canvas\/enrollments.py#L52-L83"}
{"repo_name":"uw-it-aca\/uw-restclients-canvas","method_name":"Enrollments.enroll_user","method_code":"def enroll_user(self, course_id, user_id, enrollment_type, params=None):\n        \"\"\"\"\"\"\n        url = COURSES_API.format(course_id) + \"\/enrollments\"\n\n        if not params:\n            params = {}\n\n        params[\"user_id\"] = user_id\n        params[\"type\"] = enrollment_type\n\n        data = self._post_resource(url, {\"enrollment\": params})\n        return CanvasEnrollment(data=data)","method_summary":"Enroll a user into a course.","original_method_code":"def enroll_user(self, course_id, user_id, enrollment_type, params=None):\n        \"\"\"\n        Enroll a user into a course.\n\n        https:\/\/canvas.instructure.com\/doc\/api\/enrollments.html#method.enrollments_api.create\n        \"\"\"\n        url = COURSES_API.format(course_id) + \"\/enrollments\"\n\n        if not params:\n            params = {}\n\n        params[\"user_id\"] = user_id\n        params[\"type\"] = enrollment_type\n\n        data = self._post_resource(url, {\"enrollment\": params})\n        return CanvasEnrollment(data=data)","method_path":"https:\/\/github.com\/uw-it-aca\/uw-restclients-canvas\/blob\/9845faf33d49a8f06908efc22640c001116d6ea2\/uw_canvas\/enrollments.py#L85-L100"}
{"repo_name":"uw-it-aca\/uw-restclients-canvas","method_name":"Roles.get_roles_in_account","method_code":"def get_roles_in_account(self, account_id, params={}):\n        \"\"\"\"\"\"\n        url = ACCOUNTS_API.format(account_id) + \"\/roles\"\n\n        roles = []\n        for datum in self._get_resource(url, params=params):\n            roles.append(CanvasRole(data=datum))\n        return roles","method_summary":"List the roles for an account, for the passed Canvas account ID.","original_method_code":"def get_roles_in_account(self, account_id, params={}):\n        \"\"\"\n        List the roles for an account, for the passed Canvas account ID.\n\n        https:\/\/canvas.instructure.com\/doc\/api\/roles.html#method.role_overrides.api_index\n        \"\"\"\n        url = ACCOUNTS_API.format(account_id) + \"\/roles\"\n\n        roles = []\n        for datum in self._get_resource(url, params=params):\n            roles.append(CanvasRole(data=datum))\n        return roles","method_path":"https:\/\/github.com\/uw-it-aca\/uw-restclients-canvas\/blob\/9845faf33d49a8f06908efc22640c001116d6ea2\/uw_canvas\/roles.py#L8-L19"}
{"repo_name":"uw-it-aca\/uw-restclients-canvas","method_name":"Roles.get_roles_by_account_sis_id","method_code":"def get_roles_by_account_sis_id(self, account_sis_id, params={}):\n        \"\"\"\"\"\"\n        return self.get_roles_in_account(self._sis_id(account_sis_id,\n                                                      sis_field=\"account\"),\n                                         params)","method_summary":"List the roles for an account, for the passed account SIS ID.","original_method_code":"def get_roles_by_account_sis_id(self, account_sis_id, params={}):\n        \"\"\"\n        List the roles for an account, for the passed account SIS ID.\n        \"\"\"\n        return self.get_roles_in_account(self._sis_id(account_sis_id,\n                                                      sis_field=\"account\"),\n                                         params)","method_path":"https:\/\/github.com\/uw-it-aca\/uw-restclients-canvas\/blob\/9845faf33d49a8f06908efc22640c001116d6ea2\/uw_canvas\/roles.py#L21-L27"}
{"repo_name":"uw-it-aca\/uw-restclients-canvas","method_name":"Roles.get_effective_course_roles_in_account","method_code":"def get_effective_course_roles_in_account(self, account_id):\n        \"\"\"\"\"\"\n        course_roles = []\n        params = {\"show_inherited\": \"1\"}\n        for role in self.get_roles_in_account(account_id, params):\n            if role.base_role_type != \"AccountMembership\":\n                course_roles.append(role)\n        return course_roles","method_summary":"List all course roles available to an account, for the passed Canvas account ID, including course roles inherited from parent accounts.","original_method_code":"def get_effective_course_roles_in_account(self, account_id):\n        \"\"\"\n        List all course roles available to an account, for the passed Canvas\n        account ID, including course roles inherited from parent accounts.\n        \"\"\"\n        course_roles = []\n        params = {\"show_inherited\": \"1\"}\n        for role in self.get_roles_in_account(account_id, params):\n            if role.base_role_type != \"AccountMembership\":\n                course_roles.append(role)\n        return course_roles","method_path":"https:\/\/github.com\/uw-it-aca\/uw-restclients-canvas\/blob\/9845faf33d49a8f06908efc22640c001116d6ea2\/uw_canvas\/roles.py#L29-L39"}
{"repo_name":"uw-it-aca\/uw-restclients-canvas","method_name":"Roles.get_role","method_code":"def get_role(self, account_id, role_id):\n        \"\"\"\"\"\"\n        url = ACCOUNTS_API.format(account_id) + \"\/roles\/{}\".format(role_id)\n        return CanvasRole(data=self._get_resource(url))","method_summary":"Get information about a single role, for the passed Canvas account ID.","original_method_code":"def get_role(self, account_id, role_id):\n        \"\"\"\n        Get information about a single role, for the passed Canvas account ID.\n\n        https:\/\/canvas.instructure.com\/doc\/api\/roles.html#method.role_overrides.show\n        \"\"\"\n        url = ACCOUNTS_API.format(account_id) + \"\/roles\/{}\".format(role_id)\n        return CanvasRole(data=self._get_resource(url))","method_path":"https:\/\/github.com\/uw-it-aca\/uw-restclients-canvas\/blob\/9845faf33d49a8f06908efc22640c001116d6ea2\/uw_canvas\/roles.py#L41-L48"}
{"repo_name":"chrisrink10\/basilisp","method_name":"vector","method_code":"def vector(members: Iterable[T], meta: Optional[IPersistentMap] = None) -> Vector[T]:\n    \"\"\"\"\"\"\n    return Vector(pvector(members), meta=meta)","method_summary":"Creates a new vector.","original_method_code":"def vector(members: Iterable[T], meta: Optional[IPersistentMap] = None) -> Vector[T]:\n    \"\"\"Creates a new vector.\"\"\"\n    return Vector(pvector(members), meta=meta)","method_path":"https:\/\/github.com\/chrisrink10\/basilisp\/blob\/3d82670ee218ec64eb066289c82766d14d18cc92\/src\/basilisp\/lang\/vector.py#L98-L100"}
{"repo_name":"chrisrink10\/basilisp","method_name":"v","method_code":"def v(*members: T, meta: Optional[IPersistentMap] = None) -> Vector[T]:\n    \"\"\"\"\"\"\n    return Vector(pvector(members), meta=meta)","method_summary":"Creates a new vector from members.","original_method_code":"def v(*members: T, meta: Optional[IPersistentMap] = None) -> Vector[T]:\n    \"\"\"Creates a new vector from members.\"\"\"\n    return Vector(pvector(members), meta=meta)","method_path":"https:\/\/github.com\/chrisrink10\/basilisp\/blob\/3d82670ee218ec64eb066289c82766d14d18cc92\/src\/basilisp\/lang\/vector.py#L103-L105"}
{"repo_name":"chrisrink10\/basilisp","method_name":"eval_file","method_code":"def eval_file(filename: str, ctx: compiler.CompilerContext, module: types.ModuleType):\n    \"\"\"\"\"\"\n    last = None\n    for form in reader.read_file(filename, resolver=runtime.resolve_alias):\n        last = compiler.compile_and_exec_form(form, ctx, module)\n    return last","method_summary":"Evaluate a file with the given name into a Python module AST node.","original_method_code":"def eval_file(filename: str, ctx: compiler.CompilerContext, module: types.ModuleType):\n    \"\"\"Evaluate a file with the given name into a Python module AST node.\"\"\"\n    last = None\n    for form in reader.read_file(filename, resolver=runtime.resolve_alias):\n        last = compiler.compile_and_exec_form(form, ctx, module)\n    return last","method_path":"https:\/\/github.com\/chrisrink10\/basilisp\/blob\/3d82670ee218ec64eb066289c82766d14d18cc92\/src\/basilisp\/cli.py#L55-L60"}
{"repo_name":"chrisrink10\/basilisp","method_name":"eval_stream","method_code":"def eval_stream(stream, ctx: compiler.CompilerContext, module: types.ModuleType):\n    \"\"\"\"\"\"\n    last = None\n    for form in reader.read(stream, resolver=runtime.resolve_alias):\n        last = compiler.compile_and_exec_form(form, ctx, module)\n    return last","method_summary":"Evaluate the forms in stdin into a Python module AST node.","original_method_code":"def eval_stream(stream, ctx: compiler.CompilerContext, module: types.ModuleType):\n    \"\"\"Evaluate the forms in stdin into a Python module AST node.\"\"\"\n    last = None\n    for form in reader.read(stream, resolver=runtime.resolve_alias):\n        last = compiler.compile_and_exec_form(form, ctx, module)\n    return last","method_path":"https:\/\/github.com\/chrisrink10\/basilisp\/blob\/3d82670ee218ec64eb066289c82766d14d18cc92\/src\/basilisp\/cli.py#L63-L68"}
{"repo_name":"chrisrink10\/basilisp","method_name":"eval_str","method_code":"def eval_str(s: str, ctx: compiler.CompilerContext, module: types.ModuleType, eof: Any):\n    \"\"\"\"\"\"\n    last = eof\n    for form in reader.read_str(s, resolver=runtime.resolve_alias, eof=eof):\n        last = compiler.compile_and_exec_form(form, ctx, module)\n    return last","method_summary":"Evaluate the forms in a string into a Python module AST node.","original_method_code":"def eval_str(s: str, ctx: compiler.CompilerContext, module: types.ModuleType, eof: Any):\n    \"\"\"Evaluate the forms in a string into a Python module AST node.\"\"\"\n    last = eof\n    for form in reader.read_str(s, resolver=runtime.resolve_alias, eof=eof):\n        last = compiler.compile_and_exec_form(form, ctx, module)\n    return last","method_path":"https:\/\/github.com\/chrisrink10\/basilisp\/blob\/3d82670ee218ec64eb066289c82766d14d18cc92\/src\/basilisp\/cli.py#L71-L76"}
{"repo_name":"chrisrink10\/basilisp","method_name":"bootstrap_repl","method_code":"def bootstrap_repl(which_ns: str) -> types.ModuleType:\n    \"\"\"\"\"\"\n    repl_ns = runtime.Namespace.get_or_create(sym.symbol(\"basilisp.repl\"))\n    ns = runtime.Namespace.get_or_create(sym.symbol(which_ns))\n    repl_module = importlib.import_module(\"basilisp.repl\")\n    ns.add_alias(sym.symbol(\"basilisp.repl\"), repl_ns)\n    ns.refer_all(repl_ns)\n    return repl_module","method_summary":"Bootstrap the REPL with a few useful vars and returned the bootstrapped module so it's functions can be used by the REPL command.","original_method_code":"def bootstrap_repl(which_ns: str) -> types.ModuleType:\n    \"\"\"Bootstrap the REPL with a few useful vars and returned the\n    bootstrapped module so it's functions can be used by the REPL\n    command.\"\"\"\n    repl_ns = runtime.Namespace.get_or_create(sym.symbol(\"basilisp.repl\"))\n    ns = runtime.Namespace.get_or_create(sym.symbol(which_ns))\n    repl_module = importlib.import_module(\"basilisp.repl\")\n    ns.add_alias(sym.symbol(\"basilisp.repl\"), repl_ns)\n    ns.refer_all(repl_ns)\n    return repl_module","method_path":"https:\/\/github.com\/chrisrink10\/basilisp\/blob\/3d82670ee218ec64eb066289c82766d14d18cc92\/src\/basilisp\/cli.py#L79-L88"}
{"repo_name":"chrisrink10\/basilisp","method_name":"run","method_code":"def run(  \n    file_or_code,\n    code,\n    in_ns,\n    use_var_indirection,\n    warn_on_shadowed_name,\n    warn_on_shadowed_var,\n    warn_on_var_indirection,\n):\n    \"\"\"\"\"\"\n    basilisp.init()\n    ctx = compiler.CompilerContext(\n        filename=CLI_INPUT_FILE_PATH\n        if code\n        else (\n            STDIN_INPUT_FILE_PATH if file_or_code == STDIN_FILE_NAME else file_or_code\n        ),\n        opts={\n            compiler.WARN_ON_SHADOWED_NAME: warn_on_shadowed_name,\n            compiler.WARN_ON_SHADOWED_VAR: warn_on_shadowed_var,\n            compiler.USE_VAR_INDIRECTION: use_var_indirection,\n            compiler.WARN_ON_VAR_INDIRECTION: warn_on_var_indirection,\n        },\n    )\n    eof = object()\n\n    with runtime.ns_bindings(in_ns) as ns:\n        if code:\n            print(runtime.lrepr(eval_str(file_or_code, ctx, ns.module, eof)))\n        elif file_or_code == STDIN_FILE_NAME:\n            print(\n                runtime.lrepr(\n                    eval_stream(click.get_text_stream(\"stdin\"), ctx, ns.module)\n                )\n            )\n        else:\n            print(runtime.lrepr(eval_file(file_or_code, ctx, ns.module)))","method_summary":"Run a Basilisp script or a line of code, if it is provided.","original_method_code":"def run(  # pylint: disable=too-many-arguments\n    file_or_code,\n    code,\n    in_ns,\n    use_var_indirection,\n    warn_on_shadowed_name,\n    warn_on_shadowed_var,\n    warn_on_var_indirection,\n):\n    \"\"\"Run a Basilisp script or a line of code, if it is provided.\"\"\"\n    basilisp.init()\n    ctx = compiler.CompilerContext(\n        filename=CLI_INPUT_FILE_PATH\n        if code\n        else (\n            STDIN_INPUT_FILE_PATH if file_or_code == STDIN_FILE_NAME else file_or_code\n        ),\n        opts={\n            compiler.WARN_ON_SHADOWED_NAME: warn_on_shadowed_name,\n            compiler.WARN_ON_SHADOWED_VAR: warn_on_shadowed_var,\n            compiler.USE_VAR_INDIRECTION: use_var_indirection,\n            compiler.WARN_ON_VAR_INDIRECTION: warn_on_var_indirection,\n        },\n    )\n    eof = object()\n\n    with runtime.ns_bindings(in_ns) as ns:\n        if code:\n            print(runtime.lrepr(eval_str(file_or_code, ctx, ns.module, eof)))\n        elif file_or_code == STDIN_FILE_NAME:\n            print(\n                runtime.lrepr(\n                    eval_stream(click.get_text_stream(\"stdin\"), ctx, ns.module)\n                )\n            )\n        else:\n            print(runtime.lrepr(eval_file(file_or_code, ctx, ns.module)))","method_path":"https:\/\/github.com\/chrisrink10\/basilisp\/blob\/3d82670ee218ec64eb066289c82766d14d18cc92\/src\/basilisp\/cli.py#L214-L250"}
{"repo_name":"chrisrink10\/basilisp","method_name":"multifn","method_code":"def multifn(dispatch: DispatchFunction, default=None) -> MultiFunction[T]:\n    \"\"\"\"\"\"\n    name = sym.symbol(dispatch.__qualname__, ns=dispatch.__module__)\n    return MultiFunction(name, dispatch, default)","method_summary":"Decorator function which can be used to make Python multi functions.","original_method_code":"def multifn(dispatch: DispatchFunction, default=None) -> MultiFunction[T]:\n    \"\"\"Decorator function which can be used to make Python multi functions.\"\"\"\n    name = sym.symbol(dispatch.__qualname__, ns=dispatch.__module__)\n    return MultiFunction(name, dispatch, default)","method_path":"https:\/\/github.com\/chrisrink10\/basilisp\/blob\/3d82670ee218ec64eb066289c82766d14d18cc92\/src\/basilisp\/lang\/multifn.py#L81-L84"}
{"repo_name":"chrisrink10\/basilisp","method_name":"MultiFunction.__add_method","method_code":"def __add_method(m: lmap.Map, key: T, method: Method) -> lmap.Map:\n        \"\"\"\"\"\"\n        return m.assoc(key, method)","method_summary":"Swap the methods atom to include method with key.","original_method_code":"def __add_method(m: lmap.Map, key: T, method: Method) -> lmap.Map:\n        \"\"\"Swap the methods atom to include method with key.\"\"\"\n        return m.assoc(key, method)","method_path":"https:\/\/github.com\/chrisrink10\/basilisp\/blob\/3d82670ee218ec64eb066289c82766d14d18cc92\/src\/basilisp\/lang\/multifn.py#L37-L39"}
{"repo_name":"chrisrink10\/basilisp","method_name":"MultiFunction.add_method","method_code":"def add_method(self, key: T, method: Method) -> None:\n        \"\"\"\"\"\"\n        self._methods.swap(MultiFunction.__add_method, key, method)","method_summary":"Add a new method to this function which will respond for key returned from the dispatch function.","original_method_code":"def add_method(self, key: T, method: Method) -> None:\n        \"\"\"Add a new method to this function which will respond for\n        key returned from the dispatch function.\"\"\"\n        self._methods.swap(MultiFunction.__add_method, key, method)","method_path":"https:\/\/github.com\/chrisrink10\/basilisp\/blob\/3d82670ee218ec64eb066289c82766d14d18cc92\/src\/basilisp\/lang\/multifn.py#L41-L44"}
{"repo_name":"pmacosta\/peng","method_name":"incfile","method_code":"def incfile(fname, fpointer, lrange=\"1,6-\", sdir=None):\n    r\"\"\"\"\"\"\n    \n    file_dir = (\n        sdir\n        if sdir\n        else os.environ.get(\"TRACER_DIR\", os.path.abspath(os.path.dirname(__file__)))\n    )\n    fname = os.path.join(file_dir, fname)\n    with open(fname) as fobj:\n        lines = fobj.readlines()\n    \n    tokens = [item.strip() for item in lrange.split(\",\")]\n    inc_lines = []\n    for token in tokens:\n        if \"-\" in token:\n            subtokens = token.split(\"-\")\n            lmin, lmax = (\n                int(subtokens[0]),\n                int(subtokens[1]) if subtokens[1] else len(lines),\n            )\n            for num in range(lmin, lmax + 1):\n                inc_lines.append(num)\n        else:\n            inc_lines.append(int(token))\n    \n    fpointer(\".. code-block:: python\\n\")\n    fpointer(\"\\n\")\n    for num, line in enumerate(lines):\n        if num + 1 in inc_lines:\n            fpointer(\"    \" + line.replace(\"\\t\", \"    \") if line.strip() else \"\\n\")\n    fpointer(\"\\n\")","method_summary":"r\"\"\" Include a Python source file in a docstring formatted in reStructuredText.","original_method_code":"def incfile(fname, fpointer, lrange=\"1,6-\", sdir=None):\n    r\"\"\"\n    Include a Python source file in a docstring formatted in reStructuredText.\n\n    :param fname: File name, relative to environment variable\n                  :bash:`${TRACER_DIR}`\n    :type  fname: string\n\n    :param fpointer: Output function pointer. Normally is :code:`cog.out` but\n                     :code:`print` or other functions can be used for\n                     debugging\n    :type  fpointer: function object\n\n    :param lrange: Line range to include, similar to Sphinx\n                   `literalinclude <http:\/\/sphinx-doc.org\/markup\/code.html\n                   #directive-literalinclude>`_ directive\n    :type  lrange: string\n\n    :param sdir: Source file directory. If None the :bash:`${TRACER_DIR}`\n                 environment variable is used if it is defined, otherwise\n                 the directory where the :code:`docs.support.incfile` module\n                 is located is used\n    :type  sdir: string\n\n    For example:\n\n    .. code-block:: python\n\n        def func():\n            \\\"\\\"\\\"\n            This is a docstring. This file shows how to use it:\n\n            .. =[=cog\n            .. import docs.support.incfile\n            .. docs.support.incfile.incfile('func_example.py', cog.out)\n            .. =]=\n            .. code-block:: python\n\n                # func_example.py\n                if __name__ == '__main__':\n                    func()\n\n            .. =[=end=]=\n            \\\"\\\"\\\"\n            return 'This is func output'\n    \"\"\"\n    # Read file\n    file_dir = (\n        sdir\n        if sdir\n        else os.environ.get(\"TRACER_DIR\", os.path.abspath(os.path.dirname(__file__)))\n    )\n    fname = os.path.join(file_dir, fname)\n    with open(fname) as fobj:\n        lines = fobj.readlines()\n    # Parse line specification\n    tokens = [item.strip() for item in lrange.split(\",\")]\n    inc_lines = []\n    for token in tokens:\n        if \"-\" in token:\n            subtokens = token.split(\"-\")\n            lmin, lmax = (\n                int(subtokens[0]),\n                int(subtokens[1]) if subtokens[1] else len(lines),\n            )\n            for num in range(lmin, lmax + 1):\n                inc_lines.append(num)\n        else:\n            inc_lines.append(int(token))\n    # Produce output\n    fpointer(\".. code-block:: python\\n\")\n    fpointer(\"\\n\")\n    for num, line in enumerate(lines):\n        if num + 1 in inc_lines:\n            fpointer(\"    \" + line.replace(\"\\t\", \"    \") if line.strip() else \"\\n\")\n    fpointer(\"\\n\")","method_path":"https:\/\/github.com\/pmacosta\/peng\/blob\/976935377adaa3de26fc5677aceb2cdfbd6f93a7\/docs\/support\/incfile.py#L9-L84"}
{"repo_name":"pmacosta\/peng","method_name":"_homogenize_waves","method_code":"def _homogenize_waves(wave_a, wave_b):\n    \"\"\"\"\"\"\n    indep_vector = _get_indep_vector(wave_a, wave_b)\n    dep_vector_a = _interp_dep_vector(wave_a, indep_vector)\n    dep_vector_b = _interp_dep_vector(wave_b, indep_vector)\n    return (indep_vector, dep_vector_a, dep_vector_b)","method_summary":"Generate combined independent variable vector. The combination is from two waveforms and the (possibly interpolated) dependent variable vectors of these two waveforms","original_method_code":"def _homogenize_waves(wave_a, wave_b):\n    \"\"\"\n    Generate combined independent variable vector.\n\n    The combination is from two waveforms and the (possibly interpolated)\n    dependent variable vectors of these two waveforms\n    \"\"\"\n    indep_vector = _get_indep_vector(wave_a, wave_b)\n    dep_vector_a = _interp_dep_vector(wave_a, indep_vector)\n    dep_vector_b = _interp_dep_vector(wave_b, indep_vector)\n    return (indep_vector, dep_vector_a, dep_vector_b)","method_path":"https:\/\/github.com\/pmacosta\/peng\/blob\/976935377adaa3de26fc5677aceb2cdfbd6f93a7\/peng\/wave_core.py#L87-L97"}
{"repo_name":"pmacosta\/peng","method_name":"_interp_dep_vector","method_code":"def _interp_dep_vector(wave, indep_vector):\n    \"\"\"\"\"\"\n    dep_vector_is_int = wave.dep_vector.dtype.name.startswith(\"int\")\n    dep_vector_is_complex = wave.dep_vector.dtype.name.startswith(\"complex\")\n    if (wave.interp, wave.indep_scale) == (\"CONTINUOUS\", \"LOG\"):\n        wave_interp_func = scipy.interpolate.interp1d(\n            np.log10(wave.indep_vector), wave.dep_vector\n        )\n        ret = wave_interp_func(np.log10(indep_vector))\n    elif (wave.interp, wave.indep_scale) == (\"CONTINUOUS\", \"LINEAR\"):\n        dep_vector = (\n            wave.dep_vector.astype(np.float64)\n            if not dep_vector_is_complex\n            else wave.dep_vector\n        )\n        wave_interp_func = scipy.interpolate.interp1d(wave.indep_vector, dep_vector)\n        ret = wave_interp_func(indep_vector)\n    else:  \n        wave_interp_func = scipy.interpolate.interp1d(\n            wave.indep_vector, wave.dep_vector, kind=\"zero\"\n        )\n        \n        \n        ret = wave_interp_func(indep_vector)\n        eq_comp = np.all(\n            np.isclose(wave.indep_vector[-1], indep_vector[-1], FP_RTOL, FP_ATOL)\n        )\n        if eq_comp:\n            ret[-1] = wave.dep_vector[-1]\n    round_ret = np.round(ret, 0)\n    return (\n        round_ret.astype(\"int\")\n        if (dep_vector_is_int and np.all(np.isclose(round_ret, ret, FP_RTOL, FP_ATOL)))\n        else ret\n    )","method_summary":"Create new dependent variable vector.","original_method_code":"def _interp_dep_vector(wave, indep_vector):\n    \"\"\"Create new dependent variable vector.\"\"\"\n    dep_vector_is_int = wave.dep_vector.dtype.name.startswith(\"int\")\n    dep_vector_is_complex = wave.dep_vector.dtype.name.startswith(\"complex\")\n    if (wave.interp, wave.indep_scale) == (\"CONTINUOUS\", \"LOG\"):\n        wave_interp_func = scipy.interpolate.interp1d(\n            np.log10(wave.indep_vector), wave.dep_vector\n        )\n        ret = wave_interp_func(np.log10(indep_vector))\n    elif (wave.interp, wave.indep_scale) == (\"CONTINUOUS\", \"LINEAR\"):\n        dep_vector = (\n            wave.dep_vector.astype(np.float64)\n            if not dep_vector_is_complex\n            else wave.dep_vector\n        )\n        wave_interp_func = scipy.interpolate.interp1d(wave.indep_vector, dep_vector)\n        ret = wave_interp_func(indep_vector)\n    else:  # wave.interp == 'STAIRCASE'\n        wave_interp_func = scipy.interpolate.interp1d(\n            wave.indep_vector, wave.dep_vector, kind=\"zero\"\n        )\n        # Interpolator does not return the right value for the last\n        # data point, it gives the previous \"stair\" value\n        ret = wave_interp_func(indep_vector)\n        eq_comp = np.all(\n            np.isclose(wave.indep_vector[-1], indep_vector[-1], FP_RTOL, FP_ATOL)\n        )\n        if eq_comp:\n            ret[-1] = wave.dep_vector[-1]\n    round_ret = np.round(ret, 0)\n    return (\n        round_ret.astype(\"int\")\n        if (dep_vector_is_int and np.all(np.isclose(round_ret, ret, FP_RTOL, FP_ATOL)))\n        else ret\n    )","method_path":"https:\/\/github.com\/pmacosta\/peng\/blob\/976935377adaa3de26fc5677aceb2cdfbd6f93a7\/peng\/wave_core.py#L100-L134"}
{"repo_name":"pmacosta\/peng","method_name":"_get_indep_vector","method_code":"def _get_indep_vector(wave_a, wave_b):\n    \"\"\"\"\"\"\n    exobj = pexdoc.exh.addex(RuntimeError, \"Independent variable ranges do not overlap\")\n    min_bound = max(np.min(wave_a.indep_vector), np.min(wave_b.indep_vector))\n    max_bound = min(np.max(wave_a.indep_vector), np.max(wave_b.indep_vector))\n    exobj(bool(min_bound > max_bound))\n    raw_range = np.unique(np.concatenate((wave_a.indep_vector, wave_b.indep_vector)))\n    return raw_range[np.logical_and(min_bound <= raw_range, raw_range <= max_bound)]","method_summary":"Create new independent variable vector.","original_method_code":"def _get_indep_vector(wave_a, wave_b):\n    \"\"\"Create new independent variable vector.\"\"\"\n    exobj = pexdoc.exh.addex(RuntimeError, \"Independent variable ranges do not overlap\")\n    min_bound = max(np.min(wave_a.indep_vector), np.min(wave_b.indep_vector))\n    max_bound = min(np.max(wave_a.indep_vector), np.max(wave_b.indep_vector))\n    exobj(bool(min_bound > max_bound))\n    raw_range = np.unique(np.concatenate((wave_a.indep_vector, wave_b.indep_vector)))\n    return raw_range[np.logical_and(min_bound <= raw_range, raw_range <= max_bound)]","method_path":"https:\/\/github.com\/pmacosta\/peng\/blob\/976935377adaa3de26fc5677aceb2cdfbd6f93a7\/peng\/wave_core.py#L137-L144"}
{"repo_name":"pmacosta\/peng","method_name":"_verify_compatibility","method_code":"def _verify_compatibility(wave_a, wave_b, check_dep_units=True):\n    \"\"\"\"\"\"\n    exobj = pexdoc.exh.addex(RuntimeError, \"Waveforms are not compatible\")\n    ctuple = (\n        bool(wave_a.indep_scale != wave_b.indep_scale),\n        bool(wave_a.dep_scale != wave_b.dep_scale),\n        bool(wave_a.indep_units != wave_b.indep_units),\n        (bool(wave_a.dep_units != wave_b.dep_units) if check_dep_units else False),\n        bool(wave_a.interp != wave_b.interp),\n    )\n    exobj(any(ctuple))","method_summary":"Verify that two waveforms can be combined with various mathematical functions.","original_method_code":"def _verify_compatibility(wave_a, wave_b, check_dep_units=True):\n    \"\"\"Verify that two waveforms can be combined with various mathematical functions.\"\"\"\n    exobj = pexdoc.exh.addex(RuntimeError, \"Waveforms are not compatible\")\n    ctuple = (\n        bool(wave_a.indep_scale != wave_b.indep_scale),\n        bool(wave_a.dep_scale != wave_b.dep_scale),\n        bool(wave_a.indep_units != wave_b.indep_units),\n        (bool(wave_a.dep_units != wave_b.dep_units) if check_dep_units else False),\n        bool(wave_a.interp != wave_b.interp),\n    )\n    exobj(any(ctuple))","method_path":"https:\/\/github.com\/pmacosta\/peng\/blob\/976935377adaa3de26fc5677aceb2cdfbd6f93a7\/peng\/wave_core.py#L147-L157"}
{"repo_name":"pmacosta\/peng","method_name":"trace_pars","method_code":"def trace_pars(mname):\n    \"\"\"\"\"\"\n    pickle_fname = os.path.join(os.path.dirname(__file__), \"{0}.pkl\".format(mname))\n    ddir = os.path.dirname(os.path.dirname(__file__))\n    moddb_fname = os.path.join(ddir, \"moddb.json\")\n    in_callables_fname = moddb_fname if os.path.exists(moddb_fname) else None\n    out_callables_fname = os.path.join(ddir, \"{0}.json\".format(mname))\n    noption = os.environ.get(\"NOPTION\", None)\n    exclude = [\"_pytest\", \"execnet\"]\n    partuple = collections.namedtuple(\n        \"ParTuple\",\n        [\n            \"pickle_fname\",\n            \"in_callables_fname\",\n            \"out_callables_fname\",\n            \"noption\",\n            \"exclude\",\n        ],\n    )\n    return partuple(\n        pickle_fname, in_callables_fname, out_callables_fname, noption, exclude\n    )","method_summary":"Define trace parameters.","original_method_code":"def trace_pars(mname):\n    \"\"\"Define trace parameters.\"\"\"\n    pickle_fname = os.path.join(os.path.dirname(__file__), \"{0}.pkl\".format(mname))\n    ddir = os.path.dirname(os.path.dirname(__file__))\n    moddb_fname = os.path.join(ddir, \"moddb.json\")\n    in_callables_fname = moddb_fname if os.path.exists(moddb_fname) else None\n    out_callables_fname = os.path.join(ddir, \"{0}.json\".format(mname))\n    noption = os.environ.get(\"NOPTION\", None)\n    exclude = [\"_pytest\", \"execnet\"]\n    partuple = collections.namedtuple(\n        \"ParTuple\",\n        [\n            \"pickle_fname\",\n            \"in_callables_fname\",\n            \"out_callables_fname\",\n            \"noption\",\n            \"exclude\",\n        ],\n    )\n    return partuple(\n        pickle_fname, in_callables_fname, out_callables_fname, noption, exclude\n    )","method_path":"https:\/\/github.com\/pmacosta\/peng\/blob\/976935377adaa3de26fc5677aceb2cdfbd6f93a7\/docs\/support\/trace_support.py#L27-L48"}
{"repo_name":"pmacosta\/peng","method_name":"run_trace","method_code":"def run_trace(\n    mname,\n    fname,\n    module_prefix,\n    callable_names,\n    no_print,\n    module_exclude=None,\n    callable_exclude=None,\n    debug=False,\n):\n    \"\"\"\"\"\"\n    \n    module_exclude = [] if module_exclude is None else module_exclude\n    callable_exclude = [] if callable_exclude is None else callable_exclude\n    par = trace_pars(mname)\n    start_time = datetime.datetime.now()\n    with pexdoc.exdoc.ExDocCxt(\n        exclude=par.exclude + module_exclude,\n        pickle_fname=par.pickle_fname,\n        in_callables_fname=par.in_callables_fname,\n        out_callables_fname=par.out_callables_fname,\n        _no_print=no_print,\n    ) as exdoc_obj:\n        fname = os.path.realpath(\n            os.path.join(\n                os.path.dirname(__file__),\n                \"..\",\n                \"..\",\n                \"tests\",\n                \"test_{0}.py\".format(fname),\n            )\n        )\n        test_cmd = (\n            [\"--color=yes\"]\n            + ([\"-s\", \"-vv\"] if debug else [\"-q\", \"-q\", \"-q\"])\n            + [\"--disable-warnings\"]\n            + [\"-x\"]\n            + ([par.noption] if par.noption else [])\n            + [\"-m \" + mname]\n            + [fname]\n        )\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=PytestWarning)\n            if pytest.main(test_cmd):\n                raise RuntimeError(\"Tracing did not complete successfully\")\n    stop_time = datetime.datetime.now()\n    if not no_print:\n        print(\n            \"Auto-generation of exceptions documentation time: {0}\".format(\n                pmisc.elapsed_time_string(start_time, stop_time)\n            )\n        )\n        for callable_name in callable_names:\n            callable_name = module_prefix + callable_name\n            print(\"\\nCallable: {0}\".format(callable_name))\n            print(exdoc_obj.get_sphinx_doc(callable_name, exclude=callable_exclude))\n            print(\"\\n\")\n    return copy.copy(exdoc_obj)","method_summary":"Run module tracing.","original_method_code":"def run_trace(\n    mname,\n    fname,\n    module_prefix,\n    callable_names,\n    no_print,\n    module_exclude=None,\n    callable_exclude=None,\n    debug=False,\n):\n    \"\"\"Run module tracing.\"\"\"\n    # pylint: disable=R0913\n    module_exclude = [] if module_exclude is None else module_exclude\n    callable_exclude = [] if callable_exclude is None else callable_exclude\n    par = trace_pars(mname)\n    start_time = datetime.datetime.now()\n    with pexdoc.exdoc.ExDocCxt(\n        exclude=par.exclude + module_exclude,\n        pickle_fname=par.pickle_fname,\n        in_callables_fname=par.in_callables_fname,\n        out_callables_fname=par.out_callables_fname,\n        _no_print=no_print,\n    ) as exdoc_obj:\n        fname = os.path.realpath(\n            os.path.join(\n                os.path.dirname(__file__),\n                \"..\",\n                \"..\",\n                \"tests\",\n                \"test_{0}.py\".format(fname),\n            )\n        )\n        test_cmd = (\n            [\"--color=yes\"]\n            + ([\"-s\", \"-vv\"] if debug else [\"-q\", \"-q\", \"-q\"])\n            + [\"--disable-warnings\"]\n            + [\"-x\"]\n            + ([par.noption] if par.noption else [])\n            + [\"-m \" + mname]\n            + [fname]\n        )\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=PytestWarning)\n            if pytest.main(test_cmd):\n                raise RuntimeError(\"Tracing did not complete successfully\")\n    stop_time = datetime.datetime.now()\n    if not no_print:\n        print(\n            \"Auto-generation of exceptions documentation time: {0}\".format(\n                pmisc.elapsed_time_string(start_time, stop_time)\n            )\n        )\n        for callable_name in callable_names:\n            callable_name = module_prefix + callable_name\n            print(\"\\nCallable: {0}\".format(callable_name))\n            print(exdoc_obj.get_sphinx_doc(callable_name, exclude=callable_exclude))\n            print(\"\\n\")\n    return copy.copy(exdoc_obj)","method_path":"https:\/\/github.com\/pmacosta\/peng\/blob\/976935377adaa3de26fc5677aceb2cdfbd6f93a7\/docs\/support\/trace_support.py#L51-L108"}
{"repo_name":"pmacosta\/peng","method_name":"ste","method_code":"def ste(command, nindent, mdir, fpointer):\n    r\"\"\"\"\"\"\n    term_echo(\n        \"${{PMISC_DIR}}{sep}pypkg{sep}{cmd}\".format(sep=os.path.sep, cmd=command),\n        nindent,\n        {\"PMISC_DIR\": mdir},\n        fpointer,\n    )","method_summary":"r\"\"\" Echo terminal output. Print STDOUT resulting from a given Bash shell command (relative to the package :code:`pypkg` directory) formatted in reStructuredText","original_method_code":"def ste(command, nindent, mdir, fpointer):\n    r\"\"\"\n    Echo terminal output.\n\n    Print STDOUT resulting from a given Bash shell command (relative to the\n    package :code:`pypkg` directory) formatted in reStructuredText\n\n    :param command: Bash shell command, relative to\n                    :bash:`${PMISC_DIR}\/pypkg`\n    :type  command: string\n\n    :param nindent: Indentation level\n    :type  nindent: integer\n\n    :param mdir: Module directory\n    :type  mdir: string\n\n    :param fpointer: Output function pointer. Normally is :code:`cog.out` but\n                     :code:`print` or other functions can be used for\n                     debugging\n    :type  fpointer: function object\n\n    For example::\n\n        .. This is a reStructuredText file snippet\n        .. [[[cog\n        .. import os, sys\n        .. from docs.support.term_echo import term_echo\n        .. file_name = sys.modules['docs.support.term_echo'].__file__\n        .. mdir = os.path.realpath(\n        ..     os.path.dirname(\n        ..         os.path.dirname(os.path.dirname(file_name))\n        ..     )\n        .. )\n        .. [[[cog ste('build_docs.py -h', 0, mdir, cog.out) ]]]\n\n        .. code-block:: bash\n\n        $ ${PMISC_DIR}\/pypkg\/build_docs.py -h\n        usage: build_docs.py [-h] [-d DIRECTORY] [-n NUM_CPUS]\n        ...\n\n        .. ]]]\n\n    \"\"\"\n    term_echo(\n        \"${{PMISC_DIR}}{sep}pypkg{sep}{cmd}\".format(sep=os.path.sep, cmd=command),\n        nindent,\n        {\"PMISC_DIR\": mdir},\n        fpointer,\n    )","method_path":"https:\/\/github.com\/pmacosta\/peng\/blob\/976935377adaa3de26fc5677aceb2cdfbd6f93a7\/docs\/support\/term_echo.py#L15-L65"}
{"repo_name":"pmacosta\/peng","method_name":"term_echo","method_code":"def term_echo(command, nindent=0, env=None, fpointer=None, cols=60):\n    \"\"\"\"\"\"\n    \n    \n    \n    os.environ[\"COLUMNS\"] = str(cols)\n    command_int = command\n    if env:\n        for var, repl in env.items():\n            command_int = command_int.replace(\"${\" + var + \"}\", repl)\n    tokens = command_int.split(\" \")\n    \n    \n    if (platform.system().lower() == \"windows\") and (tokens[0].endswith(\".py\")):\n        tokens = [sys.executable] + tokens\n    proc = subprocess.Popen(tokens, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    stdout = proc.communicate()[0]\n    if sys.hexversion >= 0x03000000:\n        stdout = stdout.decode(\"utf-8\")\n    stdout = stdout.split(\"\\n\")\n    indent = nindent * \" \"\n    fpointer(\"\\n\", dedent=False)\n    fpointer(\"{0}.. code-block:: bash\\n\".format(indent), dedent=False)\n    fpointer(\"\\n\", dedent=False)\n    fpointer(\"{0}    $ {1}\\n\".format(indent, command), dedent=False)\n    for line in stdout:\n        if line.strip():\n            fpointer(indent + \"    \" + line.replace(\"\\t\", \"    \") + \"\\n\", dedent=False)\n        else:\n            fpointer(\"\\n\", dedent=False)\n    fpointer(\"\\n\", dedent=False)","method_summary":"Print STDOUT resulting from a Bash shell command formatted in reStructuredText.","original_method_code":"def term_echo(command, nindent=0, env=None, fpointer=None, cols=60):\n    \"\"\"\n    Print STDOUT resulting from a Bash shell command formatted in reStructuredText.\n\n    :param command: Bash shell command\n    :type  command: string\n\n    :param nindent: Indentation level\n    :type  nindent: integer\n\n    :param env: Environment variable replacement dictionary. The Bash\n                command is pre-processed and any environment variable\n                represented in the full notation (:bash:`${...}`) is replaced.\n                The dictionary key is the environment variable name and the\n                dictionary value is the replacement value. For example, if\n                **command** is :code:`'${PYTHON_CMD} -m \"x=5\"'` and **env**\n                is :code:`{'PYTHON_CMD':'python3'}` the actual command issued\n                is :code:`'python3 -m \"x=5\"'`\n    :type  env: dictionary\n\n    :param fpointer: Output function pointer. Normally is :code:`cog.out` but\n                     :code:`print` or other functions can be used for\n                     debugging\n    :type  fpointer: function object\n\n    :param cols: Number of columns of output\n    :type  cols: integer\n    \"\"\"\n    # pylint: disable=R0204\n    # Set argparse width so that output does not need horizontal scroll\n    # bar in narrow windows or displays\n    os.environ[\"COLUMNS\"] = str(cols)\n    command_int = command\n    if env:\n        for var, repl in env.items():\n            command_int = command_int.replace(\"${\" + var + \"}\", repl)\n    tokens = command_int.split(\" \")\n    # Add Python interpreter executable for Python scripts on Windows since\n    # the shebang does not work\n    if (platform.system().lower() == \"windows\") and (tokens[0].endswith(\".py\")):\n        tokens = [sys.executable] + tokens\n    proc = subprocess.Popen(tokens, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    stdout = proc.communicate()[0]\n    if sys.hexversion >= 0x03000000:\n        stdout = stdout.decode(\"utf-8\")\n    stdout = stdout.split(\"\\n\")\n    indent = nindent * \" \"\n    fpointer(\"\\n\", dedent=False)\n    fpointer(\"{0}.. code-block:: bash\\n\".format(indent), dedent=False)\n    fpointer(\"\\n\", dedent=False)\n    fpointer(\"{0}    $ {1}\\n\".format(indent, command), dedent=False)\n    for line in stdout:\n        if line.strip():\n            fpointer(indent + \"    \" + line.replace(\"\\t\", \"    \") + \"\\n\", dedent=False)\n        else:\n            fpointer(\"\\n\", dedent=False)\n    fpointer(\"\\n\", dedent=False)","method_path":"https:\/\/github.com\/pmacosta\/peng\/blob\/976935377adaa3de26fc5677aceb2cdfbd6f93a7\/docs\/support\/term_echo.py#L68-L124"}
{"repo_name":"pmacosta\/peng","method_name":"trace_module","method_code":"def trace_module(no_print=True):\n    \"\"\"\"\"\"\n    mname = \"wave_core\"\n    fname = \"peng\"\n    module_prefix = \"peng.{0}.Waveform.\".format(mname)\n    callable_names = (\"__init__\",)\n    return docs.support.trace_support.run_trace(\n        mname, fname, module_prefix, callable_names, no_print\n    )","method_summary":"Trace eng wave module exceptions.","original_method_code":"def trace_module(no_print=True):\n    \"\"\"Trace eng wave module exceptions.\"\"\"\n    mname = \"wave_core\"\n    fname = \"peng\"\n    module_prefix = \"peng.{0}.Waveform.\".format(mname)\n    callable_names = (\"__init__\",)\n    return docs.support.trace_support.run_trace(\n        mname, fname, module_prefix, callable_names, no_print\n    )","method_path":"https:\/\/github.com\/pmacosta\/peng\/blob\/976935377adaa3de26fc5677aceb2cdfbd6f93a7\/docs\/support\/trace_ex_eng_wave_core.py#L9-L17"}
{"repo_name":"reingart\/gui2py","method_name":"set_drop_target","method_code":"def set_drop_target(obj, root, designer, inspector):\n    \"Recursively create and set the drop target for obj and childs\"\n    if obj._meta.container:\n        dt = ToolBoxDropTarget(obj, root, designer=designer, \n                                          inspector=inspector)\n        obj.drop_target = dt\n    for child in obj:\n        set_drop_target(child, root, designer, inspector)","method_summary":"Recursively create and set the drop target for obj and childs","original_method_code":"def set_drop_target(obj, root, designer, inspector):\n    \"Recursively create and set the drop target for obj and childs\"\n    if obj._meta.container:\n        dt = ToolBoxDropTarget(obj, root, designer=designer, \n                                          inspector=inspector)\n        obj.drop_target = dt\n    for child in obj:\n        set_drop_target(child, root, designer, inspector)","method_path":"https:\/\/github.com\/reingart\/gui2py\/blob\/aca0a05f6fcde55c94ad7cc058671a06608b01a4\/gui\/tools\/toolbox.py#L227-L234"}
{"repo_name":"reingart\/gui2py","method_name":"ToolBox.tool_click","method_code":"def tool_click(self, evt):\n        \"Event handler tool selection (just add to default handler)\"\n    \n        \n        ctrl = self.menu_ctrl_map[evt.GetId()]\n        \n        if self.inspector.selected_obj:\n            \n            parent = self.inspector.selected_obj\n            while parent.drop_target is None and parent.get_parent():\n                parent = parent.get_parent()\n            \n            obj = ctrl(parent, \n                       name=\"%s_%s\" % (ctrl._meta.name.lower(), wx.NewId()), \n                       pos=(0, 0), designer=self.designer)\n            \n            if obj._meta.container:\n                dt = ToolBoxDropTarget(obj, self.inspector.root_obj, \n                                       designer=self.designer, \n                                       inspector=self.inspector)\n                obj.drop_target = dt\n        \n        w, h = obj.size\n        if w <= 10:\n            obj.width = 100\n        if h <= 10:\n            obj.height = 20\n\n        \n        if self.inspector:\n            self.inspector.load_object(self.inspector.root_obj)  \n            self.inspector.inspect(obj)","method_summary":"Event handler tool selection (just add to default handler)","original_method_code":"def tool_click(self, evt):\n        \"Event handler tool selection (just add to default handler)\"\n    \n        # get the control\n        ctrl = self.menu_ctrl_map[evt.GetId()]\n        # create the control on the parent:\n        if self.inspector.selected_obj:\n            # find the first parent drop target\n            parent = self.inspector.selected_obj\n            while parent.drop_target is None and parent.get_parent():\n                parent = parent.get_parent()\n            # create the new object\n            obj = ctrl(parent, \n                       name=\"%s_%s\" % (ctrl._meta.name.lower(), wx.NewId()), \n                       pos=(0, 0), designer=self.designer)\n            # associate the object with the toolbox:\n            if obj._meta.container:\n                dt = ToolBoxDropTarget(obj, self.inspector.root_obj, \n                                       designer=self.designer, \n                                       inspector=self.inspector)\n                obj.drop_target = dt\n        # fix width and height if default is not visible\n        w, h = obj.size\n        if w <= 10:\n            obj.width = 100\n        if h <= 10:\n            obj.height = 20\n\n        # update the object at the inspector (to show the new control)\n        if self.inspector:\n            self.inspector.load_object(self.inspector.root_obj)  # refresh tree\n            self.inspector.inspect(obj)","method_path":"https:\/\/github.com\/reingart\/gui2py\/blob\/aca0a05f6fcde55c94ad7cc058671a06608b01a4\/gui\/tools\/toolbox.py#L67-L98"}
{"repo_name":"reingart\/gui2py","method_name":"ToolBox.start_drag_opperation","method_code":"def start_drag_opperation(self, evt):\n        \"Event handler for drag&drop functionality\"\n    \n        \n        ctrl = self.menu_ctrl_map[evt.GetToolId()]\n\n        \n        ldata = wx.CustomDataObject(\"gui\")\n        ldata.SetData(ctrl._meta.name)      \n\n        \n        bmp = ctrl._image.GetBitmap()\n\n        \n        \n        bdata = wx.BitmapDataObject(bmp)\n        data = wx.DataObjectComposite()\n        data.Add(ldata)\n        data.Add(bdata)\n\n        \n        \n        dropSource = wx.DropSource(self)\n        dropSource.SetData(data)\n        if DEBUG: print(\"Begining DragDrop\\n\")\n        result = dropSource.DoDragDrop(wx.Drag_AllowMove)\n        if DEBUG: print(\"DragDrop completed: %d\\n\" % result)\n\n        if result == wx.DragMove:\n            if DEBUG: print \"dragmove!\"\n            self.Refresh()","method_summary":"Event handler for drag&drop functionality","original_method_code":"def start_drag_opperation(self, evt):\n        \"Event handler for drag&drop functionality\"\n    \n        # get the control\n        ctrl = self.menu_ctrl_map[evt.GetToolId()]\n\n        # create our own data format and use it in a custom data object\n        ldata = wx.CustomDataObject(\"gui\")\n        ldata.SetData(ctrl._meta.name)      # only strings are allowed!\n\n        # Also create a Bitmap version of the drawing\n        bmp = ctrl._image.GetBitmap()\n\n        # Now make a data object for the bitmap and also a composite\n        # data object holding both of the others.\n        bdata = wx.BitmapDataObject(bmp)\n        data = wx.DataObjectComposite()\n        data.Add(ldata)\n        data.Add(bdata)\n\n        # And finally, create the drop source and begin the drag\n        # and drop opperation\n        dropSource = wx.DropSource(self)\n        dropSource.SetData(data)\n        if DEBUG: print(\"Begining DragDrop\\n\")\n        result = dropSource.DoDragDrop(wx.Drag_AllowMove)\n        if DEBUG: print(\"DragDrop completed: %d\\n\" % result)\n\n        if result == wx.DragMove:\n            if DEBUG: print \"dragmove!\"\n            self.Refresh()","method_path":"https:\/\/github.com\/reingart\/gui2py\/blob\/aca0a05f6fcde55c94ad7cc058671a06608b01a4\/gui\/tools\/toolbox.py#L101-L131"}
{"repo_name":"reingart\/gui2py","method_name":"ToolBox.set_default_tlw","method_code":"def set_default_tlw(self, tlw, designer, inspector):\n        \"track default top level window for toolbox menu default action\"\n        self.designer = designer\n        self.inspector = inspector","method_summary":"track default top level window for toolbox menu default action","original_method_code":"def set_default_tlw(self, tlw, designer, inspector):\n        \"track default top level window for toolbox menu default action\"\n        self.designer = designer\n        self.inspector = inspector","method_path":"https:\/\/github.com\/reingart\/gui2py\/blob\/aca0a05f6fcde55c94ad7cc058671a06608b01a4\/gui\/tools\/toolbox.py#L133-L136"}
{"repo_name":"reingart\/gui2py","method_name":"ToolBoxDropTarget.copy","method_code":"def copy(self):\n        \"Return a copy of the drop target (to avoid wx problems on rebuild)\"\n        return ToolBoxDropTarget(self.dv, self.root, \n                                 self.designer, self.inspector)","method_summary":"Return a copy of the drop target (to avoid wx problems on rebuild)","original_method_code":"def copy(self):\n        \"Return a copy of the drop target (to avoid wx problems on rebuild)\"\n        return ToolBoxDropTarget(self.dv, self.root, \n                                 self.designer, self.inspector)","method_path":"https:\/\/github.com\/reingart\/gui2py\/blob\/aca0a05f6fcde55c94ad7cc058671a06608b01a4\/gui\/tools\/toolbox.py#L219-L222"}
{"repo_name":"reingart\/gui2py","method_name":"inspect","method_code":"def inspect(obj):\n    \"Open the inspector windows for a given object\"\n    from gui.tools.inspector import InspectorTool\n    inspector = InspectorTool()\n    inspector.show(obj)\n    return inspector","method_summary":"Open the inspector windows for a given object","original_method_code":"def inspect(obj):\n    \"Open the inspector windows for a given object\"\n    from gui.tools.inspector import InspectorTool\n    inspector = InspectorTool()\n    inspector.show(obj)\n    return inspector","method_path":"https:\/\/github.com\/reingart\/gui2py\/blob\/aca0a05f6fcde55c94ad7cc058671a06608b01a4\/gui\/__init__.py#L59-L64"}
{"repo_name":"reingart\/gui2py","method_name":"shell","method_code":"def shell():\n    \"Open a shell\"\n    from gui.tools.debug import Shell    \n    shell = Shell()\n    shell.show()\n    return shell","method_summary":"Open a shell","original_method_code":"def shell():\n    \"Open a shell\"\n    from gui.tools.debug import Shell    \n    shell = Shell()\n    shell.show()\n    return shell","method_path":"https:\/\/github.com\/reingart\/gui2py\/blob\/aca0a05f6fcde55c94ad7cc058671a06608b01a4\/gui\/__init__.py#L66-L71"}
{"repo_name":"reingart\/gui2py","method_name":"migrate_window","method_code":"def migrate_window(bg):\n    \"Take a pythoncard background resource and convert to a gui2py window\"\n    ret = {}\n    for k, v in bg.items():\n        if k == 'type':\n            v = WIN_MAP[v]._meta.name\n        elif k == 'menubar':\n            menus = v['menus']\n            v = [migrate_control(menu) for menu in menus]\n        elif k == 'components':\n            v = [migrate_control(comp) for comp in v]\n        else:\n            k = SPEC_MAP['Widget'].get(k, k)\n        ret[k] = v\n    return ret","method_summary":"Take a pythoncard background resource and convert to a gui2py window","original_method_code":"def migrate_window(bg):\n    \"Take a pythoncard background resource and convert to a gui2py window\"\n    ret = {}\n    for k, v in bg.items():\n        if k == 'type':\n            v = WIN_MAP[v]._meta.name\n        elif k == 'menubar':\n            menus = v['menus']\n            v = [migrate_control(menu) for menu in menus]\n        elif k == 'components':\n            v = [migrate_control(comp) for comp in v]\n        else:\n            k = SPEC_MAP['Widget'].get(k, k)\n        ret[k] = v\n    return ret","method_path":"https:\/\/github.com\/reingart\/gui2py\/blob\/aca0a05f6fcde55c94ad7cc058671a06608b01a4\/gui\/tools\/migrate.py#L149-L163"}
{"repo_name":"reingart\/gui2py","method_name":"migrate_control","method_code":"def migrate_control(comp):\n    \"Take a pythoncard background resource and convert to a gui2py window\"\n    ret = {}\n    for k, v in comp.items():\n        if k == 'type':\n            v = CTRL_MAP[v]._meta.name\n        elif k == 'menubar':\n            pass\n        elif k == 'components':\n            v = [migrate_control(comp) for comp in v]\n        else:\n            k = SPEC_MAP['Widget'].get(k, k)\n            if comp['type'] in SPEC_MAP:\n                k = SPEC_MAP[comp['type']].get(k, k)\n            if k == 'font':\n                v = migrate_font(v)\n        ret[k] = v\n    return ret","method_summary":"Take a pythoncard background resource and convert to a gui2py window","original_method_code":"def migrate_control(comp):\n    \"Take a pythoncard background resource and convert to a gui2py window\"\n    ret = {}\n    for k, v in comp.items():\n        if k == 'type':\n            v = CTRL_MAP[v]._meta.name\n        elif k == 'menubar':\n            pass\n        elif k == 'components':\n            v = [migrate_control(comp) for comp in v]\n        else:\n            k = SPEC_MAP['Widget'].get(k, k)\n            if comp['type'] in SPEC_MAP:\n                k = SPEC_MAP[comp['type']].get(k, k)\n            if k == 'font':\n                v = migrate_font(v)\n        ret[k] = v\n    return ret","method_path":"https:\/\/github.com\/reingart\/gui2py\/blob\/aca0a05f6fcde55c94ad7cc058671a06608b01a4\/gui\/tools\/migrate.py#L166-L183"}
{"repo_name":"reingart\/gui2py","method_name":"migrate_font","method_code":"def migrate_font(font):\n    \"Convert PythonCard font description to gui2py style\"\n    if 'faceName' in font:\n        font['face'] = font.pop('faceName')\n    if 'family' in font and font['family'] == 'sansSerif':\n        font['family'] = 'sans serif'\n    return font","method_summary":"Convert PythonCard font description to gui2py style","original_method_code":"def migrate_font(font):\n    \"Convert PythonCard font description to gui2py style\"\n    if 'faceName' in font:\n        font['face'] = font.pop('faceName')\n    if 'family' in font and font['family'] == 'sansSerif':\n        font['family'] = 'sans serif'\n    return font","method_path":"https:\/\/github.com\/reingart\/gui2py\/blob\/aca0a05f6fcde55c94ad7cc058671a06608b01a4\/gui\/tools\/migrate.py#L186-L192"}
{"repo_name":"elliterate\/capybara.py","method_name":"add_selector","method_code":"def add_selector(name):\n    \"\"\"\"\"\"\n\n    factory = SelectorFactory(name)\n    yield factory\n    selectors[name] = factory.build_selector()","method_summary":"Builds and registers a :class:`Selector` object with the given name and configuration.","original_method_code":"def add_selector(name):\n    \"\"\"\n    Builds and registers a :class:`Selector` object with the given name and configuration.\n\n    Args:\n        name (str): The name of the selector.\n\n    Yields:\n        SelectorFactory: The factory that will build the :class:`Selector`.\n    \"\"\"\n\n    factory = SelectorFactory(name)\n    yield factory\n    selectors[name] = factory.build_selector()","method_path":"https:\/\/github.com\/elliterate\/capybara.py\/blob\/0c6ae449cc37e4445ec3cd6af95674533beedc6c\/capybara\/selector\/selector.py#L188-L201"}
{"repo_name":"elliterate\/capybara.py","method_name":"Selector.expression_filters","method_code":"def expression_filters(self):\n        \"\"\"\"\"\"\n\n        return {\n            name: filter for name, filter in iter(self.filters.items())\n            if isinstance(filter, ExpressionFilter)}","method_summary":"Dict[str, ExpressionFilter]:","original_method_code":"def expression_filters(self):\n        \"\"\" Dict[str, ExpressionFilter]: Returns the expression filters for this selector. \"\"\"\n\n        return {\n            name: filter for name, filter in iter(self.filters.items())\n            if isinstance(filter, ExpressionFilter)}","method_path":"https:\/\/github.com\/elliterate\/capybara.py\/blob\/0c6ae449cc37e4445ec3cd6af95674533beedc6c\/capybara\/selector\/selector.py#L57-L62"}
{"repo_name":"elliterate\/capybara.py","method_name":"Selector.node_filters","method_code":"def node_filters(self):\n        \"\"\"\"\"\"\n\n        return {\n            name: filter for name, filter in iter(self.filters.items())\n            if isinstance(filter, NodeFilter)}","method_summary":"Dict[str, NodeFilter]:","original_method_code":"def node_filters(self):\n        \"\"\" Dict[str, NodeFilter]: Returns the node filters for this selector. \"\"\"\n\n        return {\n            name: filter for name, filter in iter(self.filters.items())\n            if isinstance(filter, NodeFilter)}","method_path":"https:\/\/github.com\/elliterate\/capybara.py\/blob\/0c6ae449cc37e4445ec3cd6af95674533beedc6c\/capybara\/selector\/selector.py#L65-L70"}
{"repo_name":"elliterate\/capybara.py","method_name":"SelectorFactory.filter_set","method_code":"def filter_set(self, name):\n        \"\"\"\"\"\"\n\n        filter_set = filter_sets[name]\n        for name, filter in iter(filter_set.filters.items()):\n            self.filters[name] = filter\n        self.descriptions += filter_set.descriptions","method_summary":"Adds filters from a particular global :class:`FilterSet`.","original_method_code":"def filter_set(self, name):\n        \"\"\"\n        Adds filters from a particular global :class:`FilterSet`.\n\n        Args:\n            name (str): The name of the set whose filters should be added.\n        \"\"\"\n\n        filter_set = filter_sets[name]\n        for name, filter in iter(filter_set.filters.items()):\n            self.filters[name] = filter\n        self.descriptions += filter_set.descriptions","method_path":"https:\/\/github.com\/elliterate\/capybara.py\/blob\/0c6ae449cc37e4445ec3cd6af95674533beedc6c\/capybara\/selector\/selector.py#L159-L170"}
{"repo_name":"elliterate\/capybara.py","method_name":"StyleQuery.resolves_for","method_code":"def resolves_for(self, node):\n        \"\"\"\"\"\"\n\n        self.node = node\n        self.actual_styles = node.style(*self.expected_styles.keys())\n\n        return all(\n            toregex(value).search(self.actual_styles[style])\n            for style, value in iter(self.expected_styles.items()))","method_summary":"Resolves this query relative to the given node.","original_method_code":"def resolves_for(self, node):\n        \"\"\"\n        Resolves this query relative to the given node.\n\n        Args:\n            node (node.Base): The node to be evaluated.\n\n        Returns:\n            int: The number of matches found.\n        \"\"\"\n\n        self.node = node\n        self.actual_styles = node.style(*self.expected_styles.keys())\n\n        return all(\n            toregex(value).search(self.actual_styles[style])\n            for style, value in iter(self.expected_styles.items()))","method_path":"https:\/\/github.com\/elliterate\/capybara.py\/blob\/0c6ae449cc37e4445ec3cd6af95674533beedc6c\/capybara\/queries\/style_query.py#L27-L43"}
{"repo_name":"elliterate\/capybara.py","method_name":"StyleQuery.failure_message","method_code":"def failure_message(self):\n        \"\"\"\"\"\"\n        return (\n            \"Expected node to have styles {expected}. \"\n            \"Actual styles were {actual}\").format(\n                expected=desc(self.expected_styles),\n                actual=desc(self.actual_styles))","method_summary":"str: A message describing the query failure.","original_method_code":"def failure_message(self):\n        \"\"\" str: A message describing the query failure. \"\"\"\n        return (\n            \"Expected node to have styles {expected}. \"\n            \"Actual styles were {actual}\").format(\n                expected=desc(self.expected_styles),\n                actual=desc(self.actual_styles))","method_path":"https:\/\/github.com\/elliterate\/capybara.py\/blob\/0c6ae449cc37e4445ec3cd6af95674533beedc6c\/capybara\/queries\/style_query.py#L46-L52"}
{"repo_name":"elliterate\/capybara.py","method_name":"SessionMatchersMixin.assert_current_path","method_code":"def assert_current_path(self, path, **kwargs):\n        \"\"\"\"\"\"\n\n        query = CurrentPathQuery(path, **kwargs)\n\n        @self.document.synchronize\n        def assert_current_path():\n            if not query.resolves_for(self):\n                raise ExpectationNotMet(query.failure_message)\n        assert_current_path()\n\n        return True","method_summary":"Asserts that the page has the given path. By default this will compare against the path+query portion of the full URL.","original_method_code":"def assert_current_path(self, path, **kwargs):\n        \"\"\"\n        Asserts that the page has the given path. By default this will compare against the\n        path+query portion of the full URL.\n\n        Args:\n            path (str | RegexObject): The string or regex that the current \"path\" should match.\n            **kwargs: Arbitrary keyword arguments for :class:`CurrentPathQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.\n        \"\"\"\n\n        query = CurrentPathQuery(path, **kwargs)\n\n        @self.document.synchronize\n        def assert_current_path():\n            if not query.resolves_for(self):\n                raise ExpectationNotMet(query.failure_message)\n        assert_current_path()\n\n        return True","method_path":"https:\/\/github.com\/elliterate\/capybara.py\/blob\/0c6ae449cc37e4445ec3cd6af95674533beedc6c\/capybara\/session_matchers.py#L6-L30"}
{"repo_name":"elliterate\/capybara.py","method_name":"SessionMatchersMixin.assert_no_current_path","method_code":"def assert_no_current_path(self, path, **kwargs):\n        \"\"\"\"\"\"\n\n        query = CurrentPathQuery(path, **kwargs)\n\n        @self.document.synchronize\n        def assert_no_current_path():\n            if query.resolves_for(self):\n                raise ExpectationNotMet(query.negative_failure_message)\n\n        assert_no_current_path()\n\n        return True","method_summary":"Asserts that the page doesn't have the given path.","original_method_code":"def assert_no_current_path(self, path, **kwargs):\n        \"\"\"\n        Asserts that the page doesn't have the given path.\n\n        Args:\n            path (str | RegexObject): The string or regex that the current \"path\" should match.\n            **kwargs: Arbitrary keyword arguments for :class:`CurrentPathQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.\n        \"\"\"\n\n        query = CurrentPathQuery(path, **kwargs)\n\n        @self.document.synchronize\n        def assert_no_current_path():\n            if query.resolves_for(self):\n                raise ExpectationNotMet(query.negative_failure_message)\n\n        assert_no_current_path()\n\n        return True","method_path":"https:\/\/github.com\/elliterate\/capybara.py\/blob\/0c6ae449cc37e4445ec3cd6af95674533beedc6c\/capybara\/session_matchers.py#L32-L56"}
{"repo_name":"elliterate\/capybara.py","method_name":"SessionMatchersMixin.has_current_path","method_code":"def has_current_path(self, path, **kwargs):\n        \"\"\"\"\"\"\n\n        try:\n            return self.assert_current_path(path, **kwargs)\n        except ExpectationNotMet:\n            return False","method_summary":"Checks if the page has the given path.","original_method_code":"def has_current_path(self, path, **kwargs):\n        \"\"\"\n        Checks if the page has the given path.\n\n        Args:\n            path (str | RegexObject): The string or regex that the current \"path\" should match.\n            **kwargs: Arbitrary keyword arguments for :class:`CurrentPathQuery`.\n\n        Returns:\n            bool: Whether it matches.\n        \"\"\"\n\n        try:\n            return self.assert_current_path(path, **kwargs)\n        except ExpectationNotMet:\n            return False","method_path":"https:\/\/github.com\/elliterate\/capybara.py\/blob\/0c6ae449cc37e4445ec3cd6af95674533beedc6c\/capybara\/session_matchers.py#L58-L73"}
{"repo_name":"elliterate\/capybara.py","method_name":"SessionMatchersMixin.has_no_current_path","method_code":"def has_no_current_path(self, path, **kwargs):\n        \"\"\"\"\"\"\n\n        try:\n            return self.assert_no_current_path(path, **kwargs)\n        except ExpectationNotMet:\n            return False","method_summary":"Checks if the page doesn't have the given path.","original_method_code":"def has_no_current_path(self, path, **kwargs):\n        \"\"\"\n        Checks if the page doesn't have the given path.\n\n        Args:\n            path (str | RegexObject): The string or regex that the current \"path\" should match.\n            **kwargs: Arbitrary keyword arguments for :class:`CurrentPathQuery`.\n\n        Returns:\n            bool: Whether it doesn't match.\n        \"\"\"\n\n        try:\n            return self.assert_no_current_path(path, **kwargs)\n        except ExpectationNotMet:\n            return False","method_path":"https:\/\/github.com\/elliterate\/capybara.py\/blob\/0c6ae449cc37e4445ec3cd6af95674533beedc6c\/capybara\/session_matchers.py#L75-L90"}
{"repo_name":"lmjohns3\/theanets","method_name":"find","method_code":"def find(dataset, url):\n    ''''''\n    fn = os.path.join(DATASETS, dataset)\n    dn = os.path.dirname(fn)\n    if not os.path.exists(dn):\n        print('creating dataset directory: %s', dn)\n        os.makedirs(dn)\n    if not os.path.exists(fn):\n        if sys.version_info < (3, ):\n            urllib.urlretrieve(url, fn)\n        else:\n            urllib.request.urlretrieve(url, fn)\n    return fn","method_summary":"Find the location of a dataset on disk, downloading if needed.","original_method_code":"def find(dataset, url):\n    '''Find the location of a dataset on disk, downloading if needed.'''\n    fn = os.path.join(DATASETS, dataset)\n    dn = os.path.dirname(fn)\n    if not os.path.exists(dn):\n        print('creating dataset directory: %s', dn)\n        os.makedirs(dn)\n    if not os.path.exists(fn):\n        if sys.version_info < (3, ):\n            urllib.urlretrieve(url, fn)\n        else:\n            urllib.request.urlretrieve(url, fn)\n    return fn","method_path":"https:\/\/github.com\/lmjohns3\/theanets\/blob\/79db9f878ef2071f2f576a1cf5d43a752a55894a\/examples\/utils.py#L18-L30"}
{"repo_name":"lmjohns3\/theanets","method_name":"load_mnist","method_code":"def load_mnist(flatten=True, labels=False):\n    ''''''\n    fn = find('mnist.pkl.gz', 'http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz')\n    h = gzip.open(fn, 'rb')\n    if sys.version_info < (3, ):\n        (timg, tlab), (vimg, vlab), (simg, slab) = pickle.load(h)\n    else:\n        (timg, tlab), (vimg, vlab), (simg, slab) = pickle.load(h, encoding='bytes')\n    h.close()\n    if not flatten:\n        timg = timg.reshape((-1, 28, 28, 1))\n        vimg = vimg.reshape((-1, 28, 28, 1))\n        simg = simg.reshape((-1, 28, 28, 1))\n    if labels:\n        return ((timg, tlab.astype('i')),\n                (vimg, vlab.astype('i')),\n                (simg, slab.astype('i')))\n    return (timg, ), (vimg, ), (simg, )","method_summary":"Load the MNIST digits dataset.","original_method_code":"def load_mnist(flatten=True, labels=False):\n    '''Load the MNIST digits dataset.'''\n    fn = find('mnist.pkl.gz', 'http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz')\n    h = gzip.open(fn, 'rb')\n    if sys.version_info < (3, ):\n        (timg, tlab), (vimg, vlab), (simg, slab) = pickle.load(h)\n    else:\n        (timg, tlab), (vimg, vlab), (simg, slab) = pickle.load(h, encoding='bytes')\n    h.close()\n    if not flatten:\n        timg = timg.reshape((-1, 28, 28, 1))\n        vimg = vimg.reshape((-1, 28, 28, 1))\n        simg = simg.reshape((-1, 28, 28, 1))\n    if labels:\n        return ((timg, tlab.astype('i')),\n                (vimg, vlab.astype('i')),\n                (simg, slab.astype('i')))\n    return (timg, ), (vimg, ), (simg, )","method_path":"https:\/\/github.com\/lmjohns3\/theanets\/blob\/79db9f878ef2071f2f576a1cf5d43a752a55894a\/examples\/utils.py#L33-L50"}
{"repo_name":"lmjohns3\/theanets","method_name":"load_cifar","method_code":"def load_cifar(flatten=True, labels=False):\n    ''''''\n    def extract(name):\n        print('extracting data from {}'.format(name))\n        h = tar.extractfile(name)\n        if sys.version_info < (3, ):\n            d = pickle.load(h)\n        else:\n            d = pickle.load(h, encoding='bytes')\n            for k in list(d):\n                d[k.decode('utf8')] = d[k]\n        h.close()\n        img = d['data'].reshape(\n            (-1, 3, 32, 32)).transpose((0, 2, 3, 1)).astype('f') \/ 128 - 1\n        if flatten:\n            img = img.reshape((-1, 32 * 32 * 3))\n        d['data'] = img\n        return d\n\n    fn = find('cifar10.tar.gz', 'http:\/\/www.cs.toronto.edu\/~kriz\/cifar-10-python.tar.gz')\n    tar = tarfile.open(fn)\n\n    imgs = []\n    labs = []\n    for i in range(1, 6):\n        d = extract('cifar-10-batches-py\/data_batch_{}'.format(i))\n        imgs.extend(d['data'])\n        labs.extend(d['labels'])\n    timg = np.asarray(imgs[:40000])\n    tlab = np.asarray(labs[:40000], 'i')\n    vimg = np.asarray(imgs[40000:])\n    vlab = np.asarray(labs[40000:], 'i')\n\n    d = extract('cifar-10-batches-py\/test_batch')\n    simg = d['data']\n    slab = d['labels']\n\n    tar.close()\n\n    if labels:\n        return (timg, tlab), (vimg, vlab), (simg, slab)\n    return (timg, ), (vimg, ), (simg, )","method_summary":"Load the CIFAR10 image dataset.","original_method_code":"def load_cifar(flatten=True, labels=False):\n    '''Load the CIFAR10 image dataset.'''\n    def extract(name):\n        print('extracting data from {}'.format(name))\n        h = tar.extractfile(name)\n        if sys.version_info < (3, ):\n            d = pickle.load(h)\n        else:\n            d = pickle.load(h, encoding='bytes')\n            for k in list(d):\n                d[k.decode('utf8')] = d[k]\n        h.close()\n        img = d['data'].reshape(\n            (-1, 3, 32, 32)).transpose((0, 2, 3, 1)).astype('f') \/ 128 - 1\n        if flatten:\n            img = img.reshape((-1, 32 * 32 * 3))\n        d['data'] = img\n        return d\n\n    fn = find('cifar10.tar.gz', 'http:\/\/www.cs.toronto.edu\/~kriz\/cifar-10-python.tar.gz')\n    tar = tarfile.open(fn)\n\n    imgs = []\n    labs = []\n    for i in range(1, 6):\n        d = extract('cifar-10-batches-py\/data_batch_{}'.format(i))\n        imgs.extend(d['data'])\n        labs.extend(d['labels'])\n    timg = np.asarray(imgs[:40000])\n    tlab = np.asarray(labs[:40000], 'i')\n    vimg = np.asarray(imgs[40000:])\n    vlab = np.asarray(labs[40000:], 'i')\n\n    d = extract('cifar-10-batches-py\/test_batch')\n    simg = d['data']\n    slab = d['labels']\n\n    tar.close()\n\n    if labels:\n        return (timg, tlab), (vimg, vlab), (simg, slab)\n    return (timg, ), (vimg, ), (simg, )","method_path":"https:\/\/github.com\/lmjohns3\/theanets\/blob\/79db9f878ef2071f2f576a1cf5d43a752a55894a\/examples\/utils.py#L53-L94"}
{"repo_name":"lmjohns3\/theanets","method_name":"plot_layers","method_code":"def plot_layers(weights, tied_weights=False, channels=1):\n    ''''''\n    if hasattr(weights[0], 'get_value'):\n        weights = [w.get_value() for w in weights]\n    k = min(len(weights), 9)\n    imgs = np.eye(weights[0].shape[0])\n    for i, weight in enumerate(weights[:-1]):\n        imgs = np.dot(weight.T, imgs)\n        plot_images(imgs,\n                    100 + 10 * k + i + 1,\n                    channels=channels,\n                    title='Layer {}'.format(i+1))\n    weight = weights[-1]\n    n = weight.shape[1] \/ channels\n    if int(np.sqrt(n)) ** 2 != n:\n        return\n    if tied_weights:\n        imgs = np.dot(weight.T, imgs)\n        plot_images(imgs,\n                    100 + 10 * k + k,\n                    channels=channels,\n                    title='Layer {}'.format(k))\n    else:\n        plot_images(weight,\n                    100 + 10 * k + k,\n                    channels=channels,\n                    title='Decoding weights')","method_summary":"Create a plot of weights, visualized as \"bottom-level\" pixel arrays.","original_method_code":"def plot_layers(weights, tied_weights=False, channels=1):\n    '''Create a plot of weights, visualized as \"bottom-level\" pixel arrays.'''\n    if hasattr(weights[0], 'get_value'):\n        weights = [w.get_value() for w in weights]\n    k = min(len(weights), 9)\n    imgs = np.eye(weights[0].shape[0])\n    for i, weight in enumerate(weights[:-1]):\n        imgs = np.dot(weight.T, imgs)\n        plot_images(imgs,\n                    100 + 10 * k + i + 1,\n                    channels=channels,\n                    title='Layer {}'.format(i+1))\n    weight = weights[-1]\n    n = weight.shape[1] \/ channels\n    if int(np.sqrt(n)) ** 2 != n:\n        return\n    if tied_weights:\n        imgs = np.dot(weight.T, imgs)\n        plot_images(imgs,\n                    100 + 10 * k + k,\n                    channels=channels,\n                    title='Layer {}'.format(k))\n    else:\n        plot_images(weight,\n                    100 + 10 * k + k,\n                    channels=channels,\n                    title='Decoding weights')","method_path":"https:\/\/github.com\/lmjohns3\/theanets\/blob\/79db9f878ef2071f2f576a1cf5d43a752a55894a\/examples\/utils.py#L129-L155"}
{"repo_name":"lmjohns3\/theanets","method_name":"plot_filters","method_code":"def plot_filters(filters):\n    ''''''\n    imgs = filters.get_value()\n\n    N, channels, x, y = imgs.shape\n    n = int(np.sqrt(N))\n    assert n * n == N, 'filters must contain a square number of rows!'\n    assert channels == 1 or channels == 3, 'can only plot grayscale or rgb filters!'\n\n    img = np.zeros(((y+1) * n - 1, (x+1) * n - 1, channels), dtype=imgs[0].dtype)\n    for i, pix in enumerate(imgs):\n        r, c = divmod(i, n)\n        img[r * (y+1):(r+1) * (y+1) - 1,\n            c * (x+1):(c+1) * (x+1) - 1] = pix.transpose((1, 2, 0))\n\n    img -= img.min()\n    img \/= img.max()\n\n    ax = plt.gcf().add_subplot(111)\n    ax.xaxis.set_visible(False)\n    ax.yaxis.set_visible(False)\n    ax.set_frame_on(False)\n    ax.imshow(img.squeeze(), cmap=plt.cm.gray)","method_summary":"Create a plot of conv filters, visualized as pixel arrays.","original_method_code":"def plot_filters(filters):\n    '''Create a plot of conv filters, visualized as pixel arrays.'''\n    imgs = filters.get_value()\n\n    N, channels, x, y = imgs.shape\n    n = int(np.sqrt(N))\n    assert n * n == N, 'filters must contain a square number of rows!'\n    assert channels == 1 or channels == 3, 'can only plot grayscale or rgb filters!'\n\n    img = np.zeros(((y+1) * n - 1, (x+1) * n - 1, channels), dtype=imgs[0].dtype)\n    for i, pix in enumerate(imgs):\n        r, c = divmod(i, n)\n        img[r * (y+1):(r+1) * (y+1) - 1,\n            c * (x+1):(c+1) * (x+1) - 1] = pix.transpose((1, 2, 0))\n\n    img -= img.min()\n    img \/= img.max()\n\n    ax = plt.gcf().add_subplot(111)\n    ax.xaxis.set_visible(False)\n    ax.yaxis.set_visible(False)\n    ax.set_frame_on(False)\n    ax.imshow(img.squeeze(), cmap=plt.cm.gray)","method_path":"https:\/\/github.com\/lmjohns3\/theanets\/blob\/79db9f878ef2071f2f576a1cf5d43a752a55894a\/examples\/utils.py#L158-L180"}
{"repo_name":"lmjohns3\/theanets","method_name":"batches","method_code":"def batches(arrays, steps=100, batch_size=64, rng=None):\n    ''''''\n    assert batch_size >= 2, 'batch_size must be at least 2!'\n    assert isinstance(arrays, (tuple, list)), 'arrays must be a tuple or list!'\n\n    if rng is None or isinstance(rng, int):\n        rng = np.random.RandomState(rng)\n\n    def sample():\n        xs = [np.zeros((batch_size, steps, a.shape[1]), a.dtype) for a in arrays]\n        for i in range(batch_size):\n            j = rng.randint(len(arrays[0]) - steps)\n            for x, a in zip(xs, arrays):\n                x[i] = a[j:j+steps]\n        return xs\n\n    return sample","method_summary":"Create a callable that generates samples from a dataset.","original_method_code":"def batches(arrays, steps=100, batch_size=64, rng=None):\n    '''Create a callable that generates samples from a dataset.\n\n    Parameters\n    ----------\n    arrays : list of ndarray (time-steps, data-dimensions)\n        Arrays of data. Rows in these arrays are assumed to correspond to time\n        steps, and columns to variables. Multiple arrays can be given; in such\n        a case, these arrays usually correspond to [input, output]---for\n        example, for a recurrent regression problem---or [input, output,\n        weights]---for a weighted regression or classification problem.\n    steps : int, optional\n        Generate samples of this many time steps. Defaults to 100.\n    batch_size : int, optional\n        Generate this many samples per call. Defaults to 64. This must match the\n        batch_size parameter that was used when creating the recurrent network\n        that will process the data.\n    rng : :class:`numpy.random.RandomState` or int, optional\n        A random number generator, or an integer seed for a random number\n        generator. If not provided, the random number generator will be created\n        with an automatically chosen seed.\n\n    Returns\n    -------\n    callable :\n        A callable that can be used inside a dataset for training a recurrent\n        network.\n    '''\n    assert batch_size >= 2, 'batch_size must be at least 2!'\n    assert isinstance(arrays, (tuple, list)), 'arrays must be a tuple or list!'\n\n    if rng is None or isinstance(rng, int):\n        rng = np.random.RandomState(rng)\n\n    def sample():\n        xs = [np.zeros((batch_size, steps, a.shape[1]), a.dtype) for a in arrays]\n        for i in range(batch_size):\n            j = rng.randint(len(arrays[0]) - steps)\n            for x, a in zip(xs, arrays):\n                x[i] = a[j:j+steps]\n        return xs\n\n    return sample","method_path":"https:\/\/github.com\/lmjohns3\/theanets\/blob\/79db9f878ef2071f2f576a1cf5d43a752a55894a\/theanets\/recurrent.py#L12-L54"}
{"repo_name":"lmjohns3\/theanets","method_name":"Text.encode","method_code":"def encode(self, txt):\n        ''''''\n        return list(self._fwd_index.get(c, 0) for c in txt)","method_summary":"Encode a text string by replacing characters with alphabet index.","original_method_code":"def encode(self, txt):\n        '''Encode a text string by replacing characters with alphabet index.\n\n        Parameters\n        ----------\n        txt : str\n            A string to encode.\n\n        Returns\n        -------\n        classes : list of int\n            A sequence of alphabet index values corresponding to the given text.\n        '''\n        return list(self._fwd_index.get(c, 0) for c in txt)","method_path":"https:\/\/github.com\/lmjohns3\/theanets\/blob\/79db9f878ef2071f2f576a1cf5d43a752a55894a\/theanets\/recurrent.py#L97-L110"}
{"repo_name":"lmjohns3\/theanets","method_name":"Text.classifier_batches","method_code":"def classifier_batches(self, steps, batch_size, rng=None):\n        ''''''\n        assert batch_size >= 2, 'batch_size must be at least 2!'\n\n        if rng is None or isinstance(rng, int):\n            rng = np.random.RandomState(rng)\n\n        T = np.arange(steps)\n\n        def batch():\n            inputs = np.zeros((batch_size, steps, 1 + len(self.alpha)), 'f')\n            outputs = np.zeros((batch_size, steps), 'i')\n            for b in range(batch_size):\n                offset = rng.randint(len(self.text) - steps - 1)\n                enc = self.encode(self.text[offset:offset + steps + 1])\n                inputs[b, T, enc[:-1]] = 1\n                outputs[b, T] = enc[1:]\n            return [inputs, outputs]\n\n        return batch","method_summary":"Create a callable that returns a batch of training data.","original_method_code":"def classifier_batches(self, steps, batch_size, rng=None):\n        '''Create a callable that returns a batch of training data.\n\n        Parameters\n        ----------\n        steps : int\n            Number of time steps in each batch.\n        batch_size : int\n            Number of training examples per batch.\n        rng : :class:`numpy.random.RandomState` or int, optional\n            A random number generator, or an integer seed for a random number\n            generator. If not provided, the random number generator will be\n            created with an automatically chosen seed.\n\n        Returns\n        -------\n        batch : callable\n            A callable that, when called, returns a batch of data that can be\n            used to train a classifier model.\n        '''\n        assert batch_size >= 2, 'batch_size must be at least 2!'\n\n        if rng is None or isinstance(rng, int):\n            rng = np.random.RandomState(rng)\n\n        T = np.arange(steps)\n\n        def batch():\n            inputs = np.zeros((batch_size, steps, 1 + len(self.alpha)), 'f')\n            outputs = np.zeros((batch_size, steps), 'i')\n            for b in range(batch_size):\n                offset = rng.randint(len(self.text) - steps - 1)\n                enc = self.encode(self.text[offset:offset + steps + 1])\n                inputs[b, T, enc[:-1]] = 1\n                outputs[b, T] = enc[1:]\n            return [inputs, outputs]\n\n        return batch","method_path":"https:\/\/github.com\/lmjohns3\/theanets\/blob\/79db9f878ef2071f2f576a1cf5d43a752a55894a\/theanets\/recurrent.py#L127-L164"}
{"repo_name":"lmjohns3\/theanets","method_name":"Classifier.predict_sequence","method_code":"def predict_sequence(self, labels, steps, streams=1, rng=None):\n        ''''''\n        if rng is None or isinstance(rng, int):\n            rng = np.random.RandomState(rng)\n        offset = len(labels)\n        batch = max(2, streams)\n        inputs = np.zeros((batch, offset + steps, self.layers[0].output_size), 'f')\n        inputs[:, np.arange(offset), labels] = 1\n        for i in range(offset, offset + steps):\n            chars = []\n            for pdf in self.predict_proba(inputs[:i])[:, -1]:\n                try:\n                    c = rng.multinomial(1, pdf).argmax(axis=-1)\n                except ValueError:\n                    \n                    \n                    c = pdf.argmax(axis=-1)\n                chars.append(int(c))\n            inputs[np.arange(batch), i, chars] = 1\n            yield chars[0] if streams == 1 else chars","method_summary":"Draw a sequential sample of class labels from this network.","original_method_code":"def predict_sequence(self, labels, steps, streams=1, rng=None):\n        '''Draw a sequential sample of class labels from this network.\n\n        Parameters\n        ----------\n        labels : list of int\n            A list of integer class labels to get the classifier started.\n        steps : int\n            The number of time steps to sample.\n        streams : int, optional\n            Number of parallel streams to sample from the model. Defaults to 1.\n        rng : :class:`numpy.random.RandomState` or int, optional\n            A random number generator, or an integer seed for a random number\n            generator. If not provided, the random number generator will be\n            created with an automatically chosen seed.\n\n        Yields\n        ------\n        label(s) : int or list of int\n            Yields at each time step an integer class label sampled sequentially\n            from the model. If the number of requested streams is greater than\n            1, this will be a list containing the corresponding number of class\n            labels.\n        '''\n        if rng is None or isinstance(rng, int):\n            rng = np.random.RandomState(rng)\n        offset = len(labels)\n        batch = max(2, streams)\n        inputs = np.zeros((batch, offset + steps, self.layers[0].output_size), 'f')\n        inputs[:, np.arange(offset), labels] = 1\n        for i in range(offset, offset + steps):\n            chars = []\n            for pdf in self.predict_proba(inputs[:i])[:, -1]:\n                try:\n                    c = rng.multinomial(1, pdf).argmax(axis=-1)\n                except ValueError:\n                    # sometimes the pdf triggers a normalization error. just\n                    # choose greedily in this case.\n                    c = pdf.argmax(axis=-1)\n                chars.append(int(c))\n            inputs[np.arange(batch), i, chars] = 1\n            yield chars[0] if streams == 1 else chars","method_path":"https:\/\/github.com\/lmjohns3\/theanets\/blob\/79db9f878ef2071f2f576a1cf5d43a752a55894a\/theanets\/recurrent.py#L392-L433"}
{"repo_name":"lmjohns3\/theanets","method_name":"Convolution.add_conv_weights","method_code":"def add_conv_weights(self, name, mean=0, std=None, sparsity=0):\n        ''''''\n        nin = self.input_size\n        nout = self.output_size\n        mean = self.kwargs.get(\n            'mean_{}'.format(name),\n            self.kwargs.get('mean', mean))\n        std = self.kwargs.get(\n            'std_{}'.format(name),\n            self.kwargs.get('std', std or 1 \/ np.sqrt(nin + nout)))\n        sparsity = self.kwargs.get(\n            'sparsity_{}'.format(name),\n            self.kwargs.get('sparsity', sparsity))\n        arr = np.zeros((nout, nin) + self.filter_size, util.FLOAT)\n        for r in range(self.filter_size[0]):\n            for c in range(self.filter_size[1]):\n                arr[:, :, r, c] = util.random_matrix(\n                    nout, nin, mean, std, sparsity=sparsity, rng=self.rng)\n        self._params.append(theano.shared(arr, name=self._fmt(name)))","method_summary":"Add a convolutional weight array to this layer's parameters.","original_method_code":"def add_conv_weights(self, name, mean=0, std=None, sparsity=0):\n        '''Add a convolutional weight array to this layer's parameters.\n\n        Parameters\n        ----------\n        name : str\n            Name of the parameter to add.\n        mean : float, optional\n            Mean value for randomly-initialized weights. Defaults to 0.\n        std : float, optional\n            Standard deviation of initial matrix values. Defaults to\n            :math:`1 \/ sqrt(n_i + n_o)`.\n        sparsity : float, optional\n            Fraction of weights to set to zero. Defaults to 0.\n        '''\n        nin = self.input_size\n        nout = self.output_size\n        mean = self.kwargs.get(\n            'mean_{}'.format(name),\n            self.kwargs.get('mean', mean))\n        std = self.kwargs.get(\n            'std_{}'.format(name),\n            self.kwargs.get('std', std or 1 \/ np.sqrt(nin + nout)))\n        sparsity = self.kwargs.get(\n            'sparsity_{}'.format(name),\n            self.kwargs.get('sparsity', sparsity))\n        arr = np.zeros((nout, nin) + self.filter_size, util.FLOAT)\n        for r in range(self.filter_size[0]):\n            for c in range(self.filter_size[1]):\n                arr[:, :, r, c] = util.random_matrix(\n                    nout, nin, mean, std, sparsity=sparsity, rng=self.rng)\n        self._params.append(theano.shared(arr, name=self._fmt(name)))","method_path":"https:\/\/github.com\/lmjohns3\/theanets\/blob\/79db9f878ef2071f2f576a1cf5d43a752a55894a\/theanets\/layers\/convolution.py#L53-L84"}
{"repo_name":"dopefishh\/pympi","method_name":"TextGrid.from_file","method_code":"def from_file(self, ifile, codec='ascii'):\n        \"\"\"\"\"\"\n        if ifile.read(12) == b'ooBinaryFile':\n            def bin2str(ifile):\n                textlen = struct.unpack('>h', ifile.read(2))[0]\n                \n                if textlen >= 0:\n                    return ifile.read(textlen).decode('ascii')\n                \n                elif textlen == -1:\n                    textlen = struct.unpack('>h', ifile.read(2))[0]\n                    data = ifile.read(textlen*2)\n                    \n                    fun = unichr if 'unichr' in __builtins__ else chr\n                    charlist = (data[i:i+2] for i in range(0, len(data), 2))\n                    return u''.join(\n                        fun(struct.unpack('>h', i)[0]) for i in charlist)\n\n            ifile.read(ord(ifile.read(1)))  \n            self.xmin = struct.unpack('>d', ifile.read(8))[0]\n            self.xmax = struct.unpack('>d', ifile.read(8))[0]\n            ifile.read(1)  \n            self.tier_num = struct.unpack('>i', ifile.read(4))[0]\n            for i in range(self.tier_num):\n                tier_type = ifile.read(ord(ifile.read(1))).decode('ascii')\n                name = bin2str(ifile)\n                tier = Tier(0, 0, name=name, tier_type=tier_type)\n                self.tiers.append(tier)\n                tier.xmin = struct.unpack('>d', ifile.read(8))[0]\n                tier.xmax = struct.unpack('>d', ifile.read(8))[0]\n                nint = struct.unpack('>i', ifile.read(4))[0]\n                for i in range(nint):\n                    x1 = struct.unpack('>d', ifile.read(8))[0]\n                    if tier.tier_type == 'IntervalTier':\n                        x2 = struct.unpack('>d', ifile.read(8))[0]\n                    text = bin2str(ifile)\n                    if tier.tier_type == 'IntervalTier':\n                        tier.intervals.append((x1, x2, text))\n                    elif tier.tier_type == 'TextTier':\n                        tier.intervals.append((x1, text))\n                    else:\n                        raise Exception('Tiertype does not exist.')\n        else:\n            def nn(ifile, pat):\n                line = next(ifile).decode(codec)\n                return pat.search(line).group(1)\n\n            regfloat = re.compile('([\\d.]+)\\s*$', flags=re.UNICODE)\n            regint = re.compile('([\\d]+)\\s*$', flags=re.UNICODE)\n            regstr = re.compile('\"(.*)\"\\s*$', flags=re.UNICODE)\n            \n            next(ifile), next(ifile), next(ifile)\n            self.xmin = float(nn(ifile, regfloat))\n            self.xmax = float(nn(ifile, regfloat))\n            \n            line = next(ifile)\n            short = line.strip() == b'<exists>'\n            self.tier_num = int(nn(ifile, regint))\n            not short and next(ifile)\n            for i in range(self.tier_num):\n                not short and next(ifile)  \n                tier_type = nn(ifile, regstr)\n                name = nn(ifile, regstr)\n                tier = Tier(0, 0, name=name, tier_type=tier_type)\n                self.tiers.append(tier)\n                tier.xmin = float(nn(ifile, regfloat))\n                tier.xmax = float(nn(ifile, regfloat))\n                for i in range(int(nn(ifile, regint))):\n                    not short and next(ifile)  \n                    x1 = float(nn(ifile, regfloat))\n                    if tier.tier_type == 'IntervalTier':\n                        x2 = float(nn(ifile, regfloat))\n                        t = nn(ifile, regstr)\n                        tier.intervals.append((x1, x2, t))\n                    elif tier.tier_type == 'TextTier':\n                        t = nn(ifile, regstr)\n                        tier.intervals.append((x1, t))","method_summary":"Read textgrid from stream.","original_method_code":"def from_file(self, ifile, codec='ascii'):\n        \"\"\"Read textgrid from stream.\n\n        :param file ifile: Stream to read from.\n        :param str codec: Text encoding for the input. Note that this will be\n            ignored for binary TextGrids.\n        \"\"\"\n        if ifile.read(12) == b'ooBinaryFile':\n            def bin2str(ifile):\n                textlen = struct.unpack('>h', ifile.read(2))[0]\n                # Single byte characters\n                if textlen >= 0:\n                    return ifile.read(textlen).decode('ascii')\n                # Multi byte characters have initial len -1 and then \\xff bytes\n                elif textlen == -1:\n                    textlen = struct.unpack('>h', ifile.read(2))[0]\n                    data = ifile.read(textlen*2)\n                    # Hack to go from number to unicode in python3 and python2\n                    fun = unichr if 'unichr' in __builtins__ else chr\n                    charlist = (data[i:i+2] for i in range(0, len(data), 2))\n                    return u''.join(\n                        fun(struct.unpack('>h', i)[0]) for i in charlist)\n\n            ifile.read(ord(ifile.read(1)))  # skip oo type\n            self.xmin = struct.unpack('>d', ifile.read(8))[0]\n            self.xmax = struct.unpack('>d', ifile.read(8))[0]\n            ifile.read(1)  # skip <exists>\n            self.tier_num = struct.unpack('>i', ifile.read(4))[0]\n            for i in range(self.tier_num):\n                tier_type = ifile.read(ord(ifile.read(1))).decode('ascii')\n                name = bin2str(ifile)\n                tier = Tier(0, 0, name=name, tier_type=tier_type)\n                self.tiers.append(tier)\n                tier.xmin = struct.unpack('>d', ifile.read(8))[0]\n                tier.xmax = struct.unpack('>d', ifile.read(8))[0]\n                nint = struct.unpack('>i', ifile.read(4))[0]\n                for i in range(nint):\n                    x1 = struct.unpack('>d', ifile.read(8))[0]\n                    if tier.tier_type == 'IntervalTier':\n                        x2 = struct.unpack('>d', ifile.read(8))[0]\n                    text = bin2str(ifile)\n                    if tier.tier_type == 'IntervalTier':\n                        tier.intervals.append((x1, x2, text))\n                    elif tier.tier_type == 'TextTier':\n                        tier.intervals.append((x1, text))\n                    else:\n                        raise Exception('Tiertype does not exist.')\n        else:\n            def nn(ifile, pat):\n                line = next(ifile).decode(codec)\n                return pat.search(line).group(1)\n\n            regfloat = re.compile('([\\d.]+)\\s*$', flags=re.UNICODE)\n            regint = re.compile('([\\d]+)\\s*$', flags=re.UNICODE)\n            regstr = re.compile('\"(.*)\"\\s*$', flags=re.UNICODE)\n            # Skip the Headers and empty line\n            next(ifile), next(ifile), next(ifile)\n            self.xmin = float(nn(ifile, regfloat))\n            self.xmax = float(nn(ifile, regfloat))\n            # Skip <exists>\n            line = next(ifile)\n            short = line.strip() == b'<exists>'\n            self.tier_num = int(nn(ifile, regint))\n            not short and next(ifile)\n            for i in range(self.tier_num):\n                not short and next(ifile)  # skip item[]: and item[\\d]:\n                tier_type = nn(ifile, regstr)\n                name = nn(ifile, regstr)\n                tier = Tier(0, 0, name=name, tier_type=tier_type)\n                self.tiers.append(tier)\n                tier.xmin = float(nn(ifile, regfloat))\n                tier.xmax = float(nn(ifile, regfloat))\n                for i in range(int(nn(ifile, regint))):\n                    not short and next(ifile)  # skip intervals [\\d]\n                    x1 = float(nn(ifile, regfloat))\n                    if tier.tier_type == 'IntervalTier':\n                        x2 = float(nn(ifile, regfloat))\n                        t = nn(ifile, regstr)\n                        tier.intervals.append((x1, x2, t))\n                    elif tier.tier_type == 'TextTier':\n                        t = nn(ifile, regstr)\n                        tier.intervals.append((x1, t))","method_path":"https:\/\/github.com\/dopefishh\/pympi\/blob\/79c747cde45b5ba203ed93154d8c123ac9c3ef56\/pympi\/Praat.py#L49-L130"}
{"repo_name":"dopefishh\/pympi","method_name":"TextGrid.sort_tiers","method_code":"def sort_tiers(self, key=lambda x: x.name):\n        \"\"\"\"\"\"\n        self.tiers.sort(key=key)","method_summary":"Sort the tiers given the key.","original_method_code":"def sort_tiers(self, key=lambda x: x.name):\n        \"\"\"Sort the tiers given the key. Example key functions:\n\n        Sort according to the tiername in a list:\n\n        ``lambda x: ['name1', 'name2' ... 'namen'].index(x.name)``.\n\n        Sort according to the number of annotations:\n\n        ``lambda x: len(list(x.get_intervals()))``\n\n        :param func key: A key function. Default sorts alphabetically.\n        \"\"\"\n        self.tiers.sort(key=key)","method_path":"https:\/\/github.com\/dopefishh\/pympi\/blob\/79c747cde45b5ba203ed93154d8c123ac9c3ef56\/pympi\/Praat.py#L132-L145"}
{"repo_name":"dopefishh\/pympi","method_name":"TextGrid.add_tier","method_code":"def add_tier(self, name, tier_type='IntervalTier', number=None):\n        \"\"\"\"\"\"\n        if number is None:\n            number = 1 if not self.tiers else len(self.tiers)+1\n        elif number < 1 or number > len(self.tiers):\n            raise ValueError('Number not in [1..{}]'.format(len(self.tiers)))\n        elif tier_type not in Tier.P_TIERS:\n            raise ValueError('tier_type has to be in {}'.format(self.P_TIERS))\n        self.tiers.insert(number-1,\n                          Tier(self.xmin, self.xmax, name, tier_type))\n        return self.tiers[number-1]","method_summary":"Add an IntervalTier or a TextTier on the specified location.","original_method_code":"def add_tier(self, name, tier_type='IntervalTier', number=None):\n        \"\"\"Add an IntervalTier or a TextTier on the specified location.\n\n        :param str name: Name of the tier, duplicate names is allowed.\n        :param str tier_type: Type of the tier.\n        :param int number: Place to insert the tier, when ``None`` the number\n            is generated and the tier will be placed on the bottom.\n        :returns: The created tier.\n        :raises ValueError: If the number is out of bounds.\n        \"\"\"\n        if number is None:\n            number = 1 if not self.tiers else len(self.tiers)+1\n        elif number < 1 or number > len(self.tiers):\n            raise ValueError('Number not in [1..{}]'.format(len(self.tiers)))\n        elif tier_type not in Tier.P_TIERS:\n            raise ValueError('tier_type has to be in {}'.format(self.P_TIERS))\n        self.tiers.insert(number-1,\n                          Tier(self.xmin, self.xmax, name, tier_type))\n        return self.tiers[number-1]","method_path":"https:\/\/github.com\/dopefishh\/pympi\/blob\/79c747cde45b5ba203ed93154d8c123ac9c3ef56\/pympi\/Praat.py#L147-L165"}
{"repo_name":"dopefishh\/pympi","method_name":"TextGrid.remove_tier","method_code":"def remove_tier(self, name_num):\n        \"\"\"\"\"\"\n        if isinstance(name_num, int):\n            del(self.tiers[name_num-1])\n        else:\n            self.tiers = [i for i in self.tiers if i.name != name_num]","method_summary":"Remove a tier, when multiple tiers exist with that name only the first is removed.","original_method_code":"def remove_tier(self, name_num):\n        \"\"\"Remove a tier, when multiple tiers exist with that name only the\n        first is removed.\n\n        :param name_num: Name or number of the tier to remove.\n        :type name_num: int or str\n        :raises IndexError: If there is no tier with that number.\n        \"\"\"\n        if isinstance(name_num, int):\n            del(self.tiers[name_num-1])\n        else:\n            self.tiers = [i for i in self.tiers if i.name != name_num]","method_path":"https:\/\/github.com\/dopefishh\/pympi\/blob\/79c747cde45b5ba203ed93154d8c123ac9c3ef56\/pympi\/Praat.py#L167-L178"}
{"repo_name":"dopefishh\/pympi","method_name":"TextGrid.get_tier","method_code":"def get_tier(self, name_num):\n        \"\"\"\"\"\"\n        return self.tiers[name_num - 1] if isinstance(name_num, int) else\\\n            [i for i in self.tiers if i.name == name_num][0]","method_summary":"Gives a tier, when multiple tiers exist with that name only the first is returned.","original_method_code":"def get_tier(self, name_num):\n        \"\"\"Gives a tier, when multiple tiers exist with that name only the\n        first is returned.\n\n        :param name_num: Name or number of the tier to return.\n        :type name_num: int or str\n        :returns: The tier.\n        :raises IndexError: If the tier doesn't exist.\n        \"\"\"\n        return self.tiers[name_num - 1] if isinstance(name_num, int) else\\\n            [i for i in self.tiers if i.name == name_num][0]","method_path":"https:\/\/github.com\/dopefishh\/pympi\/blob\/79c747cde45b5ba203ed93154d8c123ac9c3ef56\/pympi\/Praat.py#L180-L190"}
{"repo_name":"dopefishh\/pympi","method_name":"TextGrid.to_file","method_code":"def to_file(self, filepath, codec='utf-8', mode='normal'):\n        \"\"\"\"\"\"\n        self.tier_num = len(self.tiers)\n        if mode in ['binary', 'b']:\n            with open(filepath, 'wb') as f:\n                def writebstr(s):\n                    try:\n                        bstr = s.encode('ascii')\n                    except UnicodeError:\n                        f.write(b'\\xff\\xff')\n                        bstr = b''.join(struct.pack('>h', ord(c)) for c in s)\n                    f.write(struct.pack('>h', len(s)))\n                    f.write(bstr)\n\n                f.write(b'ooBinaryFile\\x08TextGrid')\n                f.write(struct.pack('>d', self.xmin))\n                f.write(struct.pack('>d', self.xmax))\n                f.write(b'\\x01')\n                f.write(struct.pack('>i', self.tier_num))\n                for tier in self.tiers:\n                    f.write(chr(len(tier.tier_type)).encode('ascii'))\n                    f.write(tier.tier_type.encode('ascii'))\n                    writebstr(tier.name)\n                    f.write(struct.pack('>d', tier.xmin))\n                    f.write(struct.pack('>d', tier.xmax))\n                    ints = tier.get_all_intervals()\n                    f.write(struct.pack('>i', len(ints)))\n                    itier = tier.tier_type == 'IntervalTier'\n                    for c in ints:\n                        f.write(struct.pack('>d', c[0]))\n                        itier and f.write(struct.pack('>d', c[1]))\n                        writebstr(c[2 if itier else 1])\n        elif mode in ['normal', 'n', 'short', 's']:\n            with codecs.open(filepath, 'w', codec) as f:\n                short = mode[0] == 's'\n\n                def wrt(indent, prefix, value, ff=''):\n                    indent = 0 if short else indent\n                    prefix = '' if short else prefix\n                    if value is not None or not short:\n                        s = u'{{}}{{}}{}\\n'.format(ff)\n                        f.write(s.format(' '*indent, prefix, value))\n\n                f.write(u'File type = \"ooTextFile\"\\n'\n                        u'Object class = \"TextGrid\"\\n\\n')\n                wrt(0, u'xmin = ', self.xmin, '{:f}')\n                wrt(0, u'xmax = ', self.xmax, '{:f}')\n                wrt(0, u'tiers? ', u'<exists>', '{}')\n                wrt(0, u'size = ', self.tier_num, '{:d}')\n                wrt(0, u'item []:', None)\n                for tnum, tier in enumerate(self.tiers, 1):\n                    wrt(4, u'item [{:d}]:'.format(tnum), None)\n                    wrt(8, u'class = ', tier.tier_type, '\"{}\"')\n                    wrt(8, u'name = ', tier.name, '\"{}\"')\n                    wrt(8, u'xmin = ', tier.xmin, '{:f}')\n                    wrt(8, u'xmax = ', tier.xmax, '{:f}')\n                    if tier.tier_type == 'IntervalTier':\n                        ints = tier.get_all_intervals()\n                        wrt(8, u'intervals: size = ', len(ints), '{:d}')\n                        for i, c in enumerate(ints):\n                            wrt(8, 'intervals [{:d}]:'.format(i+1), None)\n                            wrt(12, 'xmin = ', c[0], '{:f}')\n                            wrt(12, 'xmax = ', c[1], '{:f}')\n                            wrt(12, 'text = ', c[2].replace('\"', '\"\"'), '\"{}\"')\n                    elif tier.tier_type == 'TextTier':\n                        wrt(8, u'points: size = ', len(tier.intervals), '{:d}')\n                        for i, c in enumerate(tier.get_intervals()):\n                            wrt(8, 'points [{:d}]:'.format(i+1), None)\n                            wrt(12, 'number = ', c[0], '{:f}')\n                            wrt(12, 'mark = ', c[1].replace('\"', '\"\"'), '\"{}\"')\n        else:\n            raise Exception('Unknown mode')","method_summary":"Write the object to a file.","original_method_code":"def to_file(self, filepath, codec='utf-8', mode='normal'):\n        \"\"\"Write the object to a file.\n\n        :param str filepath: Path of the fil.\n        :param str codec: Text encoding.\n        :param string mode: Flag to for write mode, possible modes:\n            'n'\/'normal', 's'\/'short' and 'b'\/'binary'\n        \"\"\"\n        self.tier_num = len(self.tiers)\n        if mode in ['binary', 'b']:\n            with open(filepath, 'wb') as f:\n                def writebstr(s):\n                    try:\n                        bstr = s.encode('ascii')\n                    except UnicodeError:\n                        f.write(b'\\xff\\xff')\n                        bstr = b''.join(struct.pack('>h', ord(c)) for c in s)\n                    f.write(struct.pack('>h', len(s)))\n                    f.write(bstr)\n\n                f.write(b'ooBinaryFile\\x08TextGrid')\n                f.write(struct.pack('>d', self.xmin))\n                f.write(struct.pack('>d', self.xmax))\n                f.write(b'\\x01')\n                f.write(struct.pack('>i', self.tier_num))\n                for tier in self.tiers:\n                    f.write(chr(len(tier.tier_type)).encode('ascii'))\n                    f.write(tier.tier_type.encode('ascii'))\n                    writebstr(tier.name)\n                    f.write(struct.pack('>d', tier.xmin))\n                    f.write(struct.pack('>d', tier.xmax))\n                    ints = tier.get_all_intervals()\n                    f.write(struct.pack('>i', len(ints)))\n                    itier = tier.tier_type == 'IntervalTier'\n                    for c in ints:\n                        f.write(struct.pack('>d', c[0]))\n                        itier and f.write(struct.pack('>d', c[1]))\n                        writebstr(c[2 if itier else 1])\n        elif mode in ['normal', 'n', 'short', 's']:\n            with codecs.open(filepath, 'w', codec) as f:\n                short = mode[0] == 's'\n\n                def wrt(indent, prefix, value, ff=''):\n                    indent = 0 if short else indent\n                    prefix = '' if short else prefix\n                    if value is not None or not short:\n                        s = u'{{}}{{}}{}\\n'.format(ff)\n                        f.write(s.format(' '*indent, prefix, value))\n\n                f.write(u'File type = \"ooTextFile\"\\n'\n                        u'Object class = \"TextGrid\"\\n\\n')\n                wrt(0, u'xmin = ', self.xmin, '{:f}')\n                wrt(0, u'xmax = ', self.xmax, '{:f}')\n                wrt(0, u'tiers? ', u'<exists>', '{}')\n                wrt(0, u'size = ', self.tier_num, '{:d}')\n                wrt(0, u'item []:', None)\n                for tnum, tier in enumerate(self.tiers, 1):\n                    wrt(4, u'item [{:d}]:'.format(tnum), None)\n                    wrt(8, u'class = ', tier.tier_type, '\"{}\"')\n                    wrt(8, u'name = ', tier.name, '\"{}\"')\n                    wrt(8, u'xmin = ', tier.xmin, '{:f}')\n                    wrt(8, u'xmax = ', tier.xmax, '{:f}')\n                    if tier.tier_type == 'IntervalTier':\n                        ints = tier.get_all_intervals()\n                        wrt(8, u'intervals: size = ', len(ints), '{:d}')\n                        for i, c in enumerate(ints):\n                            wrt(8, 'intervals [{:d}]:'.format(i+1), None)\n                            wrt(12, 'xmin = ', c[0], '{:f}')\n                            wrt(12, 'xmax = ', c[1], '{:f}')\n                            wrt(12, 'text = ', c[2].replace('\"', '\"\"'), '\"{}\"')\n                    elif tier.tier_type == 'TextTier':\n                        wrt(8, u'points: size = ', len(tier.intervals), '{:d}')\n                        for i, c in enumerate(tier.get_intervals()):\n                            wrt(8, 'points [{:d}]:'.format(i+1), None)\n                            wrt(12, 'number = ', c[0], '{:f}')\n                            wrt(12, 'mark = ', c[1].replace('\"', '\"\"'), '\"{}\"')\n        else:\n            raise Exception('Unknown mode')","method_path":"https:\/\/github.com\/dopefishh\/pympi\/blob\/79c747cde45b5ba203ed93154d8c123ac9c3ef56\/pympi\/Praat.py#L218-L295"}
{"repo_name":"dopefishh\/pympi","method_name":"TextGrid.to_eaf","method_code":"def to_eaf(self, skipempty=True, pointlength=0.1):\n        \"\"\"\"\"\"\n        from pympi.Elan import Eaf\n        eaf_out = Eaf()\n        if pointlength <= 0:\n            raise ValueError('Pointlength should be strictly positive')\n        for tier in self.get_tiers():\n            eaf_out.add_tier(tier.name)\n            for ann in tier.get_intervals(True):\n                if tier.tier_type == 'TextTier':\n                    ann = (ann[0], ann[0]+pointlength, ann[1])\n                if ann[2].strip() or not skipempty:\n                    eaf_out.add_annotation(tier.name, int(round(ann[0]*1000)),\n                                           int(round(ann[1]*1000)), ann[2])\n        return eaf_out","method_summary":"Convert the object to an pympi.Elan.Eaf object","original_method_code":"def to_eaf(self, skipempty=True, pointlength=0.1):\n        \"\"\"Convert the object to an pympi.Elan.Eaf object\n\n        :param int pointlength: Length of respective interval from points in\n                                seconds\n        :param bool skipempty: Skip the empty annotations\n        :returns: :class:`pympi.Elan.Eaf` object\n        :raises ImportError: If the Eaf module can't be loaded.\n        :raises ValueError: If the pointlength is not strictly positive.\n        \"\"\"\n        from pympi.Elan import Eaf\n        eaf_out = Eaf()\n        if pointlength <= 0:\n            raise ValueError('Pointlength should be strictly positive')\n        for tier in self.get_tiers():\n            eaf_out.add_tier(tier.name)\n            for ann in tier.get_intervals(True):\n                if tier.tier_type == 'TextTier':\n                    ann = (ann[0], ann[0]+pointlength, ann[1])\n                if ann[2].strip() or not skipempty:\n                    eaf_out.add_annotation(tier.name, int(round(ann[0]*1000)),\n                                           int(round(ann[1]*1000)), ann[2])\n        return eaf_out","method_path":"https:\/\/github.com\/dopefishh\/pympi\/blob\/79c747cde45b5ba203ed93154d8c123ac9c3ef56\/pympi\/Praat.py#L297-L319"}
{"repo_name":"dopefishh\/pympi","method_name":"Tier.add_point","method_code":"def add_point(self, point, value, check=True):\n        \"\"\"\"\"\"\n        if self.tier_type != 'TextTier':\n            raise Exception('Tiertype must be TextTier.')\n        if check and any(i for i in self.intervals if i[0] == point):\n                raise Exception('No overlap is allowed')\n        self.intervals.append((point, value))","method_summary":"Add a point to the TextTier","original_method_code":"def add_point(self, point, value, check=True):\n        \"\"\"Add a point to the TextTier\n\n        :param int point: Time of the point.\n        :param str value: Text of the point.\n        :param bool check: Flag to check for overlap.\n        :raises Exception: If overlap or wrong tiertype.\n        \"\"\"\n        if self.tier_type != 'TextTier':\n            raise Exception('Tiertype must be TextTier.')\n        if check and any(i for i in self.intervals if i[0] == point):\n                raise Exception('No overlap is allowed')\n        self.intervals.append((point, value))","method_path":"https:\/\/github.com\/dopefishh\/pympi\/blob\/79c747cde45b5ba203ed93154d8c123ac9c3ef56\/pympi\/Praat.py#L348-L360"}
{"repo_name":"dopefishh\/pympi","method_name":"Tier.add_interval","method_code":"def add_interval(self, begin, end, value, check=True):\n        \"\"\"\"\"\"\n        if self.tier_type != 'IntervalTier':\n            raise Exception('Tiertype must be IntervalTier')\n        if check:\n            if any(i for i in self.intervals if begin < i[1] and end > i[0]):\n                raise Exception('No overlap is allowed')\n            if begin > end:\n                raise Exception('Begin must be smaller then end')\n        self.intervals.append((begin, end, value))","method_summary":"Add an interval to the IntervalTier.","original_method_code":"def add_interval(self, begin, end, value, check=True):\n        \"\"\"Add an interval to the IntervalTier.\n\n        :param float begin: Start time of the interval.\n        :param float end: End time of the interval.\n        :param str value: Text of the interval.\n        :param bool check: Flag to check for overlap.\n        :raises Exception: If overlap, begin > end or wrong tiertype.\n        \"\"\"\n        if self.tier_type != 'IntervalTier':\n            raise Exception('Tiertype must be IntervalTier')\n        if check:\n            if any(i for i in self.intervals if begin < i[1] and end > i[0]):\n                raise Exception('No overlap is allowed')\n            if begin > end:\n                raise Exception('Begin must be smaller then end')\n        self.intervals.append((begin, end, value))","method_path":"https:\/\/github.com\/dopefishh\/pympi\/blob\/79c747cde45b5ba203ed93154d8c123ac9c3ef56\/pympi\/Praat.py#L362-L378"}
{"repo_name":"dopefishh\/pympi","method_name":"Tier.remove_interval","method_code":"def remove_interval(self, time):\n        \"\"\"\"\"\"\n        if self.tier_type != 'IntervalTier':\n            raise Exception('Tiertype must be IntervalTier.')\n        self.intervals = [i for i in self.intervals\n                          if not(i[0] <= time and i[1] >= time)]","method_summary":"Remove an interval, if no interval is found nothing happens.","original_method_code":"def remove_interval(self, time):\n        \"\"\"Remove an interval, if no interval is found nothing happens.\n\n        :param int time: Time of the interval.\n        :raises TierTypeException: If the tier is not a IntervalTier.\n        \"\"\"\n        if self.tier_type != 'IntervalTier':\n            raise Exception('Tiertype must be IntervalTier.')\n        self.intervals = [i for i in self.intervals\n                          if not(i[0] <= time and i[1] >= time)]","method_path":"https:\/\/github.com\/dopefishh\/pympi\/blob\/79c747cde45b5ba203ed93154d8c123ac9c3ef56\/pympi\/Praat.py#L380-L389"}
{"repo_name":"its-rigs\/Trolly","method_name":"Organisation.get_organisation_information","method_code":"def get_organisation_information(self, query_params=None):\n        ''''''\n        return self.fetch_json(\n            uri_path=self.base_uri,\n            query_params=query_params or {}\n        )","method_summary":"Get information fot this organisation.","original_method_code":"def get_organisation_information(self, query_params=None):\n        '''\n        Get information fot this organisation. Returns a dictionary of values.\n        '''\n        return self.fetch_json(\n            uri_path=self.base_uri,\n            query_params=query_params or {}\n        )","method_path":"https:\/\/github.com\/its-rigs\/Trolly\/blob\/483dc94c352df40dc05ead31820b059b2545cf82\/trolly\/organisation.py#L14-L21"}
{"repo_name":"its-rigs\/Trolly","method_name":"Organisation.get_boards","method_code":"def get_boards(self, **query_params):\n        ''''''\n        boards = self.get_boards_json(self.base_uri, query_params=query_params)\n\n        boards_list = []\n        for board_json in boards:\n            boards_list.append(self.create_board(board_json))\n\n        return boards_list","method_summary":"Get all the boards for this organisation.","original_method_code":"def get_boards(self, **query_params):\n        '''\n        Get all the boards for this organisation. Returns a list of Board s.\n\n        Returns:\n            list(Board): The boards attached to this organisation\n        '''\n        boards = self.get_boards_json(self.base_uri, query_params=query_params)\n\n        boards_list = []\n        for board_json in boards:\n            boards_list.append(self.create_board(board_json))\n\n        return boards_list","method_path":"https:\/\/github.com\/its-rigs\/Trolly\/blob\/483dc94c352df40dc05ead31820b059b2545cf82\/trolly\/organisation.py#L23-L36"}
{"repo_name":"its-rigs\/Trolly","method_name":"Organisation.get_members","method_code":"def get_members(self, **query_params):\n        ''''''\n        members = self.get_members_json(self.base_uri,\n                                        query_params=query_params)\n\n        members_list = []\n        for member_json in members:\n            members_list.append(self.create_member(member_json))\n\n        return members_list","method_summary":"Get all members attached to this organisation.","original_method_code":"def get_members(self, **query_params):\n        '''\n        Get all members attached to this organisation. Returns a list of\n        Member objects\n\n        Returns:\n            list(Member): The members attached to this organisation\n        '''\n        members = self.get_members_json(self.base_uri,\n                                        query_params=query_params)\n\n        members_list = []\n        for member_json in members:\n            members_list.append(self.create_member(member_json))\n\n        return members_list","method_path":"https:\/\/github.com\/its-rigs\/Trolly\/blob\/483dc94c352df40dc05ead31820b059b2545cf82\/trolly\/organisation.py#L38-L53"}
{"repo_name":"its-rigs\/Trolly","method_name":"Organisation.update_organisation","method_code":"def update_organisation(self, query_params=None):\n        ''''''\n        organisation_json = self.fetch_json(\n            uri_path=self.base_uri,\n            http_method='PUT',\n            query_params=query_params or {}\n        )\n\n        return self.create_organisation(organisation_json)","method_summary":"Update this organisations information.","original_method_code":"def update_organisation(self, query_params=None):\n        '''\n        Update this organisations information. Returns a new organisation\n        object.\n        '''\n        organisation_json = self.fetch_json(\n            uri_path=self.base_uri,\n            http_method='PUT',\n            query_params=query_params or {}\n        )\n\n        return self.create_organisation(organisation_json)","method_path":"https:\/\/github.com\/its-rigs\/Trolly\/blob\/483dc94c352df40dc05ead31820b059b2545cf82\/trolly\/organisation.py#L55-L66"}
{"repo_name":"its-rigs\/Trolly","method_name":"Organisation.remove_member","method_code":"def remove_member(self, member_id):\n        ''''''\n        return self.fetch_json(\n            uri_path=self.base_uri + '\/members\/%s' % member_id,\n            http_method='DELETE'\n        )","method_summary":"Remove a member from the organisation.","original_method_code":"def remove_member(self, member_id):\n        '''\n        Remove a member from the organisation.Returns JSON of all members if\n        successful or raises an Unauthorised exception if not.\n        '''\n        return self.fetch_json(\n            uri_path=self.base_uri + '\/members\/%s' % member_id,\n            http_method='DELETE'\n        )","method_path":"https:\/\/github.com\/its-rigs\/Trolly\/blob\/483dc94c352df40dc05ead31820b059b2545cf82\/trolly\/organisation.py#L68-L76"}
{"repo_name":"its-rigs\/Trolly","method_name":"Organisation.add_member_by_id","method_code":"def add_member_by_id(self, member_id, membership_type='normal'):\n        ''''''\n        return self.fetch_json(\n            uri_path=self.base_uri + '\/members\/%s' % member_id,\n            http_method='PUT',\n            query_params={\n                'type': membership_type\n            }\n        )","method_summary":"Add a member to the board using the id. Membership type can be normal or admin.","original_method_code":"def add_member_by_id(self, member_id, membership_type='normal'):\n        '''\n        Add a member to the board using the id. Membership type can be\n        normal or admin. Returns JSON of all members if successful or raises an\n        Unauthorised exception if not.\n        '''\n        return self.fetch_json(\n            uri_path=self.base_uri + '\/members\/%s' % member_id,\n            http_method='PUT',\n            query_params={\n                'type': membership_type\n            }\n        )","method_path":"https:\/\/github.com\/its-rigs\/Trolly\/blob\/483dc94c352df40dc05ead31820b059b2545cf82\/trolly\/organisation.py#L78-L90"}
{"repo_name":"its-rigs\/Trolly","method_name":"Organisation.add_member","method_code":"def add_member(self, email, fullname, membership_type='normal'):\n        ''''''\n        return self.fetch_json(\n            uri_path=self.base_uri + '\/members',\n            http_method='PUT',\n            query_params={\n                'email': email,\n                'fullName': fullname,\n                'type': membership_type\n            }\n        )","method_summary":"Add a member to the board. Membership type can be normal or admin.","original_method_code":"def add_member(self, email, fullname, membership_type='normal'):\n        '''\n        Add a member to the board. Membership type can be normal or admin.\n        Returns JSON of all members if successful or raises an Unauthorised\n        exception if not.\n        '''\n        return self.fetch_json(\n            uri_path=self.base_uri + '\/members',\n            http_method='PUT',\n            query_params={\n                'email': email,\n                'fullName': fullname,\n                'type': membership_type\n            }\n        )","method_path":"https:\/\/github.com\/its-rigs\/Trolly\/blob\/483dc94c352df40dc05ead31820b059b2545cf82\/trolly\/organisation.py#L92-L106"}
{"repo_name":"its-rigs\/Trolly","method_name":"List.get_list_information","method_code":"def get_list_information(self, query_params=None):\n        ''''''\n        return self.fetch_json(\n            uri_path=self.base_uri,\n            query_params=query_params or {}\n        )","method_summary":"Get information for this list.","original_method_code":"def get_list_information(self, query_params=None):\n        '''\n        Get information for this list. Returns a dictionary of values.\n        '''\n        return self.fetch_json(\n            uri_path=self.base_uri,\n            query_params=query_params or {}\n        )","method_path":"https:\/\/github.com\/its-rigs\/Trolly\/blob\/483dc94c352df40dc05ead31820b059b2545cf82\/trolly\/list.py#L18-L25"}
{"repo_name":"its-rigs\/Trolly","method_name":"List.add_card","method_code":"def add_card(self, query_params=None):\n        ''''''\n        card_json = self.fetch_json(\n            uri_path=self.base_uri + '\/cards',\n            http_method='POST',\n            query_params=query_params or {}\n        )\n\n        return self.create_card(card_json)","method_summary":"Create a card for this list.","original_method_code":"def add_card(self, query_params=None):\n        '''\n        Create a card for this list. Returns a Card object.\n        '''\n        card_json = self.fetch_json(\n            uri_path=self.base_uri + '\/cards',\n            http_method='POST',\n            query_params=query_params or {}\n        )\n\n        return self.create_card(card_json)","method_path":"https:\/\/github.com\/its-rigs\/Trolly\/blob\/483dc94c352df40dc05ead31820b059b2545cf82\/trolly\/list.py#L64-L74"}
{"repo_name":"its-rigs\/Trolly","method_name":"Label.get_label_information","method_code":"def get_label_information(self, query_params=None):\n        ''''''\n        return self.fetch_json(\n            uri_path=self.base_uri,\n            query_params=query_params or {}\n        )","method_summary":"Get all information for this Label.","original_method_code":"def get_label_information(self, query_params=None):\n        '''\n        Get all information for this Label. Returns a dictionary of values.\n        '''\n        return self.fetch_json(\n            uri_path=self.base_uri,\n            query_params=query_params or {}\n        )","method_path":"https:\/\/github.com\/its-rigs\/Trolly\/blob\/483dc94c352df40dc05ead31820b059b2545cf82\/trolly\/label.py#L20-L27"}
{"repo_name":"oscarbranson\/latools","method_name":"classifier.format_data","method_code":"def format_data(self, data, scale=True):\n        \"\"\"\"\"\"\n        if len(self.analytes) == 1:\n            \n            d = nominal_values(data[self.analytes[0]])\n            ds = np.array(list(zip(d, np.zeros(len(d)))))\n        else:\n            \n            d = [nominal_values(data[a]) for a in self.analytes]\n            ds = np.vstack(d).T\n\n        \n        finite = np.isfinite(ds).sum(1) == ds.shape[1]\n        \n        sampled = np.arange(data[self.analytes[0]].size)[finite]\n        \n        ds = ds[finite]\n\n        if scale:\n            ds = self.scaler.transform(ds)\n\n        return ds, sampled","method_summary":"Function for converting a dict to an array suitable for sklearn.","original_method_code":"def format_data(self, data, scale=True):\n        \"\"\"\n        Function for converting a dict to an array suitable for sklearn.\n\n        Parameters\n        ----------\n        data : dict\n            A dict of data, containing all elements of\n            `analytes` as items.\n        scale : bool\n            Whether or not to scale the data. Should always be\n            `True`, unless used by `classifier.fitting_data`\n            where a scaler hasn't been created yet.\n\n        Returns\n        -------\n        A data array suitable for use with `sklearn.cluster`.\n        \"\"\"\n        if len(self.analytes) == 1:\n            # if single analyte\n            d = nominal_values(data[self.analytes[0]])\n            ds = np.array(list(zip(d, np.zeros(len(d)))))\n        else:\n            # package multiple analytes\n            d = [nominal_values(data[a]) for a in self.analytes]\n            ds = np.vstack(d).T\n\n        # identify all nan values\n        finite = np.isfinite(ds).sum(1) == ds.shape[1]\n        # remember which values are sampled\n        sampled = np.arange(data[self.analytes[0]].size)[finite]\n        # remove all nan values\n        ds = ds[finite]\n\n        if scale:\n            ds = self.scaler.transform(ds)\n\n        return ds, sampled","method_path":"https:\/\/github.com\/oscarbranson\/latools\/blob\/cd25a650cfee318152f234d992708511f7047fbe\/latools\/filtering\/classifier_obj.py#L28-L65"}
{"repo_name":"oscarbranson\/latools","method_name":"classifier.fitting_data","method_code":"def fitting_data(self, data):\n        \"\"\"\"\"\"\n        ds_fit, _ = self.format_data(data, scale=False)\n\n        \n        self.scaler = preprocessing.StandardScaler().fit(ds_fit)\n\n        \n        return self.scaler.transform(ds_fit)","method_summary":"Function to format data for cluster fitting.","original_method_code":"def fitting_data(self, data):\n        \"\"\"\n        Function to format data for cluster fitting.\n\n        Parameters\n        ----------\n        data : dict\n            A dict of data, containing all elements of\n            `analytes` as items.\n\n        Returns\n        -------\n        A data array for initial cluster fitting.\n        \"\"\"\n        ds_fit, _ = self.format_data(data, scale=False)\n\n        # define scaler\n        self.scaler = preprocessing.StandardScaler().fit(ds_fit)\n\n        # scale data and return\n        return self.scaler.transform(ds_fit)","method_path":"https:\/\/github.com\/oscarbranson\/latools\/blob\/cd25a650cfee318152f234d992708511f7047fbe\/latools\/filtering\/classifier_obj.py#L67-L87"}
{"repo_name":"oscarbranson\/latools","method_name":"classifier.fit_kmeans","method_code":"def fit_kmeans(self, data, n_clusters, **kwargs):\n        \"\"\"\"\"\"\n        km = cl.KMeans(n_clusters=n_clusters, **kwargs)\n        km.fit(data)\n        return km","method_summary":"Fit KMeans clustering algorithm to data.","original_method_code":"def fit_kmeans(self, data, n_clusters, **kwargs):\n        \"\"\"\n        Fit KMeans clustering algorithm to data.\n\n        Parameters\n        ----------\n        data : array-like\n            A dataset formatted by `classifier.fitting_data`.\n        n_clusters : int\n            The number of clusters in the data.\n        **kwargs\n            passed to `sklearn.cluster.KMeans`.\n\n        Returns\n        -------\n        Fitted `sklearn.cluster.KMeans` object.\n        \"\"\"\n        km = cl.KMeans(n_clusters=n_clusters, **kwargs)\n        km.fit(data)\n        return km","method_path":"https:\/\/github.com\/oscarbranson\/latools\/blob\/cd25a650cfee318152f234d992708511f7047fbe\/latools\/filtering\/classifier_obj.py#L89-L108"}
{"repo_name":"oscarbranson\/latools","method_name":"classifier.fit_meanshift","method_code":"def fit_meanshift(self, data, bandwidth=None, bin_seeding=False, **kwargs):\n        \"\"\"\"\"\"\n        if bandwidth is None:\n            bandwidth = cl.estimate_bandwidth(data)\n        ms = cl.MeanShift(bandwidth=bandwidth, bin_seeding=bin_seeding)\n        ms.fit(data)\n        return ms","method_summary":"Fit MeanShift clustering algorithm to data.","original_method_code":"def fit_meanshift(self, data, bandwidth=None, bin_seeding=False, **kwargs):\n        \"\"\"\n        Fit MeanShift clustering algorithm to data.\n\n        Parameters\n        ----------\n        data : array-like\n            A dataset formatted by `classifier.fitting_data`.\n        bandwidth : float\n            The bandwidth value used during clustering.\n            If none, determined automatically. Note:\n            the data are scaled before clutering, so\n            this is not in the same units as the data.\n        bin_seeding : bool\n            Whether or not to use 'bin_seeding'. See\n            documentation for `sklearn.cluster.MeanShift`.\n        **kwargs\n            passed to `sklearn.cluster.MeanShift`.\n\n        Returns\n        -------\n        Fitted `sklearn.cluster.MeanShift` object.\n        \"\"\"\n        if bandwidth is None:\n            bandwidth = cl.estimate_bandwidth(data)\n        ms = cl.MeanShift(bandwidth=bandwidth, bin_seeding=bin_seeding)\n        ms.fit(data)\n        return ms","method_path":"https:\/\/github.com\/oscarbranson\/latools\/blob\/cd25a650cfee318152f234d992708511f7047fbe\/latools\/filtering\/classifier_obj.py#L110-L137"}
{"repo_name":"oscarbranson\/latools","method_name":"classifier.fit","method_code":"def fit(self, data, method='kmeans', **kwargs):\n        \"\"\"\"\"\"\n        self.method = method\n        ds_fit = self.fitting_data(data)\n        mdict = {'kmeans': self.fit_kmeans,\n                 'meanshift': self.fit_meanshift}\n        clust = mdict[method]\n\n        self.classifier = clust(data=ds_fit, **kwargs)\n\n        \n        c0 = self.classifier.cluster_centers_.T[self.sort_by]\n        self.classifier.cluster_centers_ = self.classifier.cluster_centers_[np.argsort(c0)]\n\n        \n        self.classifier.labels_ = self.classifier.predict(ds_fit)\n        self.classifier.ulabels_ = np.unique(self.classifier.labels_)\n\n        return","method_summary":"fit classifiers from large dataset.","original_method_code":"def fit(self, data, method='kmeans', **kwargs):\n        \"\"\"\n        fit classifiers from large dataset.\n\n        Parameters\n        ----------\n        data : dict\n            A dict of data for clustering. Must contain\n            items with the same name as analytes used for\n            clustering.\n        method : str\n            A string defining the clustering method used. Can be:\n\n            * 'kmeans' : K-Means clustering algorithm\n            * 'meanshift' : Meanshift algorithm\n\n        n_clusters : int\n            *K-Means only*. The numebr of clusters to identify\n        bandwidth : float\n            *Meanshift only.*\n            The bandwidth value used during clustering.\n            If none, determined automatically. Note:\n            the data are scaled before clutering, so\n            this is not in the same units as the data.\n        bin_seeding : bool\n            *Meanshift only.*\n            Whether or not to use 'bin_seeding'. See\n            documentation for `sklearn.cluster.MeanShift`.\n        **kwargs :\n            passed to `sklearn.cluster.MeanShift`.\n\n        Returns\n        -------\n        list\n        \"\"\"\n        self.method = method\n        ds_fit = self.fitting_data(data)\n        mdict = {'kmeans': self.fit_kmeans,\n                 'meanshift': self.fit_meanshift}\n        clust = mdict[method]\n\n        self.classifier = clust(data=ds_fit, **kwargs)\n\n        # sort cluster centers by value of first column, to avoid random variation.\n        c0 = self.classifier.cluster_centers_.T[self.sort_by]\n        self.classifier.cluster_centers_ = self.classifier.cluster_centers_[np.argsort(c0)]\n\n        # recalculate the labels, so it's consistent with cluster centers\n        self.classifier.labels_ = self.classifier.predict(ds_fit)\n        self.classifier.ulabels_ = np.unique(self.classifier.labels_)\n\n        return","method_path":"https:\/\/github.com\/oscarbranson\/latools\/blob\/cd25a650cfee318152f234d992708511f7047fbe\/latools\/filtering\/classifier_obj.py#L139-L190"}
{"repo_name":"oscarbranson\/latools","method_name":"classifier.predict","method_code":"def predict(self, data):\n        \"\"\"\"\"\"\n        size = data[self.analytes[0]].size\n        ds, sampled = self.format_data(data)\n\n        \n        cs = self.classifier.predict(ds)\n        \n        clusters = self.map_clusters(size, sampled, cs)\n\n        return clusters","method_summary":"Label new data with cluster identities.","original_method_code":"def predict(self, data):\n        \"\"\"\n        Label new data with cluster identities.\n\n        Parameters\n        ----------\n        data : dict\n            A data dict containing the same analytes used to\n            fit the classifier.\n        sort_by : str\n            The name of an analyte used to sort the resulting\n            clusters. If None, defaults to the first analyte\n            used in fitting.\n\n        Returns\n        -------\n        array of clusters the same length as the data.\n        \"\"\"\n        size = data[self.analytes[0]].size\n        ds, sampled = self.format_data(data)\n\n        # predict clusters\n        cs = self.classifier.predict(ds)\n        # map clusters to original index\n        clusters = self.map_clusters(size, sampled, cs)\n\n        return clusters","method_path":"https:\/\/github.com\/oscarbranson\/latools\/blob\/cd25a650cfee318152f234d992708511f7047fbe\/latools\/filtering\/classifier_obj.py#L192-L218"}
{"repo_name":"oscarbranson\/latools","method_name":"classifier.map_clusters","method_code":"def map_clusters(self, size, sampled, clusters):\n        \"\"\"\"\"\"\n        ids = np.zeros(size, dtype=int)\n        ids[:] = -2\n\n        ids[sampled] = clusters\n\n        return ids","method_summary":"Translate cluster identity back to original data size.","original_method_code":"def map_clusters(self, size, sampled, clusters):\n        \"\"\"\n        Translate cluster identity back to original data size.\n\n        Parameters\n        ----------\n        size : int\n            size of original dataset\n        sampled : array-like\n            integer array describing location of finite values\n            in original data.\n        clusters : array-like\n            integer array of cluster identities\n\n        Returns\n        -------\n        list of cluster identities the same length as original\n        data. Where original data are non-finite, returns -2.\n\n        \"\"\"\n        ids = np.zeros(size, dtype=int)\n        ids[:] = -2\n\n        ids[sampled] = clusters\n\n        return ids","method_path":"https:\/\/github.com\/oscarbranson\/latools\/blob\/cd25a650cfee318152f234d992708511f7047fbe\/latools\/filtering\/classifier_obj.py#L220-L245"}
{"repo_name":"oscarbranson\/latools","method_name":"classifier.sort_clusters","method_code":"def sort_clusters(self, data, cs, sort_by):\n        \"\"\"\"\"\"\n        \n        sdat = data[sort_by]\n\n        means = []\n        nclusts = np.arange(cs.max() + 1)\n        for c in nclusts:\n            means.append(np.nanmean(sdat[cs == c]))\n\n        \n        means = np.array(means)\n        rank = np.zeros(means.size)\n        rank[np.argsort(means)] = np.arange(means.size)\n\n        csn = cs.copy()\n        for c, o in zip(nclusts, rank):\n            csn[cs == c] = o\n\n        return csn","method_summary":"Sort clusters by the concentration of a particular analyte.","original_method_code":"def sort_clusters(self, data, cs, sort_by):\n        \"\"\"\n        Sort clusters by the concentration of a particular analyte.\n\n        Parameters\n        ----------\n        data : dict\n            A dataset containing sort_by as a key.\n        cs : array-like\n            An array of clusters, the same length as values of data.\n        sort_by : str\n            analyte to sort the clusters by\n\n        Returns\n        -------\n        array of clusters, sorted by mean value of sort_by analyte.\n        \"\"\"\n        # label the clusters according to their contents\n        sdat = data[sort_by]\n\n        means = []\n        nclusts = np.arange(cs.max() + 1)\n        for c in nclusts:\n            means.append(np.nanmean(sdat[cs == c]))\n\n        # create ranks\n        means = np.array(means)\n        rank = np.zeros(means.size)\n        rank[np.argsort(means)] = np.arange(means.size)\n\n        csn = cs.copy()\n        for c, o in zip(nclusts, rank):\n            csn[cs == c] = o\n\n        return csn","method_path":"https:\/\/github.com\/oscarbranson\/latools\/blob\/cd25a650cfee318152f234d992708511f7047fbe\/latools\/filtering\/classifier_obj.py#L247-L281"}
{"repo_name":"oscarbranson\/latools","method_name":"get_date","method_code":"def get_date(datetime, time_format=None):\n    \"\"\"\"\"\"\n    if time_format is None:\n        t = du.parser.parse(datetime)\n    else:\n        t = dt.datetime.strftime(datetime, time_format)\n    return t","method_summary":"Return a datetime oject from a string, with optional time format.","original_method_code":"def get_date(datetime, time_format=None):\n    \"\"\"\n    Return a datetime oject from a string, with optional time format.\n\n    Parameters\n    ----------\n    datetime : str\n        Date-time as string in any sensible format.\n    time_format : datetime str (optional)\n        String describing the datetime format. If missing uses\n        dateutil.parser to guess time format.\n    \"\"\"\n    if time_format is None:\n        t = du.parser.parse(datetime)\n    else:\n        t = dt.datetime.strftime(datetime, time_format)\n    return t","method_path":"https:\/\/github.com\/oscarbranson\/latools\/blob\/cd25a650cfee318152f234d992708511f7047fbe\/latools\/helpers\/helpers.py#L29-L45"}
{"repo_name":"oscarbranson\/latools","method_name":"unitpicker","method_code":"def unitpicker(a, llim=0.1, denominator=None, focus_stage=None):\n    \"\"\"\"\"\"\n\n    if not isinstance(a, (int, float)):\n        a = nominal_values(a)\n        a = np.percentile(a[~np.isnan(a)], 25)\n\n    if denominator is not None:\n        pd = pretty_element(denominator)\n    else:\n        pd = ''\n\n    if focus_stage == 'calibrated':\n        udict = {0: 'mol\/mol ' + pd,\n                 1: 'mmol\/mol ' + pd,\n                 2: '$\\mu$mol\/mol ' + pd,\n                 3: 'nmol\/mol ' + pd,\n                 4: 'pmol\/mol ' + pd,\n                 5: 'fmol\/mol ' + pd}\n    elif focus_stage == 'ratios':\n        udict = {0: 'counts\/count ' + pd,\n                 1: '$10^{-3}$ counts\/count ' + pd,\n                 2: '$10^{-6}$ counts\/count ' + pd,\n                 3: '$10^{-9}$ counts\/count ' + pd,\n                 4: '$10^{-12}$ counts\/count ' + pd,\n                 5: '$10^{-15}$ counts\/count ' + pd}\n    elif focus_stage in ('rawdata', 'despiked', 'bkgsub'):\n        udict = udict = {0: 'counts',\n                         1: '$10^{-3}$ counts',\n                         2: '$10^{-6}$ counts',\n                         3: '$10^{-9}$ counts',\n                         4: '$10^{-12}$ counts',\n                         5: '$10^{-15}$ counts'}\n    else:\n        udict = {0: '', 1: '', 2: '', 3: '', 4: '', 5: ''}\n\n    a = abs(a)\n    n = 0\n    if a < llim:\n        while a < llim:\n            a *= 1000\n            n += 1\n    return float(1000**n), udict[n]","method_summary":"Determines the most appropriate plotting unit for data.","original_method_code":"def unitpicker(a, llim=0.1, denominator=None, focus_stage=None):\n    \"\"\"\n    Determines the most appropriate plotting unit for data.\n\n    Parameters\n    ----------\n    a : float or array-like\n        number to optimise. If array like, the 25% quantile is optimised.\n    llim : float\n        minimum allowable value in scaled data.\n\n    Returns\n    -------\n    (float, str)\n        (multiplier, unit)\n    \"\"\"\n\n    if not isinstance(a, (int, float)):\n        a = nominal_values(a)\n        a = np.percentile(a[~np.isnan(a)], 25)\n\n    if denominator is not None:\n        pd = pretty_element(denominator)\n    else:\n        pd = ''\n\n    if focus_stage == 'calibrated':\n        udict = {0: 'mol\/mol ' + pd,\n                 1: 'mmol\/mol ' + pd,\n                 2: '$\\mu$mol\/mol ' + pd,\n                 3: 'nmol\/mol ' + pd,\n                 4: 'pmol\/mol ' + pd,\n                 5: 'fmol\/mol ' + pd}\n    elif focus_stage == 'ratios':\n        udict = {0: 'counts\/count ' + pd,\n                 1: '$10^{-3}$ counts\/count ' + pd,\n                 2: '$10^{-6}$ counts\/count ' + pd,\n                 3: '$10^{-9}$ counts\/count ' + pd,\n                 4: '$10^{-12}$ counts\/count ' + pd,\n                 5: '$10^{-15}$ counts\/count ' + pd}\n    elif focus_stage in ('rawdata', 'despiked', 'bkgsub'):\n        udict = udict = {0: 'counts',\n                         1: '$10^{-3}$ counts',\n                         2: '$10^{-6}$ counts',\n                         3: '$10^{-9}$ counts',\n                         4: '$10^{-12}$ counts',\n                         5: '$10^{-15}$ counts'}\n    else:\n        udict = {0: '', 1: '', 2: '', 3: '', 4: '', 5: ''}\n\n    a = abs(a)\n    n = 0\n    if a < llim:\n        while a < llim:\n            a *= 1000\n            n += 1\n    return float(1000**n), udict[n]","method_path":"https:\/\/github.com\/oscarbranson\/latools\/blob\/cd25a650cfee318152f234d992708511f7047fbe\/latools\/helpers\/helpers.py#L72-L128"}
{"repo_name":"chrisjrn\/registrasion","method_name":"ProductsForm","method_code":"def ProductsForm(category, products):\n    ''''''\n\n    \n    cat = inventory.Category\n    RENDER_TYPES = {\n        cat.RENDER_TYPE_QUANTITY: _QuantityBoxProductsForm,\n        cat.RENDER_TYPE_RADIO: _RadioButtonProductsForm,\n        cat.RENDER_TYPE_ITEM_QUANTITY: _ItemQuantityProductsForm,\n        cat.RENDER_TYPE_CHECKBOX: _CheckboxProductsForm,\n    }\n\n    \n    class ProductsForm(RENDER_TYPES[category.render_type]):\n        pass\n\n    products = list(products)\n    products.sort(key=lambda prod: prod.order)\n\n    ProductsForm.set_fields(category, products)\n\n    if category.render_type == inventory.Category.RENDER_TYPE_ITEM_QUANTITY:\n        ProductsForm = forms.formset_factory(\n            ProductsForm,\n            formset=_ItemQuantityProductsFormSet,\n        )\n\n    return ProductsForm","method_summary":"Produces an appropriate _ProductsForm subclass for the given render type.","original_method_code":"def ProductsForm(category, products):\n    ''' Produces an appropriate _ProductsForm subclass for the given render\n    type. '''\n\n    # Each Category.RENDER_TYPE value has a subclass here.\n    cat = inventory.Category\n    RENDER_TYPES = {\n        cat.RENDER_TYPE_QUANTITY: _QuantityBoxProductsForm,\n        cat.RENDER_TYPE_RADIO: _RadioButtonProductsForm,\n        cat.RENDER_TYPE_ITEM_QUANTITY: _ItemQuantityProductsForm,\n        cat.RENDER_TYPE_CHECKBOX: _CheckboxProductsForm,\n    }\n\n    # Produce a subclass of _ProductsForm which we can alter the base_fields on\n    class ProductsForm(RENDER_TYPES[category.render_type]):\n        pass\n\n    products = list(products)\n    products.sort(key=lambda prod: prod.order)\n\n    ProductsForm.set_fields(category, products)\n\n    if category.render_type == inventory.Category.RENDER_TYPE_ITEM_QUANTITY:\n        ProductsForm = forms.formset_factory(\n            ProductsForm,\n            formset=_ItemQuantityProductsFormSet,\n        )\n\n    return ProductsForm","method_path":"https:\/\/github.com\/chrisjrn\/registrasion\/blob\/461d5846c6f9f3b7099322a94f5d9911564448e4\/registrasion\/forms.py#L88-L116"}
{"repo_name":"chrisjrn\/registrasion","method_name":"staff_products_form_factory","method_code":"def staff_products_form_factory(user):\n    ''''''\n\n    products = inventory.Product.objects.all()\n    products = ProductController.available_products(user, products=products)\n\n    product_ids = [product.id for product in products]\n    product_set = inventory.Product.objects.filter(id__in=product_ids)\n\n    class StaffProductsForm(forms.Form):\n        ''''''\n\n        product = forms.ModelChoiceField(\n            widget=forms.Select,\n            queryset=product_set,\n        )\n\n        quantity = forms.IntegerField(\n            min_value=0,\n        )\n\n    return StaffProductsForm","method_summary":"Creates a StaffProductsForm that restricts the available products to those that are available to a user.","original_method_code":"def staff_products_form_factory(user):\n    ''' Creates a StaffProductsForm that restricts the available products to\n    those that are available to a user. '''\n\n    products = inventory.Product.objects.all()\n    products = ProductController.available_products(user, products=products)\n\n    product_ids = [product.id for product in products]\n    product_set = inventory.Product.objects.filter(id__in=product_ids)\n\n    class StaffProductsForm(forms.Form):\n        ''' Form for allowing staff to add an item to a user's cart. '''\n\n        product = forms.ModelChoiceField(\n            widget=forms.Select,\n            queryset=product_set,\n        )\n\n        quantity = forms.IntegerField(\n            min_value=0,\n        )\n\n    return StaffProductsForm","method_path":"https:\/\/github.com\/chrisjrn\/registrasion\/blob\/461d5846c6f9f3b7099322a94f5d9911564448e4\/registrasion\/forms.py#L437-L459"}
{"repo_name":"chrisjrn\/registrasion","method_name":"_HasProductsFields.add_product_error","method_code":"def add_product_error(self, product, error):\n        ''''''\n\n        ''''''\n\n        self.add_error(self.field_name(product), error)","method_summary":"Adds an error to the given product's field","original_method_code":"def add_product_error(self, product, error):\n        ''' Adds an error to the given product's field '''\n\n        ''' if product in field_names:\n            field = field_names[product]\n        elif isinstance(product, inventory.Product):\n            return\n        else:\n            field = None '''\n\n        self.add_error(self.field_name(product), error)","method_path":"https:\/\/github.com\/chrisjrn\/registrasion\/blob\/461d5846c6f9f3b7099322a94f5d9911564448e4\/registrasion\/forms.py#L152-L162"}
{"repo_name":"chrisjrn\/registrasion","method_name":"_ItemQuantityProductsFormSet.initial_data","method_code":"def initial_data(cls, product_quantities):\n        ''''''\n\n        f = [\n            {\n                _ItemQuantityProductsForm.CHOICE_FIELD: product.id,\n                _ItemQuantityProductsForm.QUANTITY_FIELD: quantity,\n            }\n            for product, quantity in product_quantities\n            if quantity > 0\n        ]\n        return f","method_summary":"Prepares initial data for an instance of this form. product_quantities is a sequence of (product,quantity) tuples","original_method_code":"def initial_data(cls, product_quantities):\n        ''' Prepares initial data for an instance of this form.\n        product_quantities is a sequence of (product,quantity) tuples '''\n\n        f = [\n            {\n                _ItemQuantityProductsForm.CHOICE_FIELD: product.id,\n                _ItemQuantityProductsForm.QUANTITY_FIELD: quantity,\n            }\n            for product, quantity in product_quantities\n            if quantity > 0\n        ]\n        return f","method_path":"https:\/\/github.com\/chrisjrn\/registrasion\/blob\/461d5846c6f9f3b7099322a94f5d9911564448e4\/registrasion\/forms.py#L367-L379"}
{"repo_name":"chrisjrn\/registrasion","method_name":"_ItemQuantityProductsFormSet.product_quantities","method_code":"def product_quantities(self):\n        ''''''\n\n        products = set()\n        \n        all_products = set()\n\n        for form in self:\n            if form.empty_permitted and not form.cleaned_data:\n                \n                continue\n\n            for product, quantity in form.product_quantities():\n                all_products.add(product)\n                if quantity == 0:\n                    continue\n                if product in products:\n                    form.add_error(\n                        _ItemQuantityProductsForm.CHOICE_FIELD,\n                        \"You may only choose each product type once.\",\n                    )\n                    form.add_error(\n                        _ItemQuantityProductsForm.QUANTITY_FIELD,\n                        \"You may only choose each product type once.\",\n                    )\n                products.add(product)\n                yield product, quantity\n\n        for product in (all_products - products):\n            yield product, 0","method_summary":"Yields a sequence of (product, quantity) tuples from the cleaned form data.","original_method_code":"def product_quantities(self):\n        ''' Yields a sequence of (product, quantity) tuples from the\n        cleaned form data. '''\n\n        products = set()\n        # Track everything so that we can yield some zeroes\n        all_products = set()\n\n        for form in self:\n            if form.empty_permitted and not form.cleaned_data:\n                # This is the magical empty form at the end of the list.\n                continue\n\n            for product, quantity in form.product_quantities():\n                all_products.add(product)\n                if quantity == 0:\n                    continue\n                if product in products:\n                    form.add_error(\n                        _ItemQuantityProductsForm.CHOICE_FIELD,\n                        \"You may only choose each product type once.\",\n                    )\n                    form.add_error(\n                        _ItemQuantityProductsForm.QUANTITY_FIELD,\n                        \"You may only choose each product type once.\",\n                    )\n                products.add(product)\n                yield product, quantity\n\n        for product in (all_products - products):\n            yield product, 0","method_path":"https:\/\/github.com\/chrisjrn\/registrasion\/blob\/461d5846c6f9f3b7099322a94f5d9911564448e4\/registrasion\/forms.py#L381-L411"}
{"repo_name":"chrisjrn\/registrasion","method_name":"BatchController.memoise","method_code":"def memoise(cls, func):\n        ''''''\n\n        @functools.wraps(func)\n        def f(*a):\n\n            for arg in a:\n                if isinstance(arg, User):\n                    user = arg\n                    break\n            else:\n                raise ValueError(\"One position argument must be a User\")\n\n            func_key = (func, tuple(a))\n            cache = cls.get_cache(user)\n\n            if func_key not in cache:\n                cache[func_key] = func(*a)\n\n            return cache[func_key]\n\n        return f","method_summary":"Decorator that stores the result of the stored function in the user's results cache until the batch completes. Keyword arguments are not yet supported.","original_method_code":"def memoise(cls, func):\n        ''' Decorator that stores the result of the stored function in the\n        user's results cache until the batch completes. Keyword arguments are\n        not yet supported.\n\n        Arguments:\n            func (callable(*a)): The function whose results we want\n                to store. The positional arguments, ``a``, are used as cache\n                keys.\n\n        Returns:\n            callable(*a): The memosing version of ``func``.\n\n        '''\n\n        @functools.wraps(func)\n        def f(*a):\n\n            for arg in a:\n                if isinstance(arg, User):\n                    user = arg\n                    break\n            else:\n                raise ValueError(\"One position argument must be a User\")\n\n            func_key = (func, tuple(a))\n            cache = cls.get_cache(user)\n\n            if func_key not in cache:\n                cache[func_key] = func(*a)\n\n            return cache[func_key]\n\n        return f","method_path":"https:\/\/github.com\/chrisjrn\/registrasion\/blob\/461d5846c6f9f3b7099322a94f5d9911564448e4\/registrasion\/controllers\/batch.py#L71-L104"}
{"repo_name":"chrisjrn\/registrasion","method_name":"model_fields_form_factory","method_code":"def model_fields_form_factory(model):\n    ''''''\n\n    fields = model._meta.get_fields()\n\n    choices = []\n    for field in fields:\n        if hasattr(field, \"verbose_name\"):\n            choices.append((field.name, field.verbose_name))\n\n    class ModelFieldsForm(forms.Form):\n        fields = forms.MultipleChoiceField(\n            choices=choices,\n            required=False,\n        )\n\n    return ModelFieldsForm","method_summary":"Creates a form for specifying fields from a model to display.","original_method_code":"def model_fields_form_factory(model):\n    ''' Creates a form for specifying fields from a model to display. '''\n\n    fields = model._meta.get_fields()\n\n    choices = []\n    for field in fields:\n        if hasattr(field, \"verbose_name\"):\n            choices.append((field.name, field.verbose_name))\n\n    class ModelFieldsForm(forms.Form):\n        fields = forms.MultipleChoiceField(\n            choices=choices,\n            required=False,\n        )\n\n    return ModelFieldsForm","method_path":"https:\/\/github.com\/chrisjrn\/registrasion\/blob\/461d5846c6f9f3b7099322a94f5d9911564448e4\/registrasion\/reporting\/forms.py#L81-L97"}
{"repo_name":"chrisjrn\/registrasion","method_name":"ItemController._items","method_code":"def _items(self, cart_status, category=None):\n        ''''''\n\n        if not isinstance(cart_status, Iterable):\n            cart_status = [cart_status]\n\n        status_query = (\n            Q(productitem__cart__status=status) for status in cart_status\n        )\n\n        in_cart = Q(productitem__cart__user=self.user)\n        in_cart = in_cart & reduce(operator.__or__, status_query)\n\n        quantities_in_cart = When(\n            in_cart,\n            then=\"productitem__quantity\",\n        )\n\n        quantities_or_zero = Case(\n            quantities_in_cart,\n            default=Value(0),\n        )\n\n        products = inventory.Product.objects\n\n        if category:\n            products = products.filter(category=category)\n\n        products = products.select_related(\"category\")\n        products = products.annotate(quantity=Sum(quantities_or_zero))\n        products = products.filter(quantity__gt=0)\n\n        out = []\n        for prod in products:\n            out.append(ProductAndQuantity(prod, prod.quantity))\n        return out","method_summary":"Aggregates the items that this user has purchased.","original_method_code":"def _items(self, cart_status, category=None):\n        ''' Aggregates the items that this user has purchased.\n\n        Arguments:\n            cart_status (int or Iterable(int)): etc\n            category (Optional[models.inventory.Category]): the category\n                of items to restrict to.\n\n        Returns:\n            [ProductAndQuantity, ...]: A list of product-quantity pairs,\n                aggregating like products from across multiple invoices.\n\n        '''\n\n        if not isinstance(cart_status, Iterable):\n            cart_status = [cart_status]\n\n        status_query = (\n            Q(productitem__cart__status=status) for status in cart_status\n        )\n\n        in_cart = Q(productitem__cart__user=self.user)\n        in_cart = in_cart & reduce(operator.__or__, status_query)\n\n        quantities_in_cart = When(\n            in_cart,\n            then=\"productitem__quantity\",\n        )\n\n        quantities_or_zero = Case(\n            quantities_in_cart,\n            default=Value(0),\n        )\n\n        products = inventory.Product.objects\n\n        if category:\n            products = products.filter(category=category)\n\n        products = products.select_related(\"category\")\n        products = products.annotate(quantity=Sum(quantities_or_zero))\n        products = products.filter(quantity__gt=0)\n\n        out = []\n        for prod in products:\n            out.append(ProductAndQuantity(prod, prod.quantity))\n        return out","method_path":"https:\/\/github.com\/chrisjrn\/registrasion\/blob\/461d5846c6f9f3b7099322a94f5d9911564448e4\/registrasion\/controllers\/item.py#L37-L83"}
{"repo_name":"chrisjrn\/registrasion","method_name":"ItemController.items_purchased","method_code":"def items_purchased(self, category=None):\n        ''''''\n        return self._items(commerce.Cart.STATUS_PAID, category=category)","method_summary":"Aggregates the items that this user has purchased.","original_method_code":"def items_purchased(self, category=None):\n        ''' Aggregates the items that this user has purchased.\n\n        Arguments:\n            category (Optional[models.inventory.Category]): the category\n                of items to restrict to.\n\n        Returns:\n            [ProductAndQuantity, ...]: A list of product-quantity pairs,\n                aggregating like products from across multiple invoices.\n\n        '''\n        return self._items(commerce.Cart.STATUS_PAID, category=category)","method_path":"https:\/\/github.com\/chrisjrn\/registrasion\/blob\/461d5846c6f9f3b7099322a94f5d9911564448e4\/registrasion\/controllers\/item.py#L90-L102"}
{"repo_name":"chrisjrn\/registrasion","method_name":"Sender.send_email","method_code":"def send_email(self, to, kind, **kwargs):\n        ''''''\n\n        return __send_email__(self.template_prefix, to, kind, **kwargs)","method_summary":"Sends an e-mail to the given address.","original_method_code":"def send_email(self, to, kind, **kwargs):\n        ''' Sends an e-mail to the given address.\n\n        to: The address\n        kind: the ID for an e-mail kind; it should point to a subdirectory of\n            self.template_prefix containing subject.txt and message.html, which\n            are django templates for the subject and HTML message respectively.\n\n        context: a context for rendering the e-mail.\n\n        '''\n\n        return __send_email__(self.template_prefix, to, kind, **kwargs)","method_path":"https:\/\/github.com\/chrisjrn\/registrasion\/blob\/461d5846c6f9f3b7099322a94f5d9911564448e4\/registrasion\/contrib\/mail.py#L17-L29"}
{"repo_name":"ekmmetering\/ekmmeters","method_name":"ekm_log","method_code":"def ekm_log(logstr, priority=3):\n    \"\"\"\"\"\"\n    if priority <= ekmmeters_log_level:\n        dt = datetime.datetime\n        stamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M.%f\")\n        ekmmeters_log_func(\"[EKM Meter Debug Message: \" + stamp + \"] -> \" + logstr)\n    pass","method_summary":"Send string to module level log","original_method_code":"def ekm_log(logstr, priority=3):\n    \"\"\" Send string to module level log\n\n    Args:\n        logstr (str): string to print.\n        priority (int): priority, supports 3 (default) and 4 (special).\n    \"\"\"\n    if priority <= ekmmeters_log_level:\n        dt = datetime.datetime\n        stamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M.%f\")\n        ekmmeters_log_func(\"[EKM Meter Debug Message: \" + stamp + \"] -> \" + logstr)\n    pass","method_path":"https:\/\/github.com\/ekmmetering\/ekmmeters\/blob\/b3748bdf30263bfa46ea40157bdf8df2522e1904\/ekmmeters.py#L67-L78"}
{"repo_name":"ekmmetering\/ekmmeters","method_name":"SerialPort.initPort","method_code":"def initPort(self):\n        \"\"\"\"\"\"\n        try:\n            self.m_ser = serial.Serial(port=self.m_ttyport,\n                                       baudrate=self.m_baudrate,\n                                       timeout=0,\n                                       parity=serial.PARITY_EVEN,\n                                       stopbits=serial.STOPBITS_ONE,\n                                       bytesize=serial.SEVENBITS,\n                                       rtscts=False)\n            ekm_log(\"Pyserial version = \" + serial.VERSION)\n            ekm_log(\"Port = \" + self.m_ttyport)\n            ekm_log(\"Rate = \" + str(self.m_baudrate))\n            time.sleep(self.m_init_wait)\n            return True\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        return False","method_summary":"Required initialization call, wraps pyserial constructor.","original_method_code":"def initPort(self):\n        \"\"\" Required initialization call, wraps pyserial constructor. \"\"\"\n        try:\n            self.m_ser = serial.Serial(port=self.m_ttyport,\n                                       baudrate=self.m_baudrate,\n                                       timeout=0,\n                                       parity=serial.PARITY_EVEN,\n                                       stopbits=serial.STOPBITS_ONE,\n                                       bytesize=serial.SEVENBITS,\n                                       rtscts=False)\n            ekm_log(\"Pyserial version = \" + serial.VERSION)\n            ekm_log(\"Port = \" + self.m_ttyport)\n            ekm_log(\"Rate = \" + str(self.m_baudrate))\n            time.sleep(self.m_init_wait)\n            return True\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        return False","method_path":"https:\/\/github.com\/ekmmetering\/ekmmeters\/blob\/b3748bdf30263bfa46ea40157bdf8df2522e1904\/ekmmeters.py#L964-L982"}
{"repo_name":"ekmmetering\/ekmmeters","method_name":"SerialPort.write","method_code":"def write(self, output):\n        \"\"\"\"\"\"\n        view_str = output.encode('ascii', 'ignore')\n        if (len(view_str) > 0):\n            self.m_ser.write(view_str)\n            self.m_ser.flush()\n            self.m_ser.reset_input_buffer()\n            time.sleep(self.m_force_wait)\n        pass","method_summary":"Passthrough for pyserial Serial.write().","original_method_code":"def write(self, output):\n        \"\"\"Passthrough for pyserial Serial.write().\n\n        Args:\n            output (str): Block to write to port\n        \"\"\"\n        view_str = output.encode('ascii', 'ignore')\n        if (len(view_str) > 0):\n            self.m_ser.write(view_str)\n            self.m_ser.flush()\n            self.m_ser.reset_input_buffer()\n            time.sleep(self.m_force_wait)\n        pass","method_path":"https:\/\/github.com\/ekmmetering\/ekmmeters\/blob\/b3748bdf30263bfa46ea40157bdf8df2522e1904\/ekmmeters.py#L997-L1009"}
{"repo_name":"ekmmetering\/ekmmeters","method_name":"SerialPort.setPollingValues","method_code":"def setPollingValues(self, max_waits, wait_sleep):\n        \"\"\"\"\"\"\n        self.m_max_waits = max_waits\n        self.m_wait_sleep = wait_sleep","method_summary":"Optional polling loop control","original_method_code":"def setPollingValues(self, max_waits, wait_sleep):\n        \"\"\" Optional polling loop control\n\n        Args:\n            max_waits (int):   waits\n            wait_sleep (int):  ms per wait\n        \"\"\"\n        self.m_max_waits = max_waits\n        self.m_wait_sleep = wait_sleep","method_path":"https:\/\/github.com\/ekmmetering\/ekmmeters\/blob\/b3748bdf30263bfa46ea40157bdf8df2522e1904\/ekmmeters.py#L1011-L1019"}
{"repo_name":"ekmmetering\/ekmmeters","method_name":"SerialPort.getResponse","method_code":"def getResponse(self, context=\"\"):\n        \"\"\"\"\"\"\n        waits = 0  \n        response_str = \"\"  \n        try:\n            waits = 0  \n            while (waits < self.m_max_waits):\n                bytes_to_read = self.m_ser.inWaiting()\n                if bytes_to_read > 0:\n                    next_chunk = str(self.m_ser.read(bytes_to_read)).encode('ascii', 'ignore')\n                    response_str += next_chunk\n                    if (len(response_str) == 255):\n                        time.sleep(self.m_force_wait)\n                        return response_str\n                    if (len(response_str) == 1) and (response_str.encode('hex') == '06'):\n                        time.sleep(self.m_force_wait)\n                        return response_str\n                else:  \n                    waits += 1\n                    time.sleep(self.m_force_wait)\n            response_str = \"\"\n\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        return response_str","method_summary":"Poll for finished block or first byte ACK.","original_method_code":"def getResponse(self, context=\"\"):\n        \"\"\" Poll for finished block or first byte ACK.\n        Args:\n            context (str): internal serial call context.\n\n        Returns:\n            string: Response, implict cast from byte array.\n        \"\"\"\n        waits = 0  # allowed interval counter\n        response_str = \"\"  # returned bytes in string default\n        try:\n            waits = 0  # allowed interval counter\n            while (waits < self.m_max_waits):\n                bytes_to_read = self.m_ser.inWaiting()\n                if bytes_to_read > 0:\n                    next_chunk = str(self.m_ser.read(bytes_to_read)).encode('ascii', 'ignore')\n                    response_str += next_chunk\n                    if (len(response_str) == 255):\n                        time.sleep(self.m_force_wait)\n                        return response_str\n                    if (len(response_str) == 1) and (response_str.encode('hex') == '06'):\n                        time.sleep(self.m_force_wait)\n                        return response_str\n                else:  # hang out -- half shortest expected interval (50 ms)\n                    waits += 1\n                    time.sleep(self.m_force_wait)\n            response_str = \"\"\n\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        return response_str","method_path":"https:\/\/github.com\/ekmmetering\/ekmmeters\/blob\/b3748bdf30263bfa46ea40157bdf8df2522e1904\/ekmmeters.py#L1021-L1052"}
{"repo_name":"ekmmetering\/ekmmeters","method_name":"MeterDB.combineAB","method_code":"def combineAB(self):\n        \"\"\"\"\"\"\n        v4definition_meter = V4Meter()\n        v4definition_meter.makeAB()\n        defv4 = v4definition_meter.getReadBuffer()\n\n        v3definition_meter = V3Meter()\n        v3definition_meter.makeReturnFormat()\n        defv3 = v3definition_meter.getReadBuffer()\n\n        for fld in defv3:\n            if fld not in self.m_all_fields:\n                compare_fld = fld.upper()\n                if not \"RESERVED\" in compare_fld and not \"CRC\" in compare_fld:\n                    self.m_all_fields[fld] = defv3[fld]\n\n        for fld in defv4:\n            if fld not in self.m_all_fields:\n                compare_fld = fld.upper()\n                if not \"RESERVED\" in compare_fld and not \"CRC\" in compare_fld:\n                    self.m_all_fields[fld] = defv4[fld]\n        pass","method_summary":"Use the serial block definitions in V3 and V4 to create one field list.","original_method_code":"def combineAB(self):\n        \"\"\" Use the serial block definitions in V3 and V4 to create one field list. \"\"\"\n        v4definition_meter = V4Meter()\n        v4definition_meter.makeAB()\n        defv4 = v4definition_meter.getReadBuffer()\n\n        v3definition_meter = V3Meter()\n        v3definition_meter.makeReturnFormat()\n        defv3 = v3definition_meter.getReadBuffer()\n\n        for fld in defv3:\n            if fld not in self.m_all_fields:\n                compare_fld = fld.upper()\n                if not \"RESERVED\" in compare_fld and not \"CRC\" in compare_fld:\n                    self.m_all_fields[fld] = defv3[fld]\n\n        for fld in defv4:\n            if fld not in self.m_all_fields:\n                compare_fld = fld.upper()\n                if not \"RESERVED\" in compare_fld and not \"CRC\" in compare_fld:\n                    self.m_all_fields[fld] = defv4[fld]\n        pass","method_path":"https:\/\/github.com\/ekmmetering\/ekmmeters\/blob\/b3748bdf30263bfa46ea40157bdf8df2522e1904\/ekmmeters.py#L1076-L1097"}
{"repo_name":"ekmmetering\/ekmmeters","method_name":"MeterDB.mapTypeToSql","method_code":"def mapTypeToSql(fld_type=FieldType.NoType, fld_len=0):\n        \"\"\"\"\"\"\n        if fld_type == FieldType.Float:\n            return \"FLOAT\"\n        elif fld_type == FieldType.String:\n            return \"VARCHAR(\" + str(fld_len) + \")\"\n        elif fld_type == FieldType.Int:\n            return \"INT\"\n        elif fld_type == FieldType.Hex:\n            return \"VARCHAR(\" + str(fld_len * 2) + \")\"\n        elif fld_type == FieldType.PowerFactor:\n            return \"VARCHAR(\" + str(fld_len) + \")\"\n        else:\n            ekm_log(\"Type \" + str(type) + \" not handled by mapTypeToSql, returned VARCHAR(255)\")\n            return \"VARCHAR(255)\"","method_summary":"Translate FieldType to portable SQL Type. Override if needful.","original_method_code":"def mapTypeToSql(fld_type=FieldType.NoType, fld_len=0):\n        \"\"\" Translate FieldType to portable SQL Type.  Override if needful.\n        Args:\n            fld_type (int): :class:`~ekmmeters.FieldType` in serial block.\n            fld_len (int): Binary length in serial block\n\n        Returns:\n            string: Portable SQL type and length where appropriate.\n        \"\"\"\n        if fld_type == FieldType.Float:\n            return \"FLOAT\"\n        elif fld_type == FieldType.String:\n            return \"VARCHAR(\" + str(fld_len) + \")\"\n        elif fld_type == FieldType.Int:\n            return \"INT\"\n        elif fld_type == FieldType.Hex:\n            return \"VARCHAR(\" + str(fld_len * 2) + \")\"\n        elif fld_type == FieldType.PowerFactor:\n            return \"VARCHAR(\" + str(fld_len) + \")\"\n        else:\n            ekm_log(\"Type \" + str(type) + \" not handled by mapTypeToSql, returned VARCHAR(255)\")\n            return \"VARCHAR(255)\"","method_path":"https:\/\/github.com\/ekmmetering\/ekmmeters\/blob\/b3748bdf30263bfa46ea40157bdf8df2522e1904\/ekmmeters.py#L1100-L1121"}
{"repo_name":"ekmmetering\/ekmmeters","method_name":"MeterDB.fillCreate","method_code":"def fillCreate(self, qry_str):\n        \"\"\"\"\"\"\n        count = 0\n        for fld in self.m_all_fields:\n            fld_type = self.m_all_fields[fld][MeterData.TypeValue]\n            fld_len = self.m_all_fields[fld][MeterData.SizeValue]\n            qry_spec = self.mapTypeToSql(fld_type, fld_len)\n            if count > 0:\n                qry_str += \", \\n\"\n            qry_str = qry_str + '   ' + fld + ' ' + qry_spec\n            count += 1\n\n        qry_str += (\",\\n\\t\" + Field.Time_Stamp + \" BIGINT,\\n\\t\" +\n                    \"Raw_A VARCHAR(512),\\n\\t\" +\n                    \"Raw_B VARCHAR(512)\\n)\")\n\n        return qry_str","method_summary":"Return query portion below CREATE.","original_method_code":"def fillCreate(self, qry_str):\n        \"\"\" Return query portion below CREATE.\n        Args:\n            qry_str (str): String as built.\n\n        Returns:\n            string: Passed string with fields appended.\n        \"\"\"\n        count = 0\n        for fld in self.m_all_fields:\n            fld_type = self.m_all_fields[fld][MeterData.TypeValue]\n            fld_len = self.m_all_fields[fld][MeterData.SizeValue]\n            qry_spec = self.mapTypeToSql(fld_type, fld_len)\n            if count > 0:\n                qry_str += \", \\n\"\n            qry_str = qry_str + '   ' + fld + ' ' + qry_spec\n            count += 1\n\n        qry_str += (\",\\n\\t\" + Field.Time_Stamp + \" BIGINT,\\n\\t\" +\n                    \"Raw_A VARCHAR(512),\\n\\t\" +\n                    \"Raw_B VARCHAR(512)\\n)\")\n\n        return qry_str","method_path":"https:\/\/github.com\/ekmmetering\/ekmmeters\/blob\/b3748bdf30263bfa46ea40157bdf8df2522e1904\/ekmmeters.py#L1123-L1145"}
{"repo_name":"ekmmetering\/ekmmeters","method_name":"MeterDB.sqlCreate","method_code":"def sqlCreate(self):\n        \"\"\"\"\"\"\n        count = 0\n        qry_str = \"CREATE TABLE Meter_Reads ( \\n\\r\"\n        qry_str = self.fillCreate(qry_str)\n        ekm_log(qry_str, 4)\n        return qry_str","method_summary":"Reasonably portable SQL CREATE for defined fields.","original_method_code":"def sqlCreate(self):\n        \"\"\" Reasonably portable SQL CREATE for defined fields.\n        Returns:\n            string: Portable as possible SQL Create for all-reads table.\n        \"\"\"\n        count = 0\n        qry_str = \"CREATE TABLE Meter_Reads ( \\n\\r\"\n        qry_str = self.fillCreate(qry_str)\n        ekm_log(qry_str, 4)\n        return qry_str","method_path":"https:\/\/github.com\/ekmmetering\/ekmmeters\/blob\/b3748bdf30263bfa46ea40157bdf8df2522e1904\/ekmmeters.py#L1147-L1156"}
{"repo_name":"ekmmetering\/ekmmeters","method_name":"MeterDB.sqlInsert","method_code":"def sqlInsert(def_buf, raw_a, raw_b):\n        \"\"\"\"\"\"\n        count = 0\n        qry_str = \"INSERT INTO  Meter_Reads ( \\n\\t\"\n        for fld in def_buf:\n            if count > 0:\n                qry_str += \", \\n\\t\"\n            qry_str = qry_str + fld\n            count += 1\n        qry_str += (\",\\n\\t\" + Field.Time_Stamp + \", \\n\\t\" +\n                    \"Raw_A,\\n\\t\" +\n                    \"Raw_B\\n) \\n\" +\n                    \"VALUES( \\n\\t\")\n        count = 0\n        for fld in def_buf:\n            if count > 0:\n                qry_str += \", \\n\\t\"\n            fld_type = def_buf[fld][MeterData.TypeValue]\n            fld_str_content = def_buf[fld][MeterData.StringValue]\n            delim = \"\"\n            if (fld_type == FieldType.Hex) or \\\n                    (fld_type == FieldType.String) or \\\n                    (fld_type == FieldType.PowerFactor):\n                delim = \"'\"\n            qry_str = qry_str + delim + fld_str_content + delim\n            count += 1\n        time_val = int(time.time() * 1000)\n        qry_str = (qry_str + \",\\n\\t\" + str(time_val) + \",\\n\\t'\" +\n                   binascii.b2a_hex(raw_a) + \"'\" + \",\\n\\t'\" +\n                   binascii.b2a_hex(raw_b) + \"'\\n);\")\n        ekm_log(qry_str, 4)\n        return qry_str","method_summary":"Reasonably portable SQL INSERT for from combined read buffer.","original_method_code":"def sqlInsert(def_buf, raw_a, raw_b):\n        \"\"\" Reasonably portable SQL INSERT for from combined read buffer.\n        Args:\n            def_buf (SerialBlock): Database only serial block of all fields.\n            raw_a (str): Raw A read as hex string.\n            raw_b (str): Raw B read (if exists, otherwise empty) as hex string.\n\n        Returns:\n            str: SQL insert for passed read buffer\n        \"\"\"\n        count = 0\n        qry_str = \"INSERT INTO  Meter_Reads ( \\n\\t\"\n        for fld in def_buf:\n            if count > 0:\n                qry_str += \", \\n\\t\"\n            qry_str = qry_str + fld\n            count += 1\n        qry_str += (\",\\n\\t\" + Field.Time_Stamp + \", \\n\\t\" +\n                    \"Raw_A,\\n\\t\" +\n                    \"Raw_B\\n) \\n\" +\n                    \"VALUES( \\n\\t\")\n        count = 0\n        for fld in def_buf:\n            if count > 0:\n                qry_str += \", \\n\\t\"\n            fld_type = def_buf[fld][MeterData.TypeValue]\n            fld_str_content = def_buf[fld][MeterData.StringValue]\n            delim = \"\"\n            if (fld_type == FieldType.Hex) or \\\n                    (fld_type == FieldType.String) or \\\n                    (fld_type == FieldType.PowerFactor):\n                delim = \"'\"\n            qry_str = qry_str + delim + fld_str_content + delim\n            count += 1\n        time_val = int(time.time() * 1000)\n        qry_str = (qry_str + \",\\n\\t\" + str(time_val) + \",\\n\\t'\" +\n                   binascii.b2a_hex(raw_a) + \"'\" + \",\\n\\t'\" +\n                   binascii.b2a_hex(raw_b) + \"'\\n);\")\n        ekm_log(qry_str, 4)\n        return qry_str","method_path":"https:\/\/github.com\/ekmmetering\/ekmmeters\/blob\/b3748bdf30263bfa46ea40157bdf8df2522e1904\/ekmmeters.py#L1159-L1198"}
{"repo_name":"AkihikoITOH\/capybara","method_name":"ConfigOptionParser.update_defaults","method_code":"def update_defaults(self, defaults):\n        \"\"\"\"\"\"\n        \n        config = {}\n        \n        for section in ('global', self.name):\n            config.update(\n                self.normalize_keys(self.get_config_section(section))\n            )\n        \n        if not self.isolated:\n            config.update(self.normalize_keys(self.get_environ_vars()))\n        \n        for key, val in config.items():\n            option = self.get_option(key)\n            if option is not None:\n                \n                if not val:\n                    continue\n                if option.action in ('store_true', 'store_false', 'count'):\n                    val = strtobool(val)\n                if option.action == 'append':\n                    val = val.split()\n                    val = [self.check_default(option, key, v) for v in val]\n                else:\n                    val = self.check_default(option, key, val)\n\n                defaults[option.dest] = val\n        return defaults","method_summary":"Updates the given defaults with values from the config files and the environ. Does a little special handling for certain types of options (lists).","original_method_code":"def update_defaults(self, defaults):\n        \"\"\"Updates the given defaults with values from the config files and\n        the environ. Does a little special handling for certain types of\n        options (lists).\"\"\"\n        # Then go and look for the other sources of configuration:\n        config = {}\n        # 1. config files\n        for section in ('global', self.name):\n            config.update(\n                self.normalize_keys(self.get_config_section(section))\n            )\n        # 2. environmental variables\n        if not self.isolated:\n            config.update(self.normalize_keys(self.get_environ_vars()))\n        # Then set the options with those values\n        for key, val in config.items():\n            option = self.get_option(key)\n            if option is not None:\n                # ignore empty values\n                if not val:\n                    continue\n                if option.action in ('store_true', 'store_false', 'count'):\n                    val = strtobool(val)\n                if option.action == 'append':\n                    val = val.split()\n                    val = [self.check_default(option, key, v) for v in val]\n                else:\n                    val = self.check_default(option, key, val)\n\n                defaults[option.dest] = val\n        return defaults","method_path":"https:\/\/github.com\/AkihikoITOH\/capybara\/blob\/e86c2173ea386654f4ae061148e8fbe3f25e715c\/capybara\/virtualenv\/lib\/python2.7\/site-packages\/pip\/baseparser.py#L196-L226"}
{"repo_name":"AkihikoITOH\/capybara","method_name":"ConfigOptionParser.normalize_keys","method_code":"def normalize_keys(self, items):\n        \"\"\"\"\"\"\n        normalized = {}\n        for key, val in items:\n            key = key.replace('_', '-')\n            if not key.startswith('--'):\n                key = '--%s' % key  \n            normalized[key] = val\n        return normalized","method_summary":"Return a config dictionary with normalized keys regardless of whether the keys were specified in environment variables or in config files","original_method_code":"def normalize_keys(self, items):\n        \"\"\"Return a config dictionary with normalized keys regardless of\n        whether the keys were specified in environment variables or in config\n        files\"\"\"\n        normalized = {}\n        for key, val in items:\n            key = key.replace('_', '-')\n            if not key.startswith('--'):\n                key = '--%s' % key  # only prefer long opts\n            normalized[key] = val\n        return normalized","method_path":"https:\/\/github.com\/AkihikoITOH\/capybara\/blob\/e86c2173ea386654f4ae061148e8fbe3f25e715c\/capybara\/virtualenv\/lib\/python2.7\/site-packages\/pip\/baseparser.py#L228-L238"}
{"repo_name":"AkihikoITOH\/capybara","method_name":"transform_hits","method_code":"def transform_hits(hits):\n    \"\"\"\"\"\"\n    packages = {}\n    for hit in hits:\n        name = hit['name']\n        summary = hit['summary']\n        version = hit['version']\n        score = hit['_pypi_ordering']\n        if score is None:\n            score = 0\n\n        if name not in packages.keys():\n            packages[name] = {\n                'name': name,\n                'summary': summary,\n                'versions': [version],\n                'score': score,\n            }\n        else:\n            packages[name]['versions'].append(version)\n\n            \n            if version == highest_version(packages[name]['versions']):\n                packages[name]['summary'] = summary\n                packages[name]['score'] = score\n\n    \n    \n    package_list = sorted(\n        packages.values(),\n        key=lambda x: x['score'],\n        reverse=True,\n    )\n    return package_list","method_summary":"The list from pypi is really a list of versions. We want a list of packages with the list of versions stored inline. This converts the list from pypi into one we can use.","original_method_code":"def transform_hits(hits):\n    \"\"\"\n    The list from pypi is really a list of versions. We want a list of\n    packages with the list of versions stored inline. This converts the\n    list from pypi into one we can use.\n    \"\"\"\n    packages = {}\n    for hit in hits:\n        name = hit['name']\n        summary = hit['summary']\n        version = hit['version']\n        score = hit['_pypi_ordering']\n        if score is None:\n            score = 0\n\n        if name not in packages.keys():\n            packages[name] = {\n                'name': name,\n                'summary': summary,\n                'versions': [version],\n                'score': score,\n            }\n        else:\n            packages[name]['versions'].append(version)\n\n            # if this is the highest version, replace summary and score\n            if version == highest_version(packages[name]['versions']):\n                packages[name]['summary'] = summary\n                packages[name]['score'] = score\n\n    # each record has a unique name now, so we will convert the dict into a\n    # list sorted by score\n    package_list = sorted(\n        packages.values(),\n        key=lambda x: x['score'],\n        reverse=True,\n    )\n    return package_list","method_path":"https:\/\/github.com\/AkihikoITOH\/capybara\/blob\/e86c2173ea386654f4ae061148e8fbe3f25e715c\/capybara\/virtualenv\/lib\/python2.7\/site-packages\/pip\/commands\/search.py#L64-L101"}
{"repo_name":"AkihikoITOH\/capybara","method_name":"_transform_result","method_code":"def _transform_result(typ, result):\n    \"\"\"\"\"\"\n    if issubclass(typ, bytes):\n        return tostring(result, encoding='utf-8')\n    elif issubclass(typ, unicode):\n        return tostring(result, encoding='unicode')\n    else:\n        return result","method_summary":"Convert the result back into the input type.","original_method_code":"def _transform_result(typ, result):\n    \"\"\"Convert the result back into the input type.\n    \"\"\"\n    if issubclass(typ, bytes):\n        return tostring(result, encoding='utf-8')\n    elif issubclass(typ, unicode):\n        return tostring(result, encoding='unicode')\n    else:\n        return result","method_path":"https:\/\/github.com\/AkihikoITOH\/capybara\/blob\/e86c2173ea386654f4ae061148e8fbe3f25e715c\/capybara\/virtualenv\/lib\/python2.7\/site-packages\/lxml\/html\/__init__.py#L114-L122"}
{"repo_name":"AkihikoITOH\/capybara","method_name":"parse","method_code":"def parse(filename_or_url, parser=None, base_url=None, **kw):\n    \"\"\"\"\"\"\n    if parser is None:\n        parser = html_parser\n    return etree.parse(filename_or_url, parser, base_url=base_url, **kw)","method_summary":"Parse a filename, URL, or file-like object into an HTML document tree.","original_method_code":"def parse(filename_or_url, parser=None, base_url=None, **kw):\n    \"\"\"\n    Parse a filename, URL, or file-like object into an HTML document\n    tree.  Note: this returns a tree, not an element.  Use\n    ``parse(...).getroot()`` to get the document root.\n\n    You can override the base URL with the ``base_url`` keyword.  This\n    is most useful when parsing from a file-like object.\n    \"\"\"\n    if parser is None:\n        parser = html_parser\n    return etree.parse(filename_or_url, parser, base_url=base_url, **kw)","method_path":"https:\/\/github.com\/AkihikoITOH\/capybara\/blob\/e86c2173ea386654f4ae061148e8fbe3f25e715c\/capybara\/virtualenv\/lib\/python2.7\/site-packages\/lxml\/html\/__init__.py#L778-L789"}
{"repo_name":"AkihikoITOH\/capybara","method_name":"submit_form","method_code":"def submit_form(form, extra_values=None, open_http=None):\n    \"\"\"\"\"\"\n    values = form.form_values()\n    if extra_values:\n        if hasattr(extra_values, 'items'):\n            extra_values = extra_values.items()\n        values.extend(extra_values)\n    if open_http is None:\n        open_http = open_http_urllib\n    if form.action:\n        url = form.action\n    else:\n        url = form.base_url\n    return open_http(form.method, url, values)","method_summary":"Helper function to submit a form.","original_method_code":"def submit_form(form, extra_values=None, open_http=None):\n    \"\"\"\n    Helper function to submit a form.  Returns a file-like object, as from\n    ``urllib.urlopen()``.  This object also has a ``.geturl()`` function,\n    which shows the URL if there were any redirects.\n\n    You can use this like::\n\n        form = doc.forms[0]\n        form.inputs['foo'].value = 'bar' # etc\n        response = form.submit()\n        doc = parse(response)\n        doc.make_links_absolute(response.geturl())\n\n    To change the HTTP requester, pass a function as ``open_http`` keyword\n    argument that opens the URL for you.  The function must have the following\n    signature::\n\n        open_http(method, URL, values)\n\n    The action is one of 'GET' or 'POST', the URL is the target URL as a\n    string, and the values are a sequence of ``(name, value)`` tuples with the\n    form data.\n    \"\"\"\n    values = form.form_values()\n    if extra_values:\n        if hasattr(extra_values, 'items'):\n            extra_values = extra_values.items()\n        values.extend(extra_values)\n    if open_http is None:\n        open_http = open_http_urllib\n    if form.action:\n        url = form.action\n    else:\n        url = form.base_url\n    return open_http(form.method, url, values)","method_path":"https:\/\/github.com\/AkihikoITOH\/capybara\/blob\/e86c2173ea386654f4ae061148e8fbe3f25e715c\/capybara\/virtualenv\/lib\/python2.7\/site-packages\/lxml\/html\/__init__.py#L918-L953"}
{"repo_name":"AkihikoITOH\/capybara","method_name":"html_to_xhtml","method_code":"def html_to_xhtml(html):\n    \"\"\"\"\"\"\n    try:\n        html = html.getroot()\n    except AttributeError:\n        pass\n    prefix = \"{%s}\" % XHTML_NAMESPACE\n    for el in html.iter(etree.Element):\n        tag = el.tag\n        if tag[0] != '{':\n            el.tag = prefix + tag","method_summary":"Convert all tags in an HTML tree to XHTML by moving them to the XHTML namespace.","original_method_code":"def html_to_xhtml(html):\n    \"\"\"Convert all tags in an HTML tree to XHTML by moving them to the\n    XHTML namespace.\n    \"\"\"\n    try:\n        html = html.getroot()\n    except AttributeError:\n        pass\n    prefix = \"{%s}\" % XHTML_NAMESPACE\n    for el in html.iter(etree.Element):\n        tag = el.tag\n        if tag[0] != '{':\n            el.tag = prefix + tag","method_path":"https:\/\/github.com\/AkihikoITOH\/capybara\/blob\/e86c2173ea386654f4ae061148e8fbe3f25e715c\/capybara\/virtualenv\/lib\/python2.7\/site-packages\/lxml\/html\/__init__.py#L1544-L1556"}
{"repo_name":"AkihikoITOH\/capybara","method_name":"xhtml_to_html","method_code":"def xhtml_to_html(xhtml):\n    \"\"\"\"\"\"\n    try:\n        xhtml = xhtml.getroot()\n    except AttributeError:\n        pass\n    prefix = \"{%s}\" % XHTML_NAMESPACE\n    prefix_len = len(prefix)\n    for el in xhtml.iter(prefix + \"*\"):\n        el.tag = el.tag[prefix_len:]","method_summary":"Convert all tags in an XHTML tree to HTML by removing their XHTML namespace.","original_method_code":"def xhtml_to_html(xhtml):\n    \"\"\"Convert all tags in an XHTML tree to HTML by removing their\n    XHTML namespace.\n    \"\"\"\n    try:\n        xhtml = xhtml.getroot()\n    except AttributeError:\n        pass\n    prefix = \"{%s}\" % XHTML_NAMESPACE\n    prefix_len = len(prefix)\n    for el in xhtml.iter(prefix + \"*\"):\n        el.tag = el.tag[prefix_len:]","method_path":"https:\/\/github.com\/AkihikoITOH\/capybara\/blob\/e86c2173ea386654f4ae061148e8fbe3f25e715c\/capybara\/virtualenv\/lib\/python2.7\/site-packages\/lxml\/html\/__init__.py#L1558-L1569"}
{"repo_name":"AkihikoITOH\/capybara","method_name":"tostring","method_code":"def tostring(doc, pretty_print=False, include_meta_content_type=False,\n             encoding=None, method=\"html\", with_tail=True, doctype=None):\n    \"\"\"\"\"\"\n    html = etree.tostring(doc, method=method, pretty_print=pretty_print,\n                          encoding=encoding, with_tail=with_tail,\n                          doctype=doctype)\n    if method == 'html' and not include_meta_content_type:\n        if isinstance(html, str):\n            html = __str_replace_meta_content_type('', html)\n        else:\n            html = __bytes_replace_meta_content_type(bytes(), html)\n    return html","method_summary":"Return an HTML string representation of the document.","original_method_code":"def tostring(doc, pretty_print=False, include_meta_content_type=False,\n             encoding=None, method=\"html\", with_tail=True, doctype=None):\n    \"\"\"Return an HTML string representation of the document.\n\n    Note: if include_meta_content_type is true this will create a\n    ``<meta http-equiv=\"Content-Type\" ...>`` tag in the head;\n    regardless of the value of include_meta_content_type any existing\n    ``<meta http-equiv=\"Content-Type\" ...>`` tag will be removed\n\n    The ``encoding`` argument controls the output encoding (defauts to\n    ASCII, with &#...; character references for any characters outside\n    of ASCII).  Note that you can pass the name ``'unicode'`` as\n    ``encoding`` argument to serialise to a Unicode string.\n\n    The ``method`` argument defines the output method.  It defaults to\n    'html', but can also be 'xml' for xhtml output, or 'text' to\n    serialise to plain text without markup.\n\n    To leave out the tail text of the top-level element that is being\n    serialised, pass ``with_tail=False``.\n\n    The ``doctype`` option allows passing in a plain string that will\n    be serialised before the XML tree.  Note that passing in non\n    well-formed content here will make the XML output non well-formed.\n    Also, an existing doctype in the document tree will not be removed\n    when serialising an ElementTree instance.\n\n    Example::\n\n        >>> from lxml import html\n        >>> root = html.fragment_fromstring('<p>Hello<br>world!<\/p>')\n\n        >>> html.tostring(root)\n        b'<p>Hello<br>world!<\/p>'\n        >>> html.tostring(root, method='html')\n        b'<p>Hello<br>world!<\/p>'\n\n        >>> html.tostring(root, method='xml')\n        b'<p>Hello<br\/>world!<\/p>'\n\n        >>> html.tostring(root, method='text')\n        b'Helloworld!'\n\n        >>> html.tostring(root, method='text', encoding='unicode')\n        u'Helloworld!'\n\n        >>> root = html.fragment_fromstring('<div><p>Hello<br>world!<\/p>TAIL<\/div>')\n        >>> html.tostring(root[0], method='text', encoding='unicode')\n        u'Helloworld!TAIL'\n\n        >>> html.tostring(root[0], method='text', encoding='unicode', with_tail=False)\n        u'Helloworld!'\n\n        >>> doc = html.document_fromstring('<p>Hello<br>world!<\/p>')\n        >>> html.tostring(doc, method='html', encoding='unicode')\n        u'<html><body><p>Hello<br>world!<\/p><\/body><\/html>'\n\n        >>> print(html.tostring(doc, method='html', encoding='unicode',\n        ...          doctype='<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 4.01\/\/EN\"'\n        ...                  ' \"http:\/\/www.w3.org\/TR\/html4\/strict.dtd\">'))\n        <!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 4.01\/\/EN\" \"http:\/\/www.w3.org\/TR\/html4\/strict.dtd\">\n        <html><body><p>Hello<br>world!<\/p><\/body><\/html>\n    \"\"\"\n    html = etree.tostring(doc, method=method, pretty_print=pretty_print,\n                          encoding=encoding, with_tail=with_tail,\n                          doctype=doctype)\n    if method == 'html' and not include_meta_content_type:\n        if isinstance(html, str):\n            html = __str_replace_meta_content_type('', html)\n        else:\n            html = __bytes_replace_meta_content_type(bytes(), html)\n    return html","method_path":"https:\/\/github.com\/AkihikoITOH\/capybara\/blob\/e86c2173ea386654f4ae061148e8fbe3f25e715c\/capybara\/virtualenv\/lib\/python2.7\/site-packages\/lxml\/html\/__init__.py#L1578-L1649"}
{"repo_name":"AkihikoITOH\/capybara","method_name":"open_in_browser","method_code":"def open_in_browser(doc, encoding=None):\n    \"\"\"\"\"\"\n    import os\n    import webbrowser\n    import tempfile\n    if not isinstance(doc, etree._ElementTree):\n        doc = etree.ElementTree(doc)\n    handle, fn = tempfile.mkstemp(suffix='.html')\n    f = os.fdopen(handle, 'wb')\n    try:\n        doc.write(f, method=\"html\", encoding=encoding or doc.docinfo.encoding or \"UTF-8\")\n    finally:\n        \n        f.close()\n    url = 'file:\/\/' + fn.replace(os.path.sep, '\/')\n    print(url)\n    webbrowser.open(url)","method_summary":"Open the HTML document in a web browser, saving it to a temporary file to open it. Note that this does not delete the file after use. This is mainly meant for debugging.","original_method_code":"def open_in_browser(doc, encoding=None):\n    \"\"\"\n    Open the HTML document in a web browser, saving it to a temporary\n    file to open it.  Note that this does not delete the file after\n    use.  This is mainly meant for debugging.\n    \"\"\"\n    import os\n    import webbrowser\n    import tempfile\n    if not isinstance(doc, etree._ElementTree):\n        doc = etree.ElementTree(doc)\n    handle, fn = tempfile.mkstemp(suffix='.html')\n    f = os.fdopen(handle, 'wb')\n    try:\n        doc.write(f, method=\"html\", encoding=encoding or doc.docinfo.encoding or \"UTF-8\")\n    finally:\n        # we leak the file itself here, but we should at least close it\n        f.close()\n    url = 'file:\/\/' + fn.replace(os.path.sep, '\/')\n    print(url)\n    webbrowser.open(url)","method_path":"https:\/\/github.com\/AkihikoITOH\/capybara\/blob\/e86c2173ea386654f4ae061148e8fbe3f25e715c\/capybara\/virtualenv\/lib\/python2.7\/site-packages\/lxml\/html\/__init__.py#L1653-L1673"}
{"repo_name":"rwl\/godot","method_name":"Graph.arrange_all","method_code":"def arrange_all(self):\n        \"\"\"\"\"\"\n        import godot.dot_data_parser\n\n        parser = godot.dot_data_parser.GodotDataParser()\n\n        xdot_data = self.create( format = \"xdot\" )\n\n\n\n        parser.dotparser.parseWithTabs()\n        ndata = xdot_data.replace( \"\\\\\\n\", \"\" )\n        tokens = parser.dotparser.parseString( ndata )[0]\n        parser.build_graph( graph=self, tokens=tokens[3] )\n\n        self.redraw_canvas()","method_summary":"Sets for the _draw_ and _ldraw_ attributes for each of the graph sub-elements by processing the xdot format of the graph.","original_method_code":"def arrange_all(self):\n        \"\"\" Sets for the _draw_ and _ldraw_ attributes for each of the graph\n            sub-elements by processing the xdot format of the graph.\n        \"\"\"\n        import godot.dot_data_parser\n\n        parser = godot.dot_data_parser.GodotDataParser()\n\n        xdot_data = self.create( format = \"xdot\" )\n#        print \"GRAPH DOT:\\n\", str( self )\n#        print \"XDOT DATA:\\n\", xdot_data\n\n        parser.dotparser.parseWithTabs()\n        ndata = xdot_data.replace( \"\\\\\\n\", \"\" )\n        tokens = parser.dotparser.parseString( ndata )[0]\n        parser.build_graph( graph=self, tokens=tokens[3] )\n\n        self.redraw_canvas()","method_path":"https:\/\/github.com\/rwl\/godot\/blob\/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f\/godot\/graph.py#L886-L903"}
{"repo_name":"rwl\/godot","method_name":"Graph.redraw_canvas","method_code":"def redraw_canvas(self):\n        \"\"\"\"\"\"\n        from xdot_parser import XdotAttrParser\n\n        xdot_parser = XdotAttrParser()\n        canvas = self._component_default()\n\n        for node in self.nodes:\n            components = xdot_parser.parse_xdot_data( node._draw_ )\n            canvas.add( *components )\n\n            components = xdot_parser.parse_xdot_data( node._ldraw_ )\n            canvas.add( *components )\n\n        for edge in self.edges:\n            components = xdot_parser.parse_xdot_data( edge._draw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._ldraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._hdraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._tdraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._hldraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._tldraw_ )\n            canvas.add( *components )\n\n        self.component = canvas\n        self.vp.request_redraw()","method_summary":"Parses the Xdot attributes of all graph components and adds the components to a new canvas.","original_method_code":"def redraw_canvas(self):\n        \"\"\" Parses the Xdot attributes of all graph components and adds\n            the components to a new canvas.\n        \"\"\"\n        from xdot_parser import XdotAttrParser\n\n        xdot_parser = XdotAttrParser()\n        canvas = self._component_default()\n\n        for node in self.nodes:\n            components = xdot_parser.parse_xdot_data( node._draw_ )\n            canvas.add( *components )\n\n            components = xdot_parser.parse_xdot_data( node._ldraw_ )\n            canvas.add( *components )\n\n        for edge in self.edges:\n            components = xdot_parser.parse_xdot_data( edge._draw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._ldraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._hdraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._tdraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._hldraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._tldraw_ )\n            canvas.add( *components )\n\n        self.component = canvas\n        self.vp.request_redraw()","method_path":"https:\/\/github.com\/rwl\/godot\/blob\/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f\/godot\/graph.py#L907-L938"}
{"repo_name":"rwl\/godot","method_name":"Graph._maxiter_default","method_code":"def _maxiter_default(self):\n        \"\"\"\"\"\"\n        mode = self.mode\n        if mode == \"KK\":\n            return 100 * len(self.nodes)\n        elif mode == \"major\":\n            return 200\n        else:\n            return 600","method_summary":"Trait initialiser.","original_method_code":"def _maxiter_default(self):\n        \"\"\" Trait initialiser.\n        \"\"\"\n        mode = self.mode\n        if mode == \"KK\":\n            return 100 * len(self.nodes)\n        elif mode == \"major\":\n            return 200\n        else:\n            return 600","method_path":"https:\/\/github.com\/rwl\/godot\/blob\/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f\/godot\/graph.py#L1012-L1021"}
{"repo_name":"rwl\/godot","method_name":"Graph._get_all_graphs","method_code":"def _get_all_graphs(self):\n        \"\"\"\"\"\"\n        top_graph = self\n\n        def get_subgraphs(graph):\n            assert isinstance(graph, BaseGraph)\n            subgraphs = graph.subgraphs[:]\n            for subgraph in graph.subgraphs:\n                subsubgraphs = get_subgraphs(subgraph)\n                subgraphs.extend(subsubgraphs)\n            return subgraphs\n\n        subgraphs = get_subgraphs(top_graph)\n        return [top_graph] + subgraphs","method_summary":"Property getter.","original_method_code":"def _get_all_graphs(self):\n        \"\"\" Property getter.\n        \"\"\"\n        top_graph = self\n\n        def get_subgraphs(graph):\n            assert isinstance(graph, BaseGraph)\n            subgraphs = graph.subgraphs[:]\n            for subgraph in graph.subgraphs:\n                subsubgraphs = get_subgraphs(subgraph)\n                subgraphs.extend(subsubgraphs)\n            return subgraphs\n\n        subgraphs = get_subgraphs(top_graph)\n        return [top_graph] + subgraphs","method_path":"https:\/\/github.com\/rwl\/godot\/blob\/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f\/godot\/graph.py#L1027-L1041"}
{"repo_name":"rwl\/godot","method_name":"Graph._directed_changed","method_code":"def _directed_changed(self, new):\n        \"\"\"\"\"\"\n        if new:\n            conn = \"->\"\n        else:\n            conn = \"--\"\n\n        for edge in [e for g in self.all_graphs for e in g.edges]:\n            edge.conn = conn","method_summary":"Sets the connection string for all edges.","original_method_code":"def _directed_changed(self, new):\n        \"\"\" Sets the connection string for all edges.\n        \"\"\"\n        if new:\n            conn = \"->\"\n        else:\n            conn = \"--\"\n\n        for edge in [e for g in self.all_graphs for e in g.edges]:\n            edge.conn = conn","method_path":"https:\/\/github.com\/rwl\/godot\/blob\/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f\/godot\/graph.py#L1047-L1056"}
{"repo_name":"rwl\/godot","method_name":"Graph._on_nodes","method_code":"def _on_nodes(self):\n        \"\"\"\"\"\"\n        all_graphs = self.all_graphs\n        all_nodes = [n for g in all_graphs for n in g.nodes]\n\n        for graph in all_graphs:\n            for edge in graph.edges:\n                edge._nodes = all_nodes","method_summary":"Maintains each branch's list of available nodes in order that they may move themselves (InstanceEditor values).","original_method_code":"def _on_nodes(self):\n        \"\"\" Maintains each branch's list of available nodes in order that they\n            may move themselves (InstanceEditor values).\n        \"\"\"\n        all_graphs = self.all_graphs\n        all_nodes = [n for g in all_graphs for n in g.nodes]\n\n        for graph in all_graphs:\n            for edge in graph.edges:\n                edge._nodes = all_nodes","method_path":"https:\/\/github.com\/rwl\/godot\/blob\/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f\/godot\/graph.py#L1059-L1068"}
{"repo_name":"rwl\/godot","method_name":"Graph._on_edges","method_code":"def _on_edges(self, object, name, old, new):\n        \"\"\"\"\"\"\n        if name == \"edges_items\":\n            edges = new.added\n        elif name == \"edges\":\n            edges = new\n        else:\n            edges = []\n\n        all_nodes = [n for g in self.all_graphs for n in g.nodes]\n\n        for each_edge in edges:\n            \n            if each_edge.tail_node not in all_nodes:\n                object.nodes.append( each_edge.tail_node )\n\n            if each_edge.head_node not in all_nodes:\n                object.nodes.append( each_edge.head_node )\n\n            \n            each_edge._nodes = all_nodes","method_summary":"Handles the list of edges for any graph changing.","original_method_code":"def _on_edges(self, object, name, old, new):\n        \"\"\" Handles the list of edges for any graph changing.\n        \"\"\"\n        if name == \"edges_items\":\n            edges = new.added\n        elif name == \"edges\":\n            edges = new\n        else:\n            edges = []\n\n        all_nodes = [n for g in self.all_graphs for n in g.nodes]\n\n        for each_edge in edges:\n            # Ensure the edge's nodes exist in the graph.\n            if each_edge.tail_node not in all_nodes:\n                object.nodes.append( each_edge.tail_node )\n\n            if each_edge.head_node not in all_nodes:\n                object.nodes.append( each_edge.head_node )\n\n            # Initialise the edge's list of available nodes.\n            each_edge._nodes = all_nodes","method_path":"https:\/\/github.com\/rwl\/godot\/blob\/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f\/godot\/graph.py#L1071-L1092"}
{"repo_name":"rwl\/godot","method_name":"ComponentViewer._viewport_default","method_code":"def _viewport_default(self):\n        \"\"\"\"\"\"\n\n        viewport = Viewport(component=self.canvas, enable_zoom=True)\n        viewport.tools.append(ViewportPanTool(viewport))\n        return viewport","method_summary":"Trait initialiser","original_method_code":"def _viewport_default(self):\n        \"\"\" Trait initialiser \"\"\"\n\n        viewport = Viewport(component=self.canvas, enable_zoom=True)\n        viewport.tools.append(ViewportPanTool(viewport))\n        return viewport","method_path":"https:\/\/github.com\/rwl\/godot\/blob\/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f\/godot\/component\/component_viewer.py#L67-L72"}
{"repo_name":"rwl\/godot","method_name":"ComponentViewer._component_changed","method_code":"def _component_changed(self, old, new):\n        \"\"\"\"\"\"\n        canvas = self.canvas\n        if old is not None:\n            canvas.remove(old)\n        if new is not None:\n            canvas.add(new)","method_summary":"Handles the component being changed.","original_method_code":"def _component_changed(self, old, new):\n        \"\"\" Handles the component being changed.\n        \"\"\"\n        canvas = self.canvas\n        if old is not None:\n            canvas.remove(old)\n        if new is not None:\n            canvas.add(new)","method_path":"https:\/\/github.com\/rwl\/godot\/blob\/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f\/godot\/component\/component_viewer.py#L74-L81"}
{"repo_name":"rwl\/godot","method_name":"CanvasMapping._diagram_canvas_default","method_code":"def _diagram_canvas_default(self):\n        \"\"\"\"\"\"\n\n        canvas = Canvas()\n\n        for tool in self.tools:\n            canvas.tools.append(tool(canvas))\n\n        return canvas","method_summary":"Trait initialiser","original_method_code":"def _diagram_canvas_default(self):\n        \"\"\" Trait initialiser \"\"\"\n\n        canvas = Canvas()\n\n        for tool in self.tools:\n            canvas.tools.append(tool(canvas))\n\n        return canvas","method_path":"https:\/\/github.com\/rwl\/godot\/blob\/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f\/godot\/mapping.py#L74-L82"}
{"repo_name":"bolt-project\/bolt","method_name":"ConstructSpark.array","method_code":"def array(a, context=None, axis=(0,), dtype=None, npartitions=None):\n        \"\"\"\"\"\"\n        if dtype is None:\n            arry = asarray(a)\n            dtype = arry.dtype\n        else:\n            arry = asarray(a, dtype)\n        shape = arry.shape\n        ndim = len(shape)\n\n        \n        axes = ConstructSpark._format_axes(axis, arry.shape)\n        key_axes, value_axes = get_kv_axes(arry.shape, axes)\n        permutation = key_axes + value_axes\n        arry = arry.transpose(*permutation)\n        split = len(axes)\n\n        if split < 1:\n            raise ValueError(\"split axis must be greater than 0, got %g\" % split)\n        if split > len(shape):\n            raise ValueError(\"split axis must not exceed number of axes %g, got %g\" % (ndim, split))\n\n        key_shape = shape[:split]\n        val_shape = shape[split:]\n\n        keys = zip(*unravel_index(arange(0, int(prod(key_shape))), key_shape))\n        vals = arry.reshape((prod(key_shape),) + val_shape)\n\n        rdd = context.parallelize(zip(keys, vals), npartitions)\n        return BoltArraySpark(rdd, shape=shape, split=split, dtype=dtype)","method_summary":"Create a spark bolt array from a local array.","original_method_code":"def array(a, context=None, axis=(0,), dtype=None, npartitions=None):\n        \"\"\"\n        Create a spark bolt array from a local array.\n\n        Parameters\n        ----------\n        a : array-like\n            An array, any object exposing the array interface, an\n            object whose __array__ method returns an array, or any\n            (nested) sequence.\n\n        context : SparkContext\n            A context running Spark. (see pyspark)\n\n        axis : tuple, optional, default=(0,)\n            Which axes to distribute the array along. The resulting\n            distributed object will use keys to represent these axes,\n            with the remaining axes represented by values.\n\n        dtype : data-type, optional, default=None\n            The desired data-type for the array. If None, will\n            be determined from the data. (see numpy)\n\n        npartitions : int\n            Number of partitions for parallization.\n\n        Returns\n        -------\n        BoltArraySpark\n        \"\"\"\n        if dtype is None:\n            arry = asarray(a)\n            dtype = arry.dtype\n        else:\n            arry = asarray(a, dtype)\n        shape = arry.shape\n        ndim = len(shape)\n\n        # handle the axes specification and transpose if necessary\n        axes = ConstructSpark._format_axes(axis, arry.shape)\n        key_axes, value_axes = get_kv_axes(arry.shape, axes)\n        permutation = key_axes + value_axes\n        arry = arry.transpose(*permutation)\n        split = len(axes)\n\n        if split < 1:\n            raise ValueError(\"split axis must be greater than 0, got %g\" % split)\n        if split > len(shape):\n            raise ValueError(\"split axis must not exceed number of axes %g, got %g\" % (ndim, split))\n\n        key_shape = shape[:split]\n        val_shape = shape[split:]\n\n        keys = zip(*unravel_index(arange(0, int(prod(key_shape))), key_shape))\n        vals = arry.reshape((prod(key_shape),) + val_shape)\n\n        rdd = context.parallelize(zip(keys, vals), npartitions)\n        return BoltArraySpark(rdd, shape=shape, split=split, dtype=dtype)","method_path":"https:\/\/github.com\/bolt-project\/bolt\/blob\/9cd7104aa085498da3097b72696184b9d3651c51\/bolt\/spark\/construct.py#L13-L70"}
{"repo_name":"bolt-project\/bolt","method_name":"ConstructSpark.ones","method_code":"def ones(shape, context=None, axis=(0,), dtype=float64, npartitions=None):\n        \"\"\"\"\"\"\n        from numpy import ones\n        return ConstructSpark._wrap(ones, shape, context, axis, dtype, npartitions)","method_summary":"Create a spark bolt array of ones.","original_method_code":"def ones(shape, context=None, axis=(0,), dtype=float64, npartitions=None):\n        \"\"\"\n        Create a spark bolt array of ones.\n\n        Parameters\n        ----------\n        shape : tuple\n            The desired shape of the array.\n\n        context : SparkContext\n            A context running Spark. (see pyspark)\n\n        axis : tuple, optional, default=(0,)\n            Which axes to distribute the array along. The resulting\n            distributed object will use keys to represent these axes,\n            with the remaining axes represented by values.\n\n        dtype : data-type, optional, default=float64\n            The desired data-type for the array. If None, will\n            be determined from the data. (see numpy)\n\n        npartitions : int\n            Number of partitions for parallization.\n\n        Returns\n        -------\n        BoltArraySpark\n        \"\"\"\n        from numpy import ones\n        return ConstructSpark._wrap(ones, shape, context, axis, dtype, npartitions)","method_path":"https:\/\/github.com\/bolt-project\/bolt\/blob\/9cd7104aa085498da3097b72696184b9d3651c51\/bolt\/spark\/construct.py#L73-L102"}
{"repo_name":"bolt-project\/bolt","method_name":"ConstructSpark.concatenate","method_code":"def concatenate(arrays, axis=0):\n        \"\"\"\"\"\"\n        if not isinstance(arrays, tuple):\n            raise ValueError(\"data type not understood\")\n        if not len(arrays) == 2:\n            raise NotImplementedError(\"spark concatenation only supports two arrays\")\n\n        first, second = arrays\n        if isinstance(first, BoltArraySpark):\n            return first.concatenate(second, axis)\n        elif isinstance(second, BoltArraySpark):\n            first = ConstructSpark.array(first, second._rdd.context)\n            return first.concatenate(second, axis)\n        else:\n            raise ValueError(\"at least one array must be a spark bolt array\")","method_summary":"Join two bolt arrays together, at least one of which is in spark.","original_method_code":"def concatenate(arrays, axis=0):\n        \"\"\"\n        Join two bolt arrays together, at least one of which is in spark.\n\n        Parameters\n        ----------\n        arrays : tuple\n            A pair of arrays. At least one must be a spark array,\n            the other can be a local bolt array, a local numpy array,\n            or an array-like.\n\n        axis : int, optional, default=0\n            The axis along which the arrays will be joined.\n\n        Returns\n        -------\n        BoltArraySpark\n        \"\"\"\n        if not isinstance(arrays, tuple):\n            raise ValueError(\"data type not understood\")\n        if not len(arrays) == 2:\n            raise NotImplementedError(\"spark concatenation only supports two arrays\")\n\n        first, second = arrays\n        if isinstance(first, BoltArraySpark):\n            return first.concatenate(second, axis)\n        elif isinstance(second, BoltArraySpark):\n            first = ConstructSpark.array(first, second._rdd.context)\n            return first.concatenate(second, axis)\n        else:\n            raise ValueError(\"at least one array must be a spark bolt array\")","method_path":"https:\/\/github.com\/bolt-project\/bolt\/blob\/9cd7104aa085498da3097b72696184b9d3651c51\/bolt\/spark\/construct.py#L137-L167"}
{"repo_name":"bolt-project\/bolt","method_name":"ConstructSpark._argcheck","method_code":"def _argcheck(*args, **kwargs):\n        \"\"\"\"\"\"\n        try:\n            from pyspark import SparkContext\n        except ImportError:\n            return False\n\n        cond1 = any([isinstance(arg, SparkContext) for arg in args])\n        cond2 = isinstance(kwargs.get('context', None), SparkContext)\n        cond3 = any([isinstance(arg, BoltArraySpark) for arg in args])\n        cond4 = any([any([isinstance(sub, BoltArraySpark) for sub in arg])\n                     if isinstance(arg, (tuple, list)) else False for arg in args])\n        return cond1 or cond2 or cond3 or cond4","method_summary":"Check that arguments are consistent with spark array construction. Conditions","original_method_code":"def _argcheck(*args, **kwargs):\n        \"\"\"\n        Check that arguments are consistent with spark array construction.\n\n        Conditions are:\n        (1) a positional argument is a SparkContext\n        (2) keyword arg 'context' is a SparkContext\n        (3) an argument is a BoltArraySpark, or\n        (4) an argument is a nested list containing a BoltArraySpark\n        \"\"\"\n        try:\n            from pyspark import SparkContext\n        except ImportError:\n            return False\n\n        cond1 = any([isinstance(arg, SparkContext) for arg in args])\n        cond2 = isinstance(kwargs.get('context', None), SparkContext)\n        cond3 = any([isinstance(arg, BoltArraySpark) for arg in args])\n        cond4 = any([any([isinstance(sub, BoltArraySpark) for sub in arg])\n                     if isinstance(arg, (tuple, list)) else False for arg in args])\n        return cond1 or cond2 or cond3 or cond4","method_path":"https:\/\/github.com\/bolt-project\/bolt\/blob\/9cd7104aa085498da3097b72696184b9d3651c51\/bolt\/spark\/construct.py#L170-L190"}
{"repo_name":"bolt-project\/bolt","method_name":"ConstructSpark._format_axes","method_code":"def _format_axes(axes, shape):\n        \"\"\"\"\"\"\n        if isinstance(axes, int):\n            axes = (axes,)\n        elif isinstance(axes, list) or hasattr(axes, '__iter__'):\n            axes = tuple(axes)\n        if not isinstance(axes, tuple):\n            raise ValueError(\"axes argument %s in the constructor not specified correctly\" % str(axes))\n        if min(axes) < 0 or max(axes) > len(shape) - 1:\n            raise ValueError(\"invalid key axes %s given shape %s\" % (str(axes), str(shape)))\n        return axes","method_summary":"Format target axes given an array shape","original_method_code":"def _format_axes(axes, shape):\n        \"\"\"\n        Format target axes given an array shape\n        \"\"\"\n        if isinstance(axes, int):\n            axes = (axes,)\n        elif isinstance(axes, list) or hasattr(axes, '__iter__'):\n            axes = tuple(axes)\n        if not isinstance(axes, tuple):\n            raise ValueError(\"axes argument %s in the constructor not specified correctly\" % str(axes))\n        if min(axes) < 0 or max(axes) > len(shape) - 1:\n            raise ValueError(\"invalid key axes %s given shape %s\" % (str(axes), str(shape)))\n        return axes","method_path":"https:\/\/github.com\/bolt-project\/bolt\/blob\/9cd7104aa085498da3097b72696184b9d3651c51\/bolt\/spark\/construct.py#L193-L205"}
{"repo_name":"bolt-project\/bolt","method_name":"ConstructSpark._wrap","method_code":"def _wrap(func, shape, context=None, axis=(0,), dtype=None, npartitions=None):\n        \"\"\"\"\"\"\n        if isinstance(shape, int):\n            shape = (shape,)\n        key_shape, value_shape = get_kv_shape(shape, ConstructSpark._format_axes(axis, shape))\n        split = len(key_shape)\n\n        \n        rdd = context.parallelize(list(product(*[arange(x) for x in key_shape])), npartitions)\n\n        \n        rdd = rdd.map(lambda x: (x, func(value_shape, dtype, order='C')))\n        return BoltArraySpark(rdd, shape=shape, split=split, dtype=dtype)","method_summary":"Wrap an existing numpy constructor in a parallelized construction","original_method_code":"def _wrap(func, shape, context=None, axis=(0,), dtype=None, npartitions=None):\n        \"\"\"\n        Wrap an existing numpy constructor in a parallelized construction\n        \"\"\"\n        if isinstance(shape, int):\n            shape = (shape,)\n        key_shape, value_shape = get_kv_shape(shape, ConstructSpark._format_axes(axis, shape))\n        split = len(key_shape)\n\n        # make the keys\n        rdd = context.parallelize(list(product(*[arange(x) for x in key_shape])), npartitions)\n\n        # use a map to make the arrays in parallel\n        rdd = rdd.map(lambda x: (x, func(value_shape, dtype, order='C')))\n        return BoltArraySpark(rdd, shape=shape, split=split, dtype=dtype)","method_path":"https:\/\/github.com\/bolt-project\/bolt\/blob\/9cd7104aa085498da3097b72696184b9d3651c51\/bolt\/spark\/construct.py#L208-L222"}
{"repo_name":"bolt-project\/bolt","method_name":"BoltArrayLocal.map","method_code":"def map(self, func, axis=(0,)):\n        \"\"\"\"\"\"\n        axes = sorted(tupleize(axis))\n        key_shape = [self.shape[axis] for axis in axes]\n        reshaped = self._align(axes, key_shape=key_shape)\n\n        mapped = asarray(list(map(func, reshaped)))\n        elem_shape = mapped[0].shape\n\n        \n        linearized_shape_inv = key_shape + list(elem_shape)\n        reordered = mapped.reshape(*linearized_shape_inv)\n\n        return self._constructor(reordered)","method_summary":"Apply a function across an axis. Array will be aligned so that the desired set of axes are in the keys, which may require a transpose\/reshape.","original_method_code":"def map(self, func, axis=(0,)):\n        \"\"\"\n        Apply a function across an axis.\n\n        Array will be aligned so that the desired set of axes\n        are in the keys, which may require a transpose\/reshape.\n\n        Parameters\n        ----------\n        func : function\n            Function of a single array to apply\n\n        axis : tuple or int, optional, default=(0,)\n            Axis or multiple axes to apply function along.\n\n        Returns\n        -------\n        BoltArrayLocal\n        \"\"\"\n        axes = sorted(tupleize(axis))\n        key_shape = [self.shape[axis] for axis in axes]\n        reshaped = self._align(axes, key_shape=key_shape)\n\n        mapped = asarray(list(map(func, reshaped)))\n        elem_shape = mapped[0].shape\n\n        # invert the previous reshape operation, using the shape of the map result\n        linearized_shape_inv = key_shape + list(elem_shape)\n        reordered = mapped.reshape(*linearized_shape_inv)\n\n        return self._constructor(reordered)","method_path":"https:\/\/github.com\/bolt-project\/bolt\/blob\/9cd7104aa085498da3097b72696184b9d3651c51\/bolt\/local\/array.py#L94-L124"}
{"repo_name":"bolt-project\/bolt","method_name":"BoltArrayLocal.concatenate","method_code":"def concatenate(self, arry, axis=0):\n        \"\"\"\"\"\"\n        if isinstance(arry, ndarray):\n            from bolt import concatenate\n            return concatenate((self, arry), axis)\n        else:\n            raise ValueError(\"other must be local array, got %s\" % type(arry))","method_summary":"Join this array with another array. Paramters --------- arry : ndarray or BoltArrayLocal Another array to concatenate with axis : int, optional, default=0 The axis along which arrays will be joined.","original_method_code":"def concatenate(self, arry, axis=0):\n        \"\"\"\n        Join this array with another array.\n\n        Paramters\n        ---------\n        arry : ndarray or BoltArrayLocal\n            Another array to concatenate with\n\n        axis : int, optional, default=0\n            The axis along which arrays will be joined.\n\n        Returns\n        -------\n        BoltArrayLocal\n        \"\"\"\n        if isinstance(arry, ndarray):\n            from bolt import concatenate\n            return concatenate((self, arry), axis)\n        else:\n            raise ValueError(\"other must be local array, got %s\" % type(arry))","method_path":"https:\/\/github.com\/bolt-project\/bolt\/blob\/9cd7104aa085498da3097b72696184b9d3651c51\/bolt\/local\/array.py#L172-L192"}
{"repo_name":"bolt-project\/bolt","method_name":"BoltArrayLocal.tospark","method_code":"def tospark(self, sc, axis=0):\n        \"\"\"\"\"\"\n        from bolt import array\n        return array(self.toarray(), sc, axis=axis)","method_summary":"Converts a BoltArrayLocal into a BoltArraySpark","original_method_code":"def tospark(self, sc, axis=0):\n        \"\"\"\n        Converts a BoltArrayLocal into a BoltArraySpark\n\n        Parameters\n        ----------\n        sc : SparkContext\n            The SparkContext which will be used to create the BoltArraySpark\n\n        axis : tuple or int, optional, default=0\n            The axis (or axes) across which this array will be parallelized\n\n        Returns\n        -------\n        BoltArraySpark\n        \"\"\"\n        from bolt import array\n        return array(self.toarray(), sc, axis=axis)","method_path":"https:\/\/github.com\/bolt-project\/bolt\/blob\/9cd7104aa085498da3097b72696184b9d3651c51\/bolt\/local\/array.py#L204-L221"}
{"repo_name":"bolt-project\/bolt","method_name":"BoltArrayLocal.tordd","method_code":"def tordd(self, sc, axis=0):\n        \"\"\"\"\"\"\n        from bolt import array\n        return array(self.toarray(), sc, axis=axis).tordd()","method_summary":"Converts a BoltArrayLocal into an RDD","original_method_code":"def tordd(self, sc, axis=0):\n        \"\"\"\n        Converts a BoltArrayLocal into an RDD\n\n        Parameters\n        ----------\n        sc : SparkContext\n            The SparkContext which will be used to create the BoltArraySpark\n\n        axis : tuple or int, optional, default=0\n            The axis (or axes) across which this array will be parallelized\n\n        Returns\n        -------\n        RDD[(tuple, ndarray)]\n        \"\"\"\n        from bolt import array\n        return array(self.toarray(), sc, axis=axis).tordd()","method_path":"https:\/\/github.com\/bolt-project\/bolt\/blob\/9cd7104aa085498da3097b72696184b9d3651c51\/bolt\/local\/array.py#L223-L240"}
{"repo_name":"jazzband\/django-ddp","method_name":"iter_auth_hashes","method_code":"def iter_auth_hashes(user, purpose, minutes_valid):\n    \"\"\"\"\"\"\n    now = timezone.now().replace(microsecond=0, second=0)\n    for minute in range(minutes_valid + 1):\n        yield hashlib.sha1(\n            '%s:%s:%s:%s:%s' % (\n                now - datetime.timedelta(minutes=minute),\n                user.password,\n                purpose,\n                user.pk,\n                settings.SECRET_KEY,\n            ),\n        ).hexdigest()","method_summary":"Generate auth tokens tied to user and specified purpose. The hash expires at midnight on the minute of now + minutes_valid, such that when minutes_valid=1 you get *at least","original_method_code":"def iter_auth_hashes(user, purpose, minutes_valid):\n    \"\"\"\n    Generate auth tokens tied to user and specified purpose.\n\n    The hash expires at midnight on the minute of now + minutes_valid, such\n    that when minutes_valid=1 you get *at least* 1 minute to use the token.\n    \"\"\"\n    now = timezone.now().replace(microsecond=0, second=0)\n    for minute in range(minutes_valid + 1):\n        yield hashlib.sha1(\n            '%s:%s:%s:%s:%s' % (\n                now - datetime.timedelta(minutes=minute),\n                user.password,\n                purpose,\n                user.pk,\n                settings.SECRET_KEY,\n            ),\n        ).hexdigest()","method_path":"https:\/\/github.com\/jazzband\/django-ddp\/blob\/1e1954b06fe140346acea43582515991685e4e01\/dddp\/accounts\/ddp.py#L66-L83"}
{"repo_name":"jazzband\/django-ddp","method_name":"calc_expiry_time","method_code":"def calc_expiry_time(minutes_valid):\n    \"\"\"\"\"\"\n    return (\n        timezone.now() + datetime.timedelta(minutes=minutes_valid + 1)\n    ).replace(second=0, microsecond=0)","method_summary":"Return specific time an auth_hash will expire.","original_method_code":"def calc_expiry_time(minutes_valid):\n    \"\"\"Return specific time an auth_hash will expire.\"\"\"\n    return (\n        timezone.now() + datetime.timedelta(minutes=minutes_valid + 1)\n    ).replace(second=0, microsecond=0)","method_path":"https:\/\/github.com\/jazzband\/django-ddp\/blob\/1e1954b06fe140346acea43582515991685e4e01\/dddp\/accounts\/ddp.py#L91-L95"}
{"repo_name":"jazzband\/django-ddp","method_name":"get_user_token","method_code":"def get_user_token(user, purpose, minutes_valid):\n    \"\"\"\"\"\"\n    token = ''.join(\n        dumps([\n            user.get_username(),\n            get_auth_hash(user, purpose),\n        ]).encode('base64').split('\\n')\n    )\n    return {\n        'id': get_meteor_id(user),\n        'token': token,\n        'tokenExpires': calc_expiry_time(minutes_valid),\n    }","method_summary":"Return login token info for given user.","original_method_code":"def get_user_token(user, purpose, minutes_valid):\n    \"\"\"Return login token info for given user.\"\"\"\n    token = ''.join(\n        dumps([\n            user.get_username(),\n            get_auth_hash(user, purpose),\n        ]).encode('base64').split('\\n')\n    )\n    return {\n        'id': get_meteor_id(user),\n        'token': token,\n        'tokenExpires': calc_expiry_time(minutes_valid),\n    }","method_path":"https:\/\/github.com\/jazzband\/django-ddp\/blob\/1e1954b06fe140346acea43582515991685e4e01\/dddp\/accounts\/ddp.py#L98-L110"}
{"repo_name":"jazzband\/django-ddp","method_name":"Users.serialize","method_code":"def serialize(self, obj, *args, **kwargs):\n        \"\"\"\"\"\"\n        \n        data = super(Users, self).serialize(obj, *args, **kwargs)\n\n        \n        profile = data.pop('fields')\n        profile.setdefault('name', obj.get_full_name())\n        fields = data['fields'] = {\n            'username': obj.get_username(),\n            'emails': [],\n            'profile': profile,\n            'permissions': sorted(self.model.get_all_permissions(obj)),\n        }\n\n        \n        for sensitive in [\n                'password',\n                'user_permissions_ids',\n                'is_active',\n                'is_staff',\n                'is_superuser',\n                'groups_ids',\n        ]:\n            profile.pop(sensitive, None)\n\n        \n        try:\n            fields['createdAt'] = profile.pop('date_joined')\n        except KeyError:\n            date_joined = getattr(\n                obj, 'get_date_joined',\n                lambda: getattr(obj, 'date_joined', None)\n            )()\n            if date_joined:\n                fields['createdAt'] = date_joined\n\n        \n        try:\n            email = profile.pop('email')\n        except KeyError:\n            email = getattr(\n                obj, 'get_email',\n                lambda: getattr(obj, 'email', None)\n            )()\n        if email:\n            fields['emails'].append({'address': email, 'verified': True})\n\n        return data","method_summary":"Serialize user as per Meteor accounts serialization.","original_method_code":"def serialize(self, obj, *args, **kwargs):\n        \"\"\"Serialize user as per Meteor accounts serialization.\"\"\"\n        # use default serialization, then modify to suit our needs.\n        data = super(Users, self).serialize(obj, *args, **kwargs)\n\n        # everything that isn't handled explicitly ends up in `profile`\n        profile = data.pop('fields')\n        profile.setdefault('name', obj.get_full_name())\n        fields = data['fields'] = {\n            'username': obj.get_username(),\n            'emails': [],\n            'profile': profile,\n            'permissions': sorted(self.model.get_all_permissions(obj)),\n        }\n\n        # clear out sensitive data\n        for sensitive in [\n                'password',\n                'user_permissions_ids',\n                'is_active',\n                'is_staff',\n                'is_superuser',\n                'groups_ids',\n        ]:\n            profile.pop(sensitive, None)\n\n        # createdAt (default is django.contrib.auth.models.User.date_joined)\n        try:\n            fields['createdAt'] = profile.pop('date_joined')\n        except KeyError:\n            date_joined = getattr(\n                obj, 'get_date_joined',\n                lambda: getattr(obj, 'date_joined', None)\n            )()\n            if date_joined:\n                fields['createdAt'] = date_joined\n\n        # email (default is django.contrib.auth.models.User.email)\n        try:\n            email = profile.pop('email')\n        except KeyError:\n            email = getattr(\n                obj, 'get_email',\n                lambda: getattr(obj, 'email', None)\n            )()\n        if email:\n            fields['emails'].append({'address': email, 'verified': True})\n\n        return data","method_path":"https:\/\/github.com\/jazzband\/django-ddp\/blob\/1e1954b06fe140346acea43582515991685e4e01\/dddp\/accounts\/ddp.py#L125-L173"}
{"repo_name":"jazzband\/django-ddp","method_name":"Users.deserialize_profile","method_code":"def deserialize_profile(profile, key_prefix='', pop=False):\n        \"\"\"\"\"\"\n        result = {}\n        if pop:\n            getter = profile.pop\n        else:\n            getter = profile.get\n\n        def prefixed(name):\n            \"\"\"\"\"\"\n            return '%s%s' % (key_prefix, name)\n\n        for key in profile.keys():\n            val = getter(key)\n            if key == prefixed('name'):\n                result['full_name'] = val\n            else:\n                raise MeteorError(400, 'Bad profile key: %r' % key)\n        return result","method_summary":"De-serialize user profile fields into concrete model fields.","original_method_code":"def deserialize_profile(profile, key_prefix='', pop=False):\n        \"\"\"De-serialize user profile fields into concrete model fields.\"\"\"\n        result = {}\n        if pop:\n            getter = profile.pop\n        else:\n            getter = profile.get\n\n        def prefixed(name):\n            \"\"\"Return name prefixed by `key_prefix`.\"\"\"\n            return '%s%s' % (key_prefix, name)\n\n        for key in profile.keys():\n            val = getter(key)\n            if key == prefixed('name'):\n                result['full_name'] = val\n            else:\n                raise MeteorError(400, 'Bad profile key: %r' % key)\n        return result","method_path":"https:\/\/github.com\/jazzband\/django-ddp\/blob\/1e1954b06fe140346acea43582515991685e4e01\/dddp\/accounts\/ddp.py#L176-L194"}
{"repo_name":"jazzband\/django-ddp","method_name":"Users.update","method_code":"def update(self, selector, update, options=None):\n        \"\"\"\"\"\"\n        \n        del options\n        user = get_object(\n            self.model, selector['_id'],\n            pk=this.user_id,\n        )\n        profile_update = self.deserialize_profile(\n            update['$set'], key_prefix='profile.', pop=True,\n        )\n        if len(update['$set']) != 0:\n            raise MeteorError(400, 'Invalid update fields: %r')\n\n        for key, val in profile_update.items():\n            setattr(user, key, val)\n        user.save()","method_summary":"Update user data.","original_method_code":"def update(self, selector, update, options=None):\n        \"\"\"Update user data.\"\"\"\n        # we're ignoring the `options` argument at this time\n        del options\n        user = get_object(\n            self.model, selector['_id'],\n            pk=this.user_id,\n        )\n        profile_update = self.deserialize_profile(\n            update['$set'], key_prefix='profile.', pop=True,\n        )\n        if len(update['$set']) != 0:\n            raise MeteorError(400, 'Invalid update fields: %r')\n\n        for key, val in profile_update.items():\n            setattr(user, key, val)\n        user.save()","method_path":"https:\/\/github.com\/jazzband\/django-ddp\/blob\/1e1954b06fe140346acea43582515991685e4e01\/dddp\/accounts\/ddp.py#L197-L213"}
{"repo_name":"jazzband\/django-ddp","method_name":"Auth.user_factory","method_code":"def user_factory(self):\n        \"\"\"\"\"\"\n        if this.user_id is None:\n            return None\n        return self.user_model.objects.get(pk=this.user_id)","method_summary":"Retrieve the current user (or None) from the database.","original_method_code":"def user_factory(self):\n        \"\"\"Retrieve the current user (or None) from the database.\"\"\"\n        if this.user_id is None:\n            return None\n        return self.user_model.objects.get(pk=this.user_id)","method_path":"https:\/\/github.com\/jazzband\/django-ddp\/blob\/1e1954b06fe140346acea43582515991685e4e01\/dddp\/accounts\/ddp.py#L243-L247"}
{"repo_name":"jazzband\/django-ddp","method_name":"Auth.update_subs","method_code":"def update_subs(new_user_id):\n        \"\"\"\"\"\"\n        for sub in Subscription.objects.filter(connection=this.ws.connection):\n            params = loads(sub.params_ejson)\n            pub = API.get_pub_by_name(sub.publication)\n\n            \n            pre = collections.OrderedDict([\n                (col, query) for col, query\n                in API.sub_unique_objects(sub, params, pub)\n            ])\n\n            \n            sub.user_id = new_user_id\n            sub.save()\n\n            \n            post = collections.OrderedDict([\n                (col, query) for col, query\n                in API.sub_unique_objects(sub, params, pub)\n            ])\n\n            \n            for col_post, query in post.items():\n                try:\n                    qs_pre = pre[col_post]\n                    query = query.exclude(\n                        pk__in=qs_pre.order_by().values('pk'),\n                    )\n                except KeyError:\n                    \n                    pass\n                for obj in query:\n                    this.ws.send(col_post.obj_change_as_msg(obj, ADDED))\n\n            \n            for col_pre, query in pre.items():\n                try:\n                    qs_post = post[col_pre]\n                    query = query.exclude(\n                        pk__in=qs_post.order_by().values('pk'),\n                    )\n                except KeyError:\n                    \n                    pass\n                for obj in query:\n                    this.ws.send(col_pre.obj_change_as_msg(obj, REMOVED))","method_summary":"Update subs to send added\/removed for collections with user_rel.","original_method_code":"def update_subs(new_user_id):\n        \"\"\"Update subs to send added\/removed for collections with user_rel.\"\"\"\n        for sub in Subscription.objects.filter(connection=this.ws.connection):\n            params = loads(sub.params_ejson)\n            pub = API.get_pub_by_name(sub.publication)\n\n            # calculate the querysets prior to update\n            pre = collections.OrderedDict([\n                (col, query) for col, query\n                in API.sub_unique_objects(sub, params, pub)\n            ])\n\n            # save the subscription with the updated user_id\n            sub.user_id = new_user_id\n            sub.save()\n\n            # calculate the querysets after the update\n            post = collections.OrderedDict([\n                (col, query) for col, query\n                in API.sub_unique_objects(sub, params, pub)\n            ])\n\n            # first pass, send `added` for objs unique to `post`\n            for col_post, query in post.items():\n                try:\n                    qs_pre = pre[col_post]\n                    query = query.exclude(\n                        pk__in=qs_pre.order_by().values('pk'),\n                    )\n                except KeyError:\n                    # collection not included pre-auth, everything is added.\n                    pass\n                for obj in query:\n                    this.ws.send(col_post.obj_change_as_msg(obj, ADDED))\n\n            # second pass, send `removed` for objs unique to `pre`\n            for col_pre, query in pre.items():\n                try:\n                    qs_post = post[col_pre]\n                    query = query.exclude(\n                        pk__in=qs_post.order_by().values('pk'),\n                    )\n                except KeyError:\n                    # collection not included post-auth, everything is removed.\n                    pass\n                for obj in query:\n                    this.ws.send(col_pre.obj_change_as_msg(obj, REMOVED))","method_path":"https:\/\/github.com\/jazzband\/django-ddp\/blob\/1e1954b06fe140346acea43582515991685e4e01\/dddp\/accounts\/ddp.py#L255-L301"}
{"repo_name":"jazzband\/django-ddp","method_name":"Auth.auth_failed","method_code":"def auth_failed(**credentials):\n        \"\"\"\"\"\"\n        if credentials:\n            user_login_failed.send_robust(\n                sender=__name__,\n                credentials=auth._clean_credentials(credentials),\n            )\n        raise MeteorError(403, 'Authentication failed.')","method_summary":"Consistent fail so we don't provide attackers with valuable info.","original_method_code":"def auth_failed(**credentials):\n        \"\"\"Consistent fail so we don't provide attackers with valuable info.\"\"\"\n        if credentials:\n            user_login_failed.send_robust(\n                sender=__name__,\n                credentials=auth._clean_credentials(credentials),\n            )\n        raise MeteorError(403, 'Authentication failed.')","method_path":"https:\/\/github.com\/jazzband\/django-ddp\/blob\/1e1954b06fe140346acea43582515991685e4e01\/dddp\/accounts\/ddp.py#L304-L311"}
{"repo_name":"jazzband\/django-ddp","method_name":"Auth.validated_user","method_code":"def validated_user(cls, token, purpose, minutes_valid):\n        \"\"\"\"\"\"\n        try:\n            username, auth_hash = loads(token.decode('base64'))\n        except (ValueError, Error):\n            cls.auth_failed(token=token)\n        try:\n            user = cls.user_model.objects.get(**{\n                cls.user_model.USERNAME_FIELD: username,\n                'is_active': True,\n            })\n            user.backend = 'django.contrib.auth.backends.ModelBackend'\n        except cls.user_model.DoesNotExist:\n            cls.auth_failed(username=username, token=token)\n        if auth_hash not in iter_auth_hashes(user, purpose, minutes_valid):\n            cls.auth_failed(username=username, token=token)\n        return user","method_summary":"Resolve and validate auth token, returns user object.","original_method_code":"def validated_user(cls, token, purpose, minutes_valid):\n        \"\"\"Resolve and validate auth token, returns user object.\"\"\"\n        try:\n            username, auth_hash = loads(token.decode('base64'))\n        except (ValueError, Error):\n            cls.auth_failed(token=token)\n        try:\n            user = cls.user_model.objects.get(**{\n                cls.user_model.USERNAME_FIELD: username,\n                'is_active': True,\n            })\n            user.backend = 'django.contrib.auth.backends.ModelBackend'\n        except cls.user_model.DoesNotExist:\n            cls.auth_failed(username=username, token=token)\n        if auth_hash not in iter_auth_hashes(user, purpose, minutes_valid):\n            cls.auth_failed(username=username, token=token)\n        return user","method_path":"https:\/\/github.com\/jazzband\/django-ddp\/blob\/1e1954b06fe140346acea43582515991685e4e01\/dddp\/accounts\/ddp.py#L314-L330"}
{"repo_name":"neurodata\/ndio","method_name":"to_array","method_code":"def to_array(data):\n    \"\"\"\"\"\"\n    try:\n        numpy_data = blosc.unpack_array(data)\n    except Exception as e:\n        raise ValueError(\"Could not load numpy data. {}\".format(e))\n\n    return numpy_data","method_summary":"Import a blosc array into a numpy array.","original_method_code":"def to_array(data):\n    \"\"\"\n    Import a blosc array into a numpy array.\n\n    Arguments:\n        data: A blosc packed numpy array\n\n    Returns:\n        A numpy array with data from a blosc compressed array\n    \"\"\"\n    try:\n        numpy_data = blosc.unpack_array(data)\n    except Exception as e:\n        raise ValueError(\"Could not load numpy data. {}\".format(e))\n\n    return numpy_data","method_path":"https:\/\/github.com\/neurodata\/ndio\/blob\/792dd5816bc770b05a3db2f4327da42ff6253531\/ndio\/convert\/blosc.py#L6-L21"}
{"repo_name":"neurodata\/ndio","method_name":"from_array","method_code":"def from_array(array):\n    \"\"\"\"\"\"\n    try:\n        raw_data = blosc.pack_array(array)\n    except Exception as e:\n        raise ValueError(\"Could not compress data from array. {}\".format(e))\n\n    return raw_data","method_summary":"Export a numpy array to a blosc array.","original_method_code":"def from_array(array):\n    \"\"\"\n    Export a numpy array to a blosc array.\n\n    Arguments:\n        array: The numpy array to compress to blosc array\n\n    Returns:\n        Bytes\/String. A blosc compressed array\n    \"\"\"\n    try:\n        raw_data = blosc.pack_array(array)\n    except Exception as e:\n        raise ValueError(\"Could not compress data from array. {}\".format(e))\n\n    return raw_data","method_path":"https:\/\/github.com\/neurodata\/ndio\/blob\/792dd5816bc770b05a3db2f4327da42ff6253531\/ndio\/convert\/blosc.py#L24-L39"}
{"repo_name":"neurodata\/ndio","method_name":"check_version","method_code":"def check_version():\n    \"\"\"\"\"\"\n    import requests\n    r = requests.get('https:\/\/pypi.python.org\/pypi\/ndio\/json').json()\n    r = r['info']['version']\n    if r != version:\n        print(\"A newer version of ndio is available. \" +\n              \"'pip install -U ndio' to update.\")\n    return r","method_summary":"Tells you if you have an old version of ndio.","original_method_code":"def check_version():\n    \"\"\"\n    Tells you if you have an old version of ndio.\n    \"\"\"\n    import requests\n    r = requests.get('https:\/\/pypi.python.org\/pypi\/ndio\/json').json()\n    r = r['info']['version']\n    if r != version:\n        print(\"A newer version of ndio is available. \" +\n              \"'pip install -U ndio' to update.\")\n    return r","method_path":"https:\/\/github.com\/neurodata\/ndio\/blob\/792dd5816bc770b05a3db2f4327da42ff6253531\/ndio\/__init__.py#L8-L18"}
{"repo_name":"neurodata\/ndio","method_name":"to_voxels","method_code":"def to_voxels(array):\n    \"\"\"\"\"\"\n    if type(array) is not numpy.ndarray:\n        raise ValueError(\"array argument must be of type numpy.ndarray\")\n    return numpy.argwhere(array)","method_summary":"Converts an array to its voxel list.","original_method_code":"def to_voxels(array):\n    \"\"\"\n    Converts an array to its voxel list.\n\n    Arguments:\n        array (numpy.ndarray): A numpy nd array. This must be boolean!\n\n    Returns:\n        A list of n-tuples\n    \"\"\"\n    if type(array) is not numpy.ndarray:\n        raise ValueError(\"array argument must be of type numpy.ndarray\")\n    return numpy.argwhere(array)","method_path":"https:\/\/github.com\/neurodata\/ndio\/blob\/792dd5816bc770b05a3db2f4327da42ff6253531\/ndio\/convert\/volume.py#L5-L17"}
{"repo_name":"neurodata\/ndio","method_name":"from_voxels","method_code":"def from_voxels(voxels):\n    \"\"\"\"\"\"\n    dimensions = len(voxels[0])\n\n    for d in range(len(dimensions)):\n        size.append(max([i[d] for i in voxels]))\n\n    result = numpy.zeros(dimensions)\n\n    for v in voxels:\n        result[v] = 1\n\n    return result","method_summary":"Converts a voxel list to an ndarray.","original_method_code":"def from_voxels(voxels):\n    \"\"\"\n    Converts a voxel list to an ndarray.\n\n    Arguments:\n        voxels (tuple[]): A list of coordinates indicating coordinates of\n            populated voxels in an ndarray.\n\n    Returns:\n        numpy.ndarray The result of the transformation.\n    \"\"\"\n    dimensions = len(voxels[0])\n\n    for d in range(len(dimensions)):\n        size.append(max([i[d] for i in voxels]))\n\n    result = numpy.zeros(dimensions)\n\n    for v in voxels:\n        result[v] = 1\n\n    return result","method_path":"https:\/\/github.com\/neurodata\/ndio\/blob\/792dd5816bc770b05a3db2f4327da42ff6253531\/ndio\/convert\/volume.py#L20-L41"}
{"repo_name":"neurodata\/ndio","method_name":"load","method_code":"def load(png_filename):\n    \"\"\"\"\"\"\n    \n    png_filename = os.path.expanduser(png_filename)\n\n    try:\n        img = Image.open(png_filename)\n    except Exception as e:\n        raise ValueError(\"Could not load file {0} for conversion.\"\n                         .format(png_filename))\n        raise\n\n    return numpy.array(img)","method_summary":"Import a png file into a numpy array.","original_method_code":"def load(png_filename):\n    \"\"\"\n    Import a png file into a numpy array.\n\n    Arguments:\n        png_filename (str): A string filename of a png datafile\n\n    Returns:\n        A numpy array with data from the png file\n    \"\"\"\n    # Expand filename to be absolute\n    png_filename = os.path.expanduser(png_filename)\n\n    try:\n        img = Image.open(png_filename)\n    except Exception as e:\n        raise ValueError(\"Could not load file {0} for conversion.\"\n                         .format(png_filename))\n        raise\n\n    return numpy.array(img)","method_path":"https:\/\/github.com\/neurodata\/ndio\/blob\/792dd5816bc770b05a3db2f4327da42ff6253531\/ndio\/convert\/png.py#L8-L28"}
{"repo_name":"neurodata\/ndio","method_name":"save","method_code":"def save(filename, numpy_data):\n    \"\"\"\"\"\"\n    \n    png_filename = os.path.expanduser(filename)\n\n    if type(numpy_data) is str:\n        fp = open(png_filename, \"wb\")\n        fp.write(numpy_data)\n        fp.close()\n        return png_filename\n\n    try:\n        if numpy_data.dtype.name != 'uint8':\n            m = 'I'\n            img = Image.fromarray(numpy_data, mode=m)\n        else:\n            img = Image.fromarray(numpy_data)\n        img.save(png_filename)\n    except Exception as e:\n        raise ValueError(\"Could not save png file {0}.\".format(png_filename))\n    return png_filename","method_summary":"Export a numpy array to a png file.","original_method_code":"def save(filename, numpy_data):\n    \"\"\"\n    Export a numpy array to a png file.\n\n    Arguments:\n        filename (str): A filename to which to save the png data\n        numpy_data (numpy.ndarray OR str): The numpy array to save to png.\n            OR a string: If a string is provded, it should be a binary png str\n\n    Returns:\n        str. The expanded filename that now holds the png data\n\n    Raises:\n        ValueError: If the save fails; for instance if the binary string data\n            cannot be coerced into a png, or perhaps your numpy.ndarray is\n            ill-formed?\n    \"\"\"\n    # Expand filename to be absolute\n    png_filename = os.path.expanduser(filename)\n\n    if type(numpy_data) is str:\n        fp = open(png_filename, \"wb\")\n        fp.write(numpy_data)\n        fp.close()\n        return png_filename\n\n    try:\n        if numpy_data.dtype.name != 'uint8':\n            m = 'I'\n            img = Image.fromarray(numpy_data, mode=m)\n        else:\n            img = Image.fromarray(numpy_data)\n        img.save(png_filename)\n    except Exception as e:\n        raise ValueError(\"Could not save png file {0}.\".format(png_filename))\n    return png_filename","method_path":"https:\/\/github.com\/neurodata\/ndio\/blob\/792dd5816bc770b05a3db2f4327da42ff6253531\/ndio\/convert\/png.py#L31-L66"}
{"repo_name":"neurodata\/ndio","method_name":"save_collection","method_code":"def save_collection(png_filename_base, numpy_data, start_layers_at=1):\n    \"\"\"\"\"\"\n    file_ext = png_filename_base.split('.')[-1]\n    if file_ext in ['png']:\n        \n        file_base = '.'.join(png_filename_base.split('.')[:-1])\n    else:\n        \n        \n        file_base = png_filename_base\n        file_ext = \".png\"\n\n    file_base_array = file_base.split('*')\n\n    \n    output_files = []\n\n    \n    i = start_layers_at\n    for layer in numpy_data:\n        layer_filename = (str(i).zfill(6)).join(file_base_array) + file_ext\n        output_files.append(save(layer_filename, layer))\n        i += 1\n\n    return output_files","method_summary":"Export a numpy array to a set of png files, with each Z-index 2D array as its own 2D file.","original_method_code":"def save_collection(png_filename_base, numpy_data, start_layers_at=1):\n    \"\"\"\n    Export a numpy array to a set of png files, with each Z-index 2D\n    array as its own 2D file.\n\n    Arguments:\n        png_filename_base:     A filename template, such as \"my-image-*.png\"\n                                which will lead to a collection of files named\n                                \"my-image-0.png\", \"my-image-1.png\", etc.\n        numpy_data:             The numpy array data to save to png.\n\n    Returns:\n        Array. A list of expanded filenames that hold png data.\n    \"\"\"\n    file_ext = png_filename_base.split('.')[-1]\n    if file_ext in ['png']:\n        # Filename is \"name*.ext\", set file_base to \"name*\".\n        file_base = '.'.join(png_filename_base.split('.')[:-1])\n    else:\n        # Filename is \"name*\", set file_base to \"name*\".\n        # That is, extension wasn't included.\n        file_base = png_filename_base\n        file_ext = \".png\"\n\n    file_base_array = file_base.split('*')\n\n    # The array of filenames to return\n    output_files = []\n\n    # Filename 0-padding\n    i = start_layers_at\n    for layer in numpy_data:\n        layer_filename = (str(i).zfill(6)).join(file_base_array) + file_ext\n        output_files.append(save(layer_filename, layer))\n        i += 1\n\n    return output_files","method_path":"https:\/\/github.com\/neurodata\/ndio\/blob\/792dd5816bc770b05a3db2f4327da42ff6253531\/ndio\/convert\/png.py#L69-L105"}
{"repo_name":"neurodata\/ndio","method_name":"load_collection","method_code":"def load_collection(png_filename_base):\n    \"\"\"\"\"\"\n    \n    files = glob.glob(png_filename_base)\n    files.sort()\n\n    numpy_data = []\n    for f in files:\n        numpy_data.append(load(f))\n\n    return numpy.concatenate(numpy_data)","method_summary":"Import all files matching the filename base given with `png_filename_base`. Images are ordered by alphabetical order, which means that you *MUST","original_method_code":"def load_collection(png_filename_base):\n    \"\"\"\n    Import all files matching the filename base given with `png_filename_base`.\n    Images are ordered by alphabetical order, which means that you *MUST* 0-pad\n    your numbers if they span a power of ten (e.g. 0999-1000 or 09-10). This is\n    handled automatically by its complementary function, `png.save_collection`.\n    Also, look at how nicely these documentation lines are all the same length!\n\n    Arguments:\n        png_filename_base (str): An asterisk-wildcard string that should refer\n            to all PNGs in the stack. All *s are replaced according to regular\n            cmd-line expansion rules. See the 'glob' documentation for details\n    Returns:\n        A numpy array holding a 3D dataset\n    \"\"\"\n    # We expect images to be indexed by their alphabetical order.\n    files = glob.glob(png_filename_base)\n    files.sort()\n\n    numpy_data = []\n    for f in files:\n        numpy_data.append(load(f))\n\n    return numpy.concatenate(numpy_data)","method_path":"https:\/\/github.com\/neurodata\/ndio\/blob\/792dd5816bc770b05a3db2f4327da42ff6253531\/ndio\/convert\/png.py#L108-L131"}
{"repo_name":"neurodata\/ndio","method_name":"data.get_block_size","method_code":"def get_block_size(self, token, resolution=None):\n        \"\"\"\"\"\"\n        cdims = self.get_metadata(token)['dataset']['cube_dimension']\n        if resolution is None:\n            resolution = min(cdims.keys())\n        return cdims[str(resolution)]","method_summary":"Gets the block-size for a given token at a given resolution.","original_method_code":"def get_block_size(self, token, resolution=None):\n        \"\"\"\n        Gets the block-size for a given token at a given resolution.\n\n        Arguments:\n            token (str): The token to inspect\n            resolution (int : None): The resolution at which to inspect data.\n                If none is specified, uses the minimum available.\n\n        Returns:\n            int[3]: The xyz blocksize.\n        \"\"\"\n        cdims = self.get_metadata(token)['dataset']['cube_dimension']\n        if resolution is None:\n            resolution = min(cdims.keys())\n        return cdims[str(resolution)]","method_path":"https:\/\/github.com\/neurodata\/ndio\/blob\/792dd5816bc770b05a3db2f4327da42ff6253531\/ndio\/remote\/data.py#L87-L102"}
{"repo_name":"boundary\/pulse-api-cli","method_name":"WebHookHandler.do_POST","method_code":"def do_POST(self):\n        \"\"\"\"\"\"\n        self.send_response(urllib2.httplib.OK)\n        self.end_headers()\n        content_length = int(self.headers['Content-Length'])\n        body = self.rfile.read(content_length)\n        print(\"Client: {0}\".format(str(self.client_address)))\n        print(\"headers: {0}\".format(self.headers))\n        print(\"path: {0}\".format(self.path))\n        print(\"body: {0}\".format(body))","method_summary":"Handles the POST request sent by Boundary Url Action","original_method_code":"def do_POST(self):\n        \"\"\"\n        Handles the POST request sent by Boundary Url Action\n        \"\"\"\n        self.send_response(urllib2.httplib.OK)\n        self.end_headers()\n        content_length = int(self.headers['Content-Length'])\n        body = self.rfile.read(content_length)\n        print(\"Client: {0}\".format(str(self.client_address)))\n        print(\"headers: {0}\".format(self.headers))\n        print(\"path: {0}\".format(self.path))\n        print(\"body: {0}\".format(body))","method_path":"https:\/\/github.com\/boundary\/pulse-api-cli\/blob\/b01ca65b442eed19faac309c9d62bbc3cb2c098f\/boundary\/webhook_handler.py#L182-L193"}
{"repo_name":"boundary\/pulse-api-cli","method_name":"MetricCreateBulk.load_and_parse","method_code":"def load_and_parse(self):\n        \"\"\"\"\"\"\n        f = open(self.file_path, \"r\")\n        metrics_json = f.read()\n        self.metrics = json.loads(metrics_json)","method_summary":"Load the metrics file from the given path","original_method_code":"def load_and_parse(self):\n        \"\"\"\n        Load the metrics file from the given path\n        \"\"\"\n        f = open(self.file_path, \"r\")\n        metrics_json = f.read()\n        self.metrics = json.loads(metrics_json)","method_path":"https:\/\/github.com\/boundary\/pulse-api-cli\/blob\/b01ca65b442eed19faac309c9d62bbc3cb2c098f\/boundary\/metric_create_bulk.py#L55-L61"}
{"repo_name":"boundary\/pulse-api-cli","method_name":"MetricCreateBulk.import_metrics","method_code":"def import_metrics(self):\n        \"\"\"\"\"\"\n\n        self.v2Metrics = self.metricDefinitionV2(self.metrics)\n        if self.v2Metrics:\n            metrics = self.metrics\n\n        else:\n            metrics = self.metrics['result']\n\n        \n        \n        for m in metrics:\n            if self.v2Metrics:\n                metric = metrics[m]\n                metric['name'] = m\n            else:\n                metric = m\n            self.create_update(metric)","method_summary":"1) Get command line arguments 2) Read the JSON file 3) Parse into a dictionary 4) Create or update definitions using API call","original_method_code":"def import_metrics(self):\n        \"\"\"\n        1) Get command line arguments\n        2) Read the JSON file\n        3) Parse into a dictionary\n        4) Create or update definitions using API call\n        \"\"\"\n\n        self.v2Metrics = self.metricDefinitionV2(self.metrics)\n        if self.v2Metrics:\n            metrics = self.metrics\n\n        else:\n            metrics = self.metrics['result']\n\n        # Loop through the metrics and call the API\n        # to create\/update\n        for m in metrics:\n            if self.v2Metrics:\n                metric = metrics[m]\n                metric['name'] = m\n            else:\n                metric = m\n            self.create_update(metric)","method_path":"https:\/\/github.com\/boundary\/pulse-api-cli\/blob\/b01ca65b442eed19faac309c9d62bbc3cb2c098f\/boundary\/metric_create_bulk.py#L63-L86"}
{"repo_name":"boundary\/pulse-api-cli","method_name":"MetricExport.extract_dictionary","method_code":"def extract_dictionary(self, metrics):\n        \"\"\"\"\"\"\n        new_metrics = {}\n        for m in metrics:\n            metric = self.extract_fields(m)\n            new_metrics[m['name']] = metric\n        return new_metrics","method_summary":"Extract required fields from an array","original_method_code":"def extract_dictionary(self, metrics):\n        \"\"\"\n        Extract required fields from an array\n        \"\"\"\n        new_metrics = {}\n        for m in metrics:\n            metric = self.extract_fields(m)\n            new_metrics[m['name']] = metric\n        return new_metrics","method_path":"https:\/\/github.com\/boundary\/pulse-api-cli\/blob\/b01ca65b442eed19faac309c9d62bbc3cb2c098f\/boundary\/metric_export.py#L59-L67"}
{"repo_name":"boundary\/pulse-api-cli","method_name":"MetricExport.filter","method_code":"def filter(self):\n        \"\"\"\"\"\"\n        if self.filter_expression is not None:\n            new_metrics = []\n            metrics = self.metrics['result']\n            for m in metrics:\n                if self.filter_expression.search(m['name']):\n                    new_metrics.append(m)\n        else:\n            new_metrics = self.metrics['result']\n\n        self.metrics = self.extract_dictionary(new_metrics)","method_summary":"Apply the criteria to filter out on the metrics required","original_method_code":"def filter(self):\n        \"\"\"\n        Apply the criteria to filter out on the metrics required\n        \"\"\"\n        if self.filter_expression is not None:\n            new_metrics = []\n            metrics = self.metrics['result']\n            for m in metrics:\n                if self.filter_expression.search(m['name']):\n                    new_metrics.append(m)\n        else:\n            new_metrics = self.metrics['result']\n\n        self.metrics = self.extract_dictionary(new_metrics)","method_path":"https:\/\/github.com\/boundary\/pulse-api-cli\/blob\/b01ca65b442eed19faac309c9d62bbc3cb2c098f\/boundary\/metric_export.py#L69-L82"}
{"repo_name":"boundary\/pulse-api-cli","method_name":"HostgroupGet.get_arguments","method_code":"def get_arguments(self):\n        \"\"\"\"\"\"\n        ApiCli.get_arguments(self)\n        if self.args.hostGroupId is not None:\n            self.hostGroupId = self.args.hostGroupId\n\n        self.path = \"v1\/hostgroup\/{0}\".format(str(self.hostGroupId))","method_summary":"Extracts the specific arguments of this CLI","original_method_code":"def get_arguments(self):\n        \"\"\"\n        Extracts the specific arguments of this CLI\n        \"\"\"\n        ApiCli.get_arguments(self)\n        if self.args.hostGroupId is not None:\n            self.hostGroupId = self.args.hostGroupId\n\n        self.path = \"v1\/hostgroup\/{0}\".format(str(self.hostGroupId))","method_path":"https:\/\/github.com\/boundary\/pulse-api-cli\/blob\/b01ca65b442eed19faac309c9d62bbc3cb2c098f\/boundary\/hostgroup_get.py#L36-L44"}
{"repo_name":"boundary\/pulse-api-cli","method_name":"EventCreate.get_arguments","method_code":"def get_arguments(self):\n        \"\"\"\"\"\"\n        ApiCli.get_arguments(self)\n\n        if self.args.tenant_id is not None:\n            self._tenant_id = self.args.tenant_id\n\n        if self.args.fingerprint_fields is not None:\n            self._fingerprint_fields = self.args.fingerprint_fields\n\n        if self.args.title is not None:\n            self._title = self.args.title\n\n        if self.args.source is not None:\n            self._source = self.args.source\n\n        if self.args.severity is not None:\n            self._severity = self.args.severity\n\n        if self.args.message is not None:\n            self._message = self.args.message\n\n        event = {}\n\n        if self._title is not None:\n            event['title'] = self._title\n\n        if self._severity is not None:\n            event['severity'] = self._severity\n\n        if self._message is not None:\n            event['message'] = self._message\n\n        if self._source is not None:\n            if 'source' not in event:\n                event['source'] = {}\n            if len(self._source) >= 1:\n                event['source']['ref'] = self._source[0]\n            if len(self._source) >= 2:\n                event['source']['type'] = self._source[1]\n\n        self._process_properties(self.args.properties)\n        if self._properties is not None:\n            event['properties'] = self._properties\n\n        if self._fingerprint_fields is not None:\n            event['fingerprintFields'] = self._fingerprint_fields\n\n        self.data = json.dumps(event, sort_keys=True)\n        self.headers = {'Content-Type': 'application\/json'}","method_summary":"Extracts the specific arguments of this CLI","original_method_code":"def get_arguments(self):\n        \"\"\"\n        Extracts the specific arguments of this CLI\n        \"\"\"\n        ApiCli.get_arguments(self)\n\n        if self.args.tenant_id is not None:\n            self._tenant_id = self.args.tenant_id\n\n        if self.args.fingerprint_fields is not None:\n            self._fingerprint_fields = self.args.fingerprint_fields\n\n        if self.args.title is not None:\n            self._title = self.args.title\n\n        if self.args.source is not None:\n            self._source = self.args.source\n\n        if self.args.severity is not None:\n            self._severity = self.args.severity\n\n        if self.args.message is not None:\n            self._message = self.args.message\n\n        event = {}\n\n        if self._title is not None:\n            event['title'] = self._title\n\n        if self._severity is not None:\n            event['severity'] = self._severity\n\n        if self._message is not None:\n            event['message'] = self._message\n\n        if self._source is not None:\n            if 'source' not in event:\n                event['source'] = {}\n            if len(self._source) >= 1:\n                event['source']['ref'] = self._source[0]\n            if len(self._source) >= 2:\n                event['source']['type'] = self._source[1]\n\n        self._process_properties(self.args.properties)\n        if self._properties is not None:\n            event['properties'] = self._properties\n\n        if self._fingerprint_fields is not None:\n            event['fingerprintFields'] = self._fingerprint_fields\n\n        self.data = json.dumps(event, sort_keys=True)\n        self.headers = {'Content-Type': 'application\/json'}","method_path":"https:\/\/github.com\/boundary\/pulse-api-cli\/blob\/b01ca65b442eed19faac309c9d62bbc3cb2c098f\/boundary\/event_create.py#L94-L145"}
{"repo_name":"boundary\/pulse-api-cli","method_name":"MeterClient._call_api","method_code":"def _call_api(self):\n        \"\"\"\"\"\"\n\n        \n        sockobj = socket(AF_INET, SOCK_STREAM)\n        sockobj.connect((self.rpc_host, self.rpc_port))\n        self.get_json()\n        message = [self.rpc_message.encode('utf-8')]\n\n        for line in message:\n            sockobj.send(line)\n            data = sockobj.recv(self.MAX_LINE)\n            print(data)\n            self.rpc_data.append(data)\n\n        sockobj.close()","method_summary":"Make a call to the meter via JSON RPC","original_method_code":"def _call_api(self):\n        \"\"\"\n        Make a call to the meter via JSON RPC\n        \"\"\"\n\n        # Allocate a socket and connect to the meter\n        sockobj = socket(AF_INET, SOCK_STREAM)\n        sockobj.connect((self.rpc_host, self.rpc_port))\n        self.get_json()\n        message = [self.rpc_message.encode('utf-8')]\n\n        for line in message:\n            sockobj.send(line)\n            data = sockobj.recv(self.MAX_LINE)\n            print(data)\n            self.rpc_data.append(data)\n\n        sockobj.close()","method_path":"https:\/\/github.com\/boundary\/pulse-api-cli\/blob\/b01ca65b442eed19faac309c9d62bbc3cb2c098f\/boundary\/meter_client.py#L122-L139"}
{"repo_name":"boundary\/pulse-api-cli","method_name":"HostgroupUpdate.get_arguments","method_code":"def get_arguments(self):\n        \"\"\"\"\"\"\n        HostgroupModify.get_arguments(self)\n\n        if self.args.host_group_id is not None:\n            self.host_group_id = self.args.host_group_id\n\n        self.path = \"v1\/hostgroup\/\" + str(self.host_group_id)","method_summary":"Extracts the specific arguments of this CLI","original_method_code":"def get_arguments(self):\n        \"\"\"\n        Extracts the specific arguments of this CLI\n        \"\"\"\n        HostgroupModify.get_arguments(self)\n\n        if self.args.host_group_id is not None:\n            self.host_group_id = self.args.host_group_id\n\n        self.path = \"v1\/hostgroup\/\" + str(self.host_group_id)","method_path":"https:\/\/github.com\/boundary\/pulse-api-cli\/blob\/b01ca65b442eed19faac309c9d62bbc3cb2c098f\/boundary\/hostgroup_update.py#L38-L47"}
{"repo_name":"treycucco\/pyebnf","method_name":"Parser.identifier","method_code":"def identifier(self, text):\n    \"\"\"\"\"\"\n    self._attempting(text)\n    return concatenation([\n      alternation([\n        self.alpha_character,\n        \"_\"\n      ]),\n      zero_or_more(\n        alternation([\n          self.alpha_character,\n          \"_\",\n          self.digit\n        ])\n      )\n    ], ignore_whitespace=False)(text).compressed(TokenType.identifier)","method_summary":"identifier = alpha_character | \"_\" . {alpha_character | \"_\" | digit} ;","original_method_code":"def identifier(self, text):\n    \"\"\"identifier = alpha_character | \"_\" . {alpha_character | \"_\" | digit} ;\"\"\"\n    self._attempting(text)\n    return concatenation([\n      alternation([\n        self.alpha_character,\n        \"_\"\n      ]),\n      zero_or_more(\n        alternation([\n          self.alpha_character,\n          \"_\",\n          self.digit\n        ])\n      )\n    ], ignore_whitespace=False)(text).compressed(TokenType.identifier)","method_path":"https:\/\/github.com\/treycucco\/pyebnf\/blob\/3634ddabbe5d73508bcc20f4a591f86a46634e1d\/pyebnf\/_hand_written_parser.py#L76-L91"}
{"repo_name":"treycucco\/pyebnf","method_name":"Parser.expression_terminal","method_code":"def expression_terminal(self, text):\n    \"\"\"\"\"\"\n    self._attempting(text)\n    return alternation([\n      self.identifier,\n      self.terminal,\n      self.option_group,\n      self.repetition_group,\n      self.grouping_group,\n      self.special_handling\n    ])(text)","method_summary":"expression_terminal = identifier | terminal | option_group | repetition_group | grouping_group | special_handling ;","original_method_code":"def expression_terminal(self, text):\n    \"\"\"expression_terminal = identifier\n                           | terminal\n                           | option_group\n                           | repetition_group\n                           | grouping_group\n                           | special_handling ;\n    \"\"\"\n    self._attempting(text)\n    return alternation([\n      self.identifier,\n      self.terminal,\n      self.option_group,\n      self.repetition_group,\n      self.grouping_group,\n      self.special_handling\n    ])(text)","method_path":"https:\/\/github.com\/treycucco\/pyebnf\/blob\/3634ddabbe5d73508bcc20f4a591f86a46634e1d\/pyebnf\/_hand_written_parser.py#L142-L158"}
{"repo_name":"treycucco\/pyebnf","method_name":"Parser.option_group","method_code":"def option_group(self, text):\n    \"\"\"\"\"\"\n    self._attempting(text)\n    return concatenation([\n      \"[\",\n      self.expression,\n      \"]\"\n    ], ignore_whitespace=True)(text).retyped(TokenType.option_group)","method_summary":"option_group = \"[\" , expression , \"]\" ;","original_method_code":"def option_group(self, text):\n    \"\"\"option_group = \"[\" , expression , \"]\" ;\"\"\"\n    self._attempting(text)\n    return concatenation([\n      \"[\",\n      self.expression,\n      \"]\"\n    ], ignore_whitespace=True)(text).retyped(TokenType.option_group)","method_path":"https:\/\/github.com\/treycucco\/pyebnf\/blob\/3634ddabbe5d73508bcc20f4a591f86a46634e1d\/pyebnf\/_hand_written_parser.py#L160-L167"}
{"repo_name":"treycucco\/pyebnf","method_name":"Parser.terminal","method_code":"def terminal(self, text):\n    \"\"\"\"\"\"\n    self._attempting(text)\n    return alternation([\n      concatenation([\n        '\"',\n        one_or_more(\n          exclusion(self.printable, '\"')\n        ),\n        '\"'\n      ], ignore_whitespace=False),\n      concatenation([\n        \"'\",\n        one_or_more(\n          exclusion(self.printable,\"'\")\n        ),\n        \"'\"\n      ], ignore_whitespace=False)\n    ])(text).compressed(TokenType.terminal)","method_summary":"terminal = '\"' . (printable - '\"') + . '\"' | \"'\" . (printable - \"'\") + . \"'\" ;","original_method_code":"def terminal(self, text):\n    \"\"\"terminal = '\"' . (printable - '\"') + . '\"'\n                | \"'\" . (printable - \"'\") + . \"'\" ;\n    \"\"\"\n    self._attempting(text)\n    return alternation([\n      concatenation([\n        '\"',\n        one_or_more(\n          exclusion(self.printable, '\"')\n        ),\n        '\"'\n      ], ignore_whitespace=False),\n      concatenation([\n        \"'\",\n        one_or_more(\n          exclusion(self.printable,\"'\")\n        ),\n        \"'\"\n      ], ignore_whitespace=False)\n    ])(text).compressed(TokenType.terminal)","method_path":"https:\/\/github.com\/treycucco\/pyebnf\/blob\/3634ddabbe5d73508bcc20f4a591f86a46634e1d\/pyebnf\/_hand_written_parser.py#L204-L224"}
{"repo_name":"treycucco\/pyebnf","method_name":"Parser.operator","method_code":"def operator(self, text):\n    \"\"\"\"\"\"\n    self._attempting(text)\n    return alternation([\n      \"|\",\n      \".\",\n      \",\",\n      \"-\"\n    ])(text).retyped(TokenType.operator)","method_summary":"operator = \"|\" | \".\" | \",\" | \"-\";","original_method_code":"def operator(self, text):\n    \"\"\"operator = \"|\" | \".\" | \",\" | \"-\";\"\"\"\n    self._attempting(text)\n    return alternation([\n      \"|\",\n      \".\",\n      \",\",\n      \"-\"\n    ])(text).retyped(TokenType.operator)","method_path":"https:\/\/github.com\/treycucco\/pyebnf\/blob\/3634ddabbe5d73508bcc20f4a591f86a46634e1d\/pyebnf\/_hand_written_parser.py#L226-L234"}
{"repo_name":"treycucco\/pyebnf","method_name":"Parser.op_mult","method_code":"def op_mult(self, text):\n    \"\"\"\"\"\"\n    self._attempting(text)\n    return terminal(\"*\")(text).retyped(TokenType.op_mult)","method_summary":"op_mult = \"*\" ;","original_method_code":"def op_mult(self, text):\n    \"\"\"op_mult = \"*\" ;\"\"\"\n    self._attempting(text)\n    return terminal(\"*\")(text).retyped(TokenType.op_mult)","method_path":"https:\/\/github.com\/treycucco\/pyebnf\/blob\/3634ddabbe5d73508bcc20f4a591f86a46634e1d\/pyebnf\/_hand_written_parser.py#L236-L239"}
{"repo_name":"treycucco\/pyebnf","method_name":"Parser.op_add","method_code":"def op_add(self, text):\n    \"\"\"\"\"\"\n    self._attempting(text)\n    return terminal(\"+\")(text).retyped(TokenType.op_add)","method_summary":"op_add = \"+\" ;","original_method_code":"def op_add(self, text):\n    \"\"\"op_add = \"+\" ;\"\"\"\n    self._attempting(text)\n    return terminal(\"+\")(text).retyped(TokenType.op_add)","method_path":"https:\/\/github.com\/treycucco\/pyebnf\/blob\/3634ddabbe5d73508bcc20f4a591f86a46634e1d\/pyebnf\/_hand_written_parser.py#L241-L244"}
{"repo_name":"boundary\/pulse-api-cli","method_name":"PluginGetComponents.get_arguments","method_code":"def get_arguments(self):\n        \"\"\"\"\"\"\n        ApiCli.get_arguments(self)\n        if self.args.pluginName is not None:\n            self.pluginName = self.args.pluginName\n\n        self.path = \"v1\/plugins\/{0}\/components\".format(self.pluginName)","method_summary":"Extracts the specific arguments of this CLI","original_method_code":"def get_arguments(self):\n        \"\"\"\n        Extracts the specific arguments of this CLI\n        \"\"\"\n        ApiCli.get_arguments(self)\n        if self.args.pluginName is not None:\n            self.pluginName = self.args.pluginName\n\n        self.path = \"v1\/plugins\/{0}\/components\".format(self.pluginName)","method_path":"https:\/\/github.com\/boundary\/pulse-api-cli\/blob\/b01ca65b442eed19faac309c9d62bbc3cb2c098f\/boundary\/plugin_get_components.py#L31-L39"}
{"repo_name":"treycucco\/pyebnf","method_name":"esc_split","method_code":"def esc_split(text, delimiter=\" \", maxsplit=-1, escape=\"\\\\\", *, ignore_empty=False):\n  \"\"\"\"\"\"\n  is_escaped = False\n  split_count = 0\n  yval = []\n\n  for char in text:\n    if is_escaped:\n      is_escaped = False\n      yval.append(char)\n    else:\n      if char == escape:\n        is_escaped = True\n      elif char in delimiter and split_count != maxsplit:\n        if yval or not ignore_empty:\n          yield \"\".join(yval)\n          split_count += 1\n        yval = []\n      else:\n        yval.append(char)\n\n  yield \"\".join(yval)","method_summary":"Escape-aware text","original_method_code":"def esc_split(text, delimiter=\" \", maxsplit=-1, escape=\"\\\\\", *, ignore_empty=False):\n  \"\"\"Escape-aware text splitting:\n\n  Split text on on a delimiter, recognizing escaped delimiters.\"\"\"\n  is_escaped = False\n  split_count = 0\n  yval = []\n\n  for char in text:\n    if is_escaped:\n      is_escaped = False\n      yval.append(char)\n    else:\n      if char == escape:\n        is_escaped = True\n      elif char in delimiter and split_count != maxsplit:\n        if yval or not ignore_empty:\n          yield \"\".join(yval)\n          split_count += 1\n        yval = []\n      else:\n        yval.append(char)\n\n  yield \"\".join(yval)","method_path":"https:\/\/github.com\/treycucco\/pyebnf\/blob\/3634ddabbe5d73508bcc20f4a591f86a46634e1d\/pyebnf\/util.py#L6-L29"}
{"repo_name":"treycucco\/pyebnf","method_name":"esc_join","method_code":"def esc_join(iterable, delimiter=\" \", escape=\"\\\\\"):\n  \"\"\"\"\"\"\n  rep = escape + delimiter\n  return delimiter.join(i.replace(delimiter, rep) for i in iterable)","method_summary":"Join an iterable by a delimiter, replacing instances of delimiter in items with escape + delimiter.","original_method_code":"def esc_join(iterable, delimiter=\" \", escape=\"\\\\\"):\n  \"\"\"Join an iterable by a delimiter, replacing instances of delimiter in items\n  with escape + delimiter.\n  \"\"\"\n  rep = escape + delimiter\n  return delimiter.join(i.replace(delimiter, rep) for i in iterable)","method_path":"https:\/\/github.com\/treycucco\/pyebnf\/blob\/3634ddabbe5d73508bcc20f4a591f86a46634e1d\/pyebnf\/util.py#L32-L37"}
{"repo_name":"treycucco\/pyebnf","method_name":"get_line_and_char","method_code":"def get_line_and_char(newline_positions, position):\n  \"\"\"\"\"\"\n  if newline_positions:\n    for line_no, nl_pos in enumerate(newline_positions):\n      if nl_pos >= position:\n        if line_no == 0:\n          return (line_no, position)\n        else:\n          return (line_no, position - newline_positions[line_no - 1] - 1)\n    return (line_no + 1, position - newline_positions[-1] - 1)\n  else:\n    return (0, position)","method_summary":"Given a list of newline positions, and an offset from the start of the source code that newline_positions was pulled from, return a 2-tuple of (line, char) coordinates.","original_method_code":"def get_line_and_char(newline_positions, position):\n  \"\"\"Given a list of newline positions, and an offset from the start of the source code\n  that newline_positions was pulled from, return a 2-tuple of (line, char) coordinates.\n  \"\"\"\n  if newline_positions:\n    for line_no, nl_pos in enumerate(newline_positions):\n      if nl_pos >= position:\n        if line_no == 0:\n          return (line_no, position)\n        else:\n          return (line_no, position - newline_positions[line_no - 1] - 1)\n    return (line_no + 1, position - newline_positions[-1] - 1)\n  else:\n    return (0, position)","method_path":"https:\/\/github.com\/treycucco\/pyebnf\/blob\/3634ddabbe5d73508bcc20f4a591f86a46634e1d\/pyebnf\/util.py#L51-L64"}
{"repo_name":"LionelAuroux\/pyrser","method_name":"Symbol.get_scope_list","method_code":"def get_scope_list(self) -> list:\n        \"\"\"\"\"\"\n        \n        lstparent = [self]\n        p = self.get_parent()\n        while p is not None:\n            lstparent.append(p)\n            p = p.get_parent()\n        return lstparent","method_summary":"Return the list of all contained scope from global to local","original_method_code":"def get_scope_list(self) -> list:\n        \"\"\"\n        Return the list of all contained scope from global to local\n        \"\"\"\n        # by default only return scoped name\n        lstparent = [self]\n        p = self.get_parent()\n        while p is not None:\n            lstparent.append(p)\n            p = p.get_parent()\n        return lstparent","method_path":"https:\/\/github.com\/LionelAuroux\/pyrser\/blob\/f153a97ef2b6bf915a1ed468c0252a9a59b754d5\/pyrser\/type_system\/symbol.py#L39-L49"}
{"repo_name":"LionelAuroux\/pyrser","method_name":"Symbol.get_scope_names","method_code":"def get_scope_names(self) -> list:\n        \"\"\"\"\"\"\n        \n        lscope = []\n        for scope in reversed(self.get_scope_list()):\n            if scope.name is not None:\n                \n                lscope.append(scope.name)\n        return lscope","method_summary":"Return the list of all contained scope from global to local","original_method_code":"def get_scope_names(self) -> list:\n        \"\"\"\n        Return the list of all contained scope from global to local\n        \"\"\"\n        # allow global scope to have an None string instance\n        lscope = []\n        for scope in reversed(self.get_scope_list()):\n            if scope.name is not None:\n                # handle fun\/block scope decoration\n                lscope.append(scope.name)\n        return lscope","method_path":"https:\/\/github.com\/LionelAuroux\/pyrser\/blob\/f153a97ef2b6bf915a1ed468c0252a9a59b754d5\/pyrser\/type_system\/symbol.py#L51-L61"}
{"repo_name":"LionelAuroux\/pyrser","method_name":"Cursor.position","method_code":"def position(self) -> Position:\n        \"\"\"\"\"\"\n        return Position(self._index, self._lineno, self._col_offset)","method_summary":"The current position of the cursor.","original_method_code":"def position(self) -> Position:\n        \"\"\"The current position of the cursor.\"\"\"\n        return Position(self._index, self._lineno, self._col_offset)","method_path":"https:\/\/github.com\/LionelAuroux\/pyrser\/blob\/f153a97ef2b6bf915a1ed468c0252a9a59b754d5\/pyrser\/parsing\/stream.py#L40-L42"}
{"repo_name":"LionelAuroux\/pyrser","method_name":"Cursor.max_readed_position","method_code":"def max_readed_position(self) -> Position:\n        \"\"\"\"\"\"\n        return Position(self._maxindex, self._maxline, self._maxcol)","method_summary":"The index of the deepest character readed.","original_method_code":"def max_readed_position(self) -> Position:\n        \"\"\"The index of the deepest character readed.\"\"\"\n        return Position(self._maxindex, self._maxline, self._maxcol)","method_path":"https:\/\/github.com\/LionelAuroux\/pyrser\/blob\/f153a97ef2b6bf915a1ed468c0252a9a59b754d5\/pyrser\/parsing\/stream.py#L51-L53"}
{"repo_name":"LionelAuroux\/pyrser","method_name":"Cursor.step_next_char","method_code":"def step_next_char(self):\n        \"\"\"\"\"\"\n        self._index += 1\n        self._col_offset += 1\n        if self._index > self._maxindex:\n            self._maxindex = self._index\n            self._maxcol = self._col_offset\n            self._maxline = self._lineno","method_summary":"Puts the cursor on the next character.","original_method_code":"def step_next_char(self):\n        \"\"\"Puts the cursor on the next character.\"\"\"\n        self._index += 1\n        self._col_offset += 1\n        if self._index > self._maxindex:\n            self._maxindex = self._index\n            self._maxcol = self._col_offset\n            self._maxline = self._lineno","method_path":"https:\/\/github.com\/LionelAuroux\/pyrser\/blob\/f153a97ef2b6bf915a1ed468c0252a9a59b754d5\/pyrser\/parsing\/stream.py#L55-L62"}
{"repo_name":"LionelAuroux\/pyrser","method_name":"Cursor.step_next_line","method_code":"def step_next_line(self):\n        \"\"\"\"\"\"\n        self._eol.append(self.position)\n        self._lineno += 1\n        self._col_offset = 0","method_summary":"Sets cursor as beginning of next line.","original_method_code":"def step_next_line(self):\n        \"\"\"Sets cursor as beginning of next line.\"\"\"\n        self._eol.append(self.position)\n        self._lineno += 1\n        self._col_offset = 0","method_path":"https:\/\/github.com\/LionelAuroux\/pyrser\/blob\/f153a97ef2b6bf915a1ed468c0252a9a59b754d5\/pyrser\/parsing\/stream.py#L69-L73"}
{"repo_name":"LionelAuroux\/pyrser","method_name":"Cursor.step_prev_line","method_code":"def step_prev_line(self):\n        \"\"\"\"\"\"\n        \n        \n        if len(self._eol) > 0:\n            self.position = self._eol.pop()","method_summary":"Sets cursor as end of previous line.","original_method_code":"def step_prev_line(self):\n        \"\"\"Sets cursor as end of previous line.\"\"\"\n        #TODO(bps): raise explicit error for unregistered eol\n        #assert self._eol[-1].index == self._index\n        if len(self._eol) > 0:\n            self.position = self._eol.pop()","method_path":"https:\/\/github.com\/LionelAuroux\/pyrser\/blob\/f153a97ef2b6bf915a1ed468c0252a9a59b754d5\/pyrser\/parsing\/stream.py#L75-L80"}
{"repo_name":"LionelAuroux\/pyrser","method_name":"Stream.last_readed_line","method_code":"def last_readed_line(self) -> str:\n        \"\"\"\"\"\"\n        mpos = self._cursor.max_readed_position\n        mindex = mpos.index\n        \n        prevline = mindex - 1 if mindex == self.eos_index else mindex\n        while prevline >= 0 and self._content[prevline] != '\\n':\n            prevline -= 1\n        \n        nextline = mindex\n        while nextline < self.eos_index and self._content[nextline] != '\\n':\n            nextline += 1\n        last_line = self._content[prevline + 1:nextline]\n        return last_line","method_summary":"Usefull string to compute error message.","original_method_code":"def last_readed_line(self) -> str:\n        \"\"\"Usefull string to compute error message.\"\"\"\n        mpos = self._cursor.max_readed_position\n        mindex = mpos.index\n        # search last \\n\n        prevline = mindex - 1 if mindex == self.eos_index else mindex\n        while prevline >= 0 and self._content[prevline] != '\\n':\n            prevline -= 1\n        # search next \\n\n        nextline = mindex\n        while nextline < self.eos_index and self._content[nextline] != '\\n':\n            nextline += 1\n        last_line = self._content[prevline + 1:nextline]\n        return last_line","method_path":"https:\/\/github.com\/LionelAuroux\/pyrser\/blob\/f153a97ef2b6bf915a1ed468c0252a9a59b754d5\/pyrser\/parsing\/stream.py#L156-L169"}
{"repo_name":"LionelAuroux\/pyrser","method_name":"Stream.incpos","method_code":"def incpos(self, length: int=1) -> int:\n        \"\"\"\"\"\"\n        if length < 0:\n            raise ValueError(\"length must be positive\")\n        i = 0\n        while (i < length):\n            if self._cursor.index < self._len:\n                if self.peek_char == '\\n':\n                    self._cursor.step_next_line()\n                self._cursor.step_next_char()\n            i += 1\n        return self._cursor.index","method_summary":"Increment the cursor to the next character.","original_method_code":"def incpos(self, length: int=1) -> int:\n        \"\"\"Increment the cursor to the next character.\"\"\"\n        if length < 0:\n            raise ValueError(\"length must be positive\")\n        i = 0\n        while (i < length):\n            if self._cursor.index < self._len:\n                if self.peek_char == '\\n':\n                    self._cursor.step_next_line()\n                self._cursor.step_next_char()\n            i += 1\n        return self._cursor.index","method_path":"https:\/\/github.com\/LionelAuroux\/pyrser\/blob\/f153a97ef2b6bf915a1ed468c0252a9a59b754d5\/pyrser\/parsing\/stream.py#L171-L182"}
{"repo_name":"LionelAuroux\/pyrser","method_name":"Stream.save_context","method_code":"def save_context(self) -> bool:\n        \"\"\"\"\"\"\n        self._contexts.append(self._cursor.position)\n        return True","method_summary":"Save current position.","original_method_code":"def save_context(self) -> bool:\n        \"\"\"Save current position.\"\"\"\n        self._contexts.append(self._cursor.position)\n        return True","method_path":"https:\/\/github.com\/LionelAuroux\/pyrser\/blob\/f153a97ef2b6bf915a1ed468c0252a9a59b754d5\/pyrser\/parsing\/stream.py#L199-L202"}
{"repo_name":"inveniosoftware\/invenio-migrator","method_name":"default_serializer","method_code":"def default_serializer(o):\n    \"\"\"\"\"\"\n    defs = (\n        ((datetime.date, datetime.time),\n         lambda x: x.isoformat(), ),\n        ((datetime.datetime, ),\n         lambda x: dt2utc_timestamp(x), ),\n    )\n    for types, fun in defs:\n        if isinstance(o, types):\n            return fun(o)","method_summary":"Default serializer for json.","original_method_code":"def default_serializer(o):\n    \"\"\"Default serializer for json.\"\"\"\n    defs = (\n        ((datetime.date, datetime.time),\n         lambda x: x.isoformat(), ),\n        ((datetime.datetime, ),\n         lambda x: dt2utc_timestamp(x), ),\n    )\n    for types, fun in defs:\n        if isinstance(o, types):\n            return fun(o)","method_path":"https:\/\/github.com\/inveniosoftware\/invenio-migrator\/blob\/6902c6968a39b747d15e32363f43b7dffe2622c2\/invenio_migrator\/legacy\/deposit.py#L41-L51"}
{"repo_name":"inveniosoftware\/invenio-migrator","method_name":"_get_depositions","method_code":"def _get_depositions(user=None, type=None):\n    \"\"\"\"\"\"\n    from invenio.modules.workflows.models import BibWorkflowObject, Workflow\n    from invenio.modules.deposit.models import InvalidDepositionType\n    from flask import current_app\n    from invenio.ext.sqlalchemy import db\n    from invenio.modules.deposit.models import Deposition\n    params = [\n        Workflow.module_name == 'webdeposit',\n    ]\n\n    if user:\n        params.append(BibWorkflowObject.id_user == user.get_id())\n    else:\n        params.append(BibWorkflowObject.id_user != 0)\n\n    if type:\n        params.append(Workflow.name == type.get_identifier())\n\n    objects = BibWorkflowObject.query.join(\"workflow\").options(\n        db.contains_eager('workflow')).filter(*params)\n\n    def _create_obj(o):\n        try:\n            obj = Deposition(o)\n        except InvalidDepositionType as err:\n            current_app.logger.exception(err)\n            return None\n        if type is None or obj.type == type:\n            return obj\n        return None\n\n    def mapper_filter(objs):\n        for o in objs:\n            o = _create_obj(o)\n            if o is not None:\n                yield o\n\n    return mapper_filter(objects)","method_summary":"Get list of depositions (as iterator). This is redefined Deposition.get_depositions classmethod without order-by for better performance.","original_method_code":"def _get_depositions(user=None, type=None):\n    \"\"\"Get list of depositions (as iterator).\n\n    This is redefined Deposition.get_depositions classmethod without order-by\n    for better performance.\n    \"\"\"\n    from invenio.modules.workflows.models import BibWorkflowObject, Workflow\n    from invenio.modules.deposit.models import InvalidDepositionType\n    from flask import current_app\n    from invenio.ext.sqlalchemy import db\n    from invenio.modules.deposit.models import Deposition\n    params = [\n        Workflow.module_name == 'webdeposit',\n    ]\n\n    if user:\n        params.append(BibWorkflowObject.id_user == user.get_id())\n    else:\n        params.append(BibWorkflowObject.id_user != 0)\n\n    if type:\n        params.append(Workflow.name == type.get_identifier())\n\n    objects = BibWorkflowObject.query.join(\"workflow\").options(\n        db.contains_eager('workflow')).filter(*params)\n\n    def _create_obj(o):\n        try:\n            obj = Deposition(o)\n        except InvalidDepositionType as err:\n            current_app.logger.exception(err)\n            return None\n        if type is None or obj.type == type:\n            return obj\n        return None\n\n    def mapper_filter(objs):\n        for o in objs:\n            o = _create_obj(o)\n            if o is not None:\n                yield o\n\n    return mapper_filter(objects)","method_path":"https:\/\/github.com\/inveniosoftware\/invenio-migrator\/blob\/6902c6968a39b747d15e32363f43b7dffe2622c2\/invenio_migrator\/legacy\/deposit.py#L54-L96"}
{"repo_name":"inveniosoftware\/invenio-migrator","method_name":"get","method_code":"def get(query, from_date, limit=0, **kwargs):\n    \"\"\"\"\"\"\n    dep_generator = _get_depositions()\n    total_depids = 1  \n\n    \n    if limit > 0:\n        dep_generator = islice(dep_generator, limit)\n        total_depids = limit\n    return total_depids, dep_generator","method_summary":"Get deposits.","original_method_code":"def get(query, from_date, limit=0, **kwargs):\n    \"\"\"Get deposits.\"\"\"\n    dep_generator = _get_depositions()\n    total_depids = 1  # Count of depositions is hard to determine\n\n    # If limit provided, serve only first n=limit items\n    if limit > 0:\n        dep_generator = islice(dep_generator, limit)\n        total_depids = limit\n    return total_depids, dep_generator","method_path":"https:\/\/github.com\/inveniosoftware\/invenio-migrator\/blob\/6902c6968a39b747d15e32363f43b7dffe2622c2\/invenio_migrator\/legacy\/deposit.py#L99-L108"}
{"repo_name":"inveniosoftware\/invenio-migrator","method_name":"dump","method_code":"def dump(deposition, from_date, with_json=True, latest_only=False, **kwargs):\n    \"\"\"\"\"\"\n    \n    dep_json = json.dumps(deposition.__getstate__(),\n                          default=default_serializer)\n    dep_dict = json.loads(dep_json)\n    dep_dict['_p'] = {}\n    dep_dict['_p']['id'] = deposition.id\n    dep_dict['_p']['created'] = dt2utc_timestamp(deposition.created)\n    dep_dict['_p']['modified'] = dt2utc_timestamp(deposition.modified)\n    dep_dict['_p']['user_id'] = deposition.user_id\n    dep_dict['_p']['state'] = deposition.state\n    dep_dict['_p']['has_sip'] = deposition.has_sip()\n    dep_dict['_p']['submitted'] = deposition.submitted\n    return dep_dict","method_summary":"Dump the deposition object as dictionary.","original_method_code":"def dump(deposition, from_date, with_json=True, latest_only=False, **kwargs):\n    \"\"\"Dump the deposition object as dictionary.\"\"\"\n    # Serialize the __getstate__ and fall back to default serializer\n    dep_json = json.dumps(deposition.__getstate__(),\n                          default=default_serializer)\n    dep_dict = json.loads(dep_json)\n    dep_dict['_p'] = {}\n    dep_dict['_p']['id'] = deposition.id\n    dep_dict['_p']['created'] = dt2utc_timestamp(deposition.created)\n    dep_dict['_p']['modified'] = dt2utc_timestamp(deposition.modified)\n    dep_dict['_p']['user_id'] = deposition.user_id\n    dep_dict['_p']['state'] = deposition.state\n    dep_dict['_p']['has_sip'] = deposition.has_sip()\n    dep_dict['_p']['submitted'] = deposition.submitted\n    return dep_dict","method_path":"https:\/\/github.com\/inveniosoftware\/invenio-migrator\/blob\/6902c6968a39b747d15e32363f43b7dffe2622c2\/invenio_migrator\/legacy\/deposit.py#L111-L125"}
{"repo_name":"inveniosoftware\/invenio-migrator","method_name":"_get_recids_invenio12","method_code":"def _get_recids_invenio12(from_date):\n    \"\"\"\"\"\"\n    from invenio.dbquery import run_sql\n    return (id[0] for id in run_sql(\n        'select id_bibrec from '\n        'bibrec_bibdoc as r join bibdoc as d on r.id_bibdoc=d.id '\n        'where d.modification_date >=%s',\n        (from_date, ), run_on_slave=True))","method_summary":"Get BibDocs for Invenio 1.","original_method_code":"def _get_recids_invenio12(from_date):\n    \"\"\"Get BibDocs for Invenio 1.\"\"\"\n    from invenio.dbquery import run_sql\n    return (id[0] for id in run_sql(\n        'select id_bibrec from '\n        'bibrec_bibdoc as r join bibdoc as d on r.id_bibdoc=d.id '\n        'where d.modification_date >=%s',\n        (from_date, ), run_on_slave=True))","method_path":"https:\/\/github.com\/inveniosoftware\/invenio-migrator\/blob\/6902c6968a39b747d15e32363f43b7dffe2622c2\/invenio_migrator\/legacy\/bibdocfile.py#L36-L43"}
{"repo_name":"inveniosoftware\/invenio-migrator","method_name":"_get_recids_invenio2","method_code":"def _get_recids_invenio2(from_date):\n    \"\"\"\"\"\"\n    from invenio.legacy.dbquery import run_sql\n    return (id[0] for id in run_sql(\n        'select id_bibrec from '\n        'bibrec_bibdoc as r join bibdoc as d on r.id_bibdoc=d.id '\n        'where d.modification_date >=%s',\n        (from_date, ), run_on_slave=True))","method_summary":"Get BibDocs for Invenio 2.","original_method_code":"def _get_recids_invenio2(from_date):\n    \"\"\"Get BibDocs for Invenio 2.\"\"\"\n    from invenio.legacy.dbquery import run_sql\n    return (id[0] for id in run_sql(\n        'select id_bibrec from '\n        'bibrec_bibdoc as r join bibdoc as d on r.id_bibdoc=d.id '\n        'where d.modification_date >=%s',\n        (from_date, ), run_on_slave=True))","method_path":"https:\/\/github.com\/inveniosoftware\/invenio-migrator\/blob\/6902c6968a39b747d15e32363f43b7dffe2622c2\/invenio_migrator\/legacy\/bibdocfile.py#L46-L53"}
{"repo_name":"inveniosoftware\/invenio-migrator","method_name":"_import_bibdoc","method_code":"def _import_bibdoc():\n    \"\"\"\"\"\"\n    try:\n        from invenio.bibdocfile import BibRecDocs, BibDoc\n    except ImportError:\n        from invenio.legacy.bibdocfile.api import BibRecDocs, BibDoc\n    return BibRecDocs, BibDoc","method_summary":"Import BibDocFile.","original_method_code":"def _import_bibdoc():\n    \"\"\"Import BibDocFile.\"\"\"\n    try:\n        from invenio.bibdocfile import BibRecDocs, BibDoc\n    except ImportError:\n        from invenio.legacy.bibdocfile.api import BibRecDocs, BibDoc\n    return BibRecDocs, BibDoc","method_path":"https:\/\/github.com\/inveniosoftware\/invenio-migrator\/blob\/6902c6968a39b747d15e32363f43b7dffe2622c2\/invenio_migrator\/legacy\/bibdocfile.py#L64-L70"}
{"repo_name":"inveniosoftware\/invenio-migrator","method_name":"dump_bibdoc","method_code":"def dump_bibdoc(recid, from_date, **kwargs):\n    \"\"\"\"\"\"\n    BibRecDocs, BibDoc = _import_bibdoc()\n\n    bibdocfile_dump = []\n\n    date = datetime.datetime.strptime(from_date, '%Y-%m-%d %H:%M:%S')\n    for bibdoc in BibRecDocs(recid).list_bibdocs():\n        for version in bibdoc.list_versions():\n            bibdoc_version = bibdoc.list_version_files(version)\n            for f in bibdoc_version:\n                if f.is_icon() or f.md < date:\n                    \n                    \n                    continue\n                bibdocfile_dump.append(dict(\n                    bibdocid=f.get_bibdocid(),\n                    checksum=f.get_checksum(),\n                    comment=f.get_comment(),\n                    copyright=(\n                        f.get_copyright() if hasattr(f, 'get_copyright')\n                        else None),\n                    creation_date=datetime_toutc(f.cd).isoformat(),\n                    description=f.get_description(),\n                    encoding=f.encoding,\n                    etag=f.etag,\n                    flags=f.flags,\n                    format=f.get_format(),\n                    full_name=f.get_full_name(),\n                    full_path=f.get_full_path(),\n                    hidden=f.hidden,\n                    license=(\n                        f.get_license()if hasattr(f, 'get_license') else None),\n                    modification_date=datetime_toutc(f.md).isoformat(),\n                    name=f.get_name(),\n                    mime=f.mime,\n                    path=f.get_path(),\n                    recid=f.get_recid(),\n                    recids_doctype=f.recids_doctypes,\n                    size=f.get_size(),\n                    status=f.get_status(),\n                    subformat=f.get_subformat(),\n                    superformat=f.get_superformat(),\n                    type=f.get_type(),\n                    url=f.get_url(),\n                    version=f.get_version(),\n                ))\n\n    return bibdocfile_dump","method_summary":"Dump all BibDoc metadata.","original_method_code":"def dump_bibdoc(recid, from_date, **kwargs):\n    \"\"\"Dump all BibDoc metadata.\n\n    :param docid: BibDoc ID\n    :param from_date: Dump only BibDoc revisions newer than this date.\n\n    :returns: List of version of the BibDoc formatted as a dict\n    \"\"\"\n    BibRecDocs, BibDoc = _import_bibdoc()\n\n    bibdocfile_dump = []\n\n    date = datetime.datetime.strptime(from_date, '%Y-%m-%d %H:%M:%S')\n    for bibdoc in BibRecDocs(recid).list_bibdocs():\n        for version in bibdoc.list_versions():\n            bibdoc_version = bibdoc.list_version_files(version)\n            for f in bibdoc_version:\n                if f.is_icon() or f.md < date:\n                    # Don't care about icons\n                    # Don't care about files not modified since from_date\n                    continue\n                bibdocfile_dump.append(dict(\n                    bibdocid=f.get_bibdocid(),\n                    checksum=f.get_checksum(),\n                    comment=f.get_comment(),\n                    copyright=(\n                        f.get_copyright() if hasattr(f, 'get_copyright')\n                        else None),\n                    creation_date=datetime_toutc(f.cd).isoformat(),\n                    description=f.get_description(),\n                    encoding=f.encoding,\n                    etag=f.etag,\n                    flags=f.flags,\n                    format=f.get_format(),\n                    full_name=f.get_full_name(),\n                    full_path=f.get_full_path(),\n                    hidden=f.hidden,\n                    license=(\n                        f.get_license()if hasattr(f, 'get_license') else None),\n                    modification_date=datetime_toutc(f.md).isoformat(),\n                    name=f.get_name(),\n                    mime=f.mime,\n                    path=f.get_path(),\n                    recid=f.get_recid(),\n                    recids_doctype=f.recids_doctypes,\n                    size=f.get_size(),\n                    status=f.get_status(),\n                    subformat=f.get_subformat(),\n                    superformat=f.get_superformat(),\n                    type=f.get_type(),\n                    url=f.get_url(),\n                    version=f.get_version(),\n                ))\n\n    return bibdocfile_dump","method_path":"https:\/\/github.com\/inveniosoftware\/invenio-migrator\/blob\/6902c6968a39b747d15e32363f43b7dffe2622c2\/invenio_migrator\/legacy\/bibdocfile.py#L73-L127"}
{"repo_name":"inveniosoftware\/invenio-migrator","method_name":"get_check","method_code":"def get_check():\n    \"\"\"\"\"\"\n    try:\n        from invenio.dbquery import run_sql\n    except ImportError:\n        from invenio.legacy.dbquery import run_sql\n\n    return (\n        run_sql('select count(id) from bibdoc', run_on_slave=True)[0][0],\n        [id[0] for id in run_sql('select id from bibdoc', run_on_slave=True)],\n    )","method_summary":"Get bibdocs to check.","original_method_code":"def get_check():\n    \"\"\"Get bibdocs to check.\"\"\"\n    try:\n        from invenio.dbquery import run_sql\n    except ImportError:\n        from invenio.legacy.dbquery import run_sql\n\n    return (\n        run_sql('select count(id) from bibdoc', run_on_slave=True)[0][0],\n        [id[0] for id in run_sql('select id from bibdoc', run_on_slave=True)],\n    )","method_path":"https:\/\/github.com\/inveniosoftware\/invenio-migrator\/blob\/6902c6968a39b747d15e32363f43b7dffe2622c2\/invenio_migrator\/legacy\/bibdocfile.py#L130-L140"}
{"repo_name":"inveniosoftware\/invenio-migrator","method_name":"check","method_code":"def check(id_):\n    \"\"\"\"\"\"\n    BibRecDocs, BibDoc = _import_bibdoc()\n\n    try:\n        BibDoc(id_).list_all_files()\n    except Exception:\n        click.secho(\"BibDoc {0} failed check.\".format(id_), fg='red')","method_summary":"Check bibdocs.","original_method_code":"def check(id_):\n    \"\"\"Check bibdocs.\"\"\"\n    BibRecDocs, BibDoc = _import_bibdoc()\n\n    try:\n        BibDoc(id_).list_all_files()\n    except Exception:\n        click.secho(\"BibDoc {0} failed check.\".format(id_), fg='red')","method_path":"https:\/\/github.com\/inveniosoftware\/invenio-migrator\/blob\/6902c6968a39b747d15e32363f43b7dffe2622c2\/invenio_migrator\/legacy\/bibdocfile.py#L143-L150"}
{"repo_name":"not-na\/peng3d","method_name":"BasicWidget.size","method_code":"def size(self):\n        \"\"\"\"\"\"\n        if isinstance(self._size,list) or isinstance(self._size,tuple):\n            s = self._size\n        elif callable(self._size):\n            w,h = self.submenu.size[:]\n            s = self._size(w,h)\n        else:\n            raise TypeError(\"Invalid size type\")\n        \n        s = s[:]\n        \n        if s[0]==-1:\n            s[0]=self.getMinSize()[0]\n        if s[1]==-1:\n            s[1]=self.getMinSize()[1]\n    \n        \n        s = [max(s[0],0),max(s[1],0)]\n        return _WatchingList(s,self._wlredraw_size)","method_summary":"Similar to :py:attr:`pos` but for the size instead.","original_method_code":"def size(self):\n        \"\"\"\n        Similar to :py:attr:`pos` but for the size instead.\n        \"\"\"\n        if isinstance(self._size,list) or isinstance(self._size,tuple):\n            s = self._size\n        elif callable(self._size):\n            w,h = self.submenu.size[:]\n            s = self._size(w,h)\n        else:\n            raise TypeError(\"Invalid size type\")\n        \n        s = s[:]\n        \n        if s[0]==-1:\n            s[0]=self.getMinSize()[0]\n        if s[1]==-1:\n            s[1]=self.getMinSize()[1]\n    \n        # Prevents crashes with negative size\n        s = [max(s[0],0),max(s[1],0)]\n        return _WatchingList(s,self._wlredraw_size)","method_path":"https:\/\/github.com\/not-na\/peng3d\/blob\/1151be665b26cc8a479f6307086ba919e4d32d85\/peng3d\/gui\/widgets.py#L218-L239"}
{"repo_name":"not-na\/peng3d","method_name":"Widget.on_redraw","method_code":"def on_redraw(self):\n        \"\"\"\"\"\"\n        if self.bg is not None:\n            if not self.bg.initialized:\n                self.bg.init_bg()\n                self.bg.initialized=True\n            self.bg.redraw_bg()\n        super(Widget,self).on_redraw()","method_summary":"Draws the background and the widget itself. Subclasses should use ``super()`` to call this method, or rendering may glitch out.","original_method_code":"def on_redraw(self):\n        \"\"\"\n        Draws the background and the widget itself.\n        \n        Subclasses should use ``super()`` to call this method, or rendering may glitch out.\n        \"\"\"\n        if self.bg is not None:\n            if not self.bg.initialized:\n                self.bg.init_bg()\n                self.bg.initialized=True\n            self.bg.redraw_bg()\n        super(Widget,self).on_redraw()","method_path":"https:\/\/github.com\/not-na\/peng3d\/blob\/1151be665b26cc8a479f6307086ba919e4d32d85\/peng3d\/gui\/widgets.py#L456-L467"}
{"repo_name":"not-na\/peng3d","method_name":"v_magnitude","method_code":"def v_magnitude(v):\n    \"\"\"\"\"\"\n    return math.sqrt(sum(v[i]*v[i] for i in range(len(v))))","method_summary":"Simple vector helper function returning the length of a vector. ``v`` may be any vector, with any number of dimensions","original_method_code":"def v_magnitude(v):\n    \"\"\"\n    Simple vector helper function returning the length of a vector.\n    \n    ``v`` may be any vector, with any number of dimensions\n    \"\"\"\n    return math.sqrt(sum(v[i]*v[i] for i in range(len(v))))","method_path":"https:\/\/github.com\/not-na\/peng3d\/blob\/1151be665b26cc8a479f6307086ba919e4d32d85\/peng3d\/model.py#L135-L141"}
{"repo_name":"not-na\/peng3d","method_name":"v_normalize","method_code":"def v_normalize(v):\n    \"\"\"\"\"\"\n    vmag = v_magnitude(v)\n    return [ v[i]\/vmag  for i in range(len(v)) ]","method_summary":"Normalizes the given vector. The vector given may have any number of dimensions.","original_method_code":"def v_normalize(v):\n    \"\"\"\n    Normalizes the given vector.\n    \n    The vector given may have any number of dimensions.\n    \"\"\"\n    vmag = v_magnitude(v)\n    return [ v[i]\/vmag  for i in range(len(v)) ]","method_path":"https:\/\/github.com\/not-na\/peng3d\/blob\/1151be665b26cc8a479f6307086ba919e4d32d85\/peng3d\/model.py#L143-L150"}
{"repo_name":"not-na\/peng3d","method_name":"Bone.ensureBones","method_code":"def ensureBones(self,data):\n        \"\"\"\"\"\"\n        if \"_bones\" not in data:\n            data[\"_bones\"]={}\n        if self.name not in data[\"_bones\"]:\n            data[\"_bones\"][self.name]={\"rot\":self.start_rot[:],\"length\":self.blength}","method_summary":"Helper method ensuring per-entity bone data has been properly initialized. Should be called at the start of every method accessing per-entity data. ``data`` is the entity to check in dictionary form.","original_method_code":"def ensureBones(self,data):\n        \"\"\"\n        Helper method ensuring per-entity bone data has been properly initialized.\n        \n        Should be called at the start of every method accessing per-entity data.\n        \n        ``data`` is the entity to check in dictionary form.\n        \"\"\"\n        if \"_bones\" not in data:\n            data[\"_bones\"]={}\n        if self.name not in data[\"_bones\"]:\n            data[\"_bones\"][self.name]={\"rot\":self.start_rot[:],\"length\":self.blength}","method_path":"https:\/\/github.com\/not-na\/peng3d\/blob\/1151be665b26cc8a479f6307086ba919e4d32d85\/peng3d\/model.py#L262-L273"}
{"repo_name":"not-na\/peng3d","method_name":"Bone.setLength","method_code":"def setLength(self,data,blength):\n        \"\"\"\"\"\"\n        self.ensureBones(data)\n        data[\"_bones\"][self.name][\"length\"]=blength","method_summary":"Sets the length of this bone on the given entity. ``data`` is the entity to modify in dictionary form. ``blength`` is the new length of the bone.","original_method_code":"def setLength(self,data,blength):\n        \"\"\"\n        Sets the length of this bone on the given entity.\n        \n        ``data`` is the entity to modify in dictionary form.\n        \n        ``blength`` is the new length of the bone.\n        \"\"\"\n        self.ensureBones(data)\n        data[\"_bones\"][self.name][\"length\"]=blength","method_path":"https:\/\/github.com\/not-na\/peng3d\/blob\/1151be665b26cc8a479f6307086ba919e4d32d85\/peng3d\/model.py#L295-L304"}
{"repo_name":"not-na\/peng3d","method_name":"JSONModelGroup.set_state","method_code":"def set_state(self):\n        \"\"\"\"\"\"\n        x,y,z = self.obj.pos\n        glTranslatef(x,y,z)","method_summary":"Sets the state required for this actor. Currently translates the matrix to the position of the actor.","original_method_code":"def set_state(self):\n        \"\"\"\n        Sets the state required for this actor.\n        \n        Currently translates the matrix to the position of the actor.\n        \"\"\"\n        x,y,z = self.obj.pos\n        glTranslatef(x,y,z)","method_path":"https:\/\/github.com\/not-na\/peng3d\/blob\/1151be665b26cc8a479f6307086ba919e4d32d85\/peng3d\/model.py#L791-L798"}
{"repo_name":"not-na\/peng3d","method_name":"JSONModelGroup.unset_state","method_code":"def unset_state(self):\n        \"\"\"\"\"\"\n        x,y,z = self.obj.pos\n        glTranslatef(-x,-y,-z)","method_summary":"Resets the state required for this actor to the default state. Currently resets the matrix to its previous translation.","original_method_code":"def unset_state(self):\n        \"\"\"\n        Resets the state required for this actor to the default state.\n        \n        Currently resets the matrix to its previous translation.\n        \"\"\"\n        x,y,z = self.obj.pos\n        glTranslatef(-x,-y,-z)","method_path":"https:\/\/github.com\/not-na\/peng3d\/blob\/1151be665b26cc8a479f6307086ba919e4d32d85\/peng3d\/model.py#L800-L807"}
{"repo_name":"not-na\/peng3d","method_name":"JSONRegionGroup.set_state","method_code":"def set_state(self):\n        \"\"\"\"\"\"\n        glEnable(self.region.material.target)\n        glBindTexture(self.region.material.target, self.region.material.id)\n        self.region.bone.setRotate(self.data)","method_summary":"Sets the state required for this vertex region. Currently binds and enables the texture of the material of the region.","original_method_code":"def set_state(self):\n        \"\"\"\n        Sets the state required for this vertex region.\n        \n        Currently binds and enables the texture of the material of the region.\n        \"\"\"\n        glEnable(self.region.material.target)\n        glBindTexture(self.region.material.target, self.region.material.id)\n        self.region.bone.setRotate(self.data)","method_path":"https:\/\/github.com\/not-na\/peng3d\/blob\/1151be665b26cc8a479f6307086ba919e4d32d85\/peng3d\/model.py#L834-L842"}
{"repo_name":"not-na\/peng3d","method_name":"JSONRegionGroup.unset_state","method_code":"def unset_state(self):\n        \"\"\"\"\"\"\n        glDisable(self.region.material.target)\n        self.region.bone.unsetRotate(self.data)","method_summary":"Resets the state required for this actor to the default state. Currently only disables the target of the texture of the material, it may still be bound.","original_method_code":"def unset_state(self):\n        \"\"\"\n        Resets the state required for this actor to the default state.\n        \n        Currently only disables the target of the texture of the material, it may still be bound.\n        \"\"\"\n        glDisable(self.region.material.target)\n        self.region.bone.unsetRotate(self.data)","method_path":"https:\/\/github.com\/not-na\/peng3d\/blob\/1151be665b26cc8a479f6307086ba919e4d32d85\/peng3d\/model.py#L844-L851"}
{"repo_name":"trec-kba\/streamcorpus-pipeline","method_name":"force_unicode","method_code":"def force_unicode(raw):\n    ''''''\n    converted = UnicodeDammit(raw, isHTML=True)\n    if not converted.unicode:\n        converted.unicode = unicode(raw, 'utf8', errors='ignore')\n\n    encoding_m = encoding_re.match(converted.unicode)\n    if encoding_m:\n        converted.unicode = \\\n            encoding_m.group('start_xml') + \\\n            encoding_m.group('remainder')\n\n    return converted.unicode","method_summary":"Try really really hard to get a Unicode copy of a string. First try :class:`BeautifulSoup.UnicodeDammit` to try to force to Unicode; if that fails, assume UTF-8 encoding, and ignore all errors.","original_method_code":"def force_unicode(raw):\n    '''Try really really hard to get a Unicode copy of a string.\n\n    First try :class:`BeautifulSoup.UnicodeDammit` to try to force\n    to Unicode; if that fails, assume UTF-8 encoding, and ignore\n    all errors.\n\n    :param str raw: string to coerce\n    :return: Unicode approximation of `raw`\n    :returntype: :class:`unicode`\n\n    '''\n    converted = UnicodeDammit(raw, isHTML=True)\n    if not converted.unicode:\n        converted.unicode = unicode(raw, 'utf8', errors='ignore')\n\n    encoding_m = encoding_re.match(converted.unicode)\n    if encoding_m:\n        converted.unicode = \\\n            encoding_m.group('start_xml') + \\\n            encoding_m.group('remainder')\n\n    return converted.unicode","method_path":"https:\/\/github.com\/trec-kba\/streamcorpus-pipeline\/blob\/8bb82ea1beb83c6b40ed03fa1659df2897c2292a\/streamcorpus_pipeline\/_clean_html.py#L51-L73"}
{"repo_name":"trec-kba\/streamcorpus-pipeline","method_name":"clean_html.is_matching_mime_type","method_code":"def is_matching_mime_type(self, mime_type):\n        ''''''\n        if len(self.include_mime_types) == 0:\n            return True\n        if mime_type is None:\n            return False\n        mime_type = mime_type.lower()\n        \n        \n        return any(mime_type.startswith(mt) for mt in self.include_mime_types)","method_summary":"This implements the MIME-type matching logic for deciding whether to run `make_clean_html`","original_method_code":"def is_matching_mime_type(self, mime_type):\n        '''This implements the MIME-type matching logic for deciding whether\n        to run `make_clean_html`\n\n        '''\n        if len(self.include_mime_types) == 0:\n            return True\n        if mime_type is None:\n            return False\n        mime_type = mime_type.lower()\n        # NB: startswith is necessary here, because encodings are\n        # often appended to HTTP header Content-Type\n        return any(mime_type.startswith(mt) for mt in self.include_mime_types)","method_path":"https:\/\/github.com\/trec-kba\/streamcorpus-pipeline\/blob\/8bb82ea1beb83c6b40ed03fa1659df2897c2292a\/streamcorpus_pipeline\/_clean_html.py#L250-L262"}
{"repo_name":"trec-kba\/streamcorpus-pipeline","method_name":"domain_name_cleanse","method_code":"def domain_name_cleanse(raw_string):\n    ''''''\n    try:\n        parts = urlparse(raw_string)\n        domain = parts.netloc.split(':')[0]\n    except:\n        domain = ''\n    if not domain:\n        domain = raw_string\n    if not domain:\n        return ''\n    domain = re.sub('\\\/', '', domain.strip().lower())\n    return domain","method_summary":"extract a lower-case, no-slashes domain name from a raw string that might be a URL","original_method_code":"def domain_name_cleanse(raw_string):\n    '''extract a lower-case, no-slashes domain name from a raw string\n    that might be a URL\n    '''\n    try:\n        parts = urlparse(raw_string)\n        domain = parts.netloc.split(':')[0]\n    except:\n        domain = ''\n    if not domain:\n        domain = raw_string\n    if not domain:\n        return ''\n    domain = re.sub('\\\/', '', domain.strip().lower())\n    return domain","method_path":"https:\/\/github.com\/trec-kba\/streamcorpus-pipeline\/blob\/8bb82ea1beb83c6b40ed03fa1659df2897c2292a\/streamcorpus_pipeline\/_filters.py#L253-L267"}
{"repo_name":"trec-kba\/streamcorpus-pipeline","method_name":"domain_name_left_cuts","method_code":"def domain_name_left_cuts(domain):\n    ''''''\n    cuts = []\n    if domain:\n        parts = domain.split('.')\n        for i in range(len(parts)):\n            cuts.append( '.'.join(parts[i:]))\n    return cuts","method_summary":"returns a list of strings created by splitting the domain on '.' and successively cutting off the left most portion","original_method_code":"def domain_name_left_cuts(domain):\n    '''returns a list of strings created by splitting the domain on\n    '.' and successively cutting off the left most portion\n    '''\n    cuts = []\n    if domain:\n        parts = domain.split('.')\n        for i in range(len(parts)):\n            cuts.append( '.'.join(parts[i:]))\n    return cuts","method_path":"https:\/\/github.com\/trec-kba\/streamcorpus-pipeline\/blob\/8bb82ea1beb83c6b40ed03fa1659df2897c2292a\/streamcorpus_pipeline\/_filters.py#L269-L278"}
{"repo_name":"trec-kba\/streamcorpus-pipeline","method_name":"keyword_indexer.index","method_code":"def index(self, si):\n        ''''''\n        if not si.body.clean_visible:\n            logger.warn('stream item %s has no clean_visible part, '\n                        'skipping keyword indexing', si.stream_id)\n            return\n\n        \n        \n        hash_counts = defaultdict(int)\n        hash_counts[DOCUMENT_HASH_KEY] = 1\n        hash_kw = defaultdict(int)\n        words = self.collect_words(si)\n        for tok, count in words.iteritems():\n            (tok, tok_hash) = self.make_hash_kw(tok)\n            hash_counts[tok_hash] += count\n            hash_kw[tok] = tok_hash\n\n        \n        if self.hash_docs:\n            (k1, k2) = key_for_stream_item(si)\n            kvps = [((h, k1, k2), n) for (h, n) in hash_counts.iteritems()\n                    if h != DOCUMENT_HASH_KEY]\n            self.client.put(HASH_TF_INDEX_TABLE, *kvps)\n\n        if self.hash_frequencies:\n            kvps = [((h,), 1) for h in hash_counts.iterkeys()]\n            self.client.increment(HASH_FREQUENCY_TABLE, *kvps)\n\n        if self.hash_keywords:\n            kvps = [((h, t), 1) for (t, h) in hash_kw.iteritems()]\n            self.client.increment(HASH_KEYWORD_INDEX_TABLE, *kvps)","method_summary":"Record index records for a single document. Which indexes this creates depends on the parameters to the constructor. This records all of the requested indexes for a single document.","original_method_code":"def index(self, si):\n        '''Record index records for a single document.\n\n        Which indexes this creates depends on the parameters to the\n        constructor.  This records all of the requested indexes for\n        a single document.\n\n        '''\n        if not si.body.clean_visible:\n            logger.warn('stream item %s has no clean_visible part, '\n                        'skipping keyword indexing', si.stream_id)\n            return\n\n        # Count tokens in si.clean_visible\n        # We will recycle hash==0 for \"# of documents\"\n        hash_counts = defaultdict(int)\n        hash_counts[DOCUMENT_HASH_KEY] = 1\n        hash_kw = defaultdict(int)\n        words = self.collect_words(si)\n        for tok, count in words.iteritems():\n            (tok, tok_hash) = self.make_hash_kw(tok)\n            hash_counts[tok_hash] += count\n            hash_kw[tok] = tok_hash\n\n        # Convert this and write it out\n        if self.hash_docs:\n            (k1, k2) = key_for_stream_item(si)\n            kvps = [((h, k1, k2), n) for (h, n) in hash_counts.iteritems()\n                    if h != DOCUMENT_HASH_KEY]\n            self.client.put(HASH_TF_INDEX_TABLE, *kvps)\n\n        if self.hash_frequencies:\n            kvps = [((h,), 1) for h in hash_counts.iterkeys()]\n            self.client.increment(HASH_FREQUENCY_TABLE, *kvps)\n\n        if self.hash_keywords:\n            kvps = [((h, t), 1) for (t, h) in hash_kw.iteritems()]\n            self.client.increment(HASH_KEYWORD_INDEX_TABLE, *kvps)","method_path":"https:\/\/github.com\/trec-kba\/streamcorpus-pipeline\/blob\/8bb82ea1beb83c6b40ed03fa1659df2897c2292a\/streamcorpus_pipeline\/_kvlayer_keyword_search.py#L135-L172"}
{"repo_name":"trec-kba\/streamcorpus-pipeline","method_name":"keyword_indexer.invert_hash","method_code":"def invert_hash(self, tok_hash):\n        ''''''\n        return [tok_encoded.decode('utf8')\n                for (_, tok_encoded) in\n                self.client.scan_keys(HASH_KEYWORD_INDEX_TABLE,\n                                      ((tok_hash,), (tok_hash,)))]","method_summary":"Get strings that correspond to some hash. No string will correspond to :data:`DOCUMENT_HASH_KEY`; use :data:`DOCUMENT_HASH_KEY_REPLACEMENT` instead.","original_method_code":"def invert_hash(self, tok_hash):\n        '''Get strings that correspond to some hash.\n\n        No string will correspond to :data:`DOCUMENT_HASH_KEY`; use\n        :data:`DOCUMENT_HASH_KEY_REPLACEMENT` instead.\n\n        :param int tok_hash: Murmur hash to query\n        :return: list of :class:`unicode` strings\n\n        '''\n        return [tok_encoded.decode('utf8')\n                for (_, tok_encoded) in\n                self.client.scan_keys(HASH_KEYWORD_INDEX_TABLE,\n                                      ((tok_hash,), (tok_hash,)))]","method_path":"https:\/\/github.com\/trec-kba\/streamcorpus-pipeline\/blob\/8bb82ea1beb83c6b40ed03fa1659df2897c2292a\/streamcorpus_pipeline\/_kvlayer_keyword_search.py#L174-L187"}
{"repo_name":"trec-kba\/streamcorpus-pipeline","method_name":"_make_stream_items","method_code":"def _make_stream_items(f):\n    \"\"\"\"\"\"\n    reader = ProtoStreamReader(f)\n    return itertools.ifilter(\n        lambda x: x is not None,\n        itertools.imap(_make_stream_item, reader))","method_summary":"Given a spinn3r feed, produce a sequence of valid StreamItems. Because of goopy Python interactions, you probably need to call this and re-yield its results, as","original_method_code":"def _make_stream_items(f):\n    \"\"\"Given a spinn3r feed, produce a sequence of valid StreamItems.\n\n    Because of goopy Python interactions, you probably need to call\n    this and re-yield its results, as\n\n    >>> with open(filename, 'rb') as f:\n    ...   for si in _make_stream_items(f):\n    ...     yield si\n\n    \"\"\"\n    reader = ProtoStreamReader(f)\n    return itertools.ifilter(\n        lambda x: x is not None,\n        itertools.imap(_make_stream_item, reader))","method_path":"https:\/\/github.com\/trec-kba\/streamcorpus-pipeline\/blob\/8bb82ea1beb83c6b40ed03fa1659df2897c2292a\/streamcorpus_pipeline\/_spinn3r_feed_storage.py#L226-L240"}
{"repo_name":"trec-kba\/streamcorpus-pipeline","method_name":"_make_stream_item","method_code":"def _make_stream_item(entry):\n    \"\"\"\"\"\"\n    \n    if not hasattr(entry, 'permalink_entry'):\n        return None\n    pe = entry.permalink_entry\n\n    \n    si = streamcorpus.make_stream_item(\n        pe.date_found[:-1] + '.0Z',\n        pe.canonical_link.href.encode('utf8'))\n    if not si.stream_time:\n        logger.debug('failed to generate stream_time from {0!r}'\n                     .format(pe.date_found))\n        return None\n    if not si.abs_url:\n        logger.debug('failed to generate abs_url from {0!r}'\n                     .format(pe.canonical_link.href))\n        return None\n\n    \n    si.body = _make_content_item(\n        pe.content,\n        alternate_data=entry.feed_entry.content.data)\n    if not si.body:\n        return None\n    if not si.body.raw:\n        return None\n\n    if pe.content_extract.data:\n        si.other_content['extract'] = _make_content_item(pe.content_extract)\n    si.other_content['title'] = streamcorpus.ContentItem(\n        raw=pe.title.encode('utf8'),\n        media_type=pe.content_extract.mime_type,\n        encoding='UTF-8')\n    si.other_content['feed_entry_title'] = streamcorpus.ContentItem(\n        raw=entry.feed_entry.title.encode('utf8'),\n        media_type=entry.feed_entry.content.mime_type,\n        encoding='UTF-8')\n    if entry.feed_entry.content.data:\n        si.other_content['feed_entry'] = _make_content_item(\n            entry.feed_entry.content)\n    si.source_metadata['lang'] = pe.lang[0].code\n    si.source_metadata['author'] = json.dumps(\n        dict(\n            name=pe.author[0].name,\n            email=pe.author[0].email,\n            link=pe.author[0].link[0].href,\n        )\n    )\n    si.source = entry.source.publisher_type\n    return si","method_summary":"Given a single spinn3r feed entry, produce a single StreamItem.","original_method_code":"def _make_stream_item(entry):\n    \"\"\"Given a single spinn3r feed entry, produce a single StreamItem.\n\n    Returns 'None' if a complete item can't be constructed.\n\n    \"\"\"\n    # get standard metadata, assuming it's present...\n    if not hasattr(entry, 'permalink_entry'):\n        return None\n    pe = entry.permalink_entry\n\n    # ...and create a streamitem...\n    si = streamcorpus.make_stream_item(\n        pe.date_found[:-1] + '.0Z',\n        pe.canonical_link.href.encode('utf8'))\n    if not si.stream_time:\n        logger.debug('failed to generate stream_time from {0!r}'\n                     .format(pe.date_found))\n        return None\n    if not si.abs_url:\n        logger.debug('failed to generate abs_url from {0!r}'\n                     .format(pe.canonical_link.href))\n        return None\n\n    # ...filling in the actual data\n    si.body = _make_content_item(\n        pe.content,\n        alternate_data=entry.feed_entry.content.data)\n    if not si.body:\n        return None\n    if not si.body.raw:\n        return None\n\n    if pe.content_extract.data:\n        si.other_content['extract'] = _make_content_item(pe.content_extract)\n    si.other_content['title'] = streamcorpus.ContentItem(\n        raw=pe.title.encode('utf8'),\n        media_type=pe.content_extract.mime_type,\n        encoding='UTF-8')\n    si.other_content['feed_entry_title'] = streamcorpus.ContentItem(\n        raw=entry.feed_entry.title.encode('utf8'),\n        media_type=entry.feed_entry.content.mime_type,\n        encoding='UTF-8')\n    if entry.feed_entry.content.data:\n        si.other_content['feed_entry'] = _make_content_item(\n            entry.feed_entry.content)\n    si.source_metadata['lang'] = pe.lang[0].code\n    si.source_metadata['author'] = json.dumps(\n        dict(\n            name=pe.author[0].name,\n            email=pe.author[0].email,\n            link=pe.author[0].link[0].href,\n        )\n    )\n    si.source = entry.source.publisher_type\n    return si","method_path":"https:\/\/github.com\/trec-kba\/streamcorpus-pipeline\/blob\/8bb82ea1beb83c6b40ed03fa1659df2897c2292a\/streamcorpus_pipeline\/_spinn3r_feed_storage.py#L243-L298"}
{"repo_name":"trec-kba\/streamcorpus-pipeline","method_name":"ProtoStreamReader._read","method_code":"def _read(self, n):\n        \"\"\"\"\"\"\n        if n <= len(self._prefix):\n            \n            result = self._prefix[:n]\n            self._prefix = self._prefix[n:]\n            return result\n        \n        n -= len(self._prefix)\n        result = self._prefix + self.f.read(n)\n        self._prefix = \"\"\n        return result","method_summary":"Read (up to) 'n' bytes from the underlying file. If any bytes have been pushed in with _unread() those are returned first.","original_method_code":"def _read(self, n):\n        \"\"\"Read (up to) 'n' bytes from the underlying file.  If any bytes\n        have been pushed in with _unread() those are returned first.\"\"\"\n        if n <= len(self._prefix):\n            # the read can be fulfilled entirely from the prefix\n            result = self._prefix[:n]\n            self._prefix = self._prefix[n:]\n            return result\n        # otherwise we need to read some\n        n -= len(self._prefix)\n        result = self._prefix + self.f.read(n)\n        self._prefix = \"\"\n        return result","method_path":"https:\/\/github.com\/trec-kba\/streamcorpus-pipeline\/blob\/8bb82ea1beb83c6b40ed03fa1659df2897c2292a\/streamcorpus_pipeline\/_spinn3r_feed_storage.py#L104-L116"}
{"repo_name":"trec-kba\/streamcorpus-pipeline","method_name":"ProtoStreamReader._read_varint","method_code":"def _read_varint(self):\n        \"\"\"\"\"\"\n        buf = self._read(8)\n        (n, l) = _DecodeVarint(buf, 0)\n        self._unread(buf[l:])\n        return n","method_summary":"Read exactly a varint out of the underlying file.","original_method_code":"def _read_varint(self):\n        \"\"\"Read exactly a varint out of the underlying file.\"\"\"\n        buf = self._read(8)\n        (n, l) = _DecodeVarint(buf, 0)\n        self._unread(buf[l:])\n        return n","method_path":"https:\/\/github.com\/trec-kba\/streamcorpus-pipeline\/blob\/8bb82ea1beb83c6b40ed03fa1659df2897c2292a\/streamcorpus_pipeline\/_spinn3r_feed_storage.py#L118-L123"}
{"repo_name":"xtuml\/pyxtuml","method_name":"main","method_code":"def main():\n    ''''''\n    parser = optparse.OptionParser(usage=\"%prog [options] <model_path> [another_model_path...]\",\n                                   formatter=optparse.TitledHelpFormatter())\n                                   \n    parser.set_description(__doc__.strip())\n    \n    parser.add_option(\"-f\", \"--function\", dest=\"function\", metavar=\"NAME\",\n                      help=\"append integrity checking actions to functions named NAME (required)\",\n                      action=\"store\", default=None)\n    \n    parser.add_option(\"-o\", \"--output\", dest='output', metavar=\"PATH\",\n                      help=\"save sql model instances to PATH (required)\",\n                      action=\"store\", default=None)\n    \n    parser.add_option(\"-v\", \"--verbosity\", dest='verbosity', action=\"count\",\n                      help=\"increase debug logging level\", default=2)\n\n    \n    (opts, args) = parser.parse_args()\n    if len(args) == 0 or None in [opts.output, opts.function]:\n        parser.print_help()\n        sys.exit(1)\n\n    levels = {\n              0: logging.ERROR,\n              1: logging.WARNING,\n              2: logging.INFO,\n              3: logging.DEBUG,\n    }\n    logging.basicConfig(level=levels.get(opts.verbosity, logging.DEBUG))\n\n    m = ooaofooa.load_metamodel(args)\n    for c_c in m.select_many('C_C'):\n\n        filt = lambda sel: ooaofooa.is_contained_in(sel, c_c) and sel.Name == opts.function\n        s_sync = m.select_any('S_SYNC', filt)\n        if not s_sync:\n            s_sync = m.new('S_SYNC', Name=opts.function)\n            pe_pe = m.new('PE_PE')\n            s_dt = m.select_any('S_DT', where(Name='boolean'))\n            \n            relate(pe_pe, s_sync, 8001)\n            relate(s_dt, s_sync, 25)\n\n        generate_actions(m, c_c, s_sync)\n    \n    xtuml.persist_instances(m, opts.output)","method_summary":"Parse argv for options and arguments, and start schema generation.","original_method_code":"def main():\n    '''\n    Parse argv for options and arguments, and start schema generation.\n    '''\n    parser = optparse.OptionParser(usage=\"%prog [options] <model_path> [another_model_path...]\",\n                                   formatter=optparse.TitledHelpFormatter())\n                                   \n    parser.set_description(__doc__.strip())\n    \n    parser.add_option(\"-f\", \"--function\", dest=\"function\", metavar=\"NAME\",\n                      help=\"append integrity checking actions to functions named NAME (required)\",\n                      action=\"store\", default=None)\n    \n    parser.add_option(\"-o\", \"--output\", dest='output', metavar=\"PATH\",\n                      help=\"save sql model instances to PATH (required)\",\n                      action=\"store\", default=None)\n    \n    parser.add_option(\"-v\", \"--verbosity\", dest='verbosity', action=\"count\",\n                      help=\"increase debug logging level\", default=2)\n\n    \n    (opts, args) = parser.parse_args()\n    if len(args) == 0 or None in [opts.output, opts.function]:\n        parser.print_help()\n        sys.exit(1)\n\n    levels = {\n              0: logging.ERROR,\n              1: logging.WARNING,\n              2: logging.INFO,\n              3: logging.DEBUG,\n    }\n    logging.basicConfig(level=levels.get(opts.verbosity, logging.DEBUG))\n\n    m = ooaofooa.load_metamodel(args)\n    for c_c in m.select_many('C_C'):\n\n        filt = lambda sel: ooaofooa.is_contained_in(sel, c_c) and sel.Name == opts.function\n        s_sync = m.select_any('S_SYNC', filt)\n        if not s_sync:\n            s_sync = m.new('S_SYNC', Name=opts.function)\n            pe_pe = m.new('PE_PE')\n            s_dt = m.select_any('S_DT', where(Name='boolean'))\n            \n            relate(pe_pe, s_sync, 8001)\n            relate(s_dt, s_sync, 25)\n\n        generate_actions(m, c_c, s_sync)\n    \n    xtuml.persist_instances(m, opts.output)","method_path":"https:\/\/github.com\/xtuml\/pyxtuml\/blob\/7dd9343b9a0191d1db1887ab9288d0a026608d9a\/examples\/gen_rt_integrity_check.py#L253-L302"}
{"repo_name":"xtuml\/pyxtuml","method_name":"get_defining_component","method_code":"def get_defining_component(pe_pe):\n    ''''''\n    if pe_pe is None:\n        return None\n    \n    if pe_pe.__class__.__name__ != 'PE_PE':\n        pe_pe = xtuml.navigate_one(pe_pe).PE_PE[8001]()\n    \n    \n    ep_pkg = xtuml.navigate_one(pe_pe).EP_PKG[8000]()\n    if ep_pkg:\n        return get_defining_component(ep_pkg)\n    \n    return xtuml.navigate_one(pe_pe).C_C[8003]()","method_summary":"get the C_C in which pe_pe is defined","original_method_code":"def get_defining_component(pe_pe):\n    '''\n    get the C_C in which pe_pe is defined\n    '''\n    if pe_pe is None:\n        return None\n    \n    if pe_pe.__class__.__name__ != 'PE_PE':\n        pe_pe = xtuml.navigate_one(pe_pe).PE_PE[8001]()\n    \n    \n    ep_pkg = xtuml.navigate_one(pe_pe).EP_PKG[8000]()\n    if ep_pkg:\n        return get_defining_component(ep_pkg)\n    \n    return xtuml.navigate_one(pe_pe).C_C[8003]()","method_path":"https:\/\/github.com\/xtuml\/pyxtuml\/blob\/7dd9343b9a0191d1db1887ab9288d0a026608d9a\/bridgepoint\/prebuild.py#L46-L61"}
{"repo_name":"xtuml\/pyxtuml","method_name":"prebuild_action","method_code":"def prebuild_action(instance):\n    ''''''\n    walker_map = {\n        'S_SYNC': FunctionPrebuilder,\n        'S_BRG': BridgePrebuilder,\n        'O_TFR': OperationPrebuilder,\n        'O_DBATTR': DerivedAttributePrebuilder,\n        'SM_ACT': TransitionPrebuilder,\n        'SPR_RO': RequiredOperationPrebuilder,\n        'SPR_RS': RequiredSignalPrebuilder,\n        'SPR_PO': ProvidedOperationPrebuilder,\n        'SPR_PS': ProvidedSignalPrebuilder\n    }\n    metaclass = xtuml.get_metaclass(instance)\n    walker = walker_map[metaclass.kind](metaclass.metamodel, instance)\n    logger.info('processing action %s' % walker.label)\n    \n    root = oal.parse(instance.Action_Semantics_internal)\n    return walker.accept(root)","method_summary":"Transform textual OAL actions of an *instance","original_method_code":"def prebuild_action(instance):\n    '''\n    Transform textual OAL actions of an *instance* to instances in the ooaofooa\n    subsystems Value and Body. The provided *instance* must be an instance of \n    one of the following classes:\n    \n    - S_SYNC\n    - S_BRG\n    - O_TFR\n    - O_DBATTR\n    - SM_ACT\n    - SPR_RO\n    - SPR_RS\n    - SPR_PO\n    - SPR_PS\n    '''\n    walker_map = {\n        'S_SYNC': FunctionPrebuilder,\n        'S_BRG': BridgePrebuilder,\n        'O_TFR': OperationPrebuilder,\n        'O_DBATTR': DerivedAttributePrebuilder,\n        'SM_ACT': TransitionPrebuilder,\n        'SPR_RO': RequiredOperationPrebuilder,\n        'SPR_RS': RequiredSignalPrebuilder,\n        'SPR_PO': ProvidedOperationPrebuilder,\n        'SPR_PS': ProvidedSignalPrebuilder\n    }\n    metaclass = xtuml.get_metaclass(instance)\n    walker = walker_map[metaclass.kind](metaclass.metamodel, instance)\n    logger.info('processing action %s' % walker.label)\n    # walker.visitors.append(xtuml.tools.NodePrintVisitor())\n    root = oal.parse(instance.Action_Semantics_internal)\n    return walker.accept(root)","method_path":"https:\/\/github.com\/xtuml\/pyxtuml\/blob\/7dd9343b9a0191d1db1887ab9288d0a026608d9a\/bridgepoint\/prebuild.py#L1796-L1828"}
{"repo_name":"xtuml\/pyxtuml","method_name":"prebuild_model","method_code":"def prebuild_model(metamodel):\n    ''''''\n    for kind in ['S_SYNC','S_BRG','O_TFR', 'O_DBATTR', 'SM_ACT', 'SPR_RO',\n                 'SPR_RS', 'SPR_PO', 'SPR_PS']:\n        for inst in metamodel.select_many(kind):\n            if inst.Suc_Pars:\n                prebuild_action(inst)","method_summary":"Transform textual OAL actions in a ooaofooa *metamodel","original_method_code":"def prebuild_model(metamodel):\n    '''\n    Transform textual OAL actions in a ooaofooa *metamodel* to instances in the\n    subsystems Value and Body. Instances of the following classes are supported:\n    \n    - S_SYNC\n    - S_BRG\n    - O_TFR\n    - O_DBATTR\n    - SM_ACT\n    - SPR_RO\n    - SPR_RS\n    - SPR_PO\n    - SPR_PS\n    '''\n    for kind in ['S_SYNC','S_BRG','O_TFR', 'O_DBATTR', 'SM_ACT', 'SPR_RO',\n                 'SPR_RS', 'SPR_PO', 'SPR_PS']:\n        for inst in metamodel.select_many(kind):\n            if inst.Suc_Pars:\n                prebuild_action(inst)","method_path":"https:\/\/github.com\/xtuml\/pyxtuml\/blob\/7dd9343b9a0191d1db1887ab9288d0a026608d9a\/bridgepoint\/prebuild.py#L1831-L1850"}
{"repo_name":"xtuml\/pyxtuml","method_name":"main","method_code":"def main():\n    ''''''\n    parser = optparse.OptionParser(usage=\"%prog [options] <model_path> [another_model_path..]\",\n                                   version=xtuml.version.complete_string,\n                                   formatter=optparse.TitledHelpFormatter())\n\n    parser.add_option(\"-v\", \"--verbosity\", dest='verbosity',\n                                           action=\"count\",\n                                           help=\"increase debug logging level\",\n                                           default=1)\n    \n    parser.add_option(\"-o\", \"--output\", dest=\"output\", metavar=\"PATH\",\n                                        help=\"set output to PATH\",\n                                        action=\"store\",\n                                        default=None)\n    \n    (opts, args) = parser.parse_args()\n    if len(args) == 0 or opts.output is None:\n        parser.print_help()\n        sys.exit(1)\n        \n    levels = {\n              0: logging.ERROR,\n              1: logging.WARNING,\n              2: logging.INFO,\n              3: logging.DEBUG,\n    }\n    logging.basicConfig(level=levels.get(opts.verbosity, logging.DEBUG))\n    \n    m = ooaofooa.load_metamodel(args)\n    prebuild_model(m)\n    \n    xtuml.persist_instances(m, opts.output)","method_summary":"Parse command line options and launch the prebuilder.","original_method_code":"def main():\n    '''\n    Parse command line options and launch the prebuilder.\n    '''\n    parser = optparse.OptionParser(usage=\"%prog [options] <model_path> [another_model_path..]\",\n                                   version=xtuml.version.complete_string,\n                                   formatter=optparse.TitledHelpFormatter())\n\n    parser.add_option(\"-v\", \"--verbosity\", dest='verbosity',\n                                           action=\"count\",\n                                           help=\"increase debug logging level\",\n                                           default=1)\n    \n    parser.add_option(\"-o\", \"--output\", dest=\"output\", metavar=\"PATH\",\n                                        help=\"set output to PATH\",\n                                        action=\"store\",\n                                        default=None)\n    \n    (opts, args) = parser.parse_args()\n    if len(args) == 0 or opts.output is None:\n        parser.print_help()\n        sys.exit(1)\n        \n    levels = {\n              0: logging.ERROR,\n              1: logging.WARNING,\n              2: logging.INFO,\n              3: logging.DEBUG,\n    }\n    logging.basicConfig(level=levels.get(opts.verbosity, logging.DEBUG))\n    \n    m = ooaofooa.load_metamodel(args)\n    prebuild_model(m)\n    \n    xtuml.persist_instances(m, opts.output)","method_path":"https:\/\/github.com\/xtuml\/pyxtuml\/blob\/7dd9343b9a0191d1db1887ab9288d0a026608d9a\/bridgepoint\/prebuild.py#L1853-L1887"}
{"repo_name":"xtuml\/pyxtuml","method_name":"SymbolTable.find_symbol","method_code":"def find_symbol(self, name=None, kind=None):\n        ''''''\n        for s in reversed(self.stack):\n                        \n            for symbol_name, handle in s.symbols.items():\n                symbol_kind = handle.__class__.__name__\n                \n                if name == symbol_name and kind == symbol_kind:\n                    return handle\n                \n                elif name is None and kind == handle.__class__.__name__:\n                    return handle\n                \n                elif name == symbol_name and kind is None:\n                    return handle\n            \n            if name is None and kind == s.handle.__class__.__name__:\n                return s.handle","method_summary":"Find a symbol in the symbol table by name, kind, or both.","original_method_code":"def find_symbol(self, name=None, kind=None):\n        '''\n        Find a symbol in the symbol table by name, kind, or both.\n        '''\n        for s in reversed(self.stack):\n                        \n            for symbol_name, handle in s.symbols.items():\n                symbol_kind = handle.__class__.__name__\n                \n                if name == symbol_name and kind == symbol_kind:\n                    return handle\n                \n                elif name is None and kind == handle.__class__.__name__:\n                    return handle\n                \n                elif name == symbol_name and kind is None:\n                    return handle\n            \n            if name is None and kind == s.handle.__class__.__name__:\n                return s.handle","method_path":"https:\/\/github.com\/xtuml\/pyxtuml\/blob\/7dd9343b9a0191d1db1887ab9288d0a026608d9a\/bridgepoint\/prebuild.py#L90-L109"}
{"repo_name":"xtuml\/pyxtuml","method_name":"is_contained_in","method_code":"def is_contained_in(pe_pe, root):\n    ''''''\n    if not pe_pe:\n        return False\n    \n    if type(pe_pe).__name__ != 'PE_PE':\n        pe_pe = one(pe_pe).PE_PE[8001]()\n    \n    ep_pkg = one(pe_pe).EP_PKG[8000]()\n    c_c = one(pe_pe).C_C[8003]()\n    \n    if root in [ep_pkg, c_c]:\n        return True\n    \n    elif is_contained_in(ep_pkg, root):\n        return True\n    \n    elif is_contained_in(c_c, root):\n        return True\n    \n    else:\n        return False","method_summary":"Determine if a PE_PE is contained within a EP_PKG or a C_C.","original_method_code":"def is_contained_in(pe_pe, root):\n    '''\n    Determine if a PE_PE is contained within a EP_PKG or a C_C.\n    '''\n    if not pe_pe:\n        return False\n    \n    if type(pe_pe).__name__ != 'PE_PE':\n        pe_pe = one(pe_pe).PE_PE[8001]()\n    \n    ep_pkg = one(pe_pe).EP_PKG[8000]()\n    c_c = one(pe_pe).C_C[8003]()\n    \n    if root in [ep_pkg, c_c]:\n        return True\n    \n    elif is_contained_in(ep_pkg, root):\n        return True\n    \n    elif is_contained_in(c_c, root):\n        return True\n    \n    else:\n        return False","method_path":"https:\/\/github.com\/xtuml\/pyxtuml\/blob\/7dd9343b9a0191d1db1887ab9288d0a026608d9a\/bridgepoint\/ooaofooa.py#L63-L86"}
{"repo_name":"xtuml\/pyxtuml","method_name":"is_global","method_code":"def is_global(pe_pe):\n    ''''''\n    if type(pe_pe).__name__ != 'PE_PE':\n        pe_pe = one(pe_pe).PE_PE[8001]()\n    \n    if one(pe_pe).C_C[8003]():\n        return False\n    \n    pe_pe = one(pe_pe).EP_PKG[8000].PE_PE[8001]()\n    if not pe_pe:\n        return True\n    \n    return is_global(pe_pe)","method_summary":"Check if a PE_PE is globally defined, i.e. not inside a C_C","original_method_code":"def is_global(pe_pe):\n    '''\n    Check if a PE_PE is globally defined, i.e. not inside a C_C\n    '''\n    if type(pe_pe).__name__ != 'PE_PE':\n        pe_pe = one(pe_pe).PE_PE[8001]()\n    \n    if one(pe_pe).C_C[8003]():\n        return False\n    \n    pe_pe = one(pe_pe).EP_PKG[8000].PE_PE[8001]()\n    if not pe_pe:\n        return True\n    \n    return is_global(pe_pe)","method_path":"https:\/\/github.com\/xtuml\/pyxtuml\/blob\/7dd9343b9a0191d1db1887ab9288d0a026608d9a\/bridgepoint\/ooaofooa.py#L89-L103"}
{"repo_name":"xtuml\/pyxtuml","method_name":"get_defining_component","method_code":"def get_defining_component(pe_pe):\n    ''''''\n    if pe_pe is None:\n        return None\n    \n    if type(pe_pe).__name__ != 'PE_PE':\n        pe_pe = one(pe_pe).PE_PE[8001]()\n    \n    ep_pkg = one(pe_pe).EP_PKG[8000]()\n    if ep_pkg:\n        return get_defining_component(ep_pkg)\n    \n    return one(pe_pe).C_C[8003]()","method_summary":"Get the BridgePoint component (C_C) that defines the packeable element *pe_pe*.","original_method_code":"def get_defining_component(pe_pe):\n    '''\n    Get the BridgePoint component (C_C) that defines the packeable element\n    *pe_pe*.\n    '''\n    if pe_pe is None:\n        return None\n    \n    if type(pe_pe).__name__ != 'PE_PE':\n        pe_pe = one(pe_pe).PE_PE[8001]()\n    \n    ep_pkg = one(pe_pe).EP_PKG[8000]()\n    if ep_pkg:\n        return get_defining_component(ep_pkg)\n    \n    return one(pe_pe).C_C[8003]()","method_path":"https:\/\/github.com\/xtuml\/pyxtuml\/blob\/7dd9343b9a0191d1db1887ab9288d0a026608d9a\/bridgepoint\/ooaofooa.py#L106-L121"}
{"repo_name":"xtuml\/pyxtuml","method_name":"get_attribute_type","method_code":"def get_attribute_type(o_attr):\n    ''''''\n    ref_o_attr = one(o_attr).O_RATTR[106].O_BATTR[113].O_ATTR[106]()\n    if ref_o_attr:\n        return get_attribute_type(ref_o_attr)\n    else:\n        return one(o_attr).S_DT[114]()","method_summary":"Get the base data type (S_DT) associated with a BridgePoint attribute.","original_method_code":"def get_attribute_type(o_attr):\n    '''\n    Get the base data type (S_DT) associated with a BridgePoint attribute.\n    '''\n    ref_o_attr = one(o_attr).O_RATTR[106].O_BATTR[113].O_ATTR[106]()\n    if ref_o_attr:\n        return get_attribute_type(ref_o_attr)\n    else:\n        return one(o_attr).S_DT[114]()","method_path":"https:\/\/github.com\/xtuml\/pyxtuml\/blob\/7dd9343b9a0191d1db1887ab9288d0a026608d9a\/bridgepoint\/ooaofooa.py#L124-L132"}
{"repo_name":"cloud9ers\/gurumate","method_name":"write_file","method_code":"def write_file (filename, contents):\n    \"\"\"\"\"\"\n    contents = \"\\n\".join(contents)\n    if sys.version_info >= (3,):\n        contents = contents.encode(\"utf-8\")\n    f = open(filename, \"wb\")        \n    f.write(contents)\n    f.close()","method_summary":"Create a file with the specified name and write 'contents' (a sequence of strings without line terminators) to it.","original_method_code":"def write_file (filename, contents):\n    \"\"\"Create a file with the specified name and write 'contents' (a\n    sequence of strings without line terminators) to it.\n    \"\"\"\n    contents = \"\\n\".join(contents)\n    if sys.version_info >= (3,):\n        contents = contents.encode(\"utf-8\")\n    f = open(filename, \"wb\")        # always write POSIX-style manifest\n    f.write(contents)\n    f.close()","method_path":"https:\/\/github.com\/cloud9ers\/gurumate\/blob\/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e\/environment\/lib\/python2.7\/site-packages\/distribute-0.6.31-py2.7.egg\/setuptools\/command\/egg_info.py#L381-L390"}
{"repo_name":"cloud9ers\/gurumate","method_name":"egg_info.write_file","method_code":"def write_file(self, what, filename, data):\n        \"\"\"\"\"\"\n        log.info(\"writing %s to %s\", what, filename)\n        if sys.version_info >= (3,):\n            data = data.encode(\"utf-8\")\n        if not self.dry_run:\n            f = open(filename, 'wb')\n            f.write(data)\n            f.close()","method_summary":"Write `data` to `filename` (if not a dry run) after announcing it `what` is used in a log message to identify what is being written to the file.","original_method_code":"def write_file(self, what, filename, data):\n        \"\"\"Write `data` to `filename` (if not a dry run) after announcing it\n\n        `what` is used in a log message to identify what is being written\n        to the file.\n        \"\"\"\n        log.info(\"writing %s to %s\", what, filename)\n        if sys.version_info >= (3,):\n            data = data.encode(\"utf-8\")\n        if not self.dry_run:\n            f = open(filename, 'wb')\n            f.write(data)\n            f.close()","method_path":"https:\/\/github.com\/cloud9ers\/gurumate\/blob\/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e\/environment\/lib\/python2.7\/site-packages\/distribute-0.6.31-py2.7.egg\/setuptools\/command\/egg_info.py#L144-L156"}
{"repo_name":"cloud9ers\/gurumate","method_name":"manifest_maker.write_manifest","method_code":"def write_manifest (self):\n        \"\"\"\"\"\"\n        \n        if sys.version_info >= (3,):\n            files = []\n            for file in self.filelist.files:\n                try:\n                    file.encode(\"utf-8\")\n                except UnicodeEncodeError:\n                    log.warn(\"'%s' not UTF-8 encodable -- skipping\" % file)\n                else:\n                    files.append(file)\n            self.filelist.files = files\n\n        files = self.filelist.files\n        if os.sep!='\/':\n            files = [f.replace(os.sep,'\/') for f in files]\n        self.execute(write_file, (self.manifest, files),\n                     \"writing manifest file '%s'\" % self.manifest)","method_summary":"Write the file list in 'self.filelist' (presumably as filled in by 'add_defaults()' and 'read_template()') to the manifest file named by 'self.manifest'.","original_method_code":"def write_manifest (self):\n        \"\"\"Write the file list in 'self.filelist' (presumably as filled in\n        by 'add_defaults()' and 'read_template()') to the manifest file\n        named by 'self.manifest'.\n        \"\"\"\n        # The manifest must be UTF-8 encodable. See #303.\n        if sys.version_info >= (3,):\n            files = []\n            for file in self.filelist.files:\n                try:\n                    file.encode(\"utf-8\")\n                except UnicodeEncodeError:\n                    log.warn(\"'%s' not UTF-8 encodable -- skipping\" % file)\n                else:\n                    files.append(file)\n            self.filelist.files = files\n\n        files = self.filelist.files\n        if os.sep!='\/':\n            files = [f.replace(os.sep,'\/') for f in files]\n        self.execute(write_file, (self.manifest, files),\n                     \"writing manifest file '%s'\" % self.manifest)","method_path":"https:\/\/github.com\/cloud9ers\/gurumate\/blob\/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e\/environment\/lib\/python2.7\/site-packages\/distribute-0.6.31-py2.7.egg\/setuptools\/command\/egg_info.py#L333-L354"}
{"repo_name":"cloud9ers\/gurumate","method_name":"BracketMatcher._find_match","method_code":"def _find_match(self, position):\n        \"\"\"\"\"\"\n        \n        document = self._text_edit.document()\n        start_char = document.characterAt(position)\n        search_char = self._opening_map.get(start_char)\n        if search_char:\n            increment = 1\n        else:\n            search_char = self._closing_map.get(start_char)\n            if search_char:\n                increment = -1\n            else:\n                return -1\n\n        \n        char = start_char\n        depth = 0\n        while position >= 0 and position < document.characterCount():\n            if char == start_char:\n                depth += 1\n            elif char == search_char:\n                depth -= 1\n            if depth == 0:\n                break\n            position += increment\n            char = document.characterAt(position)\n        else:\n            position = -1\n        return position","method_summary":"Given a valid position in the text document, try to find the position of the matching bracket.","original_method_code":"def _find_match(self, position):\n        \"\"\" Given a valid position in the text document, try to find the\n            position of the matching bracket. Returns -1 if unsuccessful.\n        \"\"\"\n        # Decide what character to search for and what direction to search in.\n        document = self._text_edit.document()\n        start_char = document.characterAt(position)\n        search_char = self._opening_map.get(start_char)\n        if search_char:\n            increment = 1\n        else:\n            search_char = self._closing_map.get(start_char)\n            if search_char:\n                increment = -1\n            else:\n                return -1\n\n        # Search for the character.\n        char = start_char\n        depth = 0\n        while position >= 0 and position < document.characterCount():\n            if char == start_char:\n                depth += 1\n            elif char == search_char:\n                depth -= 1\n            if depth == 0:\n                break\n            position += increment\n            char = document.characterAt(position)\n        else:\n            position = -1\n        return position","method_path":"https:\/\/github.com\/cloud9ers\/gurumate\/blob\/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e\/environment\/lib\/python2.7\/site-packages\/IPython\/frontend\/qt\/console\/bracket_matcher.py#L39-L70"}
{"repo_name":"cloud9ers\/gurumate","method_name":"BracketMatcher._selection_for_character","method_code":"def _selection_for_character(self, position):\n        \"\"\"\"\"\"\n        selection = QtGui.QTextEdit.ExtraSelection()\n        cursor = self._text_edit.textCursor()\n        cursor.setPosition(position)\n        cursor.movePosition(QtGui.QTextCursor.NextCharacter,\n                            QtGui.QTextCursor.KeepAnchor)\n        selection.cursor = cursor\n        selection.format = self.format\n        return selection","method_summary":"Convenience method for selecting a character.","original_method_code":"def _selection_for_character(self, position):\n        \"\"\" Convenience method for selecting a character.\n        \"\"\"\n        selection = QtGui.QTextEdit.ExtraSelection()\n        cursor = self._text_edit.textCursor()\n        cursor.setPosition(position)\n        cursor.movePosition(QtGui.QTextCursor.NextCharacter,\n                            QtGui.QTextCursor.KeepAnchor)\n        selection.cursor = cursor\n        selection.format = self.format\n        return selection","method_path":"https:\/\/github.com\/cloud9ers\/gurumate\/blob\/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e\/environment\/lib\/python2.7\/site-packages\/IPython\/frontend\/qt\/console\/bracket_matcher.py#L72-L82"}
{"repo_name":"cloud9ers\/gurumate","method_name":"BracketMatcher._cursor_position_changed","method_code":"def _cursor_position_changed(self):\n        \"\"\"\"\"\"\n        \n        self._text_edit.setExtraSelections([])\n\n        \n        cursor = self._text_edit.textCursor()\n        if not cursor.hasSelection():\n            position = cursor.position() - 1\n            match_position = self._find_match(position)\n            if match_position != -1:\n                extra_selections = [ self._selection_for_character(pos)\n                                     for pos in (position, match_position) ]\n                self._text_edit.setExtraSelections(extra_selections)","method_summary":"Updates the document formatting based on the new cursor position.","original_method_code":"def _cursor_position_changed(self):\n        \"\"\" Updates the document formatting based on the new cursor position.\n        \"\"\"\n        # Clear out the old formatting.\n        self._text_edit.setExtraSelections([])\n\n        # Attempt to match a bracket for the new cursor position.\n        cursor = self._text_edit.textCursor()\n        if not cursor.hasSelection():\n            position = cursor.position() - 1\n            match_position = self._find_match(position)\n            if match_position != -1:\n                extra_selections = [ self._selection_for_character(pos)\n                                     for pos in (position, match_position) ]\n                self._text_edit.setExtraSelections(extra_selections)","method_path":"https:\/\/github.com\/cloud9ers\/gurumate\/blob\/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e\/environment\/lib\/python2.7\/site-packages\/IPython\/frontend\/qt\/console\/bracket_matcher.py#L86-L100"}
{"repo_name":"cloud9ers\/gurumate","method_name":"ContextSuite._exc_info","method_code":"def _exc_info(self):\n        \"\"\"\"\"\"\n        e = self.exc_info()\n        if sys.platform == 'cli':\n            if isinstance(e[0], StringException):\n                \n                \n                \n                e = (str(e[0]), e[1], e[2])\n\n        return e","method_summary":"Bottleneck to fix up IronPython string exceptions","original_method_code":"def _exc_info(self):\n        \"\"\"Bottleneck to fix up IronPython string exceptions\n        \"\"\"\n        e = self.exc_info()\n        if sys.platform == 'cli':\n            if isinstance(e[0], StringException):\n                # IronPython throws these StringExceptions, but\n                # traceback checks type(etype) == str. Make a real\n                # string here.\n                e = (str(e[0]), e[1], e[2])\n\n        return e","method_path":"https:\/\/github.com\/cloud9ers\/gurumate\/blob\/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e\/environment\/lib\/python2.7\/site-packages\/nose\/suite.py#L183-L194"}
{"repo_name":"cloud9ers\/gurumate","method_name":"ContextSuite.run","method_code":"def run(self, result):\n        \"\"\"\"\"\"\n        \n        log.debug(\"suite %s (%s) run called, tests: %s\", id(self), self, self._tests)\n        \n        \n        if self.resultProxy:\n            result, orig = self.resultProxy(result, self), result\n        else:\n            result, orig = result, result\n        try:\n            self.setUp()\n        except KeyboardInterrupt:\n            raise\n        except:\n            self.error_context = 'setup'\n            result.addError(self, self._exc_info())\n            return\n        try:\n            for test in self._tests:\n                if result.shouldStop:\n                    log.debug(\"stopping\")\n                    break\n                \n                \n                \n                test(orig)\n        finally:\n            self.has_run = True\n            try:\n                self.tearDown()\n            except KeyboardInterrupt:\n                raise\n            except:\n                self.error_context = 'teardown'\n                result.addError(self, self._exc_info())","method_summary":"Run tests in suite inside of suite fixtures.","original_method_code":"def run(self, result):\n        \"\"\"Run tests in suite inside of suite fixtures.\n        \"\"\"\n        # proxy the result for myself\n        log.debug(\"suite %s (%s) run called, tests: %s\", id(self), self, self._tests)\n        #import pdb\n        #pdb.set_trace()\n        if self.resultProxy:\n            result, orig = self.resultProxy(result, self), result\n        else:\n            result, orig = result, result\n        try:\n            self.setUp()\n        except KeyboardInterrupt:\n            raise\n        except:\n            self.error_context = 'setup'\n            result.addError(self, self._exc_info())\n            return\n        try:\n            for test in self._tests:\n                if result.shouldStop:\n                    log.debug(\"stopping\")\n                    break\n                # each nose.case.Test will create its own result proxy\n                # so the cases need the original result, to avoid proxy\n                # chains\n                test(orig)\n        finally:\n            self.has_run = True\n            try:\n                self.tearDown()\n            except KeyboardInterrupt:\n                raise\n            except:\n                self.error_context = 'teardown'\n                result.addError(self, self._exc_info())","method_path":"https:\/\/github.com\/cloud9ers\/gurumate\/blob\/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e\/environment\/lib\/python2.7\/site-packages\/nose\/suite.py#L196-L232"}
{"repo_name":"cloud9ers\/gurumate","method_name":"ContextSuiteFactory.ancestry","method_code":"def ancestry(self, context):\n        \"\"\"\"\"\"\n        log.debug(\"get ancestry %s\", context)\n        if context is None:\n            return\n        \n        \n        \n        if hasattr(context, 'im_class'):\n            context = context.im_class\n        elif hasattr(context, '__self__'):\n            context = context.__self__.__class__\n        if hasattr(context, '__module__'):\n            ancestors = context.__module__.split('.')\n        elif hasattr(context, '__name__'):\n            ancestors = context.__name__.split('.')[:-1]\n        else:\n            raise TypeError(\"%s has no ancestors?\" % context)\n        while ancestors:\n            log.debug(\" %s ancestors %s\", context, ancestors)\n            yield resolve_name('.'.join(ancestors))\n            ancestors.pop()","method_summary":"Return the ancestry of the context (that is, all of the packages and modules containing the context), in order of descent with the outermost ancestor last. This method is a generator.","original_method_code":"def ancestry(self, context):\n        \"\"\"Return the ancestry of the context (that is, all of the\n        packages and modules containing the context), in order of\n        descent with the outermost ancestor last.\n        This method is a generator.\n        \"\"\"\n        log.debug(\"get ancestry %s\", context)\n        if context is None:\n            return\n        # Methods include reference to module they are defined in, we\n        # don't want that, instead want the module the class is in now\n        # (classes are re-ancestored elsewhere).\n        if hasattr(context, 'im_class'):\n            context = context.im_class\n        elif hasattr(context, '__self__'):\n            context = context.__self__.__class__\n        if hasattr(context, '__module__'):\n            ancestors = context.__module__.split('.')\n        elif hasattr(context, '__name__'):\n            ancestors = context.__name__.split('.')[:-1]\n        else:\n            raise TypeError(\"%s has no ancestors?\" % context)\n        while ancestors:\n            log.debug(\" %s ancestors %s\", context, ancestors)\n            yield resolve_name('.'.join(ancestors))\n            ancestors.pop()","method_path":"https:\/\/github.com\/cloud9ers\/gurumate\/blob\/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e\/environment\/lib\/python2.7\/site-packages\/nose\/suite.py#L428-L453"}
{"repo_name":"cloud9ers\/gurumate","method_name":"ContextSuiteFactory.mixedSuites","method_code":"def mixedSuites(self, tests):\n        \"\"\"\"\"\"\n        if not tests:\n            return []\n        head = tests.pop(0)\n        if not tests:\n            return [head] \n        suite = head \n        tail = tests[:]\n        context = getattr(head, 'context', None)\n        if context is not None:\n            ancestors = [context] + [a for a in self.ancestry(context)]\n            for ancestor in ancestors:\n                common = [suite] \n                remain = [] \n                for test in tail:\n                    found_common = False\n                    test_ctx = getattr(test, 'context', None)\n                    if test_ctx is None:\n                        remain.append(test)\n                        continue\n                    if test_ctx is ancestor:\n                        common.append(test)\n                        continue\n                    for test_ancestor in self.ancestry(test_ctx):\n                        if test_ancestor is ancestor:\n                            common.append(test)\n                            found_common = True\n                            break\n                    if not found_common:\n                        remain.append(test)\n                if common:\n                    suite = self.makeSuite(common, ancestor)\n                tail = self.mixedSuites(remain)\n        return [suite] + tail","method_summary":"The complex case where there are tests that don't all share the same context. Groups tests into suites with common ancestors, according to the following (essentially tail-recursive)","original_method_code":"def mixedSuites(self, tests):\n        \"\"\"The complex case where there are tests that don't all share\n        the same context. Groups tests into suites with common ancestors,\n        according to the following (essentially tail-recursive) procedure:\n\n        Starting with the context of the first test, if it is not\n        None, look for tests in the remaining tests that share that\n        ancestor. If any are found, group into a suite with that\n        ancestor as the context, and replace the current suite with\n        that suite. Continue this process for each ancestor of the\n        first test, until all ancestors have been processed. At this\n        point if any tests remain, recurse with those tests as the\n        input, returning a list of the common suite (which may be the\n        suite or test we started with, if no common tests were found)\n        plus the results of recursion.\n        \"\"\"\n        if not tests:\n            return []\n        head = tests.pop(0)\n        if not tests:\n            return [head] # short circuit when none are left to combine\n        suite = head # the common ancestry suite, so far\n        tail = tests[:]\n        context = getattr(head, 'context', None)\n        if context is not None:\n            ancestors = [context] + [a for a in self.ancestry(context)]\n            for ancestor in ancestors:\n                common = [suite] # tests with ancestor in common, so far\n                remain = [] # tests that remain to be processed\n                for test in tail:\n                    found_common = False\n                    test_ctx = getattr(test, 'context', None)\n                    if test_ctx is None:\n                        remain.append(test)\n                        continue\n                    if test_ctx is ancestor:\n                        common.append(test)\n                        continue\n                    for test_ancestor in self.ancestry(test_ctx):\n                        if test_ancestor is ancestor:\n                            common.append(test)\n                            found_common = True\n                            break\n                    if not found_common:\n                        remain.append(test)\n                if common:\n                    suite = self.makeSuite(common, ancestor)\n                tail = self.mixedSuites(remain)\n        return [suite] + tail","method_path":"https:\/\/github.com\/cloud9ers\/gurumate\/blob\/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e\/environment\/lib\/python2.7\/site-packages\/nose\/suite.py#L487-L535"}
{"repo_name":"tnkteja\/myhelp","method_name":"RequirementSet.prepare_files","method_code":"def prepare_files(self, finder):\n        \"\"\"\"\"\"\n        from pip.index import Link\n\n        unnamed = list(self.unnamed_requirements)\n        reqs = list(self.requirements.values())\n        while reqs or unnamed:\n            if unnamed:\n                req_to_install = unnamed.pop(0)\n            else:\n                req_to_install = reqs.pop(0)\n            install = True\n            best_installed = False\n            not_found = None\n\n            \n            \n            \n\n            if not self.ignore_installed and not req_to_install.editable:\n                req_to_install.check_if_exists()\n                if req_to_install.satisfied_by:\n                    if self.upgrade:\n                        if not self.force_reinstall and not req_to_install.url:\n                            try:\n                                url = finder.find_requirement(\n                                    req_to_install, self.upgrade)\n                            except BestVersionAlreadyInstalled:\n                                best_installed = True\n                                install = False\n                            except DistributionNotFound as exc:\n                                not_found = exc\n                            else:\n                                \n                                req_to_install.url = url.url\n\n                        if not best_installed:\n                            \n                            \n                            if not (self.use_user_site\n                                    and not dist_in_usersite(\n                                        req_to_install.satisfied_by\n                                    )):\n                                req_to_install.conflicts_with = \\\n                                    req_to_install.satisfied_by\n                            req_to_install.satisfied_by = None\n                    else:\n                        install = False\n                if req_to_install.satisfied_by:\n                    if best_installed:\n                        logger.info(\n                            'Requirement already up-to-date: %s',\n                            req_to_install,\n                        )\n                    else:\n                        logger.info(\n                            'Requirement already satisfied (use --upgrade to '\n                            'upgrade): %s',\n                            req_to_install,\n                        )\n            if req_to_install.editable:\n                logger.info('Obtaining %s', req_to_install)\n            elif install:\n                if (req_to_install.url\n                        and req_to_install.url.lower().startswith('file:')):\n                    path = url_to_path(req_to_install.url)\n                    logger.info('Processing %s', display_path(path))\n                else:\n                    logger.info('Collecting %s', req_to_install)\n\n            with indent_log():\n                \n                \n                \n\n                is_wheel = False\n                if req_to_install.editable:\n                    if req_to_install.source_dir is None:\n                        location = req_to_install.build_location(self.src_dir)\n                        req_to_install.source_dir = location\n                    else:\n                        location = req_to_install.source_dir\n                    if not os.path.exists(self.build_dir):\n                        _make_build_dir(self.build_dir)\n                    req_to_install.update_editable(not self.is_download)\n                    if self.is_download:\n                        req_to_install.run_egg_info()\n                        req_to_install.archive(self.download_dir)\n                    else:\n                        req_to_install.run_egg_info()\n                elif install:\n                    \n                    \n                    \n                    \n\n                    \n                    \n                    location = req_to_install.build_location(\n                        self.build_dir,\n                    )\n                    unpack = True\n                    url = None\n\n                    \n                    \n                    \n                    if os.path.exists(os.path.join(location, 'setup.py')):\n                        raise PreviousBuildDirError(\n                            \"pip can't proceed with requirements '%s' due to a\"\n                            \" pre-existing build directory (%s). This is \"\n                            \"likely due to a previous installation that failed\"\n                            \". pip is being responsible and not assuming it \"\n                            \"can delete this. Please delete it and try again.\"\n                            % (req_to_install, location)\n                        )\n                    else:\n                        \n                        \n                        if req_to_install.url is None:\n                            if not_found:\n                                raise not_found\n                            url = finder.find_requirement(\n                                req_to_install,\n                                upgrade=self.upgrade,\n                            )\n                        else:\n                            \n                            \n                            url = Link(req_to_install.url)\n                            assert url\n                        if url:\n                            try:\n\n                                if (\n                                    url.filename.endswith(wheel_ext)\n                                    and self.wheel_download_dir\n                                ):\n                                    \n                                    download_dir = self.wheel_download_dir\n                                    do_download = True\n                                else:\n                                    download_dir = self.download_dir\n                                    do_download = self.is_download\n                                unpack_url(\n                                    url, location, download_dir,\n                                    do_download, session=self.session,\n                                )\n                            except requests.HTTPError as exc:\n                                logger.critical(\n                                    'Could not install requirement %s because '\n                                    'of error %s',\n                                    req_to_install,\n                                    exc,\n                                )\n                                raise InstallationError(\n                                    'Could not install requirement %s because '\n                                    'of HTTP error %s for URL %s' %\n                                    (req_to_install, exc, url)\n                                )\n                        else:\n                            unpack = False\n                    if unpack:\n                        is_wheel = url and url.filename.endswith(wheel_ext)\n                        if self.is_download:\n                            req_to_install.source_dir = location\n                            if not is_wheel:\n                                \n                                req_to_install.run_egg_info()\n                            if url and url.scheme in vcs.all_schemes:\n                                req_to_install.archive(self.download_dir)\n                        elif is_wheel:\n                            req_to_install.source_dir = location\n                            req_to_install.url = url.url\n                        else:\n                            req_to_install.source_dir = location\n                            req_to_install.run_egg_info()\n                            req_to_install.assert_source_matches_version()\n                        \n                        \n                        \n                        if not self.ignore_installed:\n                            req_to_install.check_if_exists()\n                        if req_to_install.satisfied_by:\n                            if self.upgrade or self.ignore_installed:\n                                \n                                \n                                if not (self.use_user_site\n                                        and not dist_in_usersite(\n                                            req_to_install.satisfied_by)):\n                                    req_to_install.conflicts_with = \\\n                                        req_to_install.satisfied_by\n                                req_to_install.satisfied_by = None\n                            else:\n                                logger.info(\n                                    'Requirement already satisfied (use '\n                                    '--upgrade to upgrade): %s',\n                                    req_to_install,\n                                )\n                                install = False\n\n                \n                \n                \n                if (req_to_install.extras):\n                    logger.debug(\n                        \"Installing extra requirements: %r\",\n                        ','.join(req_to_install.extras),\n                    )\n\n                if is_wheel:\n                    dist = list(\n                        pkg_resources.find_distributions(location)\n                    )[0]\n                else:  \n                    if req_to_install.satisfied_by:\n                        dist = req_to_install.satisfied_by\n                    else:\n                        dist = req_to_install.get_dist()\n                    \n                    if dist.has_metadata('dependency_links.txt'):\n                        finder.add_dependency_links(\n                            dist.get_metadata_lines('dependency_links.txt')\n                        )\n\n                if not self.ignore_dependencies:\n                    for subreq in dist.requires(\n                            req_to_install.extras):\n                        if self.has_requirement(\n                                subreq.project_name):\n                            \n                            continue\n                        subreq = InstallRequirement(\n                            str(subreq),\n                            req_to_install,\n                            isolated=self.isolated,\n                        )\n                        reqs.append(subreq)\n                        self.add_requirement(subreq)\n\n                if not self.has_requirement(req_to_install.name):\n                    \n                    self.add_requirement(req_to_install)\n\n                \n                if (self.is_download or\n                        req_to_install._temp_build_dir is not None):\n                    self.reqs_to_cleanup.append(req_to_install)\n\n                if install:\n                    self.successfully_downloaded.append(req_to_install)","method_summary":"Prepare process. Create temp directories, download and\/or unpack files.","original_method_code":"def prepare_files(self, finder):\n        \"\"\"\n        Prepare process. Create temp directories, download and\/or unpack files.\n        \"\"\"\n        from pip.index import Link\n\n        unnamed = list(self.unnamed_requirements)\n        reqs = list(self.requirements.values())\n        while reqs or unnamed:\n            if unnamed:\n                req_to_install = unnamed.pop(0)\n            else:\n                req_to_install = reqs.pop(0)\n            install = True\n            best_installed = False\n            not_found = None\n\n            # ############################################# #\n            # # Search for archive to fulfill requirement # #\n            # ############################################# #\n\n            if not self.ignore_installed and not req_to_install.editable:\n                req_to_install.check_if_exists()\n                if req_to_install.satisfied_by:\n                    if self.upgrade:\n                        if not self.force_reinstall and not req_to_install.url:\n                            try:\n                                url = finder.find_requirement(\n                                    req_to_install, self.upgrade)\n                            except BestVersionAlreadyInstalled:\n                                best_installed = True\n                                install = False\n                            except DistributionNotFound as exc:\n                                not_found = exc\n                            else:\n                                # Avoid the need to call find_requirement again\n                                req_to_install.url = url.url\n\n                        if not best_installed:\n                            # don't uninstall conflict if user install and\n                            # conflict is not user install\n                            if not (self.use_user_site\n                                    and not dist_in_usersite(\n                                        req_to_install.satisfied_by\n                                    )):\n                                req_to_install.conflicts_with = \\\n                                    req_to_install.satisfied_by\n                            req_to_install.satisfied_by = None\n                    else:\n                        install = False\n                if req_to_install.satisfied_by:\n                    if best_installed:\n                        logger.info(\n                            'Requirement already up-to-date: %s',\n                            req_to_install,\n                        )\n                    else:\n                        logger.info(\n                            'Requirement already satisfied (use --upgrade to '\n                            'upgrade): %s',\n                            req_to_install,\n                        )\n            if req_to_install.editable:\n                logger.info('Obtaining %s', req_to_install)\n            elif install:\n                if (req_to_install.url\n                        and req_to_install.url.lower().startswith('file:')):\n                    path = url_to_path(req_to_install.url)\n                    logger.info('Processing %s', display_path(path))\n                else:\n                    logger.info('Collecting %s', req_to_install)\n\n            with indent_log():\n                # ################################ #\n                # # vcs update or unpack archive # #\n                # ################################ #\n\n                is_wheel = False\n                if req_to_install.editable:\n                    if req_to_install.source_dir is None:\n                        location = req_to_install.build_location(self.src_dir)\n                        req_to_install.source_dir = location\n                    else:\n                        location = req_to_install.source_dir\n                    if not os.path.exists(self.build_dir):\n                        _make_build_dir(self.build_dir)\n                    req_to_install.update_editable(not self.is_download)\n                    if self.is_download:\n                        req_to_install.run_egg_info()\n                        req_to_install.archive(self.download_dir)\n                    else:\n                        req_to_install.run_egg_info()\n                elif install:\n                    # @@ if filesystem packages are not marked\n                    # editable in a req, a non deterministic error\n                    # occurs when the script attempts to unpack the\n                    # build directory\n\n                    # NB: This call can result in the creation of a temporary\n                    # build directory\n                    location = req_to_install.build_location(\n                        self.build_dir,\n                    )\n                    unpack = True\n                    url = None\n\n                    # If a checkout exists, it's unwise to keep going.  version\n                    # inconsistencies are logged later, but do not fail the\n                    # installation.\n                    if os.path.exists(os.path.join(location, 'setup.py')):\n                        raise PreviousBuildDirError(\n                            \"pip can't proceed with requirements '%s' due to a\"\n                            \" pre-existing build directory (%s). This is \"\n                            \"likely due to a previous installation that failed\"\n                            \". pip is being responsible and not assuming it \"\n                            \"can delete this. Please delete it and try again.\"\n                            % (req_to_install, location)\n                        )\n                    else:\n                        # FIXME: this won't upgrade when there's an existing\n                        # package unpacked in `location`\n                        if req_to_install.url is None:\n                            if not_found:\n                                raise not_found\n                            url = finder.find_requirement(\n                                req_to_install,\n                                upgrade=self.upgrade,\n                            )\n                        else:\n                            # FIXME: should req_to_install.url already be a\n                            # link?\n                            url = Link(req_to_install.url)\n                            assert url\n                        if url:\n                            try:\n\n                                if (\n                                    url.filename.endswith(wheel_ext)\n                                    and self.wheel_download_dir\n                                ):\n                                    # when doing 'pip wheel`\n                                    download_dir = self.wheel_download_dir\n                                    do_download = True\n                                else:\n                                    download_dir = self.download_dir\n                                    do_download = self.is_download\n                                unpack_url(\n                                    url, location, download_dir,\n                                    do_download, session=self.session,\n                                )\n                            except requests.HTTPError as exc:\n                                logger.critical(\n                                    'Could not install requirement %s because '\n                                    'of error %s',\n                                    req_to_install,\n                                    exc,\n                                )\n                                raise InstallationError(\n                                    'Could not install requirement %s because '\n                                    'of HTTP error %s for URL %s' %\n                                    (req_to_install, exc, url)\n                                )\n                        else:\n                            unpack = False\n                    if unpack:\n                        is_wheel = url and url.filename.endswith(wheel_ext)\n                        if self.is_download:\n                            req_to_install.source_dir = location\n                            if not is_wheel:\n                                # FIXME:https:\/\/github.com\/pypa\/pip\/issues\/1112\n                                req_to_install.run_egg_info()\n                            if url and url.scheme in vcs.all_schemes:\n                                req_to_install.archive(self.download_dir)\n                        elif is_wheel:\n                            req_to_install.source_dir = location\n                            req_to_install.url = url.url\n                        else:\n                            req_to_install.source_dir = location\n                            req_to_install.run_egg_info()\n                            req_to_install.assert_source_matches_version()\n                        # req_to_install.req is only avail after unpack for URL\n                        # pkgs repeat check_if_exists to uninstall-on-upgrade\n                        # (#14)\n                        if not self.ignore_installed:\n                            req_to_install.check_if_exists()\n                        if req_to_install.satisfied_by:\n                            if self.upgrade or self.ignore_installed:\n                                # don't uninstall conflict if user install and\n                                # conflict is not user install\n                                if not (self.use_user_site\n                                        and not dist_in_usersite(\n                                            req_to_install.satisfied_by)):\n                                    req_to_install.conflicts_with = \\\n                                        req_to_install.satisfied_by\n                                req_to_install.satisfied_by = None\n                            else:\n                                logger.info(\n                                    'Requirement already satisfied (use '\n                                    '--upgrade to upgrade): %s',\n                                    req_to_install,\n                                )\n                                install = False\n\n                # ###################### #\n                # # parse dependencies # #\n                # ###################### #\n                if (req_to_install.extras):\n                    logger.debug(\n                        \"Installing extra requirements: %r\",\n                        ','.join(req_to_install.extras),\n                    )\n\n                if is_wheel:\n                    dist = list(\n                        pkg_resources.find_distributions(location)\n                    )[0]\n                else:  # sdists\n                    if req_to_install.satisfied_by:\n                        dist = req_to_install.satisfied_by\n                    else:\n                        dist = req_to_install.get_dist()\n                    # FIXME: shouldn't be globally added:\n                    if dist.has_metadata('dependency_links.txt'):\n                        finder.add_dependency_links(\n                            dist.get_metadata_lines('dependency_links.txt')\n                        )\n\n                if not self.ignore_dependencies:\n                    for subreq in dist.requires(\n                            req_to_install.extras):\n                        if self.has_requirement(\n                                subreq.project_name):\n                            # FIXME: check for conflict\n                            continue\n                        subreq = InstallRequirement(\n                            str(subreq),\n                            req_to_install,\n                            isolated=self.isolated,\n                        )\n                        reqs.append(subreq)\n                        self.add_requirement(subreq)\n\n                if not self.has_requirement(req_to_install.name):\n                    # 'unnamed' requirements will get added here\n                    self.add_requirement(req_to_install)\n\n                # cleanup tmp src\n                if (self.is_download or\n                        req_to_install._temp_build_dir is not None):\n                    self.reqs_to_cleanup.append(req_to_install)\n\n                if install:\n                    self.successfully_downloaded.append(req_to_install)","method_path":"https:\/\/github.com\/tnkteja\/myhelp\/blob\/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb\/virtualEnvironment\/lib\/python2.7\/site-packages\/pip\/req\/req_set.py#L207-L459"}
{"repo_name":"tnkteja\/myhelp","method_name":"RequirementSet.cleanup_files","method_code":"def cleanup_files(self):\n        \"\"\"\"\"\"\n        logger.debug('Cleaning up...')\n        with indent_log():\n            for req in self.reqs_to_cleanup:\n                req.remove_temporary_source()\n\n            if self._pip_has_created_build_dir():\n                logger.debug('Removing temporary dir %s...', self.build_dir)\n                rmtree(self.build_dir)","method_summary":"Clean up files, remove builds.","original_method_code":"def cleanup_files(self):\n        \"\"\"Clean up files, remove builds.\"\"\"\n        logger.debug('Cleaning up...')\n        with indent_log():\n            for req in self.reqs_to_cleanup:\n                req.remove_temporary_source()\n\n            if self._pip_has_created_build_dir():\n                logger.debug('Removing temporary dir %s...', self.build_dir)\n                rmtree(self.build_dir)","method_path":"https:\/\/github.com\/tnkteja\/myhelp\/blob\/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb\/virtualEnvironment\/lib\/python2.7\/site-packages\/pip\/req\/req_set.py#L461-L470"}
{"repo_name":"tnkteja\/myhelp","method_name":"RequirementSet.install","method_code":"def install(self, install_options, global_options=(), *args, **kwargs):\n        \"\"\"\"\"\"\n        to_install = [r for r in self.requirements.values()[::-1]\n                      if not r.satisfied_by]\n\n        \n        \n        \n        \n        \n        \n        distribute_req = pkg_resources.Requirement.parse(\"distribute>=0.7\")\n        for req in to_install:\n            if (req.name == 'distribute'\n                    and req.installed_version is not None\n                    and req.installed_version in distribute_req):\n                to_install.remove(req)\n                to_install.append(req)\n\n        if to_install:\n            logger.info(\n                'Installing collected packages: %s',\n                ', '.join([req.name for req in to_install]),\n            )\n\n        with indent_log():\n            for requirement in to_install:\n\n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                if requirement.name == 'setuptools':\n                    try:\n                        \n                        \n                        \n                        distribute_requirement = \\\n                            pkg_resources.Requirement.parse(\"distribute<0.7\")\n                        existing_distribute = \\\n                            pkg_resources.get_distribution(\"distribute\")\n                        if existing_distribute in distribute_requirement:\n                            requirement.conflicts_with = existing_distribute\n                    except pkg_resources.DistributionNotFound:\n                        \n                        pass\n\n                if requirement.conflicts_with:\n                    logger.info(\n                        'Found existing installation: %s',\n                        requirement.conflicts_with,\n                    )\n                    with indent_log():\n                        requirement.uninstall(auto_confirm=True)\n                try:\n                    requirement.install(\n                        install_options,\n                        global_options,\n                        *args,\n                        **kwargs\n                    )\n                except:\n                    \n                    if (requirement.conflicts_with\n                            and not requirement.install_succeeded):\n                        requirement.rollback_uninstall()\n                    raise\n                else:\n                    if (requirement.conflicts_with\n                            and requirement.install_succeeded):\n                        requirement.commit_uninstall()\n                requirement.remove_temporary_source()\n\n        self.successfully_installed = to_install","method_summary":"Install everything in this set (after having downloaded and unpacked the packages)","original_method_code":"def install(self, install_options, global_options=(), *args, **kwargs):\n        \"\"\"\n        Install everything in this set (after having downloaded and unpacked\n        the packages)\n        \"\"\"\n        to_install = [r for r in self.requirements.values()[::-1]\n                      if not r.satisfied_by]\n\n        # DISTRIBUTE TO SETUPTOOLS UPGRADE HACK (1 of 3 parts)\n        # move the distribute-0.7.X wrapper to the end because it does not\n        # install a setuptools package. by moving it to the end, we ensure it's\n        # setuptools dependency is handled first, which will provide the\n        # setuptools package\n        # TODO: take this out later\n        distribute_req = pkg_resources.Requirement.parse(\"distribute>=0.7\")\n        for req in to_install:\n            if (req.name == 'distribute'\n                    and req.installed_version is not None\n                    and req.installed_version in distribute_req):\n                to_install.remove(req)\n                to_install.append(req)\n\n        if to_install:\n            logger.info(\n                'Installing collected packages: %s',\n                ', '.join([req.name for req in to_install]),\n            )\n\n        with indent_log():\n            for requirement in to_install:\n\n                # DISTRIBUTE TO SETUPTOOLS UPGRADE HACK (1 of 3 parts)\n                # when upgrading from distribute-0.6.X to the new merged\n                # setuptools in py2, we need to force setuptools to uninstall\n                # distribute. In py3, which is always using distribute, this\n                # conversion is already happening in distribute's\n                # pkg_resources. It's ok *not* to check if setuptools>=0.7\n                # because if someone were actually trying to ugrade from\n                # distribute to setuptools 0.6.X, then all this could do is\n                # actually help, although that upgade path was certainly never\n                # \"supported\"\n                # TODO: remove this later\n                if requirement.name == 'setuptools':\n                    try:\n                        # only uninstall distribute<0.7. For >=0.7, setuptools\n                        # will also be present, and that's what we need to\n                        # uninstall\n                        distribute_requirement = \\\n                            pkg_resources.Requirement.parse(\"distribute<0.7\")\n                        existing_distribute = \\\n                            pkg_resources.get_distribution(\"distribute\")\n                        if existing_distribute in distribute_requirement:\n                            requirement.conflicts_with = existing_distribute\n                    except pkg_resources.DistributionNotFound:\n                        # distribute wasn't installed, so nothing to do\n                        pass\n\n                if requirement.conflicts_with:\n                    logger.info(\n                        'Found existing installation: %s',\n                        requirement.conflicts_with,\n                    )\n                    with indent_log():\n                        requirement.uninstall(auto_confirm=True)\n                try:\n                    requirement.install(\n                        install_options,\n                        global_options,\n                        *args,\n                        **kwargs\n                    )\n                except:\n                    # if install did not succeed, rollback previous uninstall\n                    if (requirement.conflicts_with\n                            and not requirement.install_succeeded):\n                        requirement.rollback_uninstall()\n                    raise\n                else:\n                    if (requirement.conflicts_with\n                            and requirement.install_succeeded):\n                        requirement.commit_uninstall()\n                requirement.remove_temporary_source()\n\n        self.successfully_installed = to_install","method_path":"https:\/\/github.com\/tnkteja\/myhelp\/blob\/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb\/virtualEnvironment\/lib\/python2.7\/site-packages\/pip\/req\/req_set.py#L480-L563"}
{"repo_name":"tnkteja\/myhelp","method_name":"_collapse_leading_ws","method_code":"def _collapse_leading_ws(header, txt):\n    \"\"\"\"\"\"\n    if header.lower() == 'description':  \n        return '\\n'.join([x[8:] if x.startswith(' ' * 8) else x\n                          for x in txt.strip().splitlines()])\n    else:\n        return ' '.join([x.strip() for x in txt.splitlines()])","method_summary":"``Description`` header must preserve newlines; all others need not","original_method_code":"def _collapse_leading_ws(header, txt):\n    \"\"\"\n    ``Description`` header must preserve newlines; all others need not\n    \"\"\"\n    if header.lower() == 'description':  # preserve newlines\n        return '\\n'.join([x[8:] if x.startswith(' ' * 8) else x\n                          for x in txt.strip().splitlines()])\n    else:\n        return ' '.join([x.strip() for x in txt.splitlines()])","method_path":"https:\/\/github.com\/tnkteja\/myhelp\/blob\/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb\/virtualEnvironment\/lib\/python2.7\/site-packages\/pkginfo\/distribution.py#L14-L22"}
{"repo_name":"tnkteja\/myhelp","method_name":"Git.get_refs","method_code":"def get_refs(self, location):\n        \"\"\"\"\"\"\n        output = call_subprocess([self.cmd, 'show-ref'],\n                                 show_stdout=False, cwd=location)\n        rv = {}\n        for line in output.strip().splitlines():\n            commit, ref = line.split(' ', 1)\n            ref = ref.strip()\n            ref_name = None\n            if ref.startswith('refs\/remotes\/'):\n                ref_name = ref[len('refs\/remotes\/'):]\n            elif ref.startswith('refs\/heads\/'):\n                ref_name = ref[len('refs\/heads\/'):]\n            elif ref.startswith('refs\/tags\/'):\n                ref_name = ref[len('refs\/tags\/'):]\n            if ref_name is not None:\n                rv[ref_name] = commit.strip()\n        return rv","method_summary":"Return map of named refs (branches or tags) to commit hashes.","original_method_code":"def get_refs(self, location):\n        \"\"\"Return map of named refs (branches or tags) to commit hashes.\"\"\"\n        output = call_subprocess([self.cmd, 'show-ref'],\n                                 show_stdout=False, cwd=location)\n        rv = {}\n        for line in output.strip().splitlines():\n            commit, ref = line.split(' ', 1)\n            ref = ref.strip()\n            ref_name = None\n            if ref.startswith('refs\/remotes\/'):\n                ref_name = ref[len('refs\/remotes\/'):]\n            elif ref.startswith('refs\/heads\/'):\n                ref_name = ref[len('refs\/heads\/'):]\n            elif ref.startswith('refs\/tags\/'):\n                ref_name = ref[len('refs\/tags\/'):]\n            if ref_name is not None:\n                rv[ref_name] = commit.strip()\n        return rv","method_path":"https:\/\/github.com\/tnkteja\/myhelp\/blob\/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb\/virtualEnvironment\/lib\/python2.7\/site-packages\/pip\/vcs\/git.py#L143-L160"}
{"repo_name":"tnkteja\/myhelp","method_name":"find_command","method_code":"def find_command(cmd, paths=None, pathext=None):\n    \"\"\"\"\"\"\n    if paths is None:\n        paths = os.environ.get('PATH', '').split(os.pathsep)\n    if isinstance(paths, six.string_types):\n        paths = [paths]\n    \n    if pathext is None:\n        pathext = get_pathext()\n    pathext = [ext for ext in pathext.lower().split(os.pathsep) if len(ext)]\n    \n    if os.path.splitext(cmd)[1].lower() in pathext:\n        pathext = ['']\n    \n    for path in paths:\n        \n        cmd_path = os.path.join(path, cmd)\n        for ext in pathext:\n            \n            cmd_path_ext = cmd_path + ext\n            if os.path.isfile(cmd_path_ext):\n                return cmd_path_ext\n        if os.path.isfile(cmd_path):\n            return cmd_path\n    raise BadCommand('Cannot find command %r' % cmd)","method_summary":"Searches the PATH for the given command and returns its path","original_method_code":"def find_command(cmd, paths=None, pathext=None):\n    \"\"\"Searches the PATH for the given command and returns its path\"\"\"\n    if paths is None:\n        paths = os.environ.get('PATH', '').split(os.pathsep)\n    if isinstance(paths, six.string_types):\n        paths = [paths]\n    # check if there are funny path extensions for executables, e.g. Windows\n    if pathext is None:\n        pathext = get_pathext()\n    pathext = [ext for ext in pathext.lower().split(os.pathsep) if len(ext)]\n    # don't use extensions if the command ends with one of them\n    if os.path.splitext(cmd)[1].lower() in pathext:\n        pathext = ['']\n    # check if we find the command on PATH\n    for path in paths:\n        # try without extension first\n        cmd_path = os.path.join(path, cmd)\n        for ext in pathext:\n            # then including the extension\n            cmd_path_ext = cmd_path + ext\n            if os.path.isfile(cmd_path_ext):\n                return cmd_path_ext\n        if os.path.isfile(cmd_path):\n            return cmd_path\n    raise BadCommand('Cannot find command %r' % cmd)","method_path":"https:\/\/github.com\/tnkteja\/myhelp\/blob\/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb\/virtualEnvironment\/lib\/python2.7\/site-packages\/pip\/utils\/__init__.py#L102-L126"}
{"repo_name":"tnkteja\/myhelp","method_name":"normalize_path","method_code":"def normalize_path(path):\n    \"\"\"\"\"\"\n    return os.path.normcase(os.path.realpath(os.path.expanduser(path)))","method_summary":"Convert a path to its canonical, case-normalized, absolute version.","original_method_code":"def normalize_path(path):\n    \"\"\"\n    Convert a path to its canonical, case-normalized, absolute version.\n\n    \"\"\"\n    return os.path.normcase(os.path.realpath(os.path.expanduser(path)))","method_path":"https:\/\/github.com\/tnkteja\/myhelp\/blob\/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb\/virtualEnvironment\/lib\/python2.7\/site-packages\/pip\/utils\/__init__.py#L292-L297"}
{"repo_name":"tnkteja\/myhelp","method_name":"run","method_code":"def run():\n    \"\"\"\"\"\"\n    \n    config_option_help=\"'show' - displays configured options, 'set [section] [name] [value]' - sets config under a section,'set [name] [value]' - sets configuration globally\" \n\n    parser = OptionParser()\n    \n    parser.add_option(\"-a\", \"--add\", action=\"store\", type=\"string\", dest=\"addfile\", help=\"adds a notes\")\n    parser.add_option(\"-c\", \"--config\", action=\"store\", type=\"string\", dest=\"config\", help=config_option_help)\n    parser.add_option(\"-e\", \"--edit\", action=\"store\", type=\"string\", dest=\"editfile\", help=\"edits a notes\")\n    parser.add_option(\"-o\", \"--open\", action=\"store\", type=\"string\", dest=\"openfile\", help=\"opens a notes\")\n    parser.add_option(\"-r\", \"--remove\", action=\"store\", type=\"string\", dest=\"remove\", help=\"removes a notes\")\n\n    options, args = parser.parse_args()\n    \n    if options.config:\n        if options.config == \"show\":\n            config_option_list=''\n            config_sections = config.sections()\n            for section in config_sections:\n                config_option_list=config_option_list+section+\"\\n\"\n                section_items =config.items(section)\n                for item in section_items:\n                    config_option_list=config_option_list+\"    \"+item[0]+\"    \"+item[1]+\"\\n\" \n            print config_option_list\n            quit()\n        \n    def add_notes(note_name,existing_tags):\n       call([editor,environ[\"HOME\"] + \"\/.mypy\/myhelp\/notes\/\"+note_name+\".note\"])\n       definedtags = raw_input(\"Define Tags (separated by spaces): \").split(\" \")\n       definedtags.append(note_name)\n       print definedtags\n       print existing_tags\n       definedtags=list(set(definedtags)-set(existing_tags))\n       print definedtags\n       if len(definedtags)>0:\n           modify_tags_xml(note_name,definedtags,files,rootfiles,tags,roottags,tree,TAGS_XML_DIR)  \n\n    def get_tags_from_file(note_name):\n        fil = get_file_from_files(note_name)       \n        filetags = fil.iter('tag')\n        filetaglist=[]\n        for tag in filetags:\n            filetaglist.append(tag.text)\n        return filetaglist\n\n    if options.addfile:\n       existing_tags=[]\n       if isFile(options.addfile,files):\n           existing_tags=get_tags_from_file(options.addfile)\n           raw_input(\"Note exists with tags - \"+\" \".join(existing_tags)+\"\\nDo you want to edit the notes ? [Press enter to continue]\\n\") \n       add_notes(options.addfile,existing_tags)\n       quit()\n\n    if options.editfile:\n        if isFile(options.editfile,files):\n            add_notes(note_name,[])\n        else:\n           raw_input(\"Note doesn't exist.\\nDo you want add note ? [Press enter to continue]\")     \n           add_notes(note_name)\n        \n    if options.remove:\n        pass\n    \n    if len(args) != 1:\n        print \"Please use a search term\\n example : myhelp <some tag word> \"\n        quit()\n    _key_File = \"Note\"\n    _key_Results = \"    Results                                     \"\n    table={_key_Results:[]}\n    for tag in tags:\n        if tag.attrib[\"value\"] == args[0]:\n            fileelements = tag.iter(\"file\")\n            for fileelement in fileelements:\n                f = open(\n                    environ[\"HOME\"] + \"\/.mypy\/myhelp\/notes\/\" + fileelement.text+\".note\", \"r\")\n\t\ttable[_key_Results].append(f.read()+\"\\r\\n\\tfile: ~\/.mypy\/myhelp\/notes\/\"+fileelement.text+\".note\")\n                f.close()\n\n    print tabulate(table,headers=[],tablefmt=\"rst\")","method_summary":"Main method where all logic is defined","original_method_code":"def run():\n    \"\"\"Main method where all logic is defined\"\"\"\n    \n    config_option_help=\"'show' - displays configured options, 'set [section] [name] [value]' - sets config under a section,'set [name] [value]' - sets configuration globally\" \n\n    parser = OptionParser()\n    \n    parser.add_option(\"-a\", \"--add\", action=\"store\", type=\"string\", dest=\"addfile\", help=\"adds a notes\")\n    parser.add_option(\"-c\", \"--config\", action=\"store\", type=\"string\", dest=\"config\", help=config_option_help)\n    parser.add_option(\"-e\", \"--edit\", action=\"store\", type=\"string\", dest=\"editfile\", help=\"edits a notes\")\n    parser.add_option(\"-o\", \"--open\", action=\"store\", type=\"string\", dest=\"openfile\", help=\"opens a notes\")\n    parser.add_option(\"-r\", \"--remove\", action=\"store\", type=\"string\", dest=\"remove\", help=\"removes a notes\")\n\n    options, args = parser.parse_args()\n    \n    if options.config:\n        if options.config == \"show\":\n            config_option_list=''\n            config_sections = config.sections()\n            for section in config_sections:\n                config_option_list=config_option_list+section+\"\\n\"\n                section_items =config.items(section)\n                for item in section_items:\n                    config_option_list=config_option_list+\"    \"+item[0]+\"    \"+item[1]+\"\\n\" \n            print config_option_list\n            quit()\n        \n    def add_notes(note_name,existing_tags):\n       call([editor,environ[\"HOME\"] + \"\/.mypy\/myhelp\/notes\/\"+note_name+\".note\"])\n       definedtags = raw_input(\"Define Tags (separated by spaces): \").split(\" \")\n       definedtags.append(note_name)\n       print definedtags\n       print existing_tags\n       definedtags=list(set(definedtags)-set(existing_tags))\n       print definedtags\n       if len(definedtags)>0:\n           modify_tags_xml(note_name,definedtags,files,rootfiles,tags,roottags,tree,TAGS_XML_DIR)  \n\n    def get_tags_from_file(note_name):\n        fil = get_file_from_files(note_name)       \n        filetags = fil.iter('tag')\n        filetaglist=[]\n        for tag in filetags:\n            filetaglist.append(tag.text)\n        return filetaglist\n\n    if options.addfile:\n       existing_tags=[]\n       if isFile(options.addfile,files):\n           existing_tags=get_tags_from_file(options.addfile)\n           raw_input(\"Note exists with tags - \"+\" \".join(existing_tags)+\"\\nDo you want to edit the notes ? [Press enter to continue]\\n\") \n       add_notes(options.addfile,existing_tags)\n       quit()\n\n    if options.editfile:\n        if isFile(options.editfile,files):\n            add_notes(note_name,[])\n        else:\n           raw_input(\"Note doesn't exist.\\nDo you want add note ? [Press enter to continue]\")     \n           add_notes(note_name)\n        \n    if options.remove:\n        pass\n    \n    if len(args) != 1:\n        print \"Please use a search term\\n example : myhelp <some tag word> \"\n        quit()\n    _key_File = \"Note\"\n    _key_Results = \"    Results                                     \"\n    table={_key_Results:[]}\n    for tag in tags:\n        if tag.attrib[\"value\"] == args[0]:\n            fileelements = tag.iter(\"file\")\n            for fileelement in fileelements:\n                f = open(\n                    environ[\"HOME\"] + \"\/.mypy\/myhelp\/notes\/\" + fileelement.text+\".note\", \"r\")\n\t\ttable[_key_Results].append(f.read()+\"\\r\\n\\tfile: ~\/.mypy\/myhelp\/notes\/\"+fileelement.text+\".note\")\n                f.close()\n\n    print tabulate(table,headers=[],tablefmt=\"rst\")","method_path":"https:\/\/github.com\/tnkteja\/myhelp\/blob\/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb\/myhelp\/myhelp.py#L26-L105"}
{"repo_name":"tnkteja\/myhelp","method_name":"PackageFinder._find_url_name","method_code":"def _find_url_name(self, index_url, url_name, req):\n        \"\"\"\"\"\"\n        if not index_url.url.endswith('\/'):\n            \n            \n            index_url.url += '\/'\n        page = self._get_page(index_url, req)\n        if page is None:\n            logger.critical('Cannot fetch index base URL %s', index_url)\n            return\n        norm_name = normalize_name(req.url_name)\n        for link in page.links:\n            base = posixpath.basename(link.path.rstrip('\/'))\n            if norm_name == normalize_name(base):\n                logger.debug(\n                    'Real name of requirement %s is %s', url_name, base,\n                )\n                return base\n        return None","method_summary":"Finds the true URL name of a package, when the given name isn't quite correct. This is usually used to implement case-insensitivity.","original_method_code":"def _find_url_name(self, index_url, url_name, req):\n        \"\"\"\n        Finds the true URL name of a package, when the given name isn't quite\n        correct.\n        This is usually used to implement case-insensitivity.\n        \"\"\"\n        if not index_url.url.endswith('\/'):\n            # Vaguely part of the PyPI API... weird but true.\n            # FIXME: bad to modify this?\n            index_url.url += '\/'\n        page = self._get_page(index_url, req)\n        if page is None:\n            logger.critical('Cannot fetch index base URL %s', index_url)\n            return\n        norm_name = normalize_name(req.url_name)\n        for link in page.links:\n            base = posixpath.basename(link.path.rstrip('\/'))\n            if norm_name == normalize_name(base):\n                logger.debug(\n                    'Real name of requirement %s is %s', url_name, base,\n                )\n                return base\n        return None","method_path":"https:\/\/github.com\/tnkteja\/myhelp\/blob\/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb\/virtualEnvironment\/lib\/python2.7\/site-packages\/pip\/index.py#L528-L550"}
{"repo_name":"tnkteja\/myhelp","method_name":"PackageFinder._link_package_versions","method_code":"def _link_package_versions(self, link, search_name):\n        \"\"\"\"\"\"\n        platform = get_platform()\n\n        version = None\n        if link.egg_fragment:\n            egg_info = link.egg_fragment\n        else:\n            egg_info, ext = link.splitext()\n            if not ext:\n                if link not in self.logged_links:\n                    logger.debug('Skipping link %s; not a file', link)\n                    self.logged_links.add(link)\n                return\n            if egg_info.endswith('.tar'):\n                \n                egg_info = egg_info[:-4]\n                ext = '.tar' + ext\n            if ext not in self._known_extensions():\n                if link not in self.logged_links:\n                    logger.debug(\n                        'Skipping link %s; unknown archive format: %s',\n                        link,\n                        ext,\n                    )\n                    self.logged_links.add(link)\n                return\n            if \"macosx10\" in link.path and ext == '.zip':\n                if link not in self.logged_links:\n                    logger.debug('Skipping link %s; macosx10 one', link)\n                    self.logged_links.add(link)\n                return\n            if ext == wheel_ext:\n                try:\n                    wheel = Wheel(link.filename)\n                except InvalidWheelFilename:\n                    logger.debug(\n                        'Skipping %s because the wheel filename is invalid',\n                        link\n                    )\n                    return\n                if (pkg_resources.safe_name(wheel.name).lower()\n                        != pkg_resources.safe_name(search_name).lower()):\n                    logger.debug(\n                        'Skipping link %s; wrong project name (not %s)',\n                        link,\n                        search_name,\n                    )\n                    return\n                if not wheel.supported():\n                    logger.debug(\n                        'Skipping %s because it is not compatible with this '\n                        'Python',\n                        link,\n                    )\n                    return\n                \n                \n                \n                \n                \n                \n                comes_from = getattr(link, \"comes_from\", None)\n                if (\n                        (\n                            not platform.startswith('win')\n                            and not platform.startswith('macosx')\n                            and not platform == 'cli'\n                        )\n                        and comes_from is not None\n                        and urllib_parse.urlparse(\n                            comes_from.url\n                        ).netloc.endswith(PyPI.netloc)):\n                    if not wheel.supported(tags=supported_tags_noarch):\n                        logger.debug(\n                            \"Skipping %s because it is a pypi-hosted binary \"\n                            \"Wheel on an unsupported platform\",\n                            link,\n                        )\n                        return\n                version = wheel.version\n\n        if not version:\n            version = self._egg_info_matches(egg_info, search_name, link)\n        if version is None:\n            logger.debug(\n                'Skipping link %s; wrong project name (not %s)',\n                link,\n                search_name,\n            )\n            return\n\n        if (link.internal is not None\n                and not link.internal\n                and not normalize_name(search_name).lower()\n                in self.allow_external\n                and not self.allow_all_external):\n            \n            \n            logger.debug(\"Skipping %s because it is externally hosted.\", link)\n            self.need_warn_external = True\n            return\n\n        if (link.verifiable is not None\n                and not link.verifiable\n                and not (normalize_name(search_name).lower()\n                         in self.allow_unverified)):\n            \n            \n            \n            logger.debug(\n                \"Skipping %s because it is an insecure and unverifiable file.\",\n                link,\n            )\n            self.need_warn_unverified = True\n            return\n\n        match = self._py_version_re.search(version)\n        if match:\n            version = version[:match.start()]\n            py_version = match.group(1)\n            if py_version != sys.version[:3]:\n                logger.debug(\n                    'Skipping %s because Python version is incorrect', link\n                )\n                return\n        logger.debug('Found link %s, version: %s', link, version)\n\n        return InstallationCandidate(search_name, version, link)","method_summary":"Return an iterable of triples (pkg_resources_version_key, link, python_version) that can be extracted from the given link. Meant to be overridden by subclasses, not called by clients.","original_method_code":"def _link_package_versions(self, link, search_name):\n        \"\"\"\n        Return an iterable of triples (pkg_resources_version_key,\n        link, python_version) that can be extracted from the given\n        link.\n\n        Meant to be overridden by subclasses, not called by clients.\n        \"\"\"\n        platform = get_platform()\n\n        version = None\n        if link.egg_fragment:\n            egg_info = link.egg_fragment\n        else:\n            egg_info, ext = link.splitext()\n            if not ext:\n                if link not in self.logged_links:\n                    logger.debug('Skipping link %s; not a file', link)\n                    self.logged_links.add(link)\n                return\n            if egg_info.endswith('.tar'):\n                # Special double-extension case:\n                egg_info = egg_info[:-4]\n                ext = '.tar' + ext\n            if ext not in self._known_extensions():\n                if link not in self.logged_links:\n                    logger.debug(\n                        'Skipping link %s; unknown archive format: %s',\n                        link,\n                        ext,\n                    )\n                    self.logged_links.add(link)\n                return\n            if \"macosx10\" in link.path and ext == '.zip':\n                if link not in self.logged_links:\n                    logger.debug('Skipping link %s; macosx10 one', link)\n                    self.logged_links.add(link)\n                return\n            if ext == wheel_ext:\n                try:\n                    wheel = Wheel(link.filename)\n                except InvalidWheelFilename:\n                    logger.debug(\n                        'Skipping %s because the wheel filename is invalid',\n                        link\n                    )\n                    return\n                if (pkg_resources.safe_name(wheel.name).lower()\n                        != pkg_resources.safe_name(search_name).lower()):\n                    logger.debug(\n                        'Skipping link %s; wrong project name (not %s)',\n                        link,\n                        search_name,\n                    )\n                    return\n                if not wheel.supported():\n                    logger.debug(\n                        'Skipping %s because it is not compatible with this '\n                        'Python',\n                        link,\n                    )\n                    return\n                # This is a dirty hack to prevent installing Binary Wheels from\n                # PyPI unless it is a Windows or Mac Binary Wheel. This is\n                # paired with a change to PyPI disabling uploads for the\n                # same. Once we have a mechanism for enabling support for\n                # binary wheels on linux that deals with the inherent problems\n                # of binary distribution this can be removed.\n                comes_from = getattr(link, \"comes_from\", None)\n                if (\n                        (\n                            not platform.startswith('win')\n                            and not platform.startswith('macosx')\n                            and not platform == 'cli'\n                        )\n                        and comes_from is not None\n                        and urllib_parse.urlparse(\n                            comes_from.url\n                        ).netloc.endswith(PyPI.netloc)):\n                    if not wheel.supported(tags=supported_tags_noarch):\n                        logger.debug(\n                            \"Skipping %s because it is a pypi-hosted binary \"\n                            \"Wheel on an unsupported platform\",\n                            link,\n                        )\n                        return\n                version = wheel.version\n\n        if not version:\n            version = self._egg_info_matches(egg_info, search_name, link)\n        if version is None:\n            logger.debug(\n                'Skipping link %s; wrong project name (not %s)',\n                link,\n                search_name,\n            )\n            return\n\n        if (link.internal is not None\n                and not link.internal\n                and not normalize_name(search_name).lower()\n                in self.allow_external\n                and not self.allow_all_external):\n            # We have a link that we are sure is external, so we should skip\n            #   it unless we are allowing externals\n            logger.debug(\"Skipping %s because it is externally hosted.\", link)\n            self.need_warn_external = True\n            return\n\n        if (link.verifiable is not None\n                and not link.verifiable\n                and not (normalize_name(search_name).lower()\n                         in self.allow_unverified)):\n            # We have a link that we are sure we cannot verify its integrity,\n            #   so we should skip it unless we are allowing unsafe installs\n            #   for this requirement.\n            logger.debug(\n                \"Skipping %s because it is an insecure and unverifiable file.\",\n                link,\n            )\n            self.need_warn_unverified = True\n            return\n\n        match = self._py_version_re.search(version)\n        if match:\n            version = version[:match.start()]\n            py_version = match.group(1)\n            if py_version != sys.version[:3]:\n                logger.debug(\n                    'Skipping %s because Python version is incorrect', link\n                )\n                return\n        logger.debug('Found link %s, version: %s', link, version)\n\n        return InstallationCandidate(search_name, version, link)","method_path":"https:\/\/github.com\/tnkteja\/myhelp\/blob\/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb\/virtualEnvironment\/lib\/python2.7\/site-packages\/pip\/index.py#L631-L765"}
